python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_3 --overwrite=true --save_checkpoints=false --rng_seed=22044664 --max_global_steps=240000 --tuning_ruleset=self 2>&1 | tee -a /logs/ogbg_jax_03-05-2024-10-10-44.log
I0305 10:11:05.379165 140444430841664 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax.
I0305 10:11:06.451589 140444430841664 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0305 10:11:06.452387 140444430841664 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0305 10:11:06.452561 140444430841664 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0305 10:11:07.372374 140444430841664 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1.
I0305 10:11:07.574025 140444430841664 submission_runner.py:206] Initializing dataset.
I0305 10:11:07.864684 140444430841664 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:11:07.870674 140444430841664 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 10:11:08.116464 140444430841664 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 10:11:08.178001 140444430841664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:11:08.248425 140444430841664 submission_runner.py:213] Initializing model.
I0305 10:11:13.066400 140444430841664 submission_runner.py:255] Initializing optimizer.
I0305 10:11:13.733516 140444430841664 submission_runner.py:262] Initializing metrics bundle.
I0305 10:11:13.733732 140444430841664 submission_runner.py:280] Initializing checkpoint and logger.
I0305 10:11:13.734388 140444430841664 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1 with prefix checkpoint_
I0305 10:11:13.734544 140444430841664 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1/meta_data_0.json.
I0305 10:11:13.734750 140444430841664 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0305 10:11:13.734815 140444430841664 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0305 10:11:14.069257 140444430841664 logger_utils.py:220] Unable to record git information. Continuing without it.
I0305 10:11:14.374027 140444430841664 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1/flags_0.json.
I0305 10:11:14.384133 140444430841664 submission_runner.py:314] Starting training loop.
I0305 10:11:33.771044 140279428409088 logging_writer.py:48] [0] global_step=0, grad_norm=1.9824045896530151, loss=0.7662501335144043
I0305 10:11:33.787267 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:11:33.793955 140444430841664 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:11:33.798939 140444430841664 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:11:33.875432 140444430841664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:13:28.511190 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:13:28.514850 140444430841664 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:13:28.518948 140444430841664 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:13:28.587362 140444430841664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:15:04.582728 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:15:05.413862 140444430841664 dataset_info.py:736] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ogbg_molpcba/0.1.3
I0305 10:15:05.907239 140444430841664 dataset_info.py:578] Load dataset info from /tmp/tmpfayhouwntfds
I0305 10:15:05.910824 140444430841664 dataset_info.py:669] Fields info.[description, release_notes, splits, module_name] from disk and from code do not match. Keeping the one from code.
I0305 10:15:05.911277 140444430841664 dataset_builder.py:593] Generating dataset ogbg_molpcba (/root/data/ogbg_molpcba/0.1.3)
Downloading and preparing dataset 37.70 MiB (download: 37.70 MiB, generated: 822.53 MiB, total: 860.23 MiB) to /root/data/ogbg_molpcba/0.1.3...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[AI0305 10:15:06.227432 140444430841664 download_manager.py:400] Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip into /root/data/downloads/snap.stan.edu_ogb_grap_csv_mol_down_pcbapc4I82Cv1THcU-IggPHK8IHZ8qM-BJ3VDk-q_rtqrf4.zip.tmp.bff3c0d7de27437389798a3ce653f5ca...
Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...:   0%|          | 0/37 [00:00<?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:39,  2.76s/ MiB][ADl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:39,  2.76s/ MiB][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:52,  1.49s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:52,  1.49s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:34,  1.02s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:34,  1.02s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.36 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.36 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.73 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.73 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:14,  2.07 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:14,  2.07 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:11,  2.67 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:11,  2.67 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:08,  3.29 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:08,  3.29 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:05<00:07,  3.91 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:05<00:07,  3.91 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:05<00:06,  4.48 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:05<00:06,  4.48 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:05<00:05,  4.48 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:03,  6.43 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:03,  6.43 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:03,  6.53 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:03,  6.53 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:03,  6.53 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:05<00:02,  8.24 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:05<00:02,  8.24 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  7.91 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  7.91 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 17/37 [00:05<00:02,  7.91 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:06<00:02,  9.41 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:06<00:02,  9.41 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/37 [00:06<00:01,  9.41 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 10.51 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 10.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21/37 [00:06<00:01, 10.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22/37 [00:06<00:01, 10.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:06<00:01, 13.08 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:06<00:01, 13.08 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/37 [00:06<00:00, 13.08 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.24 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.24 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26/37 [00:06<00:00, 13.24 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27/37 [00:06<00:00, 13.24 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:06<00:00, 15.08 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:06<00:00, 15.08 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/37 [00:06<00:00, 15.08 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 14.77 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 14.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/37 [00:06<00:00, 14.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [00:06<00:00, 14.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 16.22 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [00:06<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/37 [00:07<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[A
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:07<00:00, 17.29 MiB/s][ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/2 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/3 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/4 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/5 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/6 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/7 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/8 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/9 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/10 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/11 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   0%|          | 0/12 [00:07<?, ? file/s][A[A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:20,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:20,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  17%|â–ˆâ–‹        | 2/12 [00:07<01:13,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<01:05,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:58,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:51,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:07<00:43,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:36,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:07<00:29,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:07<00:21,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:14,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:07,  7.30s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.17s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.29 MiB/s][A

Extraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  7.30s/ file][A[AExtraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  1.64 file/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00,  5.06 MiB/s]
Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.31s/ url]
Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
Generating train examples...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Generating train examples...:   0%|          | 78/350343 [00:00<07:29, 779.01 examples/s][A
Generating train examples...:   0%|          | 163/350343 [00:00<07:59, 730.77 examples/s][A
Generating train examples...:   0%|          | 389/350343 [00:00<04:12, 1384.93 examples/s][A
Generating train examples...:   0%|          | 616/350343 [00:00<03:23, 1720.44 examples/s][A
Generating train examples...:   0%|          | 846/350343 [00:00<03:01, 1922.34 examples/s][A
Generating train examples...:   0%|          | 1077/350343 [00:00<02:50, 2050.46 examples/s][A
Generating train examples...:   0%|          | 1305/350343 [00:00<02:44, 2123.93 examples/s][A
Generating train examples...:   0%|          | 1538/350343 [00:00<02:39, 2188.28 examples/s][A
Generating train examples...:   1%|          | 1758/350343 [00:00<02:41, 2154.79 examples/s][A
Generating train examples...:   1%|          | 1978/350343 [00:01<02:40, 2168.24 examples/s][A
Generating train examples...:   1%|          | 2203/350343 [00:01<02:38, 2191.41 examples/s][A
Generating train examples...:   1%|          | 2434/350343 [00:01<02:36, 2226.70 examples/s][A
Generating train examples...:   1%|          | 2664/350343 [00:01<02:34, 2248.33 examples/s][A
Generating train examples...:   1%|          | 2894/350343 [00:01<02:33, 2262.94 examples/s][A
Generating train examples...:   1%|          | 3128/350343 [00:01<02:31, 2284.86 examples/s][A
Generating train examples...:   1%|          | 3358/350343 [00:01<02:31, 2286.89 examples/s][A
Generating train examples...:   1%|          | 3595/350343 [00:01<02:30, 2311.09 examples/s][A
Generating train examples...:   1%|          | 3827/350343 [00:01<02:30, 2305.06 examples/s][A
Generating train examples...:   1%|          | 4058/350343 [00:01<02:31, 2289.71 examples/s][A
Generating train examples...:   1%|          | 4288/350343 [00:02<02:33, 2257.33 examples/s][A
Generating train examples...:   1%|â–         | 4516/350343 [00:02<02:32, 2261.66 examples/s][A
Generating train examples...:   1%|â–         | 4745/350343 [00:02<02:32, 2269.77 examples/s][A
Generating train examples...:   1%|â–         | 4981/350343 [00:02<02:30, 2295.10 examples/s][A
Generating train examples...:   1%|â–         | 5212/350343 [00:02<02:30, 2296.96 examples/s][A
Generating train examples...:   2%|â–         | 5449/350343 [00:02<02:28, 2317.61 examples/s][A
Generating train examples...:   2%|â–         | 5681/350343 [00:02<02:29, 2312.71 examples/s][A
Generating train examples...:   2%|â–         | 5913/350343 [00:02<02:29, 2302.37 examples/s][A
Generating train examples...:   2%|â–         | 6145/350343 [00:02<02:29, 2305.65 examples/s][A
Generating train examples...:   2%|â–         | 6376/350343 [00:02<02:29, 2295.18 examples/s][A
Generating train examples...:   2%|â–         | 6606/350343 [00:03<02:29, 2294.95 examples/s][A
Generating train examples...:   2%|â–         | 6836/350343 [00:03<02:30, 2284.87 examples/s][A
Generating train examples...:   2%|â–         | 7069/350343 [00:03<02:29, 2296.46 examples/s][A
Generating train examples...:   2%|â–         | 7299/350343 [00:03<02:29, 2287.74 examples/s][A
Generating train examples...:   2%|â–         | 7532/350343 [00:03<02:29, 2300.03 examples/s][A
Generating train examples...:   2%|â–         | 7766/350343 [00:03<02:28, 2311.80 examples/s][A
Generating train examples...:   2%|â–         | 7998/350343 [00:03<02:34, 2217.89 examples/s][A
Generating train examples...:   2%|â–         | 8225/350343 [00:03<02:33, 2231.26 examples/s][A
Generating train examples...:   2%|â–         | 8454/350343 [00:03<02:32, 2246.37 examples/s][A
Generating train examples...:   2%|â–         | 8686/350343 [00:03<02:30, 2267.89 examples/s][A
Generating train examples...:   3%|â–Ž         | 8919/350343 [00:04<02:29, 2284.90 examples/s][A
Generating train examples...:   3%|â–Ž         | 9150/350343 [00:04<02:28, 2290.61 examples/s][A
Generating train examples...:   3%|â–Ž         | 9380/350343 [00:04<02:29, 2287.78 examples/s][A
Generating train examples...:   3%|â–Ž         | 9610/350343 [00:04<02:28, 2290.91 examples/s][A
Generating train examples...:   3%|â–Ž         | 9840/350343 [00:04<02:29, 2279.30 examples/s][A
Generating train examples...:   3%|â–Ž         | 10069/350343 [00:04<02:30, 2261.03 examples/s][A
Generating train examples...:   3%|â–Ž         | 10303/350343 [00:04<02:28, 2282.64 examples/s][A
Generating train examples...:   3%|â–Ž         | 10534/350343 [00:04<02:28, 2289.28 examples/s][A
Generating train examples...:   3%|â–Ž         | 10766/350343 [00:04<02:27, 2295.76 examples/s][A
Generating train examples...:   3%|â–Ž         | 11002/350343 [00:04<02:26, 2313.98 examples/s][A
Generating train examples...:   3%|â–Ž         | 11234/350343 [00:05<02:27, 2305.98 examples/s][A
Generating train examples...:   3%|â–Ž         | 11465/350343 [00:05<02:28, 2278.93 examples/s][A
Generating train examples...:   3%|â–Ž         | 11699/350343 [00:05<02:27, 2294.82 examples/s][A
Generating train examples...:   3%|â–Ž         | 11929/350343 [00:05<02:28, 2283.39 examples/s][A
Generating train examples...:   3%|â–Ž         | 12161/350343 [00:05<02:27, 2292.16 examples/s][A
Generating train examples...:   4%|â–Ž         | 12393/350343 [00:05<02:27, 2298.96 examples/s][A
Generating train examples...:   4%|â–Ž         | 12624/350343 [00:05<02:26, 2300.17 examples/s][A
Generating train examples...:   4%|â–Ž         | 12858/350343 [00:05<02:26, 2309.76 examples/s][A
Generating train examples...:   4%|â–Ž         | 13089/350343 [00:05<02:27, 2287.85 examples/s][A
Generating train examples...:   4%|â–         | 13320/350343 [00:05<02:26, 2293.16 examples/s][A
Generating train examples...:   4%|â–         | 13553/350343 [00:06<02:26, 2302.21 examples/s][A
Generating train examples...:   4%|â–         | 13785/350343 [00:06<02:25, 2306.00 examples/s][A
Generating train examples...:   4%|â–         | 14016/350343 [00:06<02:26, 2296.47 examples/s][A
Generating train examples...:   4%|â–         | 14246/350343 [00:06<02:27, 2280.86 examples/s][A
Generating train examples...:   4%|â–         | 14475/350343 [00:06<02:27, 2278.62 examples/s][A
Generating train examples...:   4%|â–         | 14703/350343 [00:06<02:28, 2259.77 examples/s][A
Generating train examples...:   4%|â–         | 14930/350343 [00:06<02:33, 2186.85 examples/s][A
Generating train examples...:   4%|â–         | 15164/350343 [00:06<02:30, 2230.26 examples/s][A
Generating train examples...:   4%|â–         | 15400/350343 [00:06<02:27, 2267.29 examples/s][A
Generating train examples...:   4%|â–         | 15628/350343 [00:07<02:27, 2270.08 examples/s][A
Generating train examples...:   5%|â–         | 15861/350343 [00:07<02:26, 2287.82 examples/s][A
Generating train examples...:   5%|â–         | 16091/350343 [00:07<02:26, 2276.20 examples/s][A
Generating train examples...:   5%|â–         | 16319/350343 [00:07<02:27, 2260.85 examples/s][A
Generating train examples...:   5%|â–         | 16546/350343 [00:07<02:28, 2253.37 examples/s][A
Generating train examples...:   5%|â–         | 16772/350343 [00:07<02:28, 2250.23 examples/s][A
Generating train examples...:   5%|â–         | 16999/350343 [00:07<02:27, 2256.00 examples/s][A
Generating train examples...:   5%|â–         | 17225/350343 [00:07<02:27, 2251.78 examples/s][A
Generating train examples...:   5%|â–         | 17456/350343 [00:07<02:26, 2267.51 examples/s][A
Generating train examples...:   5%|â–Œ         | 17690/350343 [00:07<02:25, 2287.33 examples/s][A
Generating train examples...:   5%|â–Œ         | 17919/350343 [00:08<02:26, 2271.64 examples/s][A
Generating train examples...:   5%|â–Œ         | 18148/350343 [00:08<02:25, 2276.57 examples/s][A
Generating train examples...:   5%|â–Œ         | 18376/350343 [00:08<02:26, 2270.69 examples/s][A
Generating train examples...:   5%|â–Œ         | 18610/350343 [00:08<02:24, 2288.63 examples/s][A
Generating train examples...:   5%|â–Œ         | 18839/350343 [00:08<02:26, 2266.79 examples/s][A
Generating train examples...:   5%|â–Œ         | 19066/350343 [00:08<02:26, 2265.76 examples/s][A
Generating train examples...:   6%|â–Œ         | 19293/350343 [00:08<02:26, 2264.70 examples/s][A
Generating train examples...:   6%|â–Œ         | 19520/350343 [00:08<02:26, 2265.41 examples/s][A
Generating train examples...:   6%|â–Œ         | 19747/350343 [00:08<02:26, 2250.35 examples/s][A
Generating train examples...:   6%|â–Œ         | 19973/350343 [00:08<02:27, 2243.83 examples/s][A
Generating train examples...:   6%|â–Œ         | 20198/350343 [00:09<02:27, 2244.16 examples/s][A
Generating train examples...:   6%|â–Œ         | 20424/350343 [00:09<02:26, 2248.41 examples/s][A
Generating train examples...:   6%|â–Œ         | 20649/350343 [00:09<02:29, 2206.11 examples/s][A
Generating train examples...:   6%|â–Œ         | 20870/350343 [00:09<02:30, 2186.04 examples/s][A
Generating train examples...:   6%|â–Œ         | 21100/350343 [00:09<02:28, 2218.10 examples/s][A
Generating train examples...:   6%|â–Œ         | 21326/350343 [00:09<02:27, 2228.84 examples/s][A
Generating train examples...:   6%|â–Œ         | 21558/350343 [00:09<02:25, 2254.84 examples/s][A
Generating train examples...:   6%|â–Œ         | 21789/350343 [00:09<02:24, 2269.21 examples/s][A
Generating train examples...:   6%|â–‹         | 22019/350343 [00:09<02:24, 2276.73 examples/s][A
Generating train examples...:   6%|â–‹         | 22247/350343 [00:09<02:24, 2274.59 examples/s][A
Generating train examples...:   6%|â–‹         | 22478/350343 [00:10<02:23, 2283.00 examples/s][A
Generating train examples...:   6%|â–‹         | 22707/350343 [00:10<02:24, 2267.46 examples/s][A
Generating train examples...:   7%|â–‹         | 22936/350343 [00:10<02:24, 2273.65 examples/s][A
Generating train examples...:   7%|â–‹         | 23166/350343 [00:10<02:23, 2280.38 examples/s][A
Generating train examples...:   7%|â–‹         | 23397/350343 [00:10<02:22, 2287.98 examples/s][A
Generating train examples...:   7%|â–‹         | 23628/350343 [00:10<02:22, 2294.16 examples/s][A
Generating train examples...:   7%|â–‹         | 23860/350343 [00:10<02:21, 2301.41 examples/s][A
Generating train examples...:   7%|â–‹         | 24091/350343 [00:10<02:22, 2288.23 examples/s][A
Generating train examples...:   7%|â–‹         | 24321/350343 [00:10<02:22, 2290.08 examples/s][A
Generating train examples...:   7%|â–‹         | 24551/350343 [00:10<02:22, 2292.90 examples/s][A
Generating train examples...:   7%|â–‹         | 24781/350343 [00:11<02:26, 2222.79 examples/s][A
Generating train examples...:   7%|â–‹         | 25011/350343 [00:11<02:24, 2244.72 examples/s][A
Generating train examples...:   7%|â–‹         | 25236/350343 [00:11<02:25, 2237.51 examples/s][A
Generating train examples...:   7%|â–‹         | 25462/350343 [00:11<02:24, 2241.70 examples/s][A
Generating train examples...:   7%|â–‹         | 25695/350343 [00:11<02:23, 2267.41 examples/s][A
Generating train examples...:   7%|â–‹         | 25929/350343 [00:11<02:21, 2288.90 examples/s][A
Generating train examples...:   7%|â–‹         | 26162/350343 [00:11<02:21, 2298.50 examples/s][A
Generating train examples...:   8%|â–Š         | 26396/350343 [00:11<02:20, 2310.82 examples/s][A
Generating train examples...:   8%|â–Š         | 26630/350343 [00:11<02:19, 2318.08 examples/s][A
Generating train examples...:   8%|â–Š         | 26862/350343 [00:11<02:22, 2276.52 examples/s][A
Generating train examples...:   8%|â–Š         | 27099/350343 [00:12<02:20, 2302.50 examples/s][A
Generating train examples...:   8%|â–Š         | 27330/350343 [00:12<02:21, 2281.44 examples/s][A
Generating train examples...:   8%|â–Š         | 27559/350343 [00:12<02:21, 2279.85 examples/s][A
Generating train examples...:   8%|â–Š         | 27789/350343 [00:12<02:21, 2285.41 examples/s][A
Generating train examples...:   8%|â–Š         | 28023/350343 [00:12<02:20, 2300.48 examples/s][A
Generating train examples...:   8%|â–Š         | 28254/350343 [00:12<02:19, 2303.13 examples/s][A
Generating train examples...:   8%|â–Š         | 28485/350343 [00:12<02:20, 2285.26 examples/s][A
Generating train examples...:   8%|â–Š         | 28716/350343 [00:12<02:20, 2290.22 examples/s][A
Generating train examples...:   8%|â–Š         | 28949/350343 [00:12<02:19, 2300.68 examples/s][A
Generating train examples...:   8%|â–Š         | 29184/350343 [00:12<02:18, 2313.47 examples/s][A
Generating train examples...:   8%|â–Š         | 29416/350343 [00:13<02:20, 2290.39 examples/s][A
Generating train examples...:   8%|â–Š         | 29646/350343 [00:13<02:20, 2283.51 examples/s][A
Generating train examples...:   9%|â–Š         | 29875/350343 [00:13<02:20, 2283.31 examples/s][A
Generating train examples...:   9%|â–Š         | 30104/350343 [00:13<02:20, 2278.65 examples/s][A
Generating train examples...:   9%|â–Š         | 30333/350343 [00:13<02:20, 2281.77 examples/s][A
Generating train examples...:   9%|â–Š         | 30565/350343 [00:13<02:19, 2290.99 examples/s][A
Generating train examples...:   9%|â–‰         | 30795/350343 [00:13<02:19, 2290.54 examples/s][A
Generating train examples...:   9%|â–‰         | 31025/350343 [00:13<02:20, 2273.61 examples/s][A
Generating train examples...:   9%|â–‰         | 31254/350343 [00:13<02:20, 2276.34 examples/s][A
Generating train examples...:   9%|â–‰         | 31483/350343 [00:13<02:19, 2278.95 examples/s][A
Generating train examples...:   9%|â–‰         | 31711/350343 [00:14<02:19, 2278.00 examples/s][A
Generating train examples...:   9%|â–‰         | 31939/350343 [00:14<02:25, 2187.50 examples/s][A
Generating train examples...:   9%|â–‰         | 32167/350343 [00:14<02:23, 2211.87 examples/s][A
Generating train examples...:   9%|â–‰         | 32393/350343 [00:14<02:22, 2225.54 examples/s][A
Generating train examples...:   9%|â–‰         | 32617/350343 [00:14<02:22, 2229.47 examples/s][A
Generating train examples...:   9%|â–‰         | 32846/350343 [00:14<02:21, 2245.30 examples/s][A
Generating train examples...:   9%|â–‰         | 33080/350343 [00:14<02:19, 2271.36 examples/s][A
Generating train examples...:  10%|â–‰         | 33308/350343 [00:14<02:20, 2256.74 examples/s][A
Generating train examples...:  10%|â–‰         | 33535/350343 [00:14<02:20, 2259.55 examples/s][A
Generating train examples...:  10%|â–‰         | 33765/350343 [00:14<02:19, 2269.38 examples/s][A
Generating train examples...:  10%|â–‰         | 33998/350343 [00:15<02:18, 2284.97 examples/s][A
Generating train examples...:  10%|â–‰         | 34227/350343 [00:15<02:19, 2259.20 examples/s][A
Generating train examples...:  10%|â–‰         | 34454/350343 [00:15<02:19, 2256.89 examples/s][A
Generating train examples...:  10%|â–‰         | 34680/350343 [00:15<02:19, 2255.73 examples/s][A
Generating train examples...:  10%|â–‰         | 34907/350343 [00:15<02:19, 2259.55 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35133/350343 [00:15<02:19, 2253.12 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35359/350343 [00:15<02:20, 2247.03 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35587/350343 [00:15<02:19, 2256.31 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35815/350343 [00:15<02:19, 2261.85 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36042/350343 [00:16<02:19, 2260.72 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36269/350343 [00:16<02:18, 2261.47 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36496/350343 [00:16<02:18, 2259.99 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36723/350343 [00:16<02:18, 2258.05 examples/s][A
Generating train examples...:  11%|â–ˆ         | 36951/350343 [00:16<02:18, 2262.45 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37178/350343 [00:16<02:18, 2260.37 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37407/350343 [00:16<02:17, 2267.83 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37634/350343 [00:16<02:18, 2261.58 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37861/350343 [00:16<02:20, 2220.51 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38084/350343 [00:16<02:21, 2209.80 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38306/350343 [00:17<02:21, 2212.78 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38528/350343 [00:17<02:20, 2214.02 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38750/350343 [00:17<02:22, 2184.93 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38970/350343 [00:17<02:22, 2188.81 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39201/350343 [00:17<02:19, 2223.08 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39433/350343 [00:17<02:18, 2251.66 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39667/350343 [00:17<02:16, 2275.71 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39897/350343 [00:17<02:16, 2282.68 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 40126/350343 [00:17<02:16, 2268.09 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40355/350343 [00:17<02:16, 2273.61 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40583/350343 [00:18<02:16, 2271.83 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40813/350343 [00:18<02:15, 2276.21 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41041/350343 [00:18<02:16, 2260.07 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41271/350343 [00:18<02:16, 2270.10 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41502/350343 [00:18<02:15, 2281.49 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41733/350343 [00:18<02:14, 2287.46 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41964/350343 [00:18<02:14, 2293.33 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42194/350343 [00:18<02:14, 2287.36 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42424/350343 [00:18<02:14, 2289.63 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42653/350343 [00:18<02:14, 2286.06 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42882/350343 [00:19<02:15, 2267.60 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43109/350343 [00:19<02:15, 2266.04 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43336/350343 [00:19<02:15, 2258.62 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43563/350343 [00:19<02:15, 2261.88 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43792/350343 [00:19<02:20, 2183.26 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44015/350343 [00:19<02:19, 2196.62 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44248/350343 [00:19<02:16, 2234.39 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44475/350343 [00:19<02:16, 2243.58 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44700/350343 [00:19<02:16, 2243.50 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44926/350343 [00:19<02:15, 2245.97 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45155/350343 [00:20<02:15, 2257.80 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45387/350343 [00:20<02:14, 2275.03 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45615/350343 [00:20<02:14, 2270.91 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45843/350343 [00:20<02:14, 2260.46 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46071/350343 [00:20<02:14, 2263.45 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46298/350343 [00:20<02:14, 2257.65 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46528/350343 [00:20<02:13, 2269.33 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46755/350343 [00:20<02:14, 2252.79 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46981/350343 [00:20<02:14, 2252.65 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 47209/350343 [00:20<02:14, 2258.88 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47437/350343 [00:21<02:13, 2263.71 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47670/350343 [00:21<02:12, 2281.01 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47899/350343 [00:21<02:14, 2253.39 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 48126/350343 [00:21<02:13, 2256.33 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48356/350343 [00:21<02:13, 2267.24 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48596/350343 [00:21<02:10, 2305.52 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48829/350343 [00:21<02:10, 2310.83 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49061/350343 [00:21<02:11, 2295.02 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49294/350343 [00:21<02:10, 2302.80 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49525/350343 [00:21<02:12, 2276.13 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49758/350343 [00:22<02:11, 2290.85 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49988/350343 [00:22<02:11, 2283.41 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50217/350343 [00:22<02:12, 2273.49 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50445/350343 [00:22<02:12, 2269.19 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50678/350343 [00:22<02:11, 2285.93 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 50907/350343 [00:22<02:11, 2273.15 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51135/350343 [00:22<02:11, 2271.03 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51363/350343 [00:22<02:12, 2260.76 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51591/350343 [00:22<02:11, 2265.76 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51825/350343 [00:22<02:10, 2287.25 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52054/350343 [00:23<02:10, 2287.94 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52286/350343 [00:23<02:09, 2295.57 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52516/350343 [00:23<02:10, 2276.87 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52744/350343 [00:23<02:16, 2178.55 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52969/350343 [00:23<02:15, 2197.24 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53196/350343 [00:23<02:13, 2218.14 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53419/350343 [00:23<02:13, 2221.57 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53642/350343 [00:23<02:20, 2118.13 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53865/350343 [00:23<02:18, 2147.88 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 54095/350343 [00:24<02:15, 2191.35 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54326/350343 [00:24<02:13, 2225.56 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54562/350343 [00:24<02:10, 2262.91 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54789/350343 [00:24<02:10, 2260.24 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55026/350343 [00:24<02:08, 2290.61 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55256/350343 [00:24<02:09, 2286.72 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55490/350343 [00:24<02:08, 2301.32 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55721/350343 [00:24<02:08, 2288.30 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55953/350343 [00:24<02:08, 2296.37 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56186/350343 [00:24<02:07, 2304.32 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56419/350343 [00:25<02:07, 2311.61 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56651/350343 [00:25<02:07, 2296.07 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56881/350343 [00:25<02:08, 2292.30 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57111/350343 [00:25<02:08, 2279.99 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57340/350343 [00:25<02:08, 2277.02 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57568/350343 [00:25<02:09, 2268.37 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57795/350343 [00:25<02:09, 2257.66 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58024/350343 [00:25<02:08, 2266.33 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58251/350343 [00:25<02:09, 2262.80 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58480/350343 [00:25<02:08, 2269.83 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58710/350343 [00:26<02:08, 2278.05 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58941/350343 [00:26<02:07, 2285.31 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59173/350343 [00:26<02:06, 2295.21 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59403/350343 [00:26<02:07, 2281.41 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59636/350343 [00:26<02:06, 2295.81 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59866/350343 [00:26<02:06, 2291.31 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60097/350343 [00:26<02:06, 2296.29 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60327/350343 [00:26<02:07, 2283.02 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60560/350343 [00:26<02:06, 2296.46 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60790/350343 [00:26<02:06, 2291.15 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 61020/350343 [00:27<02:11, 2206.27 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 61254/350343 [00:27<02:08, 2243.97 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61480/350343 [00:27<02:08, 2247.16 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61706/350343 [00:27<02:08, 2240.93 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61938/350343 [00:27<02:07, 2263.73 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62167/350343 [00:27<02:06, 2270.26 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62395/350343 [00:27<02:06, 2268.50 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62629/350343 [00:27<02:05, 2288.68 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62858/350343 [00:27<02:05, 2285.78 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63087/350343 [00:27<02:06, 2273.84 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63316/350343 [00:28<02:05, 2278.45 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63544/350343 [00:28<02:06, 2271.52 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63775/350343 [00:28<02:05, 2281.22 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64006/350343 [00:28<02:05, 2287.72 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64244/350343 [00:28<02:03, 2312.78 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64477/350343 [00:28<02:03, 2317.26 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64709/350343 [00:28<02:03, 2312.78 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 64941/350343 [00:28<02:03, 2307.09 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65174/350343 [00:28<02:03, 2311.42 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65406/350343 [00:28<02:03, 2309.32 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65637/350343 [00:29<02:03, 2303.37 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 65868/350343 [00:29<02:04, 2291.01 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66098/350343 [00:29<02:05, 2271.18 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66326/350343 [00:29<02:05, 2264.71 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66556/350343 [00:29<02:04, 2272.80 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66786/350343 [00:29<02:04, 2278.65 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67015/350343 [00:29<02:04, 2280.52 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67246/350343 [00:29<02:03, 2288.49 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67475/350343 [00:29<02:03, 2287.18 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67705/350343 [00:29<02:03, 2288.82 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67938/350343 [00:30<02:02, 2301.09 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68169/350343 [00:30<02:02, 2297.53 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68401/350343 [00:30<02:02, 2303.25 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68632/350343 [00:30<02:08, 2194.35 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68857/350343 [00:30<02:07, 2210.29 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69081/350343 [00:30<02:06, 2217.22 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69304/350343 [00:30<02:06, 2219.08 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69527/350343 [00:30<02:06, 2217.83 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69754/350343 [00:30<02:05, 2231.22 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69984/350343 [00:30<02:04, 2250.49 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70210/350343 [00:31<02:14, 2077.62 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70421/350343 [00:31<02:28, 1879.95 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70636/350343 [00:31<02:23, 1950.23 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70836/350343 [00:31<02:31, 1846.19 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71025/350343 [00:31<02:30, 1853.91 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71234/350343 [00:31<02:25, 1919.11 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71452/350343 [00:31<02:20, 1991.78 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71681/350343 [00:31<02:14, 2077.13 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 71910/350343 [00:31<02:10, 2139.13 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72139/350343 [00:32<02:07, 2183.32 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72368/350343 [00:32<02:05, 2214.66 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72598/350343 [00:32<02:04, 2237.92 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72830/350343 [00:32<02:02, 2260.21 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73057/350343 [00:32<02:03, 2250.77 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73287/350343 [00:32<02:02, 2263.32 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73520/350343 [00:32<02:01, 2282.88 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73752/350343 [00:32<02:00, 2292.11 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73982/350343 [00:32<02:00, 2290.26 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74214/350343 [00:32<02:00, 2297.73 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74444/350343 [00:33<02:04, 2209.14 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74672/350343 [00:33<02:03, 2227.79 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74904/350343 [00:33<02:02, 2253.37 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 75139/350343 [00:33<02:00, 2280.53 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75368/350343 [00:33<02:00, 2282.68 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75597/350343 [00:33<02:00, 2279.08 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75831/350343 [00:33<01:59, 2297.02 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76061/350343 [00:33<02:00, 2279.34 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76293/350343 [00:33<01:59, 2290.77 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76523/350343 [00:34<02:00, 2265.85 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76756/350343 [00:34<01:59, 2283.33 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76985/350343 [00:34<01:59, 2280.11 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77221/350343 [00:34<01:58, 2302.78 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77452/350343 [00:34<01:58, 2296.67 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77685/350343 [00:34<01:58, 2306.25 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77916/350343 [00:34<01:58, 2290.43 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78146/350343 [00:34<01:59, 2286.92 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78375/350343 [00:34<01:59, 2283.25 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78604/350343 [00:34<01:59, 2276.58 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 78832/350343 [00:35<01:59, 2271.82 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79060/350343 [00:35<01:59, 2267.18 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79287/350343 [00:35<01:59, 2265.35 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79516/350343 [00:35<01:59, 2271.19 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79744/350343 [00:35<01:59, 2265.13 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79975/350343 [00:35<01:58, 2276.83 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80203/350343 [00:35<01:58, 2272.70 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80431/350343 [00:35<01:58, 2272.42 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80659/350343 [00:35<01:59, 2250.67 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80887/350343 [00:35<01:59, 2259.26 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81113/350343 [00:36<02:00, 2240.37 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81342/350343 [00:36<01:59, 2254.70 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81569/350343 [00:36<01:59, 2257.88 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81795/350343 [00:36<01:59, 2252.16 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82026/350343 [00:36<01:58, 2267.94 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82258/350343 [00:36<01:57, 2283.03 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82487/350343 [00:36<01:57, 2274.51 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82715/350343 [00:36<02:01, 2196.83 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82944/350343 [00:36<02:00, 2221.79 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 83172/350343 [00:36<01:59, 2237.64 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83401/350343 [00:37<01:58, 2252.62 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83633/350343 [00:37<01:57, 2270.83 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83862/350343 [00:37<01:57, 2274.89 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84092/350343 [00:37<01:56, 2280.12 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84321/350343 [00:37<01:56, 2281.32 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84550/350343 [00:37<01:56, 2282.47 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84779/350343 [00:37<01:57, 2266.28 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85010/350343 [00:37<01:56, 2278.48 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85239/350343 [00:37<01:56, 2280.61 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85468/350343 [00:37<01:56, 2279.23 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85703/350343 [00:38<01:55, 2299.52 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 85933/350343 [00:38<01:55, 2290.35 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86163/350343 [00:38<01:55, 2288.51 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86393/350343 [00:38<01:55, 2291.27 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86627/350343 [00:38<01:54, 2304.95 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86858/350343 [00:38<01:54, 2298.06 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87088/350343 [00:38<01:55, 2288.40 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87318/350343 [00:38<01:54, 2289.82 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87551/350343 [00:38<01:54, 2301.46 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 87783/350343 [00:38<01:53, 2305.64 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88014/350343 [00:39<01:54, 2297.19 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88244/350343 [00:39<01:54, 2291.92 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88474/350343 [00:39<01:54, 2280.05 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88704/350343 [00:39<01:54, 2284.35 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88933/350343 [00:39<01:54, 2279.05 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 89164/350343 [00:39<01:54, 2286.41 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89393/350343 [00:39<01:54, 2281.57 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89626/350343 [00:39<01:53, 2294.45 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89859/350343 [00:39<01:53, 2304.35 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90090/350343 [00:39<01:53, 2294.68 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90320/350343 [00:40<01:57, 2204.97 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90548/350343 [00:40<01:56, 2226.64 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90773/350343 [00:40<01:56, 2232.71 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91004/350343 [00:40<01:55, 2254.59 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91233/350343 [00:40<01:54, 2263.77 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91460/350343 [00:40<01:54, 2261.24 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91687/350343 [00:40<01:54, 2259.55 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91915/350343 [00:40<01:54, 2263.91 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92148/350343 [00:40<01:53, 2281.25 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92378/350343 [00:40<01:52, 2286.77 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92612/350343 [00:41<01:52, 2300.44 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 92843/350343 [00:41<01:52, 2297.75 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93073/350343 [00:41<01:52, 2292.83 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93303/350343 [00:41<01:52, 2286.73 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93532/350343 [00:41<01:53, 2268.54 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93760/350343 [00:41<01:52, 2271.70 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93988/350343 [00:41<01:53, 2251.76 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94217/350343 [00:41<01:53, 2261.81 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94450/350343 [00:41<01:52, 2280.41 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94681/350343 [00:41<01:51, 2288.45 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94914/350343 [00:42<01:51, 2298.81 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95145/350343 [00:42<01:50, 2301.75 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95376/350343 [00:42<01:51, 2292.52 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95608/350343 [00:42<01:50, 2299.03 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95839/350343 [00:42<01:50, 2299.93 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 96070/350343 [00:42<01:50, 2295.60 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 96300/350343 [00:42<01:51, 2282.10 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96531/350343 [00:42<01:50, 2290.24 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96762/350343 [00:42<01:50, 2294.10 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96992/350343 [00:43<01:50, 2289.63 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97221/350343 [00:43<01:50, 2283.08 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97450/350343 [00:43<01:51, 2273.21 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97681/350343 [00:43<01:50, 2282.92 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97910/350343 [00:43<01:50, 2276.57 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98141/350343 [00:43<01:50, 2285.99 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98372/350343 [00:43<01:49, 2292.79 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98602/350343 [00:43<01:54, 2194.05 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98829/350343 [00:43<01:53, 2214.77 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99058/350343 [00:43<01:52, 2235.51 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99288/350343 [00:44<01:51, 2253.65 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99518/350343 [00:44<01:50, 2264.68 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99745/350343 [00:44<01:50, 2260.22 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 99976/350343 [00:44<01:50, 2274.05 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100207/350343 [00:44<01:49, 2283.91 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100437/350343 [00:44<01:49, 2286.36 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100668/350343 [00:44<01:48, 2292.18 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 100898/350343 [00:44<01:49, 2284.47 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101131/350343 [00:44<01:48, 2297.94 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101365/350343 [00:44<01:47, 2308.24 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101596/350343 [00:45<01:48, 2296.05 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101828/350343 [00:45<01:47, 2301.17 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102059/350343 [00:45<01:47, 2301.66 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102293/350343 [00:45<01:47, 2311.32 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102526/350343 [00:45<01:47, 2315.27 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102758/350343 [00:45<01:46, 2316.27 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102990/350343 [00:45<01:51, 2209.46 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 103212/350343 [00:45<01:54, 2155.31 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103438/350343 [00:45<01:53, 2183.98 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103658/350343 [00:45<01:52, 2187.69 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103886/350343 [00:46<01:51, 2212.80 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104117/350343 [00:46<01:49, 2241.09 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104346/350343 [00:46<01:49, 2255.29 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104576/350343 [00:46<01:48, 2267.03 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104809/350343 [00:46<01:47, 2285.40 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 105039/350343 [00:46<01:47, 2287.33 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105273/350343 [00:46<01:46, 2302.99 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105504/350343 [00:46<01:46, 2292.23 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105735/350343 [00:46<01:46, 2296.08 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105965/350343 [00:46<01:46, 2294.93 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106198/350343 [00:47<01:45, 2303.33 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106430/350343 [00:47<01:45, 2308.03 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106661/350343 [00:47<01:46, 2278.94 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 106889/350343 [00:47<01:51, 2180.36 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107115/350343 [00:47<01:50, 2202.13 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107336/350343 [00:47<01:52, 2161.33 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107560/350343 [00:47<01:51, 2181.72 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107785/350343 [00:47<01:50, 2200.49 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108008/350343 [00:47<01:49, 2207.63 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108241/350343 [00:47<01:47, 2242.15 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108468/350343 [00:48<01:47, 2248.47 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108698/350343 [00:48<01:46, 2263.45 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108927/350343 [00:48<01:46, 2270.96 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109157/350343 [00:48<01:45, 2279.17 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109385/350343 [00:48<01:45, 2278.78 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109613/350343 [00:48<01:45, 2275.40 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109842/350343 [00:48<01:45, 2279.28 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110070/350343 [00:48<01:45, 2272.13 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110300/350343 [00:48<01:45, 2279.03 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110530/350343 [00:48<01:45, 2283.57 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110759/350343 [00:49<01:45, 2274.70 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110987/350343 [00:49<01:45, 2275.17 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111215/350343 [00:49<01:45, 2266.04 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111446/350343 [00:49<01:44, 2278.15 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111678/350343 [00:49<01:44, 2289.99 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111908/350343 [00:49<01:44, 2279.47 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112136/350343 [00:49<01:45, 2247.50 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112361/350343 [00:49<01:46, 2233.44 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112588/350343 [00:49<01:46, 2242.57 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112817/350343 [00:50<01:45, 2256.09 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113043/350343 [00:50<01:45, 2250.98 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113273/350343 [00:50<01:44, 2264.24 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113503/350343 [00:50<01:44, 2273.86 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113731/350343 [00:50<01:44, 2269.91 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 113959/350343 [00:50<01:44, 2267.85 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114186/350343 [00:50<01:44, 2267.26 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114417/350343 [00:50<01:43, 2278.71 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114645/350343 [00:50<01:43, 2268.74 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114877/350343 [00:50<01:43, 2281.96 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115106/350343 [00:51<01:43, 2275.91 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115334/350343 [00:51<01:44, 2258.85 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115563/350343 [00:51<01:43, 2266.68 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115790/350343 [00:51<01:43, 2265.83 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116020/350343 [00:51<01:42, 2275.01 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116254/350343 [00:51<01:42, 2293.00 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116484/350343 [00:51<01:42, 2292.26 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116718/350343 [00:51<01:41, 2305.66 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116949/350343 [00:51<01:41, 2299.93 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117180/350343 [00:51<01:45, 2212.39 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117409/350343 [00:52<01:44, 2234.35 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117638/350343 [00:52<01:43, 2250.04 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117864/350343 [00:52<01:43, 2252.79 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 118090/350343 [00:52<01:43, 2253.90 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118320/350343 [00:52<01:42, 2265.53 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118547/350343 [00:52<01:42, 2253.55 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118773/350343 [00:52<01:42, 2250.99 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119003/350343 [00:52<01:42, 2263.98 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119230/350343 [00:52<01:42, 2255.28 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119460/350343 [00:52<01:41, 2266.13 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119690/350343 [00:53<01:41, 2274.25 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119918/350343 [00:53<01:41, 2269.65 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120148/350343 [00:53<01:41, 2276.27 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120376/350343 [00:53<01:41, 2275.01 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120604/350343 [00:53<01:41, 2261.93 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120834/350343 [00:53<01:40, 2273.04 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121066/350343 [00:53<01:40, 2286.31 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121295/350343 [00:53<01:40, 2283.59 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121524/350343 [00:53<01:40, 2276.22 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121752/350343 [00:53<01:40, 2265.48 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121982/350343 [00:54<01:40, 2274.97 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122212/350343 [00:54<01:40, 2280.07 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122441/350343 [00:54<01:40, 2277.31 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122672/350343 [00:54<01:39, 2285.17 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122901/350343 [00:54<01:39, 2280.68 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123130/350343 [00:54<01:40, 2257.12 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123356/350343 [00:54<01:41, 2247.39 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123581/350343 [00:54<01:41, 2241.55 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123807/350343 [00:54<01:40, 2246.41 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 124036/350343 [00:54<01:40, 2257.88 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 124264/350343 [00:55<01:39, 2263.92 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124497/350343 [00:55<01:38, 2281.89 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124726/350343 [00:55<01:38, 2279.04 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124955/350343 [00:55<01:38, 2280.95 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125184/350343 [00:55<01:39, 2253.23 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125410/350343 [00:55<01:40, 2247.60 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125635/350343 [00:55<01:40, 2240.98 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125861/350343 [00:55<01:39, 2245.67 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126093/350343 [00:55<01:38, 2265.49 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126320/350343 [00:55<01:39, 2258.80 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126550/350343 [00:56<01:38, 2270.12 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126778/350343 [00:56<01:38, 2272.43 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127010/350343 [00:56<01:37, 2285.00 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127239/350343 [00:56<01:37, 2282.37 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127468/350343 [00:56<01:42, 2182.55 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127692/350343 [00:56<01:41, 2198.29 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 127922/350343 [00:56<01:39, 2225.70 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128149/350343 [00:56<01:39, 2238.14 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128374/350343 [00:56<01:39, 2231.93 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128600/350343 [00:56<01:39, 2239.55 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128827/350343 [00:57<01:38, 2246.55 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129055/350343 [00:57<01:38, 2255.03 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129281/350343 [00:57<01:39, 2216.10 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129503/350343 [00:57<01:40, 2199.78 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129724/350343 [00:57<01:40, 2188.12 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129943/350343 [00:57<01:40, 2186.28 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130165/350343 [00:57<01:40, 2195.11 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130385/350343 [00:57<01:40, 2190.31 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130605/350343 [00:57<01:40, 2177.07 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130823/350343 [00:57<01:40, 2177.12 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131046/350343 [00:58<01:40, 2186.94 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131270/350343 [00:58<01:39, 2197.27 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131494/350343 [00:58<01:39, 2190.37 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131719/350343 [00:58<01:39, 2195.10 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131943/350343 [00:58<01:40, 2180.57 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132167/350343 [00:58<01:39, 2197.23 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132392/350343 [00:58<01:38, 2212.39 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132619/350343 [00:58<01:37, 2228.98 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132842/350343 [00:58<01:37, 2228.50 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133065/350343 [00:59<01:38, 2216.97 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133291/350343 [00:59<01:37, 2227.41 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133517/350343 [00:59<01:36, 2237.04 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133741/350343 [00:59<01:37, 2223.69 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133964/350343 [00:59<01:37, 2222.09 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134187/350343 [00:59<01:37, 2210.01 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134409/350343 [00:59<01:37, 2205.30 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134631/350343 [00:59<01:38, 2200.00 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134855/350343 [00:59<01:38, 2189.17 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135078/350343 [00:59<01:38, 2182.00 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135302/350343 [01:00<01:38, 2179.43 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135524/350343 [01:00<01:39, 2159.34 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135748/350343 [01:00<01:39, 2146.84 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 135971/350343 [01:00<01:40, 2143.46 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136194/350343 [01:00<01:40, 2139.51 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136418/350343 [01:00<01:39, 2153.36 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136642/350343 [01:00<01:38, 2158.68 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136867/350343 [01:00<01:38, 2170.80 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137090/350343 [01:00<01:38, 2161.17 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137313/350343 [01:00<01:38, 2157.89 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137536/350343 [01:01<01:38, 2164.61 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137760/350343 [01:01<01:38, 2157.85 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137984/350343 [01:01<01:38, 2151.33 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138208/350343 [01:01<01:38, 2153.38 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138432/350343 [01:01<01:38, 2156.56 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138656/350343 [01:01<01:37, 2162.50 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138879/350343 [01:01<01:38, 2156.66 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139103/350343 [01:01<01:37, 2157.33 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139326/350343 [01:01<01:38, 2150.38 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139550/350343 [01:01<01:37, 2161.16 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139774/350343 [01:02<01:37, 2165.51 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139997/350343 [01:02<01:36, 2181.27 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140229/350343 [01:02<01:34, 2220.76 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140456/350343 [01:02<01:33, 2233.20 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140689/350343 [01:02<01:32, 2260.54 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140919/350343 [01:02<01:32, 2271.46 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141152/350343 [01:02<01:31, 2286.38 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141384/350343 [01:02<01:31, 2295.69 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141614/350343 [01:02<01:30, 2294.84 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141844/350343 [01:03<01:31, 2284.86 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142073/350343 [01:03<01:31, 2278.12 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142302/350343 [01:03<01:31, 2281.29 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142531/350343 [01:03<01:31, 2268.96 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142766/350343 [01:03<01:30, 2292.71 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143000/350343 [01:03<01:29, 2304.89 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143231/350343 [01:03<01:30, 2298.34 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143461/350343 [01:03<01:30, 2291.38 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143691/350343 [01:03<01:30, 2292.99 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143921/350343 [01:03<01:30, 2293.26 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144151/350343 [01:04<01:29, 2292.33 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144381/350343 [01:04<01:29, 2293.61 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144611/350343 [01:04<01:30, 2270.80 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144839/350343 [01:04<01:30, 2263.17 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145068/350343 [01:04<01:30, 2269.08 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145300/350343 [01:04<01:29, 2284.18 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145535/350343 [01:04<01:28, 2303.27 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145766/350343 [01:04<01:29, 2293.91 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145999/350343 [01:04<01:28, 2303.59 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146230/350343 [01:04<01:28, 2295.26 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146464/350343 [01:05<01:28, 2307.62 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146697/350343 [01:05<01:28, 2311.74 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146929/350343 [01:05<01:29, 2282.97 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147159/350343 [01:05<01:31, 2210.60 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147390/350343 [01:05<01:30, 2238.64 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147623/350343 [01:05<01:29, 2263.79 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147851/350343 [01:05<01:29, 2266.56 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148079/350343 [01:05<01:29, 2269.82 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148312/350343 [01:05<01:28, 2286.67 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148541/350343 [01:05<01:28, 2270.46 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148771/350343 [01:06<01:28, 2278.07 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149004/350343 [01:06<01:27, 2292.42 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149234/350343 [01:06<01:28, 2277.11 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149464/350343 [01:06<01:28, 2282.55 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149694/350343 [01:06<01:27, 2287.41 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149923/350343 [01:06<01:27, 2285.98 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150152/350343 [01:06<01:27, 2282.93 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150382/350343 [01:06<01:27, 2287.89 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150611/350343 [01:06<01:27, 2286.74 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150840/350343 [01:06<01:27, 2286.53 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151072/350343 [01:07<01:26, 2295.32 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151302/350343 [01:07<01:26, 2296.31 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151532/350343 [01:07<01:26, 2290.67 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151762/350343 [01:07<01:26, 2291.76 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151992/350343 [01:07<01:27, 2270.65 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152220/350343 [01:07<01:27, 2273.22 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152452/350343 [01:07<01:26, 2285.70 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152681/350343 [01:07<01:26, 2281.56 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152911/350343 [01:07<01:26, 2284.49 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153140/350343 [01:07<01:26, 2282.94 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153371/350343 [01:08<01:26, 2289.84 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153603/350343 [01:08<01:25, 2296.88 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153833/350343 [01:08<01:25, 2293.18 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154065/350343 [01:08<01:25, 2299.04 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154297/350343 [01:08<01:25, 2302.56 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154528/350343 [01:08<01:25, 2296.60 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154758/350343 [01:08<01:25, 2292.91 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154988/350343 [01:08<01:26, 2266.62 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155215/350343 [01:08<01:28, 2198.10 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155442/350343 [01:08<01:27, 2217.40 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155673/350343 [01:09<01:26, 2242.18 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155898/350343 [01:09<01:26, 2241.62 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156123/350343 [01:09<01:26, 2236.73 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156348/350343 [01:09<01:26, 2240.45 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156573/350343 [01:09<01:26, 2237.63 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156800/350343 [01:09<01:26, 2246.63 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157027/350343 [01:09<01:25, 2252.46 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157253/350343 [01:09<01:26, 2236.76 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157478/350343 [01:09<01:26, 2240.21 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157704/350343 [01:09<01:25, 2245.12 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157935/350343 [01:10<01:24, 2263.85 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158168/350343 [01:10<01:24, 2283.36 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158397/350343 [01:10<01:24, 2275.87 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158631/350343 [01:10<01:23, 2294.28 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158861/350343 [01:10<01:23, 2291.18 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159091/350343 [01:10<01:23, 2291.07 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159321/350343 [01:10<01:23, 2289.68 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159550/350343 [01:10<01:23, 2289.75 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159780/350343 [01:10<01:23, 2292.77 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160010/350343 [01:10<01:23, 2289.08 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160239/350343 [01:11<01:23, 2288.51 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160470/350343 [01:11<01:22, 2292.92 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160700/350343 [01:11<01:23, 2259.74 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160927/350343 [01:11<01:24, 2249.48 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161155/350343 [01:11<01:23, 2256.65 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161384/350343 [01:11<01:23, 2266.13 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161612/350343 [01:11<01:23, 2268.37 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161839/350343 [01:11<01:23, 2267.75 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162066/350343 [01:11<01:23, 2267.96 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162297/350343 [01:11<01:22, 2279.63 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162528/350343 [01:12<01:22, 2287.97 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162761/350343 [01:12<01:21, 2300.49 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162994/350343 [01:12<01:21, 2308.45 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163225/350343 [01:12<01:21, 2297.87 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163455/350343 [01:12<01:21, 2297.33 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163685/350343 [01:12<01:21, 2279.44 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163916/350343 [01:12<01:21, 2285.77 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164145/350343 [01:12<01:21, 2283.36 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164375/350343 [01:12<01:21, 2287.39 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164609/350343 [01:12<01:20, 2301.99 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164840/350343 [01:13<01:23, 2213.84 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165063/350343 [01:13<01:23, 2217.67 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165286/350343 [01:13<01:23, 2210.10 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165511/350343 [01:13<01:23, 2221.68 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165734/350343 [01:13<01:23, 2218.01 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165962/350343 [01:13<01:22, 2235.17 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166191/350343 [01:13<01:21, 2249.81 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166417/350343 [01:13<01:21, 2252.23 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166648/350343 [01:13<01:20, 2268.07 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166879/350343 [01:14<01:20, 2280.35 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167108/350343 [01:14<01:20, 2277.64 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167336/350343 [01:14<01:20, 2277.84 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167564/350343 [01:14<01:20, 2275.78 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167793/350343 [01:14<01:20, 2278.51 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168021/350343 [01:14<01:20, 2271.31 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168251/350343 [01:14<01:19, 2277.98 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168482/350343 [01:14<01:19, 2286.01 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168713/350343 [01:14<01:19, 2292.15 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168943/350343 [01:14<01:19, 2290.07 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169173/350343 [01:15<01:19, 2284.86 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169402/350343 [01:15<01:19, 2269.01 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169631/350343 [01:15<01:19, 2273.65 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169859/350343 [01:15<01:19, 2270.36 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170094/350343 [01:15<01:18, 2291.62 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170324/350343 [01:15<01:18, 2290.40 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170554/350343 [01:15<01:18, 2280.12 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170784/350343 [01:15<01:18, 2283.10 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171013/350343 [01:15<01:18, 2279.18 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171244/350343 [01:15<01:18, 2287.30 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171473/350343 [01:16<01:18, 2275.89 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171701/350343 [01:16<01:18, 2271.19 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171930/350343 [01:16<01:18, 2274.87 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172158/350343 [01:16<01:18, 2263.32 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172386/350343 [01:16<01:18, 2267.55 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172613/350343 [01:16<01:18, 2261.56 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172840/350343 [01:16<01:18, 2261.96 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173069/350343 [01:16<01:18, 2268.56 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173296/350343 [01:16<01:18, 2260.33 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173526/350343 [01:16<01:17, 2270.41 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173758/350343 [01:17<01:17, 2282.71 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173988/350343 [01:17<01:17, 2286.32 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174217/350343 [01:17<01:17, 2274.45 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174445/350343 [01:17<01:17, 2257.49 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174673/350343 [01:17<01:20, 2179.97 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174896/350343 [01:17<01:20, 2190.19 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 175120/350343 [01:17<01:19, 2202.75 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175347/350343 [01:17<01:18, 2220.51 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175570/350343 [01:17<01:19, 2199.93 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175795/350343 [01:17<01:18, 2212.33 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176017/350343 [01:18<01:18, 2212.61 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176245/350343 [01:18<01:18, 2231.19 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176472/350343 [01:18<01:17, 2240.19 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176699/350343 [01:18<01:17, 2247.65 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176924/350343 [01:18<01:17, 2245.23 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177154/350343 [01:18<01:16, 2261.32 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177385/350343 [01:18<01:16, 2273.64 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177613/350343 [01:18<01:16, 2264.54 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177844/350343 [01:18<01:15, 2277.37 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178074/350343 [01:18<01:15, 2282.10 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178303/350343 [01:19<01:15, 2283.04 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178533/350343 [01:19<01:15, 2286.16 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178762/350343 [01:19<01:15, 2273.39 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178990/350343 [01:19<01:15, 2266.27 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179217/350343 [01:19<01:15, 2265.90 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179444/350343 [01:19<01:15, 2266.48 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179671/350343 [01:19<01:16, 2244.34 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179896/350343 [01:19<01:16, 2235.83 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180121/350343 [01:19<01:16, 2238.29 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180345/350343 [01:19<01:16, 2232.65 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180577/350343 [01:20<01:15, 2257.75 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180805/350343 [01:20<01:14, 2263.24 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181032/350343 [01:20<01:14, 2259.99 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181261/350343 [01:20<01:14, 2268.69 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181493/350343 [01:20<01:13, 2282.44 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181726/350343 [01:20<01:13, 2295.35 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181956/350343 [01:20<01:13, 2292.75 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182189/350343 [01:20<01:13, 2301.74 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182420/350343 [01:20<01:12, 2301.60 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182651/350343 [01:20<01:13, 2288.08 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182882/350343 [01:21<01:13, 2292.79 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183112/350343 [01:21<01:13, 2284.17 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183341/350343 [01:21<01:13, 2273.34 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183569/350343 [01:21<01:13, 2273.13 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183797/350343 [01:21<01:13, 2252.07 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184023/350343 [01:21<01:14, 2245.68 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184249/350343 [01:21<01:13, 2247.70 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184477/350343 [01:21<01:13, 2255.86 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184703/350343 [01:21<01:13, 2251.83 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184929/350343 [01:21<01:13, 2251.67 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185161/350343 [01:22<01:12, 2271.24 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185389/350343 [01:22<01:12, 2268.69 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185616/350343 [01:22<01:12, 2258.60 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185842/350343 [01:22<01:12, 2255.48 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186070/350343 [01:22<01:12, 2262.60 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186301/350343 [01:22<01:14, 2196.38 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186530/350343 [01:22<01:13, 2221.72 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186760/350343 [01:22<01:12, 2243.37 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186991/350343 [01:22<01:12, 2260.94 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187224/350343 [01:23<01:11, 2280.56 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187453/350343 [01:23<01:11, 2276.93 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187681/350343 [01:23<01:11, 2265.87 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187908/350343 [01:23<01:11, 2263.02 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188137/350343 [01:23<01:11, 2270.71 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188365/350343 [01:23<01:11, 2267.15 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188592/350343 [01:23<01:11, 2264.37 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188821/350343 [01:23<01:11, 2270.79 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189052/350343 [01:23<01:10, 2281.77 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189283/350343 [01:23<01:10, 2287.78 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189512/350343 [01:24<01:10, 2287.10 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189742/350343 [01:24<01:10, 2289.30 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189971/350343 [01:24<01:10, 2286.21 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190200/350343 [01:24<01:10, 2284.66 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190431/350343 [01:24<01:09, 2290.34 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190665/350343 [01:24<01:09, 2303.22 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190896/350343 [01:24<01:09, 2298.98 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191126/350343 [01:24<01:09, 2292.68 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191357/350343 [01:24<01:09, 2296.09 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191587/350343 [01:24<01:09, 2291.04 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191817/350343 [01:25<01:09, 2286.06 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192046/350343 [01:25<01:09, 2278.65 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192274/350343 [01:25<01:09, 2268.16 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192501/350343 [01:25<01:09, 2260.63 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192728/350343 [01:25<01:09, 2258.78 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192956/350343 [01:25<01:09, 2263.12 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193184/350343 [01:25<01:09, 2267.20 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193414/350343 [01:25<01:08, 2276.24 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193643/350343 [01:25<01:08, 2277.50 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193871/350343 [01:25<01:09, 2264.31 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194098/350343 [01:26<01:10, 2228.18 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194321/350343 [01:26<01:10, 2217.12 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194543/350343 [01:26<01:10, 2206.21 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194774/350343 [01:26<01:09, 2235.31 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195004/350343 [01:26<01:08, 2252.49 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195232/350343 [01:26<01:08, 2259.78 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195463/350343 [01:26<01:08, 2273.42 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195694/350343 [01:26<01:07, 2281.74 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195923/350343 [01:26<01:07, 2282.13 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196152/350343 [01:26<01:10, 2191.04 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196378/350343 [01:27<01:09, 2209.61 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196608/350343 [01:27<01:08, 2234.12 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196836/350343 [01:27<01:08, 2245.27 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197068/350343 [01:27<01:07, 2266.54 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197300/350343 [01:27<01:07, 2280.20 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197529/350343 [01:27<01:07, 2275.56 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197758/350343 [01:27<01:06, 2277.92 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197993/350343 [01:27<01:06, 2297.86 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198224/350343 [01:27<01:06, 2301.39 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198455/350343 [01:27<01:06, 2275.41 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198686/350343 [01:28<01:06, 2283.83 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198917/350343 [01:28<01:06, 2289.46 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199147/350343 [01:28<01:06, 2280.73 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199376/350343 [01:28<01:06, 2276.70 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199604/350343 [01:28<01:06, 2272.21 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199832/350343 [01:28<01:06, 2252.57 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200064/350343 [01:28<01:06, 2272.01 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200293/350343 [01:28<01:05, 2276.67 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200522/350343 [01:28<01:05, 2279.24 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200759/350343 [01:28<01:04, 2304.20 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200990/350343 [01:29<01:05, 2295.45 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201220/350343 [01:29<01:05, 2289.85 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201450/350343 [01:29<01:05, 2279.85 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201679/350343 [01:29<01:05, 2280.17 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201908/350343 [01:29<01:05, 2282.83 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202137/350343 [01:29<01:05, 2262.79 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202364/350343 [01:29<01:05, 2255.47 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202593/350343 [01:29<01:05, 2263.95 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202820/350343 [01:29<01:05, 2264.94 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203049/350343 [01:29<01:04, 2272.10 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203282/350343 [01:30<01:04, 2288.70 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203511/350343 [01:30<01:04, 2261.53 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203739/350343 [01:30<01:04, 2266.43 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203966/350343 [01:30<01:04, 2265.47 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204197/350343 [01:30<01:04, 2277.42 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204425/350343 [01:30<01:08, 2144.71 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204642/350343 [01:30<01:14, 1947.68 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204841/350343 [01:30<01:18, 1851.53 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205030/350343 [01:30<01:20, 1806.49 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205213/350343 [01:31<01:21, 1775.38 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205404/350343 [01:31<01:20, 1811.29 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205617/350343 [01:31<01:16, 1899.28 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 205836/350343 [01:31<01:12, 1982.29 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206060/350343 [01:31<01:10, 2056.99 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206287/350343 [01:31<01:07, 2118.50 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206516/350343 [01:31<01:06, 2168.60 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206740/350343 [01:31<01:05, 2187.16 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206966/350343 [01:31<01:04, 2208.11 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207198/350343 [01:31<01:03, 2239.35 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207423/350343 [01:32<01:04, 2232.19 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207647/350343 [01:32<01:05, 2176.70 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207873/350343 [01:32<01:04, 2200.32 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208102/350343 [01:32<01:03, 2226.57 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208328/350343 [01:32<01:03, 2235.72 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208558/350343 [01:32<01:02, 2254.33 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208789/350343 [01:32<01:02, 2269.27 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209017/350343 [01:32<01:02, 2259.42 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209244/350343 [01:32<01:02, 2260.64 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209471/350343 [01:32<01:02, 2262.01 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209698/350343 [01:33<01:02, 2262.34 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209927/350343 [01:33<01:01, 2270.36 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210155/350343 [01:33<01:01, 2271.48 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210384/350343 [01:33<01:01, 2275.12 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210612/350343 [01:33<01:01, 2259.90 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210839/350343 [01:33<01:01, 2253.76 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211069/350343 [01:33<01:01, 2266.56 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211296/350343 [01:33<01:01, 2261.04 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211524/350343 [01:33<01:01, 2266.49 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211754/350343 [01:33<01:00, 2275.16 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211982/350343 [01:34<01:00, 2273.93 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212210/350343 [01:34<01:01, 2264.01 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212437/350343 [01:34<01:01, 2251.60 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212665/350343 [01:34<01:00, 2258.84 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212891/350343 [01:34<01:01, 2250.05 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213117/350343 [01:34<01:01, 2246.82 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213342/350343 [01:34<01:01, 2239.29 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213566/350343 [01:34<01:01, 2228.50 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213797/350343 [01:34<01:00, 2252.13 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214023/350343 [01:35<01:00, 2244.84 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214248/350343 [01:35<01:00, 2236.45 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214476/350343 [01:35<01:00, 2247.61 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214701/350343 [01:35<01:00, 2233.78 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214928/350343 [01:35<01:00, 2242.55 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215153/350343 [01:35<01:00, 2242.07 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215378/350343 [01:35<01:00, 2238.60 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215602/350343 [01:35<01:00, 2228.01 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215825/350343 [01:35<01:00, 2226.16 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216051/350343 [01:35<01:00, 2235.30 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216275/350343 [01:36<01:00, 2225.15 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216501/350343 [01:36<01:02, 2158.73 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216731/350343 [01:36<01:00, 2199.70 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216957/350343 [01:36<01:00, 2216.69 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217185/350343 [01:36<00:59, 2233.32 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217418/350343 [01:36<00:58, 2261.74 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217647/350343 [01:36<00:58, 2269.64 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217876/350343 [01:36<00:58, 2275.21 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218104/350343 [01:36<00:58, 2257.96 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218330/350343 [01:36<00:58, 2244.38 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218556/350343 [01:37<00:58, 2247.76 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218781/350343 [01:37<00:58, 2236.23 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219009/350343 [01:37<00:58, 2246.73 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219239/350343 [01:37<00:58, 2260.33 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219468/350343 [01:37<00:57, 2266.59 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219700/350343 [01:37<00:57, 2280.67 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219929/350343 [01:37<00:57, 2281.90 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220160/350343 [01:37<00:56, 2288.22 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220389/350343 [01:37<00:57, 2270.96 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220617/350343 [01:37<00:57, 2263.83 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220844/350343 [01:38<00:57, 2247.20 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221069/350343 [01:38<00:57, 2245.85 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221300/350343 [01:38<00:57, 2263.16 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221529/350343 [01:38<00:56, 2270.82 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221757/350343 [01:38<00:56, 2265.15 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221990/350343 [01:38<00:56, 2284.36 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222219/350343 [01:38<00:56, 2275.80 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222447/350343 [01:38<00:56, 2264.48 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222674/350343 [01:38<00:56, 2246.44 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222899/350343 [01:38<00:57, 2226.35 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223122/350343 [01:39<00:57, 2221.60 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223348/350343 [01:39<00:56, 2232.19 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223575/350343 [01:39<00:56, 2242.03 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223803/350343 [01:39<00:56, 2250.74 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224029/350343 [01:39<00:56, 2245.24 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224254/350343 [01:39<00:56, 2244.96 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224479/350343 [01:39<00:56, 2246.01 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224704/350343 [01:39<00:56, 2242.24 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224929/350343 [01:39<00:55, 2243.70 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225154/350343 [01:39<00:55, 2245.10 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225379/350343 [01:40<00:57, 2174.64 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225597/350343 [01:40<00:59, 2109.45 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225825/350343 [01:40<00:57, 2158.06 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226050/350343 [01:40<00:56, 2182.51 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226277/350343 [01:40<00:56, 2206.02 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226502/350343 [01:40<00:55, 2217.76 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226733/350343 [01:40<00:55, 2243.80 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226959/350343 [01:40<00:54, 2247.21 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227184/350343 [01:40<00:55, 2230.41 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227410/350343 [01:40<00:54, 2237.27 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227638/350343 [01:41<00:54, 2248.15 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 227865/350343 [01:41<00:54, 2253.60 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228092/350343 [01:41<00:54, 2256.09 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228322/350343 [01:41<00:53, 2267.78 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228551/350343 [01:41<00:53, 2273.00 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228781/350343 [01:41<00:53, 2280.60 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229011/350343 [01:41<00:53, 2285.63 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229240/350343 [01:41<00:53, 2271.52 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229468/350343 [01:41<00:53, 2262.23 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229695/350343 [01:41<00:53, 2259.15 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229921/350343 [01:42<00:55, 2183.67 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230152/350343 [01:42<00:54, 2218.67 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230380/350343 [01:42<00:53, 2235.54 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230611/350343 [01:42<00:53, 2256.25 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230838/350343 [01:42<00:52, 2258.48 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231067/350343 [01:42<00:52, 2267.25 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231295/350343 [01:42<00:52, 2270.53 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231523/350343 [01:42<00:52, 2269.71 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231751/350343 [01:42<00:52, 2257.02 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231978/350343 [01:43<00:52, 2259.21 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232204/350343 [01:43<00:52, 2256.03 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232435/350343 [01:43<00:51, 2270.96 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232663/350343 [01:43<00:52, 2253.90 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232889/350343 [01:43<00:52, 2251.46 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233115/350343 [01:43<00:52, 2241.78 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233340/350343 [01:43<00:52, 2238.00 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233567/350343 [01:43<00:51, 2246.52 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233792/350343 [01:43<00:52, 2239.75 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234020/350343 [01:43<00:51, 2250.42 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234248/350343 [01:44<00:51, 2256.62 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234479/350343 [01:44<00:51, 2269.87 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234709/350343 [01:44<00:50, 2278.15 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234937/350343 [01:44<00:50, 2273.68 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235165/350343 [01:44<00:50, 2264.53 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235393/350343 [01:44<00:50, 2267.48 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235625/350343 [01:44<00:50, 2282.54 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235861/350343 [01:44<00:49, 2305.43 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236092/350343 [01:44<00:50, 2245.12 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236321/350343 [01:44<00:50, 2257.59 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236548/350343 [01:45<00:50, 2259.19 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236777/350343 [01:45<00:50, 2268.01 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237004/350343 [01:45<00:50, 2261.38 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237231/350343 [01:45<00:50, 2257.48 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237458/350343 [01:45<00:49, 2259.65 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237685/350343 [01:45<00:49, 2259.89 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237912/350343 [01:45<00:50, 2245.71 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238137/350343 [01:45<00:52, 2143.53 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238353/350343 [01:45<00:52, 2147.97 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238580/350343 [01:45<00:51, 2181.52 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238806/350343 [01:46<00:50, 2202.54 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239032/350343 [01:46<00:50, 2218.04 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239258/350343 [01:46<00:49, 2228.58 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239484/350343 [01:46<00:49, 2237.84 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239709/350343 [01:46<00:49, 2241.37 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239934/350343 [01:46<00:49, 2229.40 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240158/350343 [01:46<00:49, 2230.28 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240386/350343 [01:46<00:48, 2245.05 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240611/350343 [01:46<00:49, 2234.83 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240838/350343 [01:46<00:48, 2242.87 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241063/350343 [01:47<00:48, 2241.49 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241288/350343 [01:47<00:48, 2234.78 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241514/350343 [01:47<00:48, 2241.34 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241744/350343 [01:47<00:48, 2258.78 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241974/350343 [01:47<00:47, 2269.85 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242202/350343 [01:47<00:47, 2270.87 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242430/350343 [01:47<00:47, 2259.50 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242656/350343 [01:47<00:47, 2247.19 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242881/350343 [01:47<00:47, 2239.10 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243106/350343 [01:47<00:47, 2241.12 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243335/350343 [01:48<00:47, 2255.63 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243565/350343 [01:48<00:47, 2266.66 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243792/350343 [01:48<00:49, 2170.46 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244023/350343 [01:48<00:48, 2209.44 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244251/350343 [01:48<00:47, 2228.73 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244479/350343 [01:48<00:47, 2240.23 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244712/350343 [01:48<00:46, 2265.39 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244942/350343 [01:48<00:46, 2273.80 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 245170/350343 [01:48<00:46, 2275.55 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245398/350343 [01:48<00:46, 2264.72 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245628/350343 [01:49<00:46, 2275.18 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245856/350343 [01:49<00:46, 2259.73 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246083/350343 [01:49<00:46, 2259.14 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246316/350343 [01:49<00:45, 2277.72 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246544/350343 [01:49<00:45, 2277.82 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246774/350343 [01:49<00:45, 2282.02 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247003/350343 [01:49<00:45, 2280.12 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247232/350343 [01:49<00:45, 2274.54 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247466/350343 [01:49<00:44, 2292.52 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247696/350343 [01:49<00:44, 2291.60 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247926/350343 [01:50<00:44, 2279.13 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248154/350343 [01:50<00:45, 2268.06 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248384/350343 [01:50<00:44, 2275.41 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248612/350343 [01:50<00:44, 2262.64 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248840/350343 [01:50<00:44, 2266.61 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249067/350343 [01:50<00:44, 2266.25 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249295/350343 [01:50<00:44, 2270.32 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249524/350343 [01:50<00:44, 2274.03 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249752/350343 [01:50<00:44, 2273.37 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249980/350343 [01:51<00:44, 2269.08 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250214/350343 [01:51<00:43, 2288.98 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250443/350343 [01:51<00:44, 2269.60 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250670/350343 [01:51<00:45, 2200.55 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250895/350343 [01:51<00:44, 2212.73 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251123/350343 [01:51<00:44, 2232.26 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251350/350343 [01:51<00:44, 2243.26 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251581/350343 [01:51<00:43, 2262.34 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251808/350343 [01:51<00:43, 2261.45 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252041/350343 [01:51<00:43, 2279.21 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252272/350343 [01:52<00:42, 2287.84 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252501/350343 [01:52<00:42, 2282.07 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252730/350343 [01:52<00:42, 2281.31 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252959/350343 [01:52<00:44, 2196.66 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253190/350343 [01:52<00:43, 2229.26 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253417/350343 [01:52<00:43, 2240.41 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253642/350343 [01:52<00:43, 2241.41 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253874/350343 [01:52<00:42, 2264.03 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254101/350343 [01:52<00:42, 2265.44 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254328/350343 [01:52<00:42, 2258.61 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254554/350343 [01:53<00:42, 2252.22 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254784/350343 [01:53<00:42, 2265.91 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255011/350343 [01:53<00:42, 2264.12 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255239/350343 [01:53<00:41, 2268.19 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255469/350343 [01:53<00:41, 2277.37 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255697/350343 [01:53<00:41, 2272.05 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255925/350343 [01:53<00:41, 2269.89 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256155/350343 [01:53<00:41, 2276.82 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256383/350343 [01:53<00:41, 2271.73 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256613/350343 [01:53<00:41, 2278.81 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256843/350343 [01:54<00:40, 2285.09 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257072/350343 [01:54<00:40, 2278.69 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257300/350343 [01:54<00:41, 2259.00 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257526/350343 [01:54<00:41, 2254.56 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257758/350343 [01:54<00:40, 2273.36 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257987/350343 [01:54<00:40, 2275.89 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 258215/350343 [01:54<00:40, 2268.54 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258449/350343 [01:54<00:40, 2288.41 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258678/350343 [01:54<00:40, 2283.20 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258907/350343 [01:54<00:40, 2281.59 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259136/350343 [01:55<00:39, 2282.71 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259365/350343 [01:55<00:39, 2280.92 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259594/350343 [01:55<00:39, 2270.07 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259822/350343 [01:55<00:39, 2268.96 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260049/350343 [01:55<00:39, 2264.50 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260276/350343 [01:55<00:40, 2247.87 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260501/350343 [01:55<00:40, 2237.68 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260725/350343 [01:55<00:40, 2230.50 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260949/350343 [01:55<00:40, 2231.48 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261178/350343 [01:55<00:39, 2247.30 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261406/350343 [01:56<00:39, 2255.21 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261632/350343 [01:56<00:39, 2255.60 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261858/350343 [01:56<00:39, 2255.45 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262085/350343 [01:56<00:39, 2257.98 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262311/350343 [01:56<00:39, 2253.15 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262537/350343 [01:56<00:39, 2247.48 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262762/350343 [01:56<00:39, 2245.02 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262987/350343 [01:56<00:38, 2245.57 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263214/350343 [01:56<00:38, 2251.48 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263442/350343 [01:56<00:38, 2259.51 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263674/350343 [01:57<00:38, 2276.06 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263902/350343 [01:57<00:37, 2275.74 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264130/350343 [01:57<00:38, 2265.00 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264357/350343 [01:57<00:38, 2252.26 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264583/350343 [01:57<00:39, 2157.27 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264812/350343 [01:57<00:38, 2195.36 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265040/350343 [01:57<00:38, 2217.98 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265270/350343 [01:57<00:37, 2240.00 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265500/350343 [01:57<00:37, 2256.96 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265730/350343 [01:57<00:37, 2269.27 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265958/350343 [01:58<00:37, 2270.05 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266186/350343 [01:58<00:37, 2264.81 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266419/350343 [01:58<00:36, 2282.99 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266648/350343 [01:58<00:36, 2268.61 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266880/350343 [01:58<00:36, 2281.76 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 267109/350343 [01:58<00:36, 2281.03 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267338/350343 [01:58<00:36, 2274.62 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267566/350343 [01:58<00:36, 2261.41 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267796/350343 [01:58<00:36, 2272.35 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268024/350343 [01:58<00:36, 2260.55 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268253/350343 [01:59<00:36, 2267.72 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268481/350343 [01:59<00:36, 2270.18 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268710/350343 [01:59<00:35, 2274.99 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268944/350343 [01:59<00:35, 2292.19 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269174/350343 [01:59<00:35, 2289.08 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269406/350343 [01:59<00:35, 2296.71 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269640/350343 [01:59<00:34, 2307.37 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269871/350343 [01:59<00:35, 2297.66 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270101/350343 [01:59<00:35, 2285.52 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270330/350343 [01:59<00:35, 2280.52 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270559/350343 [02:00<00:35, 2274.18 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270787/350343 [02:00<00:34, 2275.62 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271015/350343 [02:00<00:34, 2271.02 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271247/350343 [02:00<00:34, 2282.93 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271477/350343 [02:00<00:34, 2287.29 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271706/350343 [02:00<00:34, 2283.29 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271937/350343 [02:00<00:34, 2291.04 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272167/350343 [02:00<00:34, 2292.06 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272397/350343 [02:00<00:34, 2288.03 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272626/350343 [02:01<00:33, 2286.41 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272858/350343 [02:01<00:33, 2296.36 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273088/350343 [02:01<00:35, 2204.03 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273313/350343 [02:01<00:34, 2216.08 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273536/350343 [02:01<00:34, 2214.88 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273768/350343 [02:01<00:34, 2244.60 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273998/350343 [02:01<00:33, 2260.16 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274225/350343 [02:01<00:34, 2237.75 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274454/350343 [02:01<00:33, 2250.83 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274680/350343 [02:01<00:33, 2246.15 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274911/350343 [02:02<00:33, 2263.61 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275141/350343 [02:02<00:33, 2273.96 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275372/350343 [02:02<00:32, 2284.23 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275601/350343 [02:02<00:32, 2285.13 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275830/350343 [02:02<00:32, 2270.96 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276061/350343 [02:02<00:32, 2280.37 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276290/350343 [02:02<00:32, 2279.26 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276518/350343 [02:02<00:32, 2273.87 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276747/350343 [02:02<00:32, 2277.11 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276975/350343 [02:02<00:32, 2267.46 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277206/350343 [02:03<00:32, 2277.61 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277438/350343 [02:03<00:31, 2288.97 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277667/350343 [02:03<00:31, 2282.93 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277896/350343 [02:03<00:32, 2256.08 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278122/350343 [02:03<00:32, 2254.91 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278348/350343 [02:03<00:32, 2246.02 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278576/350343 [02:03<00:31, 2255.43 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278802/350343 [02:03<00:31, 2255.91 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279028/350343 [02:03<00:31, 2254.66 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279256/350343 [02:03<00:31, 2261.78 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279483/350343 [02:04<00:31, 2249.76 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279709/350343 [02:04<00:31, 2251.94 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279941/350343 [02:04<00:31, 2270.41 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280169/350343 [02:04<00:30, 2270.61 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280399/350343 [02:04<00:30, 2279.32 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280627/350343 [02:04<00:30, 2275.43 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280863/350343 [02:04<00:30, 2299.80 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281093/350343 [02:04<00:30, 2288.50 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281322/350343 [02:04<00:30, 2273.29 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281550/350343 [02:04<00:30, 2268.39 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281777/350343 [02:05<00:30, 2266.22 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282004/350343 [02:05<00:30, 2265.12 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282231/350343 [02:05<00:30, 2261.90 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282458/350343 [02:05<00:30, 2249.77 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282683/350343 [02:05<00:30, 2240.66 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282908/350343 [02:05<00:30, 2234.90 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283132/350343 [02:05<00:30, 2231.92 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283360/350343 [02:05<00:29, 2245.79 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283588/350343 [02:05<00:30, 2170.94 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283817/350343 [02:05<00:30, 2204.79 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284045/350343 [02:06<00:29, 2224.90 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284275/350343 [02:06<00:29, 2246.09 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284500/350343 [02:06<00:29, 2231.16 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284728/350343 [02:06<00:29, 2243.33 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284953/350343 [02:06<00:29, 2241.02 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285181/350343 [02:06<00:28, 2252.17 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285411/350343 [02:06<00:28, 2264.66 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285640/350343 [02:06<00:28, 2270.22 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285870/350343 [02:06<00:28, 2277.80 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286098/350343 [02:06<00:28, 2272.60 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286326/350343 [02:07<00:28, 2274.04 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286559/350343 [02:07<00:27, 2288.42 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286788/350343 [02:07<00:27, 2280.87 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287017/350343 [02:07<00:28, 2254.32 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287243/350343 [02:07<00:28, 2236.30 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287468/350343 [02:07<00:28, 2238.55 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287694/350343 [02:07<00:27, 2244.04 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287920/350343 [02:07<00:27, 2248.27 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288154/350343 [02:07<00:27, 2273.63 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288384/350343 [02:07<00:27, 2280.20 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288613/350343 [02:08<00:27, 2275.67 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288842/350343 [02:08<00:26, 2279.47 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289070/350343 [02:08<00:26, 2275.18 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289298/350343 [02:08<00:27, 2260.16 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289528/350343 [02:08<00:26, 2270.25 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289756/350343 [02:08<00:26, 2248.79 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289983/350343 [02:08<00:26, 2254.23 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290209/350343 [02:08<00:26, 2255.62 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290437/350343 [02:08<00:26, 2260.95 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290666/350343 [02:08<00:26, 2268.86 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290893/350343 [02:09<00:26, 2252.23 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291123/350343 [02:09<00:26, 2265.31 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291350/350343 [02:09<00:26, 2261.18 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291577/350343 [02:09<00:25, 2263.02 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291804/350343 [02:09<00:25, 2252.32 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292032/350343 [02:09<00:25, 2258.52 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292261/350343 [02:09<00:25, 2267.42 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292488/350343 [02:09<00:25, 2263.23 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292715/350343 [02:09<00:25, 2264.20 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292942/350343 [02:10<00:25, 2242.15 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293171/350343 [02:10<00:25, 2256.30 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293403/350343 [02:10<00:25, 2272.82 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293631/350343 [02:10<00:24, 2269.60 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293867/350343 [02:10<00:24, 2294.12 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294097/350343 [02:10<00:25, 2203.38 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294329/350343 [02:10<00:25, 2236.60 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294563/350343 [02:10<00:24, 2266.56 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294793/350343 [02:10<00:24, 2275.28 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295024/350343 [02:10<00:24, 2285.34 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295253/350343 [02:11<00:24, 2282.03 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295484/350343 [02:11<00:23, 2289.04 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295714/350343 [02:11<00:24, 2267.95 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295941/350343 [02:11<00:24, 2260.34 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296168/350343 [02:11<00:23, 2259.07 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296394/350343 [02:11<00:23, 2258.84 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296622/350343 [02:11<00:23, 2264.60 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296851/350343 [02:11<00:23, 2270.47 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297083/350343 [02:11<00:23, 2283.80 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297312/350343 [02:11<00:23, 2282.55 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297541/350343 [02:12<00:23, 2284.39 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297770/350343 [02:12<00:23, 2261.39 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 297997/350343 [02:12<00:23, 2252.29 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298223/350343 [02:12<00:23, 2245.18 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298448/350343 [02:12<00:23, 2244.52 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298673/350343 [02:12<00:23, 2246.00 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298898/350343 [02:12<00:22, 2242.89 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299123/350343 [02:12<00:22, 2234.40 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299351/350343 [02:12<00:22, 2247.35 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299577/350343 [02:12<00:22, 2250.81 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299809/350343 [02:13<00:22, 2269.45 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300036/350343 [02:13<00:22, 2263.60 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300264/350343 [02:13<00:22, 2268.08 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300495/350343 [02:13<00:21, 2279.34 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300725/350343 [02:13<00:21, 2284.43 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300954/350343 [02:13<00:21, 2279.88 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301182/350343 [02:13<00:21, 2271.75 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301414/350343 [02:13<00:21, 2285.21 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301644/350343 [02:13<00:21, 2287.09 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301873/350343 [02:13<00:21, 2285.86 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 302106/350343 [02:14<00:21, 2296.11 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302337/350343 [02:14<00:20, 2298.09 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302567/350343 [02:14<00:20, 2297.38 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302800/350343 [02:14<00:20, 2305.82 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303031/350343 [02:14<00:20, 2300.84 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303262/350343 [02:14<00:20, 2297.47 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303492/350343 [02:14<00:21, 2214.58 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303716/350343 [02:14<00:20, 2220.86 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303943/350343 [02:14<00:20, 2233.83 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304170/350343 [02:14<00:20, 2243.71 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304401/350343 [02:15<00:20, 2260.69 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304628/350343 [02:15<00:20, 2256.78 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304857/350343 [02:15<00:20, 2265.40 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305086/350343 [02:15<00:19, 2272.35 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305314/350343 [02:15<00:19, 2268.51 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305546/350343 [02:15<00:19, 2282.91 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305775/350343 [02:15<00:19, 2274.16 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306003/350343 [02:15<00:19, 2271.19 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306232/350343 [02:15<00:19, 2276.20 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306460/350343 [02:15<00:19, 2277.17 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306688/350343 [02:16<00:19, 2261.10 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306915/350343 [02:16<00:19, 2257.65 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307141/350343 [02:16<00:19, 2249.25 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307366/350343 [02:16<00:19, 2247.90 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307595/350343 [02:16<00:18, 2258.77 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307824/350343 [02:16<00:18, 2266.84 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308054/350343 [02:16<00:18, 2275.00 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308282/350343 [02:16<00:18, 2266.80 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308509/350343 [02:16<00:18, 2266.33 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308739/350343 [02:16<00:18, 2274.74 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308967/350343 [02:17<00:18, 2269.91 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309196/350343 [02:17<00:18, 2273.77 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309424/350343 [02:17<00:18, 2261.18 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309651/350343 [02:17<00:18, 2258.90 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309877/350343 [02:17<00:17, 2252.44 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310106/350343 [02:17<00:17, 2262.69 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310333/350343 [02:17<00:17, 2249.06 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310564/350343 [02:17<00:17, 2264.68 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310792/350343 [02:17<00:17, 2268.83 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311020/350343 [02:17<00:17, 2269.74 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311248/350343 [02:18<00:17, 2271.44 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311478/350343 [02:18<00:17, 2279.79 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311712/350343 [02:18<00:16, 2296.07 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311943/350343 [02:18<00:16, 2298.43 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312173/350343 [02:18<00:16, 2284.22 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312404/350343 [02:18<00:16, 2291.20 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312634/350343 [02:18<00:16, 2276.01 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312862/350343 [02:18<00:16, 2269.60 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313089/350343 [02:18<00:16, 2263.12 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313316/350343 [02:18<00:16, 2253.15 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313543/350343 [02:19<00:16, 2256.71 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313770/350343 [02:19<00:16, 2174.89 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313995/350343 [02:19<00:16, 2193.79 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314221/350343 [02:19<00:16, 2211.39 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314448/350343 [02:19<00:16, 2228.29 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314674/350343 [02:19<00:15, 2236.08 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314898/350343 [02:19<00:15, 2218.87 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 315121/350343 [02:19<00:15, 2209.91 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315349/350343 [02:19<00:15, 2229.39 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315573/350343 [02:20<00:15, 2220.18 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315802/350343 [02:20<00:15, 2240.29 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316032/350343 [02:20<00:15, 2256.77 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316258/350343 [02:20<00:15, 2248.51 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316486/350343 [02:20<00:15, 2255.82 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316713/350343 [02:20<00:14, 2257.39 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316941/350343 [02:20<00:14, 2263.61 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317168/350343 [02:20<00:14, 2255.93 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317396/350343 [02:20<00:14, 2262.62 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317625/350343 [02:20<00:14, 2270.60 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317853/350343 [02:21<00:14, 2261.31 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318081/350343 [02:21<00:14, 2266.35 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318308/350343 [02:21<00:14, 2251.63 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318534/350343 [02:21<00:14, 2251.69 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318766/350343 [02:21<00:13, 2271.34 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318994/350343 [02:21<00:13, 2265.92 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319221/350343 [02:21<00:14, 2197.31 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319442/350343 [02:21<00:14, 2176.84 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319667/350343 [02:21<00:13, 2196.01 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319897/350343 [02:21<00:13, 2226.52 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320126/350343 [02:22<00:13, 2245.24 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320357/350343 [02:22<00:13, 2262.55 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320584/350343 [02:22<00:13, 2258.54 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320810/350343 [02:22<00:13, 2258.83 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321036/350343 [02:22<00:13, 2254.09 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321263/350343 [02:22<00:12, 2257.73 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321490/350343 [02:22<00:12, 2261.27 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321722/350343 [02:22<00:12, 2277.55 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321950/350343 [02:22<00:12, 2271.43 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322179/350343 [02:22<00:12, 2274.11 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322407/350343 [02:23<00:12, 2272.30 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322637/350343 [02:23<00:12, 2280.33 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322866/350343 [02:23<00:12, 2267.38 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323093/350343 [02:23<00:12, 2252.06 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323319/350343 [02:23<00:12, 2251.32 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323545/350343 [02:23<00:11, 2246.18 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323770/350343 [02:23<00:11, 2240.35 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323995/350343 [02:23<00:11, 2234.84 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324219/350343 [02:23<00:11, 2235.69 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324447/350343 [02:23<00:11, 2248.62 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324678/350343 [02:24<00:11, 2265.27 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324908/350343 [02:24<00:11, 2273.99 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325136/350343 [02:24<00:11, 2270.26 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325366/350343 [02:24<00:10, 2276.94 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325595/350343 [02:24<00:10, 2278.88 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325823/350343 [02:24<00:10, 2277.34 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326055/350343 [02:24<00:10, 2289.09 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326284/350343 [02:24<00:10, 2275.79 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326512/350343 [02:24<00:10, 2275.36 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326740/350343 [02:24<00:10, 2276.34 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326968/350343 [02:25<00:10, 2179.95 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327197/350343 [02:25<00:10, 2210.09 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327428/350343 [02:25<00:10, 2238.14 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327657/350343 [02:25<00:10, 2253.34 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327888/350343 [02:25<00:09, 2268.26 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328121/350343 [02:25<00:09, 2285.49 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328352/350343 [02:25<00:09, 2290.28 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328582/350343 [02:25<00:09, 2284.21 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328811/350343 [02:25<00:09, 2266.93 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329038/350343 [02:25<00:09, 2252.79 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329265/350343 [02:26<00:09, 2256.20 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329491/350343 [02:26<00:09, 2255.90 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329721/350343 [02:26<00:09, 2268.40 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329948/350343 [02:26<00:08, 2268.42 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330176/350343 [02:26<00:08, 2270.13 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330404/350343 [02:26<00:08, 2268.26 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330635/350343 [02:26<00:08, 2278.02 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330864/350343 [02:26<00:08, 2279.44 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331093/350343 [02:26<00:08, 2282.17 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331322/350343 [02:26<00:08, 2276.60 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331555/350343 [02:27<00:08, 2292.03 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331785/350343 [02:27<00:08, 2286.24 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332014/350343 [02:27<00:08, 2286.21 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332243/350343 [02:27<00:07, 2264.58 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332472/350343 [02:27<00:07, 2272.08 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332701/350343 [02:27<00:07, 2275.93 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 332929/350343 [02:27<00:07, 2275.84 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333157/350343 [02:27<00:07, 2267.88 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333384/350343 [02:27<00:07, 2266.18 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333611/350343 [02:27<00:07, 2262.47 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333838/350343 [02:28<00:07, 2204.83 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334059/350343 [02:28<00:07, 2164.51 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334283/350343 [02:28<00:07, 2184.53 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334509/350343 [02:28<00:07, 2204.52 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334730/350343 [02:28<00:07, 2202.50 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334952/350343 [02:28<00:06, 2207.48 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335184/350343 [02:28<00:06, 2239.13 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335411/350343 [02:28<00:06, 2247.84 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335639/350343 [02:28<00:06, 2256.37 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335869/350343 [02:29<00:06, 2268.27 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336100/350343 [02:29<00:06, 2279.92 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336329/350343 [02:29<00:06, 2273.56 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336557/350343 [02:29<00:06, 2272.93 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336788/350343 [02:29<00:05, 2283.23 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 337017/350343 [02:29<00:05, 2268.88 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337249/350343 [02:29<00:05, 2283.62 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337478/350343 [02:29<00:05, 2195.58 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337702/350343 [02:29<00:05, 2208.16 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337933/350343 [02:29<00:05, 2235.43 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338160/350343 [02:30<00:05, 2243.90 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338393/350343 [02:30<00:05, 2266.09 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338620/350343 [02:30<00:05, 2265.49 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338849/350343 [02:30<00:05, 2271.92 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339078/350343 [02:30<00:04, 2276.28 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339307/350343 [02:30<00:04, 2280.14 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339541/350343 [02:30<00:04, 2295.70 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339771/350343 [02:30<00:04, 2293.60 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340001/350343 [02:30<00:04, 2282.22 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340230/350343 [02:30<00:04, 2272.72 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340458/350343 [02:31<00:04, 2272.62 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340686/350343 [02:31<00:04, 2269.94 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340914/350343 [02:31<00:04, 1943.72 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341117/350343 [02:31<00:04, 1848.58 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341308/350343 [02:31<00:05, 1804.81 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341493/350343 [02:31<00:04, 1810.64 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341680/350343 [02:31<00:04, 1826.21 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341886/350343 [02:31<00:04, 1890.72 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342107/350343 [02:31<00:04, 1982.02 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342321/350343 [02:32<00:03, 2027.66 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342546/350343 [02:32<00:03, 2091.06 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342772/350343 [02:32<00:03, 2139.39 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342995/350343 [02:32<00:03, 2165.96 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343223/350343 [02:32<00:03, 2198.88 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343449/350343 [02:32<00:03, 2216.01 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343680/350343 [02:32<00:02, 2241.35 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343909/350343 [02:32<00:02, 2253.94 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344135/350343 [02:32<00:02, 2246.17 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344363/350343 [02:32<00:02, 2253.74 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344589/350343 [02:33<00:02, 2238.77 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344813/350343 [02:33<00:02, 2237.60 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345038/350343 [02:33<00:02, 2239.72 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345264/350343 [02:33<00:02, 2245.35 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345490/350343 [02:33<00:02, 2248.61 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345715/350343 [02:33<00:02, 2243.03 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345943/350343 [02:33<00:01, 2251.37 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346170/350343 [02:33<00:01, 2254.54 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346397/350343 [02:33<00:01, 2258.09 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346623/350343 [02:33<00:01, 2253.32 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346851/350343 [02:34<00:01, 2260.92 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347082/350343 [02:34<00:01, 2274.79 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347310/350343 [02:34<00:01, 2265.19 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347539/350343 [02:34<00:01, 2272.43 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347767/350343 [02:34<00:01, 2180.58 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347994/350343 [02:34<00:01, 2206.01 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348223/350343 [02:34<00:00, 2228.16 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348451/350343 [02:34<00:00, 2241.66 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348682/350343 [02:34<00:00, 2261.76 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348909/350343 [02:34<00:00, 2257.55 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349135/350343 [02:35<00:00, 2256.38 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349366/350343 [02:35<00:00, 2271.12 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349594/350343 [02:35<00:00, 2257.74 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349823/350343 [02:35<00:00, 2267.11 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350053/350343 [02:35<00:00, 2275.19 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350287/350343 [02:35<00:00, 2293.67 examples/s][A
                                                                                              [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:   0%|          | 1/350343 [00:00<41:50:27,  2.33 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:   4%|â–         | 13890/350343 [00:00<00:09, 34640.12 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:   8%|â–Š         | 28476/350343 [00:00<00:05, 63854.01 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  12%|â–ˆâ–        | 43215/350343 [00:00<00:03, 86644.35 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  17%|â–ˆâ–‹        | 58052/350343 [00:00<00:02, 103953.45 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  21%|â–ˆâ–ˆ        | 73039/350343 [00:00<00:02, 117084.70 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  25%|â–ˆâ–ˆâ–Œ       | 87993/350343 [00:01<00:02, 126497.59 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  29%|â–ˆâ–ˆâ–‰       | 103077/350343 [00:01<00:01, 133624.58 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 118019/350343 [00:01<00:01, 138283.02 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133112/350343 [00:01<00:01, 142028.36 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148216/350343 [00:01<00:01, 144704.03 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163274/350343 [00:01<00:01, 146454.29 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178371/350343 [00:01<00:01, 147799.34 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193466/350343 [00:01<00:01, 148736.15 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208628/350343 [00:01<00:00, 149595.96 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223721/350343 [00:01<00:00, 149991.78 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238850/350343 [00:02<00:00, 150374.12 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254036/350343 [00:02<00:00, 150816.14 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269153/350343 [00:02<00:00, 150891.00 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284345/350343 [00:02<00:00, 151194.50 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299611/350343 [00:02<00:00, 151630.99 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314824/350343 [00:02<00:00, 151778.31 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330011/350343 [00:02<00:00, 146986.22 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344749/350343 [00:02<00:00, 144030.02 examples/s][A
                                                                                                                                                            [AI0305 10:18:08.969228 140444430841664 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-train.tfrecord*. Number of examples: 350343 (shards: [43793, 43793, 43793, 43793, 43792, 43793, 43793, 43793])
Generating splits...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:38<05:16, 158.48s/ splits]
Generating validation examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating validation examples...:   0%|          | 108/43793 [00:00<00:48, 895.48 examples/s][A
Generating validation examples...:   1%|          | 330/43793 [00:00<00:27, 1580.18 examples/s][A
Generating validation examples...:   1%|â–         | 552/43793 [00:00<00:23, 1846.19 examples/s][A
Generating validation examples...:   2%|â–         | 774/43793 [00:00<00:21, 1980.05 examples/s][A
Generating validation examples...:   2%|â–         | 996/43793 [00:00<00:20, 2054.27 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1218/43793 [00:00<00:20, 2104.01 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1440/43793 [00:00<00:19, 2136.47 examples/s][A
Generating validation examples...:   4%|â–         | 1662/43793 [00:00<00:19, 2144.90 examples/s][A
Generating validation examples...:   4%|â–         | 1885/43793 [00:00<00:19, 2166.73 examples/s][A
Generating validation examples...:   5%|â–         | 2108/43793 [00:01<00:19, 2184.08 examples/s][A
Generating validation examples...:   5%|â–Œ         | 2329/43793 [00:01<00:18, 2184.11 examples/s][A
Generating validation examples...:   6%|â–Œ         | 2552/43793 [00:01<00:18, 2196.68 examples/s][A
Generating validation examples...:   6%|â–‹         | 2773/43793 [00:01<00:18, 2195.38 examples/s][A
Generating validation examples...:   7%|â–‹         | 2997/43793 [00:01<00:18, 2198.47 examples/s][A
Generating validation examples...:   7%|â–‹         | 3220/43793 [00:01<00:18, 2205.94 examples/s][A
Generating validation examples...:   8%|â–Š         | 3441/43793 [00:01<00:18, 2191.64 examples/s][A
Generating validation examples...:   8%|â–Š         | 3663/43793 [00:01<00:18, 2191.02 examples/s][A
Generating validation examples...:   9%|â–‰         | 3886/43793 [00:01<00:18, 2194.60 examples/s][A
Generating validation examples...:   9%|â–‰         | 4108/43793 [00:01<00:18, 2193.66 examples/s][A
Generating validation examples...:  10%|â–‰         | 4330/43793 [00:02<00:18, 2184.55 examples/s][A
Generating validation examples...:  10%|â–ˆ         | 4552/43793 [00:02<00:17, 2189.20 examples/s][A
Generating validation examples...:  11%|â–ˆ         | 4774/43793 [00:02<00:17, 2192.44 examples/s][A
Generating validation examples...:  11%|â–ˆâ–        | 4997/43793 [00:02<00:17, 2196.29 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5222/43793 [00:02<00:17, 2211.01 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5444/43793 [00:02<00:17, 2207.72 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5665/43793 [00:02<00:17, 2207.68 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5888/43793 [00:02<00:17, 2213.70 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6110/43793 [00:02<00:17, 2205.50 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6331/43793 [00:02<00:17, 2199.70 examples/s][A
Generating validation examples...:  15%|â–ˆâ–        | 6556/43793 [00:03<00:16, 2213.94 examples/s][A
Generating validation examples...:  15%|â–ˆâ–Œ        | 6781/43793 [00:03<00:16, 2223.01 examples/s][A
Generating validation examples...:  16%|â–ˆâ–Œ        | 7004/43793 [00:03<00:16, 2222.10 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7227/43793 [00:03<00:16, 2218.97 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7453/43793 [00:03<00:16, 2228.41 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7676/43793 [00:03<00:16, 2206.68 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7897/43793 [00:03<00:16, 2202.20 examples/s][A
Generating validation examples...:  19%|â–ˆâ–Š        | 8118/43793 [00:03<00:16, 2198.52 examples/s][A
Generating validation examples...:  19%|â–ˆâ–‰        | 8338/43793 [00:03<00:16, 2197.74 examples/s][A
Generating validation examples...:  20%|â–ˆâ–‰        | 8562/43793 [00:03<00:15, 2209.75 examples/s][A
Generating validation examples...:  20%|â–ˆâ–ˆ        | 8783/43793 [00:04<00:15, 2206.26 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9004/43793 [00:04<00:15, 2197.93 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9224/43793 [00:04<00:15, 2194.81 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9446/43793 [00:04<00:15, 2194.94 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9669/43793 [00:04<00:15, 2200.32 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 9894/43793 [00:04<00:15, 2214.66 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 10116/43793 [00:04<00:15, 2214.14 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–Ž       | 10338/43793 [00:04<00:15, 2213.22 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–       | 10561/43793 [00:04<00:15, 2214.29 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–       | 10783/43793 [00:04<00:14, 2206.86 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–Œ       | 11005/43793 [00:05<00:14, 2204.14 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11227/43793 [00:05<00:14, 2195.08 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11450/43793 [00:05<00:14, 2192.93 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11672/43793 [00:05<00:14, 2194.08 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11894/43793 [00:05<00:14, 2189.26 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12117/43793 [00:05<00:14, 2190.96 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12341/43793 [00:05<00:14, 2204.29 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–Š       | 12562/43793 [00:05<00:14, 2191.31 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–‰       | 12783/43793 [00:05<00:14, 2192.04 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–‰       | 13007/43793 [00:05<00:14, 2197.91 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13230/43793 [00:06<00:13, 2206.26 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13453/43793 [00:06<00:13, 2213.03 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13675/43793 [00:06<00:13, 2201.61 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13898/43793 [00:06<00:13, 2209.22 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14119/43793 [00:06<00:13, 2209.18 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14342/43793 [00:06<00:13, 2214.24 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14564/43793 [00:06<00:13, 2203.68 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14786/43793 [00:06<00:13, 2207.04 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 15009/43793 [00:06<00:13, 2209.91 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15232/43793 [00:06<00:12, 2212.10 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15454/43793 [00:07<00:12, 2201.23 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15678/43793 [00:07<00:12, 2210.78 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 15900/43793 [00:07<00:13, 2132.27 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16117/43793 [00:07<00:12, 2141.86 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16332/43793 [00:07<00:13, 2100.93 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16543/43793 [00:07<00:13, 2095.35 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16759/43793 [00:07<00:12, 2112.26 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 16975/43793 [00:07<00:12, 2126.23 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17195/43793 [00:07<00:12, 2147.35 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17415/43793 [00:08<00:12, 2162.20 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17633/43793 [00:08<00:12, 2165.20 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17850/43793 [00:08<00:11, 2166.04 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18069/43793 [00:08<00:11, 2172.64 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18289/43793 [00:08<00:11, 2176.99 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18508/43793 [00:08<00:11, 2177.54 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18728/43793 [00:08<00:11, 2182.54 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18947/43793 [00:08<00:11, 2178.52 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19168/43793 [00:08<00:11, 2185.72 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19387/43793 [00:08<00:11, 2172.82 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19605/43793 [00:09<00:11, 2174.13 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19823/43793 [00:09<00:11, 2174.24 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20041/43793 [00:09<00:10, 2175.55 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20260/43793 [00:09<00:10, 2177.09 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20478/43793 [00:09<00:10, 2166.06 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20702/43793 [00:09<00:10, 2187.75 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20925/43793 [00:09<00:10, 2198.28 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21145/43793 [00:09<00:10, 2187.45 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21364/43793 [00:09<00:10, 2178.76 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21588/43793 [00:09<00:10, 2196.74 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21808/43793 [00:10<00:10, 2194.12 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22033/43793 [00:10<00:09, 2208.23 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22256/43793 [00:10<00:09, 2214.36 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22478/43793 [00:10<00:09, 2209.35 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22699/43793 [00:10<00:09, 2203.49 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22920/43793 [00:10<00:09, 2196.19 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23143/43793 [00:10<00:09, 2206.01 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23364/43793 [00:10<00:09, 2203.66 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23585/43793 [00:10<00:09, 2181.17 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23809/43793 [00:10<00:09, 2196.62 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24035/43793 [00:11<00:08, 2214.80 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24257/43793 [00:11<00:08, 2216.31 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24479/43793 [00:11<00:08, 2215.38 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24701/43793 [00:11<00:08, 2215.40 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24923/43793 [00:11<00:08, 2197.17 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25150/43793 [00:11<00:08, 2217.36 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25372/43793 [00:11<00:08, 2210.07 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25594/43793 [00:11<00:08, 2199.96 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25815/43793 [00:11<00:08, 2188.04 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26034/43793 [00:11<00:08, 2173.01 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26252/43793 [00:12<00:08, 2169.67 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26471/43793 [00:12<00:07, 2174.49 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26689/43793 [00:12<00:07, 2172.61 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26910/43793 [00:12<00:07, 2183.48 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27134/43793 [00:12<00:07, 2198.46 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27360/43793 [00:12<00:07, 2214.60 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27582/43793 [00:12<00:07, 2212.80 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27806/43793 [00:12<00:07, 2218.72 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28028/43793 [00:12<00:07, 2193.50 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28253/43793 [00:12<00:07, 2210.16 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28475/43793 [00:13<00:07, 2184.25 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28699/43793 [00:13<00:06, 2200.69 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28920/43793 [00:13<00:06, 2191.13 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29145/43793 [00:13<00:06, 2208.32 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29366/43793 [00:13<00:06, 2208.09 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29587/43793 [00:13<00:06, 2206.37 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29810/43793 [00:13<00:06, 2213.30 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 30032/43793 [00:13<00:06, 2209.68 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30253/43793 [00:13<00:06, 2196.15 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30473/43793 [00:13<00:06, 2146.48 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30688/43793 [00:14<00:06, 2106.90 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30910/43793 [00:14<00:06, 2137.99 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31128/43793 [00:14<00:05, 2148.57 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31344/43793 [00:14<00:05, 2148.29 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31565/43793 [00:14<00:05, 2165.31 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31786/43793 [00:14<00:05, 2176.30 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32008/43793 [00:14<00:05, 2187.08 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32228/43793 [00:14<00:05, 2187.28 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32453/43793 [00:14<00:05, 2204.81 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32677/43793 [00:14<00:05, 2213.76 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32899/43793 [00:15<00:04, 2203.76 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33121/43793 [00:15<00:04, 2205.48 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33343/43793 [00:15<00:04, 2207.70 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33565/43793 [00:15<00:04, 2209.34 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33786/43793 [00:15<00:04, 2172.54 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34004/43793 [00:15<00:04, 2174.65 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34227/43793 [00:15<00:04, 2190.26 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34447/43793 [00:15<00:04, 2187.18 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34672/43793 [00:15<00:04, 2204.12 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34893/43793 [00:15<00:04, 2200.48 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35114/43793 [00:16<00:03, 2181.88 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35333/43793 [00:16<00:03, 2136.52 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35551/43793 [00:16<00:03, 2146.78 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35771/43793 [00:16<00:03, 2162.04 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35996/43793 [00:16<00:03, 2186.97 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36218/43793 [00:16<00:03, 2195.10 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36440/43793 [00:16<00:03, 2202.31 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36661/43793 [00:16<00:03, 2181.70 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36881/43793 [00:16<00:03, 2184.49 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37100/43793 [00:17<00:03, 2160.99 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37317/43793 [00:17<00:03, 2152.10 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37538/43793 [00:17<00:02, 2167.41 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37755/43793 [00:17<00:02, 2167.84 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37976/43793 [00:17<00:02, 2178.86 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38198/43793 [00:17<00:02, 2190.55 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38418/43793 [00:17<00:02, 2167.54 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38636/43793 [00:17<00:02, 2170.82 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38854/43793 [00:17<00:02, 2170.79 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39072/43793 [00:17<00:02, 2171.58 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39290/43793 [00:18<00:02, 2172.55 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39512/43793 [00:18<00:01, 2184.59 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39737/43793 [00:18<00:01, 2203.99 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39958/43793 [00:18<00:01, 2203.10 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40183/43793 [00:18<00:01, 2215.03 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40405/43793 [00:18<00:01, 2215.69 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40627/43793 [00:18<00:01, 2216.57 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40849/43793 [00:18<00:01, 2199.03 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41073/43793 [00:18<00:01, 2208.79 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41294/43793 [00:18<00:01, 2201.45 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41515/43793 [00:19<00:01, 2196.07 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41735/43793 [00:19<00:00, 2194.89 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41958/43793 [00:19<00:00, 2202.73 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42180/43793 [00:19<00:00, 2206.49 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42401/43793 [00:19<00:00, 2195.50 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42624/43793 [00:19<00:00, 2203.75 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42845/43793 [00:19<00:00, 2194.19 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43065/43793 [00:19<00:00, 2193.07 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43286/43793 [00:19<00:00, 2196.81 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43506/43793 [00:19<00:00, 2193.29 examples/s][A
Generating validation examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43726/43793 [00:20<00:00, 2192.73 examples/s][A
                                                                                                 [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-validation.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-validation.tfrecord*...:  19%|â–ˆâ–‰        | 8230/43793 [00:00<00:00, 82296.55 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-validation.tfrecord*...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20575/43793 [00:00<00:00, 106500.07 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-validation.tfrecord*...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33184/43793 [00:00<00:00, 115439.75 examples/s][A
                                                                                                                                                               [AI0305 10:18:29.501744 140444430841664 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-validation.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:58<01:17, 77.29s/ splits] 
Generating test examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating test examples...:   0%|          | 161/43793 [00:00<00:27, 1602.56 examples/s][A
Generating test examples...:   1%|          | 370/43793 [00:00<00:23, 1886.80 examples/s][A
Generating test examples...:   1%|â–         | 587/43793 [00:00<00:21, 2014.33 examples/s][A
Generating test examples...:   2%|â–         | 799/43793 [00:00<00:20, 2053.37 examples/s][A
Generating test examples...:   2%|â–         | 1020/43793 [00:00<00:20, 2106.99 examples/s][A
Generating test examples...:   3%|â–Ž         | 1240/43793 [00:00<00:19, 2137.80 examples/s][A
Generating test examples...:   3%|â–Ž         | 1460/43793 [00:00<00:19, 2157.55 examples/s][A
Generating test examples...:   4%|â–         | 1683/43793 [00:00<00:19, 2179.92 examples/s][A
Generating test examples...:   4%|â–         | 1907/43793 [00:00<00:19, 2196.92 examples/s][A
Generating test examples...:   5%|â–         | 2130/43793 [00:01<00:18, 2206.97 examples/s][A
Generating test examples...:   5%|â–Œ         | 2356/43793 [00:01<00:18, 2221.65 examples/s][A
Generating test examples...:   6%|â–Œ         | 2579/43793 [00:01<00:19, 2166.01 examples/s][A
Generating test examples...:   6%|â–‹         | 2800/43793 [00:01<00:18, 2179.01 examples/s][A
Generating test examples...:   7%|â–‹         | 3021/43793 [00:01<00:18, 2186.26 examples/s][A
Generating test examples...:   7%|â–‹         | 3244/43793 [00:01<00:18, 2197.09 examples/s][A
Generating test examples...:   8%|â–Š         | 3468/43793 [00:01<00:18, 2209.17 examples/s][A
Generating test examples...:   8%|â–Š         | 3690/43793 [00:01<00:18, 2210.31 examples/s][A
Generating test examples...:   9%|â–‰         | 3912/43793 [00:01<00:18, 2203.34 examples/s][A
Generating test examples...:   9%|â–‰         | 4133/43793 [00:01<00:18, 2196.77 examples/s][A
Generating test examples...:  10%|â–‰         | 4354/43793 [00:02<00:17, 2200.01 examples/s][A
Generating test examples...:  10%|â–ˆ         | 4575/43793 [00:02<00:17, 2201.59 examples/s][A
Generating test examples...:  11%|â–ˆ         | 4796/43793 [00:02<00:17, 2197.14 examples/s][A
Generating test examples...:  11%|â–ˆâ–        | 5018/43793 [00:02<00:17, 2202.45 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5242/43793 [00:02<00:17, 2211.89 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5464/43793 [00:02<00:17, 2191.90 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5685/43793 [00:02<00:17, 2196.22 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5907/43793 [00:02<00:17, 2203.20 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6128/43793 [00:02<00:17, 2201.69 examples/s][A
Generating test examples...:  15%|â–ˆâ–        | 6355/43793 [00:02<00:16, 2219.88 examples/s][A
Generating test examples...:  15%|â–ˆâ–Œ        | 6578/43793 [00:03<00:16, 2212.81 examples/s][A
Generating test examples...:  16%|â–ˆâ–Œ        | 6800/43793 [00:03<00:16, 2198.47 examples/s][A
Generating test examples...:  16%|â–ˆâ–Œ        | 7020/43793 [00:03<00:16, 2178.84 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7238/43793 [00:03<00:16, 2167.92 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7461/43793 [00:03<00:16, 2183.93 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7680/43793 [00:03<00:16, 2178.36 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7903/43793 [00:03<00:16, 2193.67 examples/s][A
Generating test examples...:  19%|â–ˆâ–Š        | 8123/43793 [00:03<00:16, 2184.92 examples/s][A
Generating test examples...:  19%|â–ˆâ–‰        | 8342/43793 [00:03<00:16, 2169.67 examples/s][A
Generating test examples...:  20%|â–ˆâ–‰        | 8562/43793 [00:03<00:16, 2175.19 examples/s][A
Generating test examples...:  20%|â–ˆâ–ˆ        | 8780/43793 [00:04<00:16, 2173.49 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 8999/43793 [00:04<00:15, 2177.33 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 9219/43793 [00:04<00:15, 2182.90 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9439/43793 [00:04<00:15, 2187.88 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9665/43793 [00:04<00:15, 2207.44 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 9886/43793 [00:04<00:15, 2206.05 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 10110/43793 [00:04<00:15, 2215.77 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–Ž       | 10333/43793 [00:04<00:15, 2218.86 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–       | 10555/43793 [00:04<00:14, 2218.78 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–       | 10777/43793 [00:04<00:14, 2216.88 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–Œ       | 10999/43793 [00:05<00:14, 2208.82 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11220/43793 [00:05<00:14, 2189.76 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11440/43793 [00:05<00:14, 2186.46 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11659/43793 [00:05<00:14, 2182.40 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11880/43793 [00:05<00:14, 2189.94 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12100/43793 [00:05<00:14, 2186.33 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12320/43793 [00:05<00:14, 2190.24 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–Š       | 12540/43793 [00:05<00:14, 2186.19 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–‰       | 12759/43793 [00:05<00:14, 2184.02 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–‰       | 12978/43793 [00:05<00:14, 2175.70 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13196/43793 [00:06<00:14, 2172.72 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13419/43793 [00:06<00:13, 2189.65 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13638/43793 [00:06<00:13, 2180.99 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13859/43793 [00:06<00:13, 2187.13 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14080/43793 [00:06<00:13, 2179.25 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14303/43793 [00:06<00:13, 2183.73 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14525/43793 [00:06<00:13, 2189.91 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 14747/43793 [00:06<00:13, 2179.85 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14968/43793 [00:06<00:13, 2173.67 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15190/43793 [00:06<00:13, 2153.11 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15410/43793 [00:07<00:13, 2157.82 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15632/43793 [00:07<00:13, 2102.04 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15850/43793 [00:07<00:13, 2124.39 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16071/43793 [00:07<00:12, 2148.39 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16287/43793 [00:07<00:12, 2143.33 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16505/43793 [00:07<00:12, 2152.85 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16723/43793 [00:07<00:12, 2159.02 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 16940/43793 [00:07<00:12, 2146.67 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17155/43793 [00:07<00:12, 2144.77 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17370/43793 [00:07<00:12, 2145.56 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17590/43793 [00:08<00:12, 2160.33 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17812/43793 [00:08<00:11, 2177.60 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 18033/43793 [00:08<00:11, 2185.48 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18256/43793 [00:08<00:11, 2198.57 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18476/43793 [00:08<00:11, 2185.99 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18695/43793 [00:08<00:11, 2185.44 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18914/43793 [00:08<00:11, 2180.19 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19136/43793 [00:08<00:11, 2190.92 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19356/43793 [00:08<00:11, 2190.23 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19577/43793 [00:08<00:11, 2193.88 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19797/43793 [00:09<00:11, 2174.79 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20019/43793 [00:09<00:10, 2186.73 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20239/43793 [00:09<00:10, 2190.04 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20459/43793 [00:09<00:10, 2178.27 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20677/43793 [00:09<00:10, 2175.82 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20895/43793 [00:09<00:10, 2170.25 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21113/43793 [00:09<00:10, 2172.58 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21331/43793 [00:09<00:10, 2167.84 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21555/43793 [00:09<00:10, 2188.31 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21775/43793 [00:09<00:10, 2189.48 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22001/43793 [00:10<00:09, 2209.70 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22225/43793 [00:10<00:09, 2218.68 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22450/43793 [00:10<00:09, 2226.33 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22673/43793 [00:10<00:09, 2219.52 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22897/43793 [00:10<00:09, 2224.20 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23121/43793 [00:10<00:09, 2227.85 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23344/43793 [00:10<00:09, 2220.17 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23567/43793 [00:10<00:09, 2211.05 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23791/43793 [00:10<00:09, 2218.72 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24013/43793 [00:11<00:08, 2214.87 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24235/43793 [00:11<00:08, 2208.49 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24459/43793 [00:11<00:08, 2215.91 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24681/43793 [00:11<00:08, 2214.12 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24903/43793 [00:11<00:08, 2201.77 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25124/43793 [00:11<00:08, 2196.94 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25344/43793 [00:11<00:08, 2195.99 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25564/43793 [00:11<00:08, 2182.01 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25783/43793 [00:11<00:08, 2170.48 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26001/43793 [00:11<00:08, 2171.73 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26222/43793 [00:12<00:08, 2181.20 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26441/43793 [00:12<00:08, 2165.54 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26659/43793 [00:12<00:07, 2168.30 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26879/43793 [00:12<00:07, 2177.48 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27097/43793 [00:12<00:07, 2176.73 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27323/43793 [00:12<00:07, 2201.52 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27544/43793 [00:12<00:07, 2196.12 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27764/43793 [00:12<00:07, 2186.13 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27984/43793 [00:12<00:07, 2189.52 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28203/43793 [00:12<00:07, 2189.01 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28422/43793 [00:13<00:07, 2184.31 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28643/43793 [00:13<00:06, 2189.50 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28862/43793 [00:13<00:06, 2172.08 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29080/43793 [00:13<00:06, 2164.91 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29299/43793 [00:13<00:06, 2171.15 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29521/43793 [00:13<00:06, 2182.63 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29744/43793 [00:13<00:06, 2196.17 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29964/43793 [00:13<00:06, 2194.96 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30185/43793 [00:13<00:06, 2198.64 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30407/43793 [00:13<00:06, 2203.04 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30628/43793 [00:14<00:05, 2198.58 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30852/43793 [00:14<00:05, 2210.45 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31074/43793 [00:14<00:05, 2203.29 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31295/43793 [00:14<00:05, 2192.39 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31518/43793 [00:14<00:05, 2202.37 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31739/43793 [00:14<00:05, 2192.97 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31961/43793 [00:14<00:05, 2200.51 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32185/43793 [00:14<00:05, 2211.22 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32407/43793 [00:14<00:05, 2193.56 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32627/43793 [00:14<00:05, 2168.99 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32851/43793 [00:15<00:04, 2188.46 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33070/43793 [00:15<00:04, 2173.63 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33288/43793 [00:15<00:04, 2172.20 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33506/43793 [00:15<00:04, 2157.18 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33722/43793 [00:15<00:04, 2157.84 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33941/43793 [00:15<00:04, 2166.02 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34158/43793 [00:15<00:04, 2159.52 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34377/43793 [00:15<00:04, 2167.60 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34600/43793 [00:15<00:04, 2184.27 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34819/43793 [00:15<00:04, 2178.38 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35037/43793 [00:16<00:04, 2177.63 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35259/43793 [00:16<00:03, 2188.52 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35478/43793 [00:16<00:03, 2186.89 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35699/43793 [00:16<00:03, 2192.85 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35922/43793 [00:16<00:03, 2202.71 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36146/43793 [00:16<00:03, 2212.62 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36368/43793 [00:16<00:03, 2204.98 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36589/43793 [00:16<00:03, 2193.87 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36811/43793 [00:16<00:03, 2199.74 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37031/43793 [00:16<00:03, 2184.88 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37250/43793 [00:17<00:03, 2176.50 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37468/43793 [00:17<00:02, 2165.15 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37687/43793 [00:17<00:02, 2172.35 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37910/43793 [00:17<00:02, 2188.00 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38129/43793 [00:17<00:02, 2185.29 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38350/43793 [00:17<00:02, 2191.59 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38570/43793 [00:17<00:02, 2186.77 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38790/43793 [00:17<00:02, 2190.15 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39010/43793 [00:17<00:02, 2172.56 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39231/43793 [00:17<00:02, 2182.84 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39453/43793 [00:18<00:01, 2193.71 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39677/43793 [00:18<00:01, 2206.39 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39898/43793 [00:18<00:01, 2190.52 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40118/43793 [00:18<00:01, 2187.53 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40338/43793 [00:18<00:01, 2191.20 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40558/43793 [00:18<00:01, 2191.78 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40778/43793 [00:18<00:01, 2190.59 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 41000/43793 [00:18<00:01, 2197.03 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41220/43793 [00:18<00:01, 2192.10 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41440/43793 [00:18<00:01, 2159.96 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41659/43793 [00:19<00:00, 2163.34 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41878/43793 [00:19<00:00, 2170.73 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42096/43793 [00:19<00:00, 2170.61 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42319/43793 [00:19<00:00, 2187.35 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42539/43793 [00:19<00:00, 2189.59 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42758/43793 [00:19<00:00, 2189.61 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42977/43793 [00:19<00:00, 2184.59 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43196/43793 [00:19<00:00, 2182.60 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43415/43793 [00:19<00:00, 2164.59 examples/s][A
Generating test examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43636/43793 [00:19<00:00, 2176.31 examples/s][A
                                                                                           [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-test.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-test.tfrecord*...:  23%|â–ˆâ–ˆâ–Ž       | 9993/43793 [00:00<00:00, 99906.99 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-test.tfrecord*...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25001/43793 [00:00<00:00, 129412.90 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-test.tfrecord*...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40226/43793 [00:00<00:00, 139835.39 examples/s][A
                                                                                                                                                         [AI0305 10:18:49.905211 140444430841664 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteNUBZXN/ogbg_molpcba-test.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:19<00:00, 51.32s/ splits]                                                                        I0305 10:18:49.992752 140444430841664 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /root/data/ogbg_molpcba/0.1.3
I0305 10:20:29.002958 140444430841664 submission_runner.py:411] Time since start: 554.62s, 	Step: 1, 	{'train/accuracy': 0.46428531408309937, 'train/loss': 0.7602338194847107, 'train/mean_average_precision': 0.02287309917833224, 'validation/accuracy': 0.4643697440624237, 'validation/loss': 0.7589086294174194, 'validation/mean_average_precision': 0.02746526181437259, 'validation/num_examples': 43793, 'test/accuracy': 0.4646042585372925, 'test/loss': 0.7593051791191101, 'test/mean_average_precision': 0.028571746956970227, 'test/num_examples': 43793, 'score': 19.403101921081543, 'total_duration': 554.6187665462494, 'accumulated_submission_time': 19.403101921081543, 'accumulated_eval_time': 535.2156233787537, 'accumulated_logging_time': 0}
I0305 10:20:29.019603 140275591976704 logging_writer.py:48] [1] accumulated_eval_time=535.215623, accumulated_logging_time=0, accumulated_submission_time=19.403102, global_step=1, preemption_count=0, score=19.403102, test/accuracy=0.464604, test/loss=0.759305, test/mean_average_precision=0.028572, test/num_examples=43793, total_duration=554.618767, train/accuracy=0.464285, train/loss=0.760234, train/mean_average_precision=0.022873, validation/accuracy=0.464370, validation/loss=0.758909, validation/mean_average_precision=0.027465, validation/num_examples=43793
I0305 10:21:01.420006 140276829759232 logging_writer.py:48] [100] global_step=100, grad_norm=0.3149909973144531, loss=0.2891714572906494
I0305 10:21:33.868385 140275591976704 logging_writer.py:48] [200] global_step=200, grad_norm=0.09732901304960251, loss=0.10826517641544342
I0305 10:22:06.369850 140276829759232 logging_writer.py:48] [300] global_step=300, grad_norm=0.032088834792375565, loss=0.0664319396018982
I0305 10:22:38.656165 140275591976704 logging_writer.py:48] [400] global_step=400, grad_norm=0.016926245763897896, loss=0.0582168772816658
I0305 10:23:11.159241 140276829759232 logging_writer.py:48] [500] global_step=500, grad_norm=0.01253413874655962, loss=0.057437505573034286
I0305 10:23:43.337119 140275591976704 logging_writer.py:48] [600] global_step=600, grad_norm=0.021659085527062416, loss=0.05011656507849693
I0305 10:24:15.908968 140276829759232 logging_writer.py:48] [700] global_step=700, grad_norm=0.021550549194216728, loss=0.05231104791164398
I0305 10:24:29.110222 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:26:22.611643 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:26:25.698428 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:26:28.763072 140444430841664 submission_runner.py:411] Time since start: 914.38s, 	Step: 742, 	{'train/accuracy': 0.9867706298828125, 'train/loss': 0.051478225737810135, 'train/mean_average_precision': 0.052824441202643874, 'validation/accuracy': 0.9842023849487305, 'validation/loss': 0.06077352538704872, 'validation/mean_average_precision': 0.05033699973000773, 'validation/num_examples': 43793, 'test/accuracy': 0.9831854701042175, 'test/loss': 0.06394501030445099, 'test/mean_average_precision': 0.0520615109430797, 'test/num_examples': 43793, 'score': 259.46107959747314, 'total_duration': 914.3788795471191, 'accumulated_submission_time': 259.46107959747314, 'accumulated_eval_time': 654.8684275150299, 'accumulated_logging_time': 0.027963638305664062}
I0305 10:26:28.778557 140276755326720 logging_writer.py:48] [742] accumulated_eval_time=654.868428, accumulated_logging_time=0.027964, accumulated_submission_time=259.461080, global_step=742, preemption_count=0, score=259.461080, test/accuracy=0.983185, test/loss=0.063945, test/mean_average_precision=0.052062, test/num_examples=43793, total_duration=914.378880, train/accuracy=0.986771, train/loss=0.051478, train/mean_average_precision=0.052824, validation/accuracy=0.984202, validation/loss=0.060774, validation/mean_average_precision=0.050337, validation/num_examples=43793
I0305 10:26:48.243193 140276963518208 logging_writer.py:48] [800] global_step=800, grad_norm=0.015347271226346493, loss=0.04824095591902733
I0305 10:27:21.009417 140276755326720 logging_writer.py:48] [900] global_step=900, grad_norm=0.021862659603357315, loss=0.052045609802007675
I0305 10:27:53.360209 140276963518208 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.012887547723948956, loss=0.04713726043701172
I0305 10:28:25.984011 140276755326720 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.028286481276154518, loss=0.04562433063983917
I0305 10:28:57.983439 140276963518208 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.025978585705161095, loss=0.042253367602825165
I0305 10:29:30.531219 140276755326720 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.014359783381223679, loss=0.04767495021224022
I0305 10:30:02.711616 140276963518208 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.027266260236501694, loss=0.053739141672849655
I0305 10:30:28.816408 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:32:25.296633 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:32:28.389989 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:32:31.336398 140444430841664 submission_runner.py:411] Time since start: 1276.95s, 	Step: 1482, 	{'train/accuracy': 0.9872096180915833, 'train/loss': 0.04890992492437363, 'train/mean_average_precision': 0.07529236448381428, 'validation/accuracy': 0.9844236373901367, 'validation/loss': 0.058875441551208496, 'validation/mean_average_precision': 0.06714620590008055, 'validation/num_examples': 43793, 'test/accuracy': 0.9834840893745422, 'test/loss': 0.06203994154930115, 'test/mean_average_precision': 0.06890655275109649, 'test/num_examples': 43793, 'score': 499.4656026363373, 'total_duration': 1276.9522080421448, 'accumulated_submission_time': 499.4656026363373, 'accumulated_eval_time': 777.3883748054504, 'accumulated_logging_time': 0.05576014518737793}
I0305 10:32:31.351713 140276838151936 logging_writer.py:48] [1482] accumulated_eval_time=777.388375, accumulated_logging_time=0.055760, accumulated_submission_time=499.465603, global_step=1482, preemption_count=0, score=499.465603, test/accuracy=0.983484, test/loss=0.062040, test/mean_average_precision=0.068907, test/num_examples=43793, total_duration=1276.952208, train/accuracy=0.987210, train/loss=0.048910, train/mean_average_precision=0.075292, validation/accuracy=0.984424, validation/loss=0.058875, validation/mean_average_precision=0.067146, validation/num_examples=43793
I0305 10:32:37.642445 140277079156480 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.023898255079984665, loss=0.051980528980493546
I0305 10:33:10.066977 140276838151936 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.012898704968392849, loss=0.051150642335414886
I0305 10:33:42.652495 140277079156480 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.017734138295054436, loss=0.049501027911901474
I0305 10:34:14.594092 140276838151936 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.01551070436835289, loss=0.05032359063625336
I0305 10:34:46.874802 140277079156480 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.010517042130231857, loss=0.04347442835569382
I0305 10:35:19.277781 140276838151936 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.02993394434452057, loss=0.04322407394647598
I0305 10:35:51.803235 140277079156480 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01554243452847004, loss=0.03698376566171646
I0305 10:36:24.052643 140276838151936 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.010590264573693275, loss=0.04143620654940605
I0305 10:36:31.372558 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:38:25.336523 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:38:28.339202 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:38:31.371363 140444430841664 submission_runner.py:411] Time since start: 1636.99s, 	Step: 2224, 	{'train/accuracy': 0.9875986576080322, 'train/loss': 0.044645387679338455, 'train/mean_average_precision': 0.12088607373852893, 'validation/accuracy': 0.9848376512527466, 'validation/loss': 0.053868964314460754, 'validation/mean_average_precision': 0.12295809056478879, 'validation/num_examples': 43793, 'test/accuracy': 0.9838589429855347, 'test/loss': 0.05699586495757103, 'test/mean_average_precision': 0.12305350375508577, 'test/num_examples': 43793, 'score': 739.4543447494507, 'total_duration': 1636.9871473312378, 'accumulated_submission_time': 739.4543447494507, 'accumulated_eval_time': 897.3871071338654, 'accumulated_logging_time': 0.08219432830810547}
I0305 10:38:31.389207 140276755326720 logging_writer.py:48] [2224] accumulated_eval_time=897.387107, accumulated_logging_time=0.082194, accumulated_submission_time=739.454345, global_step=2224, preemption_count=0, score=739.454345, test/accuracy=0.983859, test/loss=0.056996, test/mean_average_precision=0.123054, test/num_examples=43793, total_duration=1636.987147, train/accuracy=0.987599, train/loss=0.044645, train/mean_average_precision=0.120886, validation/accuracy=0.984838, validation/loss=0.053869, validation/mean_average_precision=0.122958, validation/num_examples=43793
I0305 10:38:56.084290 140276963518208 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.011561129242181778, loss=0.046836767345666885
I0305 10:39:27.911396 140276755326720 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.016919096931815147, loss=0.04156429320573807
I0305 10:39:59.714690 140276963518208 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.010673466138541698, loss=0.04299379140138626
I0305 10:40:32.040759 140276755326720 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.025255955755710602, loss=0.044977132230997086
I0305 10:41:04.186960 140276963518208 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.011172421276569366, loss=0.04243752732872963
I0305 10:41:36.104038 140276755326720 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.010908820666372776, loss=0.04164363071322441
I0305 10:42:08.422230 140276963518208 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01802346110343933, loss=0.04436616599559784
I0305 10:42:31.594694 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:44:30.251212 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:44:33.268448 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:44:36.267768 140444430841664 submission_runner.py:411] Time since start: 2001.88s, 	Step: 2973, 	{'train/accuracy': 0.987941324710846, 'train/loss': 0.04250074177980423, 'train/mean_average_precision': 0.1518332776698262, 'validation/accuracy': 0.9851689338684082, 'validation/loss': 0.051614854484796524, 'validation/mean_average_precision': 0.14716346938793062, 'validation/num_examples': 43793, 'test/accuracy': 0.9842245578765869, 'test/loss': 0.0544905923306942, 'test/mean_average_precision': 0.14643783532774093, 'test/num_examples': 43793, 'score': 979.6255490779877, 'total_duration': 2001.8835163116455, 'accumulated_submission_time': 979.6255490779877, 'accumulated_eval_time': 1022.0600764751434, 'accumulated_logging_time': 0.11286187171936035}
I0305 10:44:36.282849 140276838151936 logging_writer.py:48] [2973] accumulated_eval_time=1022.060076, accumulated_logging_time=0.112862, accumulated_submission_time=979.625549, global_step=2973, preemption_count=0, score=979.625549, test/accuracy=0.984225, test/loss=0.054491, test/mean_average_precision=0.146438, test/num_examples=43793, total_duration=2001.883516, train/accuracy=0.987941, train/loss=0.042501, train/mean_average_precision=0.151833, validation/accuracy=0.985169, validation/loss=0.051615, validation/mean_average_precision=0.147163, validation/num_examples=43793
I0305 10:44:45.272611 140277079156480 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.008985239081084728, loss=0.04496266692876816
I0305 10:45:17.448560 140276838151936 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.012682310305535793, loss=0.04076063632965088
I0305 10:45:49.753326 140277079156480 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.015028380788862705, loss=0.043703824281692505
I0305 10:46:22.580183 140276838151936 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.01633748970925808, loss=0.040187954902648926
I0305 10:46:54.543952 140277079156480 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.009634820744395256, loss=0.04025101661682129
I0305 10:47:27.027251 140276838151936 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.014798002317547798, loss=0.037203285843133926
I0305 10:47:59.446480 140277079156480 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.013163083232939243, loss=0.036291103810071945
I0305 10:48:32.491815 140276838151936 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.012731749564409256, loss=0.04323321953415871
I0305 10:48:36.401041 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:50:36.680093 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:50:39.784540 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:50:42.740542 140444430841664 submission_runner.py:411] Time since start: 2368.36s, 	Step: 3713, 	{'train/accuracy': 0.9883215427398682, 'train/loss': 0.040549714118242264, 'train/mean_average_precision': 0.18738881272369848, 'validation/accuracy': 0.9853605031967163, 'validation/loss': 0.049864307045936584, 'validation/mean_average_precision': 0.17231764114353332, 'validation/num_examples': 43793, 'test/accuracy': 0.9843947291374207, 'test/loss': 0.052686017006635666, 'test/mean_average_precision': 0.16917548741008423, 'test/num_examples': 43793, 'score': 1219.710813999176, 'total_duration': 2368.356336593628, 'accumulated_submission_time': 1219.710813999176, 'accumulated_eval_time': 1148.3995158672333, 'accumulated_logging_time': 0.13927745819091797}
I0305 10:50:42.758336 140276829759232 logging_writer.py:48] [3713] accumulated_eval_time=1148.399516, accumulated_logging_time=0.139277, accumulated_submission_time=1219.710814, global_step=3713, preemption_count=0, score=1219.710814, test/accuracy=0.984395, test/loss=0.052686, test/mean_average_precision=0.169175, test/num_examples=43793, total_duration=2368.356337, train/accuracy=0.988322, train/loss=0.040550, train/mean_average_precision=0.187389, validation/accuracy=0.985361, validation/loss=0.049864, validation/mean_average_precision=0.172318, validation/num_examples=43793
I0305 10:51:11.508065 140276963518208 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.013368668965995312, loss=0.0445912703871727
I0305 10:51:43.878893 140276829759232 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.014724941924214363, loss=0.04521990194916725
I0305 10:52:16.169106 140276963518208 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.010295332409441471, loss=0.03835782781243324
I0305 10:52:48.719992 140276829759232 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.011130561120808125, loss=0.04116841033101082
I0305 10:53:21.234313 140276963518208 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.011083492077887058, loss=0.039428986608982086
I0305 10:53:53.550305 140276829759232 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.012482562102377415, loss=0.039008162915706635
I0305 10:54:25.859738 140276963518208 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.01714884117245674, loss=0.03519080579280853
I0305 10:54:43.057092 140444430841664 spec.py:321] Evaluating on the training split.
I0305 10:56:46.917524 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 10:56:49.925324 140444430841664 spec.py:349] Evaluating on the test split.
I0305 10:56:52.892100 140444430841664 submission_runner.py:411] Time since start: 2738.51s, 	Step: 4454, 	{'train/accuracy': 0.9883311986923218, 'train/loss': 0.04012526944279671, 'train/mean_average_precision': 0.20392519948533677, 'validation/accuracy': 0.9854956865310669, 'validation/loss': 0.04980768635869026, 'validation/mean_average_precision': 0.18144165406849042, 'validation/num_examples': 43793, 'test/accuracy': 0.9845686554908752, 'test/loss': 0.052750419825315475, 'test/mean_average_precision': 0.1856167206647187, 'test/num_examples': 43793, 'score': 1459.977115869522, 'total_duration': 2738.5078999996185, 'accumulated_submission_time': 1459.977115869522, 'accumulated_eval_time': 1278.234468460083, 'accumulated_logging_time': 0.16867780685424805}
I0305 10:56:52.908373 140283577931520 logging_writer.py:48] [4454] accumulated_eval_time=1278.234468, accumulated_logging_time=0.168678, accumulated_submission_time=1459.977116, global_step=4454, preemption_count=0, score=1459.977116, test/accuracy=0.984569, test/loss=0.052750, test/mean_average_precision=0.185617, test/num_examples=43793, total_duration=2738.507900, train/accuracy=0.988331, train/loss=0.040125, train/mean_average_precision=0.203925, validation/accuracy=0.985496, validation/loss=0.049808, validation/mean_average_precision=0.181442, validation/num_examples=43793
I0305 10:57:08.587119 140381870089984 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.009666790254414082, loss=0.04016125202178955
I0305 10:57:40.616666 140283577931520 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.01453440636396408, loss=0.04551228880882263
I0305 10:58:13.353479 140381870089984 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.01931079663336277, loss=0.041925135999917984
I0305 10:58:46.327336 140283577931520 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.010306584648787975, loss=0.040630046278238297
I0305 10:59:19.387426 140381870089984 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.012871132232248783, loss=0.038603879511356354
I0305 10:59:52.250860 140283577931520 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.011937917210161686, loss=0.0352158322930336
I0305 11:00:25.445021 140381870089984 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012910268269479275, loss=0.0412592813372612
I0305 11:00:53.051473 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:02:59.433501 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:03:02.963119 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:03:06.262790 140444430841664 submission_runner.py:411] Time since start: 3111.88s, 	Step: 5185, 	{'train/accuracy': 0.9888564944267273, 'train/loss': 0.038242947310209274, 'train/mean_average_precision': 0.23384081677107818, 'validation/accuracy': 0.9857197999954224, 'validation/loss': 0.048068199306726456, 'validation/mean_average_precision': 0.19336923472957543, 'validation/num_examples': 43793, 'test/accuracy': 0.9848546385765076, 'test/loss': 0.0508074052631855, 'test/mean_average_precision': 0.196022459140312, 'test/num_examples': 43793, 'score': 1700.0847454071045, 'total_duration': 3111.8785684108734, 'accumulated_submission_time': 1700.0847454071045, 'accumulated_eval_time': 1411.4457199573517, 'accumulated_logging_time': 0.19603347778320312}
I0305 11:03:06.281820 140277112727296 logging_writer.py:48] [5185] accumulated_eval_time=1411.445720, accumulated_logging_time=0.196033, accumulated_submission_time=1700.084745, global_step=5185, preemption_count=0, score=1700.084745, test/accuracy=0.984855, test/loss=0.050807, test/mean_average_precision=0.196022, test/num_examples=43793, total_duration=3111.878568, train/accuracy=0.988856, train/loss=0.038243, train/mean_average_precision=0.233841, validation/accuracy=0.985720, validation/loss=0.048068, validation/mean_average_precision=0.193369, validation/num_examples=43793
I0305 11:03:11.637113 140381878482688 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.01106291078031063, loss=0.03689353168010712
I0305 11:03:44.355561 140277112727296 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.012535137124359608, loss=0.038584038615226746
I0305 11:04:17.140156 140381878482688 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.015708204358816147, loss=0.03772426396608353
I0305 11:04:49.540325 140277112727296 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.021110745146870613, loss=0.038661591708660126
I0305 11:05:22.353197 140381878482688 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.016147272661328316, loss=0.036650050431489944
I0305 11:05:54.662527 140277112727296 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.034851640462875366, loss=0.04346716031432152
I0305 11:06:27.844539 140381878482688 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.013786999508738518, loss=0.0365411750972271
I0305 11:07:00.824930 140277112727296 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.01351145375519991, loss=0.03549077361822128
I0305 11:07:06.528328 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:09:09.966720 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:09:13.029428 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:09:16.071638 140444430841664 submission_runner.py:411] Time since start: 3481.69s, 	Step: 5918, 	{'train/accuracy': 0.989027202129364, 'train/loss': 0.03728177025914192, 'train/mean_average_precision': 0.2582431101463912, 'validation/accuracy': 0.9858922958374023, 'validation/loss': 0.04819972813129425, 'validation/mean_average_precision': 0.2058692487304153, 'validation/num_examples': 43793, 'test/accuracy': 0.9849650263786316, 'test/loss': 0.05096153914928436, 'test/mean_average_precision': 0.20837914973360552, 'test/num_examples': 43793, 'score': 1940.293526172638, 'total_duration': 3481.6874401569366, 'accumulated_submission_time': 1940.293526172638, 'accumulated_eval_time': 1540.9889826774597, 'accumulated_logging_time': 0.22872018814086914}
I0305 11:09:16.089929 140283569538816 logging_writer.py:48] [5918] accumulated_eval_time=1540.988983, accumulated_logging_time=0.228720, accumulated_submission_time=1940.293526, global_step=5918, preemption_count=0, score=1940.293526, test/accuracy=0.984965, test/loss=0.050962, test/mean_average_precision=0.208379, test/num_examples=43793, total_duration=3481.687440, train/accuracy=0.989027, train/loss=0.037282, train/mean_average_precision=0.258243, validation/accuracy=0.985892, validation/loss=0.048200, validation/mean_average_precision=0.205869, validation/num_examples=43793
I0305 11:09:43.573340 140283577931520 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.019328661262989044, loss=0.04353848844766617
I0305 11:10:16.774652 140283569538816 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.020046386867761612, loss=0.03950579836964607
I0305 11:10:49.505942 140283577931520 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.012970161624252796, loss=0.040355999022722244
I0305 11:11:22.289967 140283569538816 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.012783091515302658, loss=0.03510362282395363
I0305 11:11:54.518445 140283577931520 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.01223635021597147, loss=0.04141382500529289
I0305 11:12:27.199731 140283569538816 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.02536657825112343, loss=0.041214264929294586
I0305 11:12:59.583713 140283577931520 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.013189486227929592, loss=0.034861307591199875
I0305 11:13:16.301463 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:15:17.501132 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:15:21.192881 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:15:24.270070 140444430841664 submission_runner.py:411] Time since start: 3849.89s, 	Step: 6652, 	{'train/accuracy': 0.9893152713775635, 'train/loss': 0.03640811890363693, 'train/mean_average_precision': 0.26265603652395375, 'validation/accuracy': 0.9861143231391907, 'validation/loss': 0.046385668218135834, 'validation/mean_average_precision': 0.2199867776480811, 'validation/num_examples': 43793, 'test/accuracy': 0.9852867722511292, 'test/loss': 0.048968322575092316, 'test/mean_average_precision': 0.21580382686634927, 'test/num_examples': 43793, 'score': 2180.472153902054, 'total_duration': 3849.8858761787415, 'accumulated_submission_time': 2180.472153902054, 'accumulated_eval_time': 1668.957560300827, 'accumulated_logging_time': 0.25841641426086426}
I0305 11:15:24.285993 140277112727296 logging_writer.py:48] [6652] accumulated_eval_time=1668.957560, accumulated_logging_time=0.258416, accumulated_submission_time=2180.472154, global_step=6652, preemption_count=0, score=2180.472154, test/accuracy=0.985287, test/loss=0.048968, test/mean_average_precision=0.215804, test/num_examples=43793, total_duration=3849.885876, train/accuracy=0.989315, train/loss=0.036408, train/mean_average_precision=0.262656, validation/accuracy=0.986114, validation/loss=0.046386, validation/mean_average_precision=0.219987, validation/num_examples=43793
I0305 11:15:40.178454 140381878482688 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.010470161214470863, loss=0.03648342564702034
I0305 11:16:12.168224 140277112727296 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.0174652598798275, loss=0.03592121601104736
I0305 11:16:44.261358 140381878482688 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.01809597760438919, loss=0.03669910132884979
I0305 11:17:16.343330 140277112727296 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.017581522464752197, loss=0.041337985545396805
I0305 11:17:48.592805 140381878482688 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.014017882756888866, loss=0.03684943541884422
I0305 11:18:21.061956 140277112727296 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.014512017369270325, loss=0.03593932464718819
I0305 11:18:53.944783 140381878482688 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.01985391415655613, loss=0.039982687681913376
I0305 11:19:24.292116 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:21:32.391895 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:21:35.471199 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:21:38.476848 140444430841664 submission_runner.py:411] Time since start: 4224.09s, 	Step: 7394, 	{'train/accuracy': 0.9894617795944214, 'train/loss': 0.035730551928281784, 'train/mean_average_precision': 0.2777985762334495, 'validation/accuracy': 0.9862133860588074, 'validation/loss': 0.04589434713125229, 'validation/mean_average_precision': 0.22058258163892933, 'validation/num_examples': 43793, 'test/accuracy': 0.9853141903877258, 'test/loss': 0.04837311431765556, 'test/mean_average_precision': 0.22411920752356312, 'test/num_examples': 43793, 'score': 2420.4435505867004, 'total_duration': 4224.092652320862, 'accumulated_submission_time': 2420.4435505867004, 'accumulated_eval_time': 1803.142256975174, 'accumulated_logging_time': 0.28579068183898926}
I0305 11:21:38.494222 140283569538816 logging_writer.py:48] [7394] accumulated_eval_time=1803.142257, accumulated_logging_time=0.285791, accumulated_submission_time=2420.443551, global_step=7394, preemption_count=0, score=2420.443551, test/accuracy=0.985314, test/loss=0.048373, test/mean_average_precision=0.224119, test/num_examples=43793, total_duration=4224.092652, train/accuracy=0.989462, train/loss=0.035731, train/mean_average_precision=0.277799, validation/accuracy=0.986213, validation/loss=0.045894, validation/mean_average_precision=0.220583, validation/num_examples=43793
I0305 11:21:40.769518 140381870089984 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.015656812116503716, loss=0.03586522489786148
I0305 11:22:13.818032 140283569538816 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.020543603226542473, loss=0.0349353589117527
I0305 11:22:46.409195 140381870089984 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.017970582470297813, loss=0.03578563407063484
I0305 11:23:18.853416 140283569538816 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01805727370083332, loss=0.037360161542892456
I0305 11:23:51.165478 140381870089984 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.018587801605463028, loss=0.03834156319499016
I0305 11:24:23.641295 140283569538816 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.01477602869272232, loss=0.034707725048065186
I0305 11:24:56.052041 140381870089984 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.014350378885865211, loss=0.03248515725135803
I0305 11:25:28.371586 140283569538816 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.01436998788267374, loss=0.033744264394044876
I0305 11:25:38.546469 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:27:42.505246 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:27:45.559655 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:27:48.500648 140444430841664 submission_runner.py:411] Time since start: 4594.12s, 	Step: 8133, 	{'train/accuracy': 0.9896026849746704, 'train/loss': 0.035033080726861954, 'train/mean_average_precision': 0.3017076079581593, 'validation/accuracy': 0.9864204525947571, 'validation/loss': 0.045406751334667206, 'validation/mean_average_precision': 0.2327738536063612, 'validation/num_examples': 43793, 'test/accuracy': 0.9854860305786133, 'test/loss': 0.047999218106269836, 'test/mean_average_precision': 0.23239103190491153, 'test/num_examples': 43793, 'score': 2660.463634490967, 'total_duration': 4594.116451025009, 'accumulated_submission_time': 2660.463634490967, 'accumulated_eval_time': 1933.0963923931122, 'accumulated_logging_time': 0.31459808349609375}
I0305 11:27:48.518143 140283577931520 logging_writer.py:48] [8133] accumulated_eval_time=1933.096392, accumulated_logging_time=0.314598, accumulated_submission_time=2660.463634, global_step=8133, preemption_count=0, score=2660.463634, test/accuracy=0.985486, test/loss=0.047999, test/mean_average_precision=0.232391, test/num_examples=43793, total_duration=4594.116451, train/accuracy=0.989603, train/loss=0.035033, train/mean_average_precision=0.301708, validation/accuracy=0.986420, validation/loss=0.045407, validation/mean_average_precision=0.232774, validation/num_examples=43793
I0305 11:28:10.459904 140381878482688 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.012943295761942863, loss=0.03727109357714653
I0305 11:28:42.490942 140283577931520 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.025249239057302475, loss=0.03499045595526695
I0305 11:29:14.864819 140381878482688 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.014626196585595608, loss=0.03495388850569725
I0305 11:29:46.978279 140283577931520 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01760009676218033, loss=0.036064788699150085
I0305 11:30:19.549290 140381878482688 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.016520150005817413, loss=0.03217121586203575
I0305 11:30:51.770109 140283577931520 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.016068758442997932, loss=0.03565108776092529
I0305 11:31:24.183257 140381878482688 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.015323920175433159, loss=0.03583856299519539
I0305 11:31:48.648385 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:33:52.048531 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:33:55.136484 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:33:58.176668 140444430841664 submission_runner.py:411] Time since start: 4963.79s, 	Step: 8876, 	{'train/accuracy': 0.9896164536476135, 'train/loss': 0.03482969105243683, 'train/mean_average_precision': 0.3129953349495222, 'validation/accuracy': 0.9864743947982788, 'validation/loss': 0.04550886154174805, 'validation/mean_average_precision': 0.23620245219076616, 'validation/num_examples': 43793, 'test/accuracy': 0.9855205416679382, 'test/loss': 0.04822637140750885, 'test/mean_average_precision': 0.2397758718441605, 'test/num_examples': 43793, 'score': 2900.5610976219177, 'total_duration': 4963.792473316193, 'accumulated_submission_time': 2900.5610976219177, 'accumulated_eval_time': 2062.624636888504, 'accumulated_logging_time': 0.3434135913848877}
I0305 11:33:58.194280 140283569538816 logging_writer.py:48] [8876] accumulated_eval_time=2062.624637, accumulated_logging_time=0.343414, accumulated_submission_time=2900.561098, global_step=8876, preemption_count=0, score=2900.561098, test/accuracy=0.985521, test/loss=0.048226, test/mean_average_precision=0.239776, test/num_examples=43793, total_duration=4963.792473, train/accuracy=0.989616, train/loss=0.034830, train/mean_average_precision=0.312995, validation/accuracy=0.986474, validation/loss=0.045509, validation/mean_average_precision=0.236202, validation/num_examples=43793
I0305 11:34:06.668379 140381870089984 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.020609956234693527, loss=0.03347249701619148
I0305 11:34:39.069949 140283569538816 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.017557233572006226, loss=0.03114367090165615
I0305 11:35:11.111600 140381870089984 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.03364705666899681, loss=0.03976920247077942
I0305 11:35:43.100727 140283569538816 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01703285239636898, loss=0.035406842827796936
I0305 11:36:15.139543 140381870089984 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.02648197114467621, loss=0.04086262732744217
I0305 11:36:47.622689 140283569538816 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.030525976791977882, loss=0.029125383123755455
I0305 11:37:20.322027 140381870089984 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.031758327037096024, loss=0.041319943964481354
I0305 11:37:52.479856 140283569538816 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.027349065989255905, loss=0.03344138711690903
I0305 11:37:58.263111 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:40:00.981214 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:40:04.219203 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:40:07.191272 140444430841664 submission_runner.py:411] Time since start: 5332.81s, 	Step: 9619, 	{'train/accuracy': 0.9899412989616394, 'train/loss': 0.033649660646915436, 'train/mean_average_precision': 0.3282852467346069, 'validation/accuracy': 0.9865624904632568, 'validation/loss': 0.04502420499920845, 'validation/mean_average_precision': 0.24260415420829937, 'validation/num_examples': 43793, 'test/accuracy': 0.9856747388839722, 'test/loss': 0.047865305095911026, 'test/mean_average_precision': 0.24188524851051374, 'test/num_examples': 43793, 'score': 3140.5976366996765, 'total_duration': 5332.80707859993, 'accumulated_submission_time': 3140.5976366996765, 'accumulated_eval_time': 2191.552747964859, 'accumulated_logging_time': 0.3722200393676758}
I0305 11:40:07.208137 140277112727296 logging_writer.py:48] [9619] accumulated_eval_time=2191.552748, accumulated_logging_time=0.372220, accumulated_submission_time=3140.597637, global_step=9619, preemption_count=0, score=3140.597637, test/accuracy=0.985675, test/loss=0.047865, test/mean_average_precision=0.241885, test/num_examples=43793, total_duration=5332.807079, train/accuracy=0.989941, train/loss=0.033650, train/mean_average_precision=0.328285, validation/accuracy=0.986562, validation/loss=0.045024, validation/mean_average_precision=0.242604, validation/num_examples=43793
I0305 11:40:33.775440 140283577931520 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.02368389628827572, loss=0.03383168950676918
I0305 11:41:06.462958 140277112727296 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.01758066564798355, loss=0.03508097305893898
I0305 11:41:38.725554 140283577931520 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.018639568239450455, loss=0.03566514328122139
I0305 11:42:11.661709 140277112727296 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.016881432384252548, loss=0.03648393973708153
I0305 11:42:44.043060 140283577931520 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.017986204475164413, loss=0.03540788218379021
I0305 11:43:16.320157 140277112727296 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01847991906106472, loss=0.032325949519872665
I0305 11:43:49.485114 140283577931520 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.02370666339993477, loss=0.03318515419960022
I0305 11:44:07.215979 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:46:10.364392 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:46:13.428480 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:46:16.486622 140444430841664 submission_runner.py:411] Time since start: 5702.10s, 	Step: 10355, 	{'train/accuracy': 0.9902228116989136, 'train/loss': 0.03266623616218567, 'train/mean_average_precision': 0.3493272232299749, 'validation/accuracy': 0.9866660237312317, 'validation/loss': 0.044837482273578644, 'validation/mean_average_precision': 0.2521623679673248, 'validation/num_examples': 43793, 'test/accuracy': 0.9856966137886047, 'test/loss': 0.04765224829316139, 'test/mean_average_precision': 0.24604642736691884, 'test/num_examples': 43793, 'score': 3380.571000099182, 'total_duration': 5702.102422714233, 'accumulated_submission_time': 3380.571000099182, 'accumulated_eval_time': 2320.8233363628387, 'accumulated_logging_time': 0.4018385410308838}
I0305 11:46:16.503702 140283569538816 logging_writer.py:48] [10355] accumulated_eval_time=2320.823336, accumulated_logging_time=0.401839, accumulated_submission_time=3380.571000, global_step=10355, preemption_count=0, score=3380.571000, test/accuracy=0.985697, test/loss=0.047652, test/mean_average_precision=0.246046, test/num_examples=43793, total_duration=5702.102423, train/accuracy=0.990223, train/loss=0.032666, train/mean_average_precision=0.349327, validation/accuracy=0.986666, validation/loss=0.044837, validation/mean_average_precision=0.252162, validation/num_examples=43793
I0305 11:46:31.567721 140381870089984 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.020892605185508728, loss=0.03686125949025154
I0305 11:47:04.092424 140283569538816 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.01866098679602146, loss=0.03433963656425476
I0305 11:47:36.522746 140381870089984 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.01798473671078682, loss=0.03490537405014038
I0305 11:48:08.652472 140283569538816 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.02127891592681408, loss=0.033824339509010315
I0305 11:48:40.602583 140381870089984 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.02352253906428814, loss=0.03384178504347801
I0305 11:49:13.061836 140283569538816 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.021958092227578163, loss=0.039004847407341
I0305 11:49:44.949951 140381870089984 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.02202184870839119, loss=0.0372747965157032
I0305 11:50:16.634739 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:52:23.903260 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:52:26.998401 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:52:29.981349 140444430841664 submission_runner.py:411] Time since start: 6075.60s, 	Step: 11097, 	{'train/accuracy': 0.9903243780136108, 'train/loss': 0.03213879093527794, 'train/mean_average_precision': 0.3718836815702409, 'validation/accuracy': 0.9866465330123901, 'validation/loss': 0.04455002024769783, 'validation/mean_average_precision': 0.2481188682180836, 'validation/num_examples': 43793, 'test/accuracy': 0.9858115911483765, 'test/loss': 0.04721446707844734, 'test/mean_average_precision': 0.25401400621206205, 'test/num_examples': 43793, 'score': 3620.668796777725, 'total_duration': 6075.597155094147, 'accumulated_submission_time': 3620.668796777725, 'accumulated_eval_time': 2454.1698989868164, 'accumulated_logging_time': 0.43018245697021484}
I0305 11:52:29.999206 140277079156480 logging_writer.py:48] [11097] accumulated_eval_time=2454.169899, accumulated_logging_time=0.430182, accumulated_submission_time=3620.668797, global_step=11097, preemption_count=0, score=3620.668797, test/accuracy=0.985812, test/loss=0.047214, test/mean_average_precision=0.254014, test/num_examples=43793, total_duration=6075.597155, train/accuracy=0.990324, train/loss=0.032139, train/mean_average_precision=0.371884, validation/accuracy=0.986647, validation/loss=0.044550, validation/mean_average_precision=0.248119, validation/num_examples=43793
I0305 11:52:31.312087 140283577931520 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.01745833083987236, loss=0.03023323230445385
I0305 11:53:04.174971 140277079156480 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.032018426805734634, loss=0.035410989075899124
I0305 11:53:35.983491 140283577931520 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.026022043079137802, loss=0.034896086901426315
I0305 11:54:07.975404 140277079156480 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02567056007683277, loss=0.03482174873352051
I0305 11:54:39.849148 140283577931520 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.021421581506729126, loss=0.033294063061475754
I0305 11:55:12.097168 140277079156480 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.028040502220392227, loss=0.03425796329975128
I0305 11:55:44.129876 140283577931520 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.021281661465764046, loss=0.03232843056321144
I0305 11:56:16.543770 140277079156480 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.026033034548163414, loss=0.03528491407632828
I0305 11:56:30.055208 140444430841664 spec.py:321] Evaluating on the training split.
I0305 11:58:34.246247 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 11:58:37.303787 140444430841664 spec.py:349] Evaluating on the test split.
I0305 11:58:40.322928 140444430841664 submission_runner.py:411] Time since start: 6445.94s, 	Step: 11843, 	{'train/accuracy': 0.9905675053596497, 'train/loss': 0.031368955969810486, 'train/mean_average_precision': 0.3800155912255726, 'validation/accuracy': 0.9866924285888672, 'validation/loss': 0.04442405700683594, 'validation/mean_average_precision': 0.2527786570240431, 'validation/num_examples': 43793, 'test/accuracy': 0.9857800006866455, 'test/loss': 0.04716132581233978, 'test/mean_average_precision': 0.24721160401434744, 'test/num_examples': 43793, 'score': 3860.6907708644867, 'total_duration': 6445.938730955124, 'accumulated_submission_time': 3860.6907708644867, 'accumulated_eval_time': 2584.4375660419464, 'accumulated_logging_time': 0.4595632553100586}
I0305 11:58:40.340925 140283569538816 logging_writer.py:48] [11843] accumulated_eval_time=2584.437566, accumulated_logging_time=0.459563, accumulated_submission_time=3860.690771, global_step=11843, preemption_count=0, score=3860.690771, test/accuracy=0.985780, test/loss=0.047161, test/mean_average_precision=0.247212, test/num_examples=43793, total_duration=6445.938731, train/accuracy=0.990568, train/loss=0.031369, train/mean_average_precision=0.380016, validation/accuracy=0.986692, validation/loss=0.044424, validation/mean_average_precision=0.252779, validation/num_examples=43793
I0305 11:58:59.101694 140381870089984 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.02463340014219284, loss=0.03366181254386902
I0305 11:59:31.241547 140283569538816 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02639187127351761, loss=0.03488482907414436
I0305 12:00:04.012813 140381870089984 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.02347586117684841, loss=0.03317641094326973
I0305 12:00:36.371538 140283569538816 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.02986922115087509, loss=0.03435872867703438
I0305 12:01:09.200256 140381870089984 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.042298417538404465, loss=0.03308798745274544
I0305 12:01:41.941195 140283569538816 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.022026659920811653, loss=0.03267897292971611
I0305 12:02:14.493735 140381870089984 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.023705629631876945, loss=0.03370681405067444
I0305 12:02:40.484096 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:04:46.168312 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:04:49.244436 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:04:52.251356 140444430841664 submission_runner.py:411] Time since start: 6817.87s, 	Step: 12581, 	{'train/accuracy': 0.9902974367141724, 'train/loss': 0.03216349706053734, 'train/mean_average_precision': 0.35626095225588705, 'validation/accuracy': 0.9867252707481384, 'validation/loss': 0.044447436928749084, 'validation/mean_average_precision': 0.26201550155997333, 'validation/num_examples': 43793, 'test/accuracy': 0.9857972860336304, 'test/loss': 0.047402143478393555, 'test/mean_average_precision': 0.25240617179308367, 'test/num_examples': 43793, 'score': 4100.80003118515, 'total_duration': 6817.867141008377, 'accumulated_submission_time': 4100.80003118515, 'accumulated_eval_time': 2716.2047567367554, 'accumulated_logging_time': 0.48949599266052246}
I0305 12:04:52.269287 140283577931520 logging_writer.py:48] [12581] accumulated_eval_time=2716.204757, accumulated_logging_time=0.489496, accumulated_submission_time=4100.800031, global_step=12581, preemption_count=0, score=4100.800031, test/accuracy=0.985797, test/loss=0.047402, test/mean_average_precision=0.252406, test/num_examples=43793, total_duration=6817.867141, train/accuracy=0.990297, train/loss=0.032163, train/mean_average_precision=0.356261, validation/accuracy=0.986725, validation/loss=0.044447, validation/mean_average_precision=0.262016, validation/num_examples=43793
I0305 12:04:58.649483 140381878482688 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.03643514961004257, loss=0.032585326582193375
I0305 12:05:31.049328 140283577931520 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.024072401225566864, loss=0.03318837657570839
I0305 12:06:03.427478 140381878482688 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.026190180331468582, loss=0.03058730438351631
I0305 12:06:35.402556 140283577931520 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.027186963707208633, loss=0.031955648213624954
I0305 12:07:07.501224 140381878482688 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.025895096361637115, loss=0.033178217709064484
I0305 12:07:39.339085 140283577931520 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.025311728939414024, loss=0.03486030176281929
I0305 12:08:12.081919 140381878482688 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.025248143821954727, loss=0.032261479645967484
I0305 12:08:44.078743 140283577931520 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.028357496485114098, loss=0.03247563913464546
I0305 12:08:52.330364 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:10:51.370845 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:10:54.439366 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:10:57.421981 140444430841664 submission_runner.py:411] Time since start: 7183.04s, 	Step: 13326, 	{'train/accuracy': 0.9902656674385071, 'train/loss': 0.03223968297243118, 'train/mean_average_precision': 0.3608134814237923, 'validation/accuracy': 0.986819863319397, 'validation/loss': 0.04451136663556099, 'validation/mean_average_precision': 0.26813830141520595, 'validation/num_examples': 43793, 'test/accuracy': 0.9858890771865845, 'test/loss': 0.04764237254858017, 'test/mean_average_precision': 0.25477645264258997, 'test/num_examples': 43793, 'score': 4340.828438043594, 'total_duration': 7183.03778886795, 'accumulated_submission_time': 4340.828438043594, 'accumulated_eval_time': 2841.2963259220123, 'accumulated_logging_time': 0.5184030532836914}
I0305 12:10:57.441064 140277079156480 logging_writer.py:48] [13326] accumulated_eval_time=2841.296326, accumulated_logging_time=0.518403, accumulated_submission_time=4340.828438, global_step=13326, preemption_count=0, score=4340.828438, test/accuracy=0.985889, test/loss=0.047642, test/mean_average_precision=0.254776, test/num_examples=43793, total_duration=7183.037789, train/accuracy=0.990266, train/loss=0.032240, train/mean_average_precision=0.360813, validation/accuracy=0.986820, validation/loss=0.044511, validation/mean_average_precision=0.268138, validation/num_examples=43793
I0305 12:11:21.527328 140381870089984 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.031646355986595154, loss=0.031136173754930496
I0305 12:11:53.794513 140277079156480 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.035716280341148376, loss=0.03535349667072296
I0305 12:12:26.128674 140381870089984 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.02967134676873684, loss=0.032685600221157074
I0305 12:12:57.883334 140277079156480 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.031119976192712784, loss=0.03178797662258148
I0305 12:13:30.291918 140381870089984 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03001605160534382, loss=0.031743284314870834
I0305 12:14:03.194611 140277079156480 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.027742557227611542, loss=0.03169545531272888
I0305 12:14:35.819703 140381870089984 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.030141105875372887, loss=0.0311205442994833
I0305 12:14:57.422273 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:17:00.734786 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:17:04.224044 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:17:07.546477 140444430841664 submission_runner.py:411] Time since start: 7553.16s, 	Step: 14067, 	{'train/accuracy': 0.9905223846435547, 'train/loss': 0.031429897993803024, 'train/mean_average_precision': 0.37606789224080694, 'validation/accuracy': 0.9869217872619629, 'validation/loss': 0.043913647532463074, 'validation/mean_average_precision': 0.27488151943901024, 'validation/num_examples': 43793, 'test/accuracy': 0.9859737753868103, 'test/loss': 0.046831123530864716, 'test/mean_average_precision': 0.2589879758092704, 'test/num_examples': 43793, 'score': 4580.7753620147705, 'total_duration': 7553.162268877029, 'accumulated_submission_time': 4580.7753620147705, 'accumulated_eval_time': 2971.4204857349396, 'accumulated_logging_time': 0.5491487979888916}
I0305 12:17:07.565942 140283569538816 logging_writer.py:48] [14067] accumulated_eval_time=2971.420486, accumulated_logging_time=0.549149, accumulated_submission_time=4580.775362, global_step=14067, preemption_count=0, score=4580.775362, test/accuracy=0.985974, test/loss=0.046831, test/mean_average_precision=0.258988, test/num_examples=43793, total_duration=7553.162269, train/accuracy=0.990522, train/loss=0.031430, train/mean_average_precision=0.376068, validation/accuracy=0.986922, validation/loss=0.043914, validation/mean_average_precision=0.274882, validation/num_examples=43793
I0305 12:17:18.882917 140381878482688 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03524570167064667, loss=0.030099598690867424
I0305 12:17:51.970588 140283569538816 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.036905258893966675, loss=0.036393292248249054
I0305 12:18:24.710525 140381878482688 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.04314400255680084, loss=0.03258867934346199
I0305 12:18:57.484329 140283569538816 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.03235895559191704, loss=0.03042040765285492
I0305 12:19:30.571444 140381878482688 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.04246515780687332, loss=0.0343562476336956
I0305 12:20:03.499726 140283569538816 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.03407398983836174, loss=0.03187913820147514
I0305 12:20:36.034406 140381878482688 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.030499784275889397, loss=0.03155943751335144
I0305 12:21:07.647939 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:23:11.965997 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:23:15.016914 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:23:18.056430 140444430841664 submission_runner.py:411] Time since start: 7923.67s, 	Step: 14798, 	{'train/accuracy': 0.9905580878257751, 'train/loss': 0.031193077564239502, 'train/mean_average_precision': 0.3902971635582445, 'validation/accuracy': 0.9866790175437927, 'validation/loss': 0.044609855860471725, 'validation/mean_average_precision': 0.26873626543366375, 'validation/num_examples': 43793, 'test/accuracy': 0.9856927990913391, 'test/loss': 0.04757380485534668, 'test/mean_average_precision': 0.25555612480518847, 'test/num_examples': 43793, 'score': 4820.819413661957, 'total_duration': 7923.672146081924, 'accumulated_submission_time': 4820.819413661957, 'accumulated_eval_time': 3101.82884144783, 'accumulated_logging_time': 0.5812263488769531}
I0305 12:23:18.075200 140283577931520 logging_writer.py:48] [14798] accumulated_eval_time=3101.828841, accumulated_logging_time=0.581226, accumulated_submission_time=4820.819414, global_step=14798, preemption_count=0, score=4820.819414, test/accuracy=0.985693, test/loss=0.047574, test/mean_average_precision=0.255556, test/num_examples=43793, total_duration=7923.672146, train/accuracy=0.990558, train/loss=0.031193, train/mean_average_precision=0.390297, validation/accuracy=0.986679, validation/loss=0.044610, validation/mean_average_precision=0.268736, validation/num_examples=43793
I0305 12:23:19.104642 140381870089984 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.032494258135557175, loss=0.033123429864645004
I0305 12:23:51.939260 140283577931520 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.04492725059390068, loss=0.03269452974200249
I0305 12:24:24.928951 140381870089984 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.02831333689391613, loss=0.03218230605125427
I0305 12:24:57.546433 140283577931520 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.032517239451408386, loss=0.03355470299720764
I0305 12:25:30.577307 140381870089984 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.040197450667619705, loss=0.03196509927511215
I0305 12:26:03.916166 140283577931520 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04638610780239105, loss=0.034151703119277954
I0305 12:26:37.149671 140381870089984 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.041479773819446564, loss=0.036342862993478775
I0305 12:27:10.765273 140283577931520 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.03338424861431122, loss=0.03065299242734909
I0305 12:27:18.233332 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:29:21.683867 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:29:24.798629 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:29:27.839945 140444430841664 submission_runner.py:411] Time since start: 8293.46s, 	Step: 15524, 	{'train/accuracy': 0.9906007647514343, 'train/loss': 0.03069312870502472, 'train/mean_average_precision': 0.40379776921794225, 'validation/accuracy': 0.9867488145828247, 'validation/loss': 0.04422121122479439, 'validation/mean_average_precision': 0.26879628724417715, 'validation/num_examples': 43793, 'test/accuracy': 0.985888659954071, 'test/loss': 0.046939872205257416, 'test/mean_average_precision': 0.2664834005963081, 'test/num_examples': 43793, 'score': 5060.941811084747, 'total_duration': 8293.455752372742, 'accumulated_submission_time': 5060.941811084747, 'accumulated_eval_time': 3231.4354071617126, 'accumulated_logging_time': 0.6112728118896484}
I0305 12:29:27.858792 140283569538816 logging_writer.py:48] [15524] accumulated_eval_time=3231.435407, accumulated_logging_time=0.611273, accumulated_submission_time=5060.941811, global_step=15524, preemption_count=0, score=5060.941811, test/accuracy=0.985889, test/loss=0.046940, test/mean_average_precision=0.266483, test/num_examples=43793, total_duration=8293.455752, train/accuracy=0.990601, train/loss=0.030693, train/mean_average_precision=0.403798, validation/accuracy=0.986749, validation/loss=0.044221, validation/mean_average_precision=0.268796, validation/num_examples=43793
I0305 12:29:53.311102 140381878482688 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.032699599862098694, loss=0.03355046734213829
I0305 12:30:25.686157 140283569538816 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.03689545765519142, loss=0.03747447207570076
I0305 12:30:58.256514 140381878482688 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.0356338769197464, loss=0.033927418291568756
I0305 12:31:31.261377 140283569538816 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.0348215289413929, loss=0.03385452181100845
I0305 12:32:03.521285 140381878482688 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.032284557819366455, loss=0.030964771285653114
I0305 12:32:36.374068 140283569538816 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03820911794900894, loss=0.030665172263979912
I0305 12:33:08.340616 140381878482688 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.047126539051532745, loss=0.03168236091732979
I0305 12:33:28.169370 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:35:32.134407 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:35:35.188581 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:35:38.189696 140444430841664 submission_runner.py:411] Time since start: 8663.81s, 	Step: 16263, 	{'train/accuracy': 0.9907249808311462, 'train/loss': 0.030366413295269012, 'train/mean_average_precision': 0.40921699850730814, 'validation/accuracy': 0.9867720007896423, 'validation/loss': 0.043939169496297836, 'validation/mean_average_precision': 0.268025885787124, 'validation/num_examples': 43793, 'test/accuracy': 0.9859308004379272, 'test/loss': 0.04655764624476433, 'test/mean_average_precision': 0.26440022465576724, 'test/num_examples': 43793, 'score': 5301.218809843063, 'total_duration': 8663.80548620224, 'accumulated_submission_time': 5301.218809843063, 'accumulated_eval_time': 3361.455674648285, 'accumulated_logging_time': 0.6414785385131836}
I0305 12:35:38.207824 140283577931520 logging_writer.py:48] [16263] accumulated_eval_time=3361.455675, accumulated_logging_time=0.641479, accumulated_submission_time=5301.218810, global_step=16263, preemption_count=0, score=5301.218810, test/accuracy=0.985931, test/loss=0.046558, test/mean_average_precision=0.264400, test/num_examples=43793, total_duration=8663.805486, train/accuracy=0.990725, train/loss=0.030366, train/mean_average_precision=0.409217, validation/accuracy=0.986772, validation/loss=0.043939, validation/mean_average_precision=0.268026, validation/num_examples=43793
I0305 12:35:50.593988 140381870089984 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.033464960753917694, loss=0.030345434322953224
I0305 12:36:23.340499 140283577931520 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.044316671788692474, loss=0.03226805105805397
I0305 12:36:55.915569 140381870089984 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03731641545891762, loss=0.02852356620132923
I0305 12:37:28.559949 140283577931520 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03728361427783966, loss=0.03363025560975075
I0305 12:38:01.426074 140381870089984 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.04627200588583946, loss=0.032518982887268066
I0305 12:38:33.856628 140283577931520 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.030467918142676353, loss=0.0321136936545372
I0305 12:39:06.408927 140381870089984 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.03795451670885086, loss=0.031036769971251488
I0305 12:39:38.254126 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:41:40.923238 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:41:44.002198 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:41:47.030162 140444430841664 submission_runner.py:411] Time since start: 9032.65s, 	Step: 16999, 	{'train/accuracy': 0.9909940361976624, 'train/loss': 0.02934557944536209, 'train/mean_average_precision': 0.4297240381765557, 'validation/accuracy': 0.9869027137756348, 'validation/loss': 0.044033072888851166, 'validation/mean_average_precision': 0.27278173310326165, 'validation/num_examples': 43793, 'test/accuracy': 0.9860049486160278, 'test/loss': 0.04712457209825516, 'test/mean_average_precision': 0.26232762618243494, 'test/num_examples': 43793, 'score': 5541.2301506996155, 'total_duration': 9032.645967960358, 'accumulated_submission_time': 5541.2301506996155, 'accumulated_eval_time': 3490.2316720485687, 'accumulated_logging_time': 0.672264575958252}
I0305 12:41:47.050238 140283569538816 logging_writer.py:48] [16999] accumulated_eval_time=3490.231672, accumulated_logging_time=0.672265, accumulated_submission_time=5541.230151, global_step=16999, preemption_count=0, score=5541.230151, test/accuracy=0.986005, test/loss=0.047125, test/mean_average_precision=0.262328, test/num_examples=43793, total_duration=9032.645968, train/accuracy=0.990994, train/loss=0.029346, train/mean_average_precision=0.429724, validation/accuracy=0.986903, validation/loss=0.044033, validation/mean_average_precision=0.272782, validation/num_examples=43793
I0305 12:41:47.712216 140381878482688 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03372131660580635, loss=0.029844965785741806
I0305 12:42:20.409527 140283569538816 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.043030496686697006, loss=0.03439006581902504
I0305 12:42:53.192414 140381878482688 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03607248514890671, loss=0.02961648628115654
I0305 12:43:25.827280 140283569538816 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.031469810754060745, loss=0.027068769559264183
I0305 12:43:58.374145 140381878482688 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.04956204816699028, loss=0.03584069013595581
I0305 12:44:30.979496 140283569538816 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03877260908484459, loss=0.03413793072104454
I0305 12:45:03.821606 140381878482688 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.03377732262015343, loss=0.031094465404748917
I0305 12:45:36.421546 140283569538816 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.04653893783688545, loss=0.03437524288892746
I0305 12:45:47.331208 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:47:53.927707 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:47:56.980208 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:48:00.016875 140444430841664 submission_runner.py:411] Time since start: 9405.63s, 	Step: 17734, 	{'train/accuracy': 0.9909370541572571, 'train/loss': 0.02974214404821396, 'train/mean_average_precision': 0.41728955213000285, 'validation/accuracy': 0.9867074489593506, 'validation/loss': 0.0442817248404026, 'validation/mean_average_precision': 0.266266108306227, 'validation/num_examples': 43793, 'test/accuracy': 0.9858478307723999, 'test/loss': 0.04725059121847153, 'test/mean_average_precision': 0.2620539259508231, 'test/num_examples': 43793, 'score': 5781.477691650391, 'total_duration': 9405.632671833038, 'accumulated_submission_time': 5781.477691650391, 'accumulated_eval_time': 3622.917279958725, 'accumulated_logging_time': 0.7036941051483154}
I0305 12:48:00.035409 140277079156480 logging_writer.py:48] [17734] accumulated_eval_time=3622.917280, accumulated_logging_time=0.703694, accumulated_submission_time=5781.477692, global_step=17734, preemption_count=0, score=5781.477692, test/accuracy=0.985848, test/loss=0.047251, test/mean_average_precision=0.262054, test/num_examples=43793, total_duration=9405.632672, train/accuracy=0.990937, train/loss=0.029742, train/mean_average_precision=0.417290, validation/accuracy=0.986707, validation/loss=0.044282, validation/mean_average_precision=0.266266, validation/num_examples=43793
I0305 12:48:21.941544 140381870089984 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.03531397879123688, loss=0.03214937821030617
I0305 12:48:54.396496 140277079156480 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04065321013331413, loss=0.03183623030781746
I0305 12:49:27.232585 140381870089984 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.039741285145282745, loss=0.031177932396531105
I0305 12:50:00.088993 140277079156480 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.03737908974289894, loss=0.030556052923202515
I0305 12:50:33.405661 140381870089984 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.03593776002526283, loss=0.029689135029911995
I0305 12:51:05.987012 140277079156480 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.03654343634843826, loss=0.029931724071502686
I0305 12:51:38.647647 140381870089984 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.036379020661115646, loss=0.032201822847127914
I0305 12:52:00.327259 140444430841664 spec.py:321] Evaluating on the training split.
I0305 12:54:02.946503 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 12:54:06.799109 140444430841664 spec.py:349] Evaluating on the test split.
I0305 12:54:09.755677 140444430841664 submission_runner.py:411] Time since start: 9775.37s, 	Step: 18468, 	{'train/accuracy': 0.9910364151000977, 'train/loss': 0.029528234153985977, 'train/mean_average_precision': 0.423870057287632, 'validation/accuracy': 0.9868448376655579, 'validation/loss': 0.043834198266267776, 'validation/mean_average_precision': 0.27833126499579314, 'validation/num_examples': 43793, 'test/accuracy': 0.9860584139823914, 'test/loss': 0.04644297808408737, 'test/mean_average_precision': 0.266278519375825, 'test/num_examples': 43793, 'score': 6021.736469745636, 'total_duration': 9775.371485948563, 'accumulated_submission_time': 6021.736469745636, 'accumulated_eval_time': 3752.345653772354, 'accumulated_logging_time': 0.7333090305328369}
I0305 12:54:09.774572 140283569538816 logging_writer.py:48] [18468] accumulated_eval_time=3752.345654, accumulated_logging_time=0.733309, accumulated_submission_time=6021.736470, global_step=18468, preemption_count=0, score=6021.736470, test/accuracy=0.986058, test/loss=0.046443, test/mean_average_precision=0.266279, test/num_examples=43793, total_duration=9775.371486, train/accuracy=0.991036, train/loss=0.029528, train/mean_average_precision=0.423870, validation/accuracy=0.986845, validation/loss=0.043834, validation/mean_average_precision=0.278331, validation/num_examples=43793
I0305 12:54:20.402026 140381878482688 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.038715675473213196, loss=0.030649524182081223
I0305 12:54:52.859683 140283569538816 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.036824990063905716, loss=0.02806464210152626
I0305 12:55:25.819250 140381878482688 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.045741911977529526, loss=0.03302217274904251
I0305 12:55:58.211704 140283569538816 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.038841646164655685, loss=0.03069073148071766
I0305 12:56:31.323143 140381878482688 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.03906561806797981, loss=0.03197023272514343
I0305 12:57:04.062012 140283569538816 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.04155328869819641, loss=0.03245527297258377
I0305 12:57:36.409229 140381878482688 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.041407715529203415, loss=0.028167424723505974
I0305 12:58:09.272083 140283569538816 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.036623213440179825, loss=0.026606183499097824
I0305 12:58:09.926270 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:00:12.212680 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:00:15.254232 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:00:18.267736 140444430841664 submission_runner.py:411] Time since start: 10143.88s, 	Step: 19203, 	{'train/accuracy': 0.9909691214561462, 'train/loss': 0.029665526002645493, 'train/mean_average_precision': 0.42618701446624574, 'validation/accuracy': 0.9869157075881958, 'validation/loss': 0.04378636181354523, 'validation/mean_average_precision': 0.278916097311652, 'validation/num_examples': 43793, 'test/accuracy': 0.9859741926193237, 'test/loss': 0.046578411012887955, 'test/mean_average_precision': 0.26738046020964407, 'test/num_examples': 43793, 'score': 6261.852321386337, 'total_duration': 10143.883538722992, 'accumulated_submission_time': 6261.852321386337, 'accumulated_eval_time': 3880.687066555023, 'accumulated_logging_time': 0.7648305892944336}
I0305 13:00:18.289398 140277079156480 logging_writer.py:48] [19203] accumulated_eval_time=3880.687067, accumulated_logging_time=0.764831, accumulated_submission_time=6261.852321, global_step=19203, preemption_count=0, score=6261.852321, test/accuracy=0.985974, test/loss=0.046578, test/mean_average_precision=0.267380, test/num_examples=43793, total_duration=10143.883539, train/accuracy=0.990969, train/loss=0.029666, train/mean_average_precision=0.426187, validation/accuracy=0.986916, validation/loss=0.043786, validation/mean_average_precision=0.278916, validation/num_examples=43793
I0305 13:00:51.341081 140283577931520 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.04075351357460022, loss=0.032397907227277756
I0305 13:01:23.852975 140277079156480 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04258183017373085, loss=0.03110508807003498
I0305 13:01:56.437080 140283577931520 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04637472704052925, loss=0.02798166312277317
I0305 13:02:29.550968 140277079156480 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04485604912042618, loss=0.031279876828193665
I0305 13:03:02.516020 140283577931520 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04167890548706055, loss=0.03144349157810211
I0305 13:03:34.821327 140277079156480 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.03964877873659134, loss=0.02645130455493927
I0305 13:04:07.274653 140283577931520 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04384332895278931, loss=0.03688478842377663
I0305 13:04:18.505448 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:06:23.306591 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:06:26.396129 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:06:29.608791 140444430841664 submission_runner.py:411] Time since start: 10515.22s, 	Step: 19936, 	{'train/accuracy': 0.9907914400100708, 'train/loss': 0.030123768374323845, 'train/mean_average_precision': 0.4203923085594159, 'validation/accuracy': 0.9866023063659668, 'validation/loss': 0.04420877993106842, 'validation/mean_average_precision': 0.274155667844338, 'validation/num_examples': 43793, 'test/accuracy': 0.9857467412948608, 'test/loss': 0.04686388373374939, 'test/mean_average_precision': 0.2608550174941025, 'test/num_examples': 43793, 'score': 6502.033669233322, 'total_duration': 10515.224586963654, 'accumulated_submission_time': 6502.033669233322, 'accumulated_eval_time': 4011.7903487682343, 'accumulated_logging_time': 0.7990889549255371}
I0305 13:06:29.629343 140283569538816 logging_writer.py:48] [19936] accumulated_eval_time=4011.790349, accumulated_logging_time=0.799089, accumulated_submission_time=6502.033669, global_step=19936, preemption_count=0, score=6502.033669, test/accuracy=0.985747, test/loss=0.046864, test/mean_average_precision=0.260855, test/num_examples=43793, total_duration=10515.224587, train/accuracy=0.990791, train/loss=0.030124, train/mean_average_precision=0.420392, validation/accuracy=0.986602, validation/loss=0.044209, validation/mean_average_precision=0.274156, validation/num_examples=43793
I0305 13:06:50.542794 140381878482688 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04186136648058891, loss=0.03206806629896164
I0305 13:07:23.237819 140283569538816 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.043888382613658905, loss=0.031153395771980286
I0305 13:07:55.573119 140381878482688 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04355354607105255, loss=0.03331036493182182
I0305 13:08:27.995683 140283569538816 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.06215881556272507, loss=0.03399985656142235
I0305 13:09:00.319503 140381878482688 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.04086954891681671, loss=0.03101339004933834
I0305 13:09:32.866966 140283569538816 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.047088269144296646, loss=0.0281169842928648
I0305 13:10:05.631008 140381878482688 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04998905211687088, loss=0.032800447195768356
I0305 13:10:29.712937 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:12:30.853917 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:12:34.001796 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:12:37.057532 140444430841664 submission_runner.py:411] Time since start: 10882.67s, 	Step: 20676, 	{'train/accuracy': 0.990973174571991, 'train/loss': 0.02962084673345089, 'train/mean_average_precision': 0.41989086156415745, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.04406379908323288, 'validation/mean_average_precision': 0.27451953577384397, 'validation/num_examples': 43793, 'test/accuracy': 0.9859619736671448, 'test/loss': 0.04670894145965576, 'test/mean_average_precision': 0.2625949482700543, 'test/num_examples': 43793, 'score': 6742.084539890289, 'total_duration': 10882.673321723938, 'accumulated_submission_time': 6742.084539890289, 'accumulated_eval_time': 4139.134879112244, 'accumulated_logging_time': 0.8307468891143799}
I0305 13:12:37.077479 140283577931520 logging_writer.py:48] [20676] accumulated_eval_time=4139.134879, accumulated_logging_time=0.830747, accumulated_submission_time=6742.084540, global_step=20676, preemption_count=0, score=6742.084540, test/accuracy=0.985962, test/loss=0.046709, test/mean_average_precision=0.262595, test/num_examples=43793, total_duration=10882.673322, train/accuracy=0.990973, train/loss=0.029621, train/mean_average_precision=0.419891, validation/accuracy=0.986768, validation/loss=0.044064, validation/mean_average_precision=0.274520, validation/num_examples=43793
I0305 13:12:45.505632 140381870089984 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04581214115023613, loss=0.03503360226750374
I0305 13:13:18.235533 140283577931520 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.05024028941988945, loss=0.033468641340732574
I0305 13:13:50.764498 140381870089984 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.03873045742511749, loss=0.033803943544626236
I0305 13:14:23.749516 140283577931520 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05767720192670822, loss=0.03285009786486626
I0305 13:14:56.505589 140381870089984 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.043705012649297714, loss=0.031170198693871498
I0305 13:15:29.197139 140283577931520 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.04164806753396988, loss=0.03226146474480629
I0305 13:16:01.935855 140381870089984 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.03918185457587242, loss=0.030550191178917885
I0305 13:16:34.714554 140283577931520 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.05339688062667847, loss=0.031064564362168312
I0305 13:16:37.249648 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:18:44.566679 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:18:47.606585 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:18:50.648500 140444430841664 submission_runner.py:411] Time since start: 11256.26s, 	Step: 21409, 	{'train/accuracy': 0.9911744594573975, 'train/loss': 0.02877720817923546, 'train/mean_average_precision': 0.4454445866037658, 'validation/accuracy': 0.9868689775466919, 'validation/loss': 0.04363427311182022, 'validation/mean_average_precision': 0.2769052961180017, 'validation/num_examples': 43793, 'test/accuracy': 0.9859548211097717, 'test/loss': 0.04636829346418381, 'test/mean_average_precision': 0.2663493902990147, 'test/num_examples': 43793, 'score': 6982.223841190338, 'total_duration': 11256.264298200607, 'accumulated_submission_time': 6982.223841190338, 'accumulated_eval_time': 4272.533679008484, 'accumulated_logging_time': 0.861682653427124}
I0305 13:18:50.669429 140283569538816 logging_writer.py:48] [21409] accumulated_eval_time=4272.533679, accumulated_logging_time=0.861683, accumulated_submission_time=6982.223841, global_step=21409, preemption_count=0, score=6982.223841, test/accuracy=0.985955, test/loss=0.046368, test/mean_average_precision=0.266349, test/num_examples=43793, total_duration=11256.264298, train/accuracy=0.991174, train/loss=0.028777, train/mean_average_precision=0.445445, validation/accuracy=0.986869, validation/loss=0.043634, validation/mean_average_precision=0.276905, validation/num_examples=43793
I0305 13:19:21.004367 140381878482688 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.05047907307744026, loss=0.033930666744709015
I0305 13:19:53.867633 140283569538816 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.050172850489616394, loss=0.029724933207035065
I0305 13:20:26.303944 140381878482688 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.06168101355433464, loss=0.025899164378643036
I0305 13:20:59.175180 140283569538816 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.052617114037275314, loss=0.03246621415019035
I0305 13:21:32.161703 140381878482688 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.03953710198402405, loss=0.031425632536411285
I0305 13:22:05.303005 140283569538816 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.049629997462034225, loss=0.030395440757274628
I0305 13:22:37.893279 140381878482688 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.04908332601189613, loss=0.032960765063762665
I0305 13:22:50.771021 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:24:52.815698 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:24:55.888080 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:24:58.955388 140444430841664 submission_runner.py:411] Time since start: 11624.57s, 	Step: 22141, 	{'train/accuracy': 0.9913221597671509, 'train/loss': 0.028284713625907898, 'train/mean_average_precision': 0.4633205340969653, 'validation/accuracy': 0.9869778156280518, 'validation/loss': 0.04384014382958412, 'validation/mean_average_precision': 0.28310263021989046, 'validation/num_examples': 43793, 'test/accuracy': 0.986151933670044, 'test/loss': 0.04661700874567032, 'test/mean_average_precision': 0.2707095616703951, 'test/num_examples': 43793, 'score': 7222.290660858154, 'total_duration': 11624.571064710617, 'accumulated_submission_time': 7222.290660858154, 'accumulated_eval_time': 4400.717868804932, 'accumulated_logging_time': 0.8936870098114014}
I0305 13:24:58.975501 140277079156480 logging_writer.py:48] [22141] accumulated_eval_time=4400.717869, accumulated_logging_time=0.893687, accumulated_submission_time=7222.290661, global_step=22141, preemption_count=0, score=7222.290661, test/accuracy=0.986152, test/loss=0.046617, test/mean_average_precision=0.270710, test/num_examples=43793, total_duration=11624.571065, train/accuracy=0.991322, train/loss=0.028285, train/mean_average_precision=0.463321, validation/accuracy=0.986978, validation/loss=0.043840, validation/mean_average_precision=0.283103, validation/num_examples=43793
I0305 13:25:18.455497 140283577931520 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.04511278122663498, loss=0.027069073170423508
I0305 13:25:51.538438 140277079156480 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.047259654849767685, loss=0.03211710602045059
I0305 13:26:24.312853 140283577931520 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.0436931811273098, loss=0.03047364577651024
I0305 13:26:56.994927 140277079156480 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.056124381721019745, loss=0.03129347413778305
I0305 13:27:28.893409 140283577931520 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.05573948472738266, loss=0.03247654065489769
I0305 13:28:01.480652 140277079156480 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.06461700797080994, loss=0.03591843694448471
I0305 13:28:34.031647 140283577931520 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04752777889370918, loss=0.03357202187180519
I0305 13:28:59.294107 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:31:03.734778 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:31:06.789951 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:31:09.816870 140444430841664 submission_runner.py:411] Time since start: 11995.43s, 	Step: 22880, 	{'train/accuracy': 0.9915106296539307, 'train/loss': 0.027569672092795372, 'train/mean_average_precision': 0.4735089752188432, 'validation/accuracy': 0.9869562983512878, 'validation/loss': 0.04415281489491463, 'validation/mean_average_precision': 0.2790978605654699, 'validation/num_examples': 43793, 'test/accuracy': 0.9861325621604919, 'test/loss': 0.04681672900915146, 'test/mean_average_precision': 0.26884340209445046, 'test/num_examples': 43793, 'score': 7462.577006340027, 'total_duration': 11995.432676315308, 'accumulated_submission_time': 7462.577006340027, 'accumulated_eval_time': 4531.240593194962, 'accumulated_logging_time': 0.9250195026397705}
I0305 13:31:09.836358 140283569538816 logging_writer.py:48] [22880] accumulated_eval_time=4531.240593, accumulated_logging_time=0.925020, accumulated_submission_time=7462.577006, global_step=22880, preemption_count=0, score=7462.577006, test/accuracy=0.986133, test/loss=0.046817, test/mean_average_precision=0.268843, test/num_examples=43793, total_duration=11995.432676, train/accuracy=0.991511, train/loss=0.027570, train/mean_average_precision=0.473509, validation/accuracy=0.986956, validation/loss=0.044153, validation/mean_average_precision=0.279098, validation/num_examples=43793
I0305 13:31:16.596341 140381878482688 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.05634003132581711, loss=0.03413702920079231
I0305 13:31:48.881936 140283569538816 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.043136849999427795, loss=0.030192917212843895
I0305 13:32:20.941989 140381878482688 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.0518309511244297, loss=0.03347035124897957
I0305 13:32:52.886286 140283569538816 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.05647163465619087, loss=0.03301587700843811
I0305 13:33:25.374277 140381878482688 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.04953230917453766, loss=0.030658572912216187
I0305 13:33:57.417346 140283569538816 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.049407243728637695, loss=0.032884642481803894
I0305 13:34:29.793583 140381878482688 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.042743731290102005, loss=0.03022892028093338
I0305 13:35:01.843606 140283569538816 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.060203250497579575, loss=0.02961220219731331
I0305 13:35:09.883498 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:37:12.369089 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:37:15.421791 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:37:18.459589 140444430841664 submission_runner.py:411] Time since start: 12364.08s, 	Step: 23626, 	{'train/accuracy': 0.9915829300880432, 'train/loss': 0.0273137167096138, 'train/mean_average_precision': 0.4782024616666336, 'validation/accuracy': 0.9868547916412354, 'validation/loss': 0.044268690049648285, 'validation/mean_average_precision': 0.27107560951549214, 'validation/num_examples': 43793, 'test/accuracy': 0.9860613942146301, 'test/loss': 0.047044456005096436, 'test/mean_average_precision': 0.2641718235610194, 'test/num_examples': 43793, 'score': 7702.5916657447815, 'total_duration': 12364.075269460678, 'accumulated_submission_time': 7702.5916657447815, 'accumulated_eval_time': 4659.816511154175, 'accumulated_logging_time': 0.9558253288269043}
I0305 13:37:18.479422 140277079156480 logging_writer.py:48] [23626] accumulated_eval_time=4659.816511, accumulated_logging_time=0.955825, accumulated_submission_time=7702.591666, global_step=23626, preemption_count=0, score=7702.591666, test/accuracy=0.986061, test/loss=0.047044, test/mean_average_precision=0.264172, test/num_examples=43793, total_duration=12364.075269, train/accuracy=0.991583, train/loss=0.027314, train/mean_average_precision=0.478202, validation/accuracy=0.986855, validation/loss=0.044269, validation/mean_average_precision=0.271076, validation/num_examples=43793
I0305 13:37:42.283955 140283577931520 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.07889658957719803, loss=0.029284998774528503
I0305 13:38:14.600133 140277079156480 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.052221789956092834, loss=0.0338636189699173
I0305 13:38:46.589747 140283577931520 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.047531627118587494, loss=0.027856064960360527
I0305 13:39:18.803808 140277079156480 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05426667258143425, loss=0.03143671154975891
I0305 13:39:50.911104 140283577931520 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.05003373697400093, loss=0.030896048992872238
I0305 13:40:23.067202 140277079156480 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.05182895064353943, loss=0.02942098118364811
I0305 13:40:55.286790 140283577931520 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.044618669897317886, loss=0.029807934537529945
I0305 13:41:18.490308 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:43:22.635670 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:43:25.720439 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:43:28.710409 140444430841664 submission_runner.py:411] Time since start: 12734.33s, 	Step: 24373, 	{'train/accuracy': 0.991339385509491, 'train/loss': 0.028113512322306633, 'train/mean_average_precision': 0.45992866373453756, 'validation/accuracy': 0.9870126843452454, 'validation/loss': 0.043771956115961075, 'validation/mean_average_precision': 0.27956455584700257, 'validation/num_examples': 43793, 'test/accuracy': 0.9861839413642883, 'test/loss': 0.04654020071029663, 'test/mean_average_precision': 0.27017914849261426, 'test/num_examples': 43793, 'score': 7942.569582223892, 'total_duration': 12734.326212882996, 'accumulated_submission_time': 7942.569582223892, 'accumulated_eval_time': 4790.036564588547, 'accumulated_logging_time': 0.98685622215271}
I0305 13:43:28.730553 140283569538816 logging_writer.py:48] [24373] accumulated_eval_time=4790.036565, accumulated_logging_time=0.986856, accumulated_submission_time=7942.569582, global_step=24373, preemption_count=0, score=7942.569582, test/accuracy=0.986184, test/loss=0.046540, test/mean_average_precision=0.270179, test/num_examples=43793, total_duration=12734.326213, train/accuracy=0.991339, train/loss=0.028114, train/mean_average_precision=0.459929, validation/accuracy=0.987013, validation/loss=0.043772, validation/mean_average_precision=0.279565, validation/num_examples=43793
I0305 13:43:37.742074 140381878482688 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.05324587970972061, loss=0.030487101525068283
I0305 13:44:09.966837 140283569538816 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.042935218662023544, loss=0.029390376061201096
I0305 13:44:41.846007 140381878482688 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.06757968664169312, loss=0.033997438848018646
I0305 13:45:14.035185 140283569538816 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.06930578500032425, loss=0.028890740126371384
I0305 13:45:46.046880 140381878482688 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.060048092156648636, loss=0.03245897591114044
I0305 13:46:18.167452 140283569538816 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.04635726287961006, loss=0.03157418221235275
I0305 13:46:50.858951 140381878482688 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.051625605672597885, loss=0.028051991015672684
I0305 13:47:23.437824 140283569538816 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.053638506680727005, loss=0.03303320333361626
I0305 13:47:28.942664 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:49:27.694670 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:49:30.770632 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:49:33.784089 140444430841664 submission_runner.py:411] Time since start: 13099.40s, 	Step: 25118, 	{'train/accuracy': 0.9912641048431396, 'train/loss': 0.028504066169261932, 'train/mean_average_precision': 0.44777100259166897, 'validation/accuracy': 0.9869189262390137, 'validation/loss': 0.044187549501657486, 'validation/mean_average_precision': 0.28543353691925233, 'validation/num_examples': 43793, 'test/accuracy': 0.9860343933105469, 'test/loss': 0.04705650731921196, 'test/mean_average_precision': 0.26565273335062595, 'test/num_examples': 43793, 'score': 8182.74741435051, 'total_duration': 13099.399793863297, 'accumulated_submission_time': 8182.74741435051, 'accumulated_eval_time': 4914.877838134766, 'accumulated_logging_time': 1.0195541381835938}
I0305 13:49:33.805159 140277079156480 logging_writer.py:48] [25118] accumulated_eval_time=4914.877838, accumulated_logging_time=1.019554, accumulated_submission_time=8182.747414, global_step=25118, preemption_count=0, score=8182.747414, test/accuracy=0.986034, test/loss=0.047057, test/mean_average_precision=0.265653, test/num_examples=43793, total_duration=13099.399794, train/accuracy=0.991264, train/loss=0.028504, train/mean_average_precision=0.447771, validation/accuracy=0.986919, validation/loss=0.044188, validation/mean_average_precision=0.285434, validation/num_examples=43793
I0305 13:50:00.564935 140381870089984 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.04154790937900543, loss=0.028547149151563644
I0305 13:50:33.295248 140277079156480 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.05669870972633362, loss=0.030824078246951103
I0305 13:51:06.344511 140381870089984 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.054088570177555084, loss=0.031874243170022964
I0305 13:51:38.647809 140277079156480 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.057594265788793564, loss=0.034308817237615585
I0305 13:52:11.715703 140381870089984 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.061461858451366425, loss=0.02987261302769184
I0305 13:52:44.405540 140277079156480 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.05647099018096924, loss=0.031985968351364136
I0305 13:53:17.071956 140381870089984 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.05234222114086151, loss=0.030160393565893173
I0305 13:53:33.997821 140444430841664 spec.py:321] Evaluating on the training split.
I0305 13:55:38.480230 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 13:55:41.561771 140444430841664 spec.py:349] Evaluating on the test split.
I0305 13:55:44.541534 140444430841664 submission_runner.py:411] Time since start: 13470.16s, 	Step: 25853, 	{'train/accuracy': 0.9912461638450623, 'train/loss': 0.028466306626796722, 'train/mean_average_precision': 0.45411843934102525, 'validation/accuracy': 0.986922562122345, 'validation/loss': 0.04375268146395683, 'validation/mean_average_precision': 0.2801145594678734, 'validation/num_examples': 43793, 'test/accuracy': 0.98613041639328, 'test/loss': 0.046415720134973526, 'test/mean_average_precision': 0.2715346448095588, 'test/num_examples': 43793, 'score': 8422.905773878098, 'total_duration': 13470.157238006592, 'accumulated_submission_time': 8422.905773878098, 'accumulated_eval_time': 5045.421402454376, 'accumulated_logging_time': 1.0534796714782715}
I0305 13:55:44.561469 140283577931520 logging_writer.py:48] [25853] accumulated_eval_time=5045.421402, accumulated_logging_time=1.053480, accumulated_submission_time=8422.905774, global_step=25853, preemption_count=0, score=8422.905774, test/accuracy=0.986130, test/loss=0.046416, test/mean_average_precision=0.271535, test/num_examples=43793, total_duration=13470.157238, train/accuracy=0.991246, train/loss=0.028466, train/mean_average_precision=0.454118, validation/accuracy=0.986923, validation/loss=0.043753, validation/mean_average_precision=0.280115, validation/num_examples=43793
I0305 13:56:00.193156 140381878482688 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.049117930233478546, loss=0.02851487509906292
I0305 13:56:32.611707 140283577931520 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.060459449887275696, loss=0.032469529658555984
I0305 13:57:05.263160 140381878482688 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.04704563319683075, loss=0.028957555070519447
I0305 13:57:38.262117 140283577931520 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04392746463418007, loss=0.03218189999461174
I0305 13:58:10.644843 140381878482688 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.0744742676615715, loss=0.030599163845181465
I0305 13:58:42.873075 140283577931520 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.06095021590590477, loss=0.02929881401360035
I0305 13:59:15.620609 140381878482688 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.06599119305610657, loss=0.0290821623057127
I0305 13:59:44.641364 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:01:46.116517 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:01:49.192732 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:01:52.191198 140444430841664 submission_runner.py:411] Time since start: 13837.81s, 	Step: 26590, 	{'train/accuracy': 0.9915059804916382, 'train/loss': 0.02787797525525093, 'train/mean_average_precision': 0.45282565462978963, 'validation/accuracy': 0.9869843125343323, 'validation/loss': 0.044001273810863495, 'validation/mean_average_precision': 0.2870869938781617, 'validation/num_examples': 43793, 'test/accuracy': 0.986127495765686, 'test/loss': 0.046985313296318054, 'test/mean_average_precision': 0.26823349837840704, 'test/num_examples': 43793, 'score': 8662.952923297882, 'total_duration': 13837.806899309158, 'accumulated_submission_time': 8662.952923297882, 'accumulated_eval_time': 5172.9710857868195, 'accumulated_logging_time': 1.0846607685089111}
I0305 14:01:52.211577 140277079156480 logging_writer.py:48] [26590] accumulated_eval_time=5172.971086, accumulated_logging_time=1.084661, accumulated_submission_time=8662.952923, global_step=26590, preemption_count=0, score=8662.952923, test/accuracy=0.986127, test/loss=0.046985, test/mean_average_precision=0.268233, test/num_examples=43793, total_duration=13837.806899, train/accuracy=0.991506, train/loss=0.027878, train/mean_average_precision=0.452826, validation/accuracy=0.986984, validation/loss=0.044001, validation/mean_average_precision=0.287087, validation/num_examples=43793
I0305 14:01:55.839958 140283569538816 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.0471317283809185, loss=0.027756204828619957
I0305 14:02:28.577390 140277079156480 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.05777774751186371, loss=0.031059760600328445
I0305 14:03:00.732156 140283569538816 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.06313960999250412, loss=0.029495269060134888
I0305 14:03:32.937955 140277079156480 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.050492651760578156, loss=0.030610905960202217
I0305 14:04:05.697237 140283569538816 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.07147639244794846, loss=0.029315246269106865
I0305 14:04:37.994717 140277079156480 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.0481540821492672, loss=0.02894728258252144
I0305 14:05:10.816007 140283569538816 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.04955519735813141, loss=0.029444877058267593
I0305 14:05:43.370857 140277079156480 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05410589650273323, loss=0.028002727776765823
I0305 14:05:52.450339 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:07:53.605746 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:07:56.985404 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:08:00.414704 140444430841664 submission_runner.py:411] Time since start: 14206.03s, 	Step: 27329, 	{'train/accuracy': 0.991453230381012, 'train/loss': 0.027622630819678307, 'train/mean_average_precision': 0.4781773408012578, 'validation/accuracy': 0.9870342016220093, 'validation/loss': 0.0437435507774353, 'validation/mean_average_precision': 0.29145533012031893, 'validation/num_examples': 43793, 'test/accuracy': 0.9861692190170288, 'test/loss': 0.04656269773840904, 'test/mean_average_precision': 0.27768227532223655, 'test/num_examples': 43793, 'score': 8903.157333612442, 'total_duration': 14206.030374526978, 'accumulated_submission_time': 8903.157333612442, 'accumulated_eval_time': 5300.935266256332, 'accumulated_logging_time': 1.1179378032684326}
I0305 14:08:00.436983 140275669853952 logging_writer.py:48] [27329] accumulated_eval_time=5300.935266, accumulated_logging_time=1.117938, accumulated_submission_time=8903.157334, global_step=27329, preemption_count=0, score=8903.157334, test/accuracy=0.986169, test/loss=0.046563, test/mean_average_precision=0.277682, test/num_examples=43793, total_duration=14206.030375, train/accuracy=0.991453, train/loss=0.027623, train/mean_average_precision=0.478177, validation/accuracy=0.987034, validation/loss=0.043744, validation/mean_average_precision=0.291455, validation/num_examples=43793
I0305 14:08:24.533817 140283577931520 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.06980700045824051, loss=0.03268430754542351
I0305 14:08:57.468400 140275669853952 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06186959892511368, loss=0.029370345175266266
I0305 14:09:30.135577 140283577931520 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.06707615405321121, loss=0.03148943930864334
I0305 14:10:02.606631 140275669853952 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.06569907814264297, loss=0.03332594782114029
I0305 14:10:34.785676 140283577931520 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.05168496072292328, loss=0.03260989487171173
I0305 14:11:07.272802 140275669853952 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04366132616996765, loss=0.02891557291150093
I0305 14:11:39.455469 140283577931520 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.06265505403280258, loss=0.03125489130616188
I0305 14:12:00.654710 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:14:04.507523 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:14:07.586894 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:14:10.565379 140444430841664 submission_runner.py:411] Time since start: 14576.18s, 	Step: 28067, 	{'train/accuracy': 0.9916055798530579, 'train/loss': 0.027238789945840836, 'train/mean_average_precision': 0.4788237662347861, 'validation/accuracy': 0.9869899749755859, 'validation/loss': 0.043802615255117416, 'validation/mean_average_precision': 0.2826635722662011, 'validation/num_examples': 43793, 'test/accuracy': 0.9861502647399902, 'test/loss': 0.04644906893372536, 'test/mean_average_precision': 0.2764971134633485, 'test/num_examples': 43793, 'score': 9143.340360164642, 'total_duration': 14576.181078672409, 'accumulated_submission_time': 9143.340360164642, 'accumulated_eval_time': 5430.845782995224, 'accumulated_logging_time': 1.1518630981445312}
I0305 14:14:10.585829 140277079156480 logging_writer.py:48] [28067] accumulated_eval_time=5430.845783, accumulated_logging_time=1.151863, accumulated_submission_time=9143.340360, global_step=28067, preemption_count=0, score=9143.340360, test/accuracy=0.986150, test/loss=0.046449, test/mean_average_precision=0.276497, test/num_examples=43793, total_duration=14576.181079, train/accuracy=0.991606, train/loss=0.027239, train/mean_average_precision=0.478824, validation/accuracy=0.986990, validation/loss=0.043803, validation/mean_average_precision=0.282664, validation/num_examples=43793
I0305 14:14:21.623188 140283569538816 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05431589111685753, loss=0.028948292136192322
I0305 14:14:54.227488 140277079156480 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.05472037196159363, loss=0.027950625866651535
I0305 14:15:26.771672 140283569538816 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.056146975606679916, loss=0.03233986720442772
I0305 14:15:58.812783 140277079156480 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.049206919968128204, loss=0.028737952932715416
I0305 14:16:31.033908 140283569538816 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0612441822886467, loss=0.032931216061115265
I0305 14:17:03.270886 140277079156480 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.06192189082503319, loss=0.029635131359100342
I0305 14:17:35.748467 140283569538816 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.07071595638990402, loss=0.03151385858654976
I0305 14:18:08.102841 140277079156480 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.061119139194488525, loss=0.029457103461027145
I0305 14:18:10.647215 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:20:13.502234 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:20:16.574914 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:20:19.582945 140444430841664 submission_runner.py:411] Time since start: 14945.20s, 	Step: 28809, 	{'train/accuracy': 0.991728663444519, 'train/loss': 0.026828888803720474, 'train/mean_average_precision': 0.4913344138715968, 'validation/accuracy': 0.9869307279586792, 'validation/loss': 0.04369683936238289, 'validation/mean_average_precision': 0.2770742675956774, 'validation/num_examples': 43793, 'test/accuracy': 0.9860761165618896, 'test/loss': 0.04629123583436012, 'test/mean_average_precision': 0.27338015242067687, 'test/num_examples': 43793, 'score': 9383.368975162506, 'total_duration': 14945.198653936386, 'accumulated_submission_time': 9383.368975162506, 'accumulated_eval_time': 5559.781363964081, 'accumulated_logging_time': 1.1833415031433105}
I0305 14:20:19.603704 140283577931520 logging_writer.py:48] [28809] accumulated_eval_time=5559.781364, accumulated_logging_time=1.183342, accumulated_submission_time=9383.368975, global_step=28809, preemption_count=0, score=9383.368975, test/accuracy=0.986076, test/loss=0.046291, test/mean_average_precision=0.273380, test/num_examples=43793, total_duration=14945.198654, train/accuracy=0.991729, train/loss=0.026829, train/mean_average_precision=0.491334, validation/accuracy=0.986931, validation/loss=0.043697, validation/mean_average_precision=0.277074, validation/num_examples=43793
I0305 14:20:49.560621 140381878482688 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.04292946308851242, loss=0.02727396972477436
I0305 14:21:21.963595 140283577931520 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.05761662498116493, loss=0.028630301356315613
I0305 14:21:54.595366 140381878482688 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.05322476103901863, loss=0.030688632279634476
I0305 14:22:27.018875 140283577931520 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.07806918770074844, loss=0.030215660110116005
I0305 14:22:58.994920 140381878482688 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.07767598330974579, loss=0.03126046434044838
I0305 14:23:31.280964 140283577931520 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.06523963063955307, loss=0.030941594392061234
I0305 14:24:03.403938 140381878482688 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.06098678335547447, loss=0.030250312760472298
I0305 14:24:19.826864 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:26:21.877260 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:26:24.930780 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:26:27.920460 140444430841664 submission_runner.py:411] Time since start: 15313.54s, 	Step: 29552, 	{'train/accuracy': 0.9918919205665588, 'train/loss': 0.02621607296168804, 'train/mean_average_precision': 0.5069955294043473, 'validation/accuracy': 0.9869245886802673, 'validation/loss': 0.04410369694232941, 'validation/mean_average_precision': 0.28677555014648837, 'validation/num_examples': 43793, 'test/accuracy': 0.9861944913864136, 'test/loss': 0.04671134427189827, 'test/mean_average_precision': 0.2743730009782902, 'test/num_examples': 43793, 'score': 9623.559691429138, 'total_duration': 15313.5361597538, 'accumulated_submission_time': 9623.559691429138, 'accumulated_eval_time': 5687.874805927277, 'accumulated_logging_time': 1.215362310409546}
I0305 14:26:27.942149 140276755326720 logging_writer.py:48] [29552] accumulated_eval_time=5687.874806, accumulated_logging_time=1.215362, accumulated_submission_time=9623.559691, global_step=29552, preemption_count=0, score=9623.559691, test/accuracy=0.986194, test/loss=0.046711, test/mean_average_precision=0.274373, test/num_examples=43793, total_duration=15313.536160, train/accuracy=0.991892, train/loss=0.026216, train/mean_average_precision=0.506996, validation/accuracy=0.986925, validation/loss=0.044104, validation/mean_average_precision=0.286776, validation/num_examples=43793
I0305 14:26:44.440700 140277079156480 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.04548810049891472, loss=0.029785986989736557
I0305 14:27:17.265308 140276755326720 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.057696398347616196, loss=0.0294780470430851
I0305 14:27:49.504259 140277079156480 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.06067106872797012, loss=0.029243076220154762
I0305 14:28:22.174213 140276755326720 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05348418653011322, loss=0.030493373051285744
I0305 14:28:54.266685 140277079156480 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.07671230286359787, loss=0.033673107624053955
I0305 14:29:26.526279 140276755326720 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.050645262002944946, loss=0.02690596505999565
I0305 14:29:58.936106 140277079156480 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.08727322518825531, loss=0.03122146986424923
I0305 14:30:28.138669 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:32:32.978256 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:32:36.037388 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:32:39.051447 140444430841664 submission_runner.py:411] Time since start: 15684.67s, 	Step: 30291, 	{'train/accuracy': 0.9920125007629395, 'train/loss': 0.026100628077983856, 'train/mean_average_precision': 0.5134227238878066, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.0438586100935936, 'validation/mean_average_precision': 0.2853673628324881, 'validation/num_examples': 43793, 'test/accuracy': 0.9861359000205994, 'test/loss': 0.04630785062909126, 'test/mean_average_precision': 0.27347957026623393, 'test/num_examples': 43793, 'score': 9863.722933769226, 'total_duration': 15684.667104959488, 'accumulated_submission_time': 9863.722933769226, 'accumulated_eval_time': 5818.787388086319, 'accumulated_logging_time': 1.248405933380127}
I0305 14:32:39.072575 140283569538816 logging_writer.py:48] [30291] accumulated_eval_time=5818.787388, accumulated_logging_time=1.248406, accumulated_submission_time=9863.722934, global_step=30291, preemption_count=0, score=9863.722934, test/accuracy=0.986136, test/loss=0.046308, test/mean_average_precision=0.273480, test/num_examples=43793, total_duration=15684.667105, train/accuracy=0.992013, train/loss=0.026101, train/mean_average_precision=0.513423, validation/accuracy=0.986856, validation/loss=0.043859, validation/mean_average_precision=0.285367, validation/num_examples=43793
I0305 14:32:42.306015 140381878482688 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.053555678576231, loss=0.03227410465478897
I0305 14:33:14.704112 140283569538816 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.06335422396659851, loss=0.03042740747332573
I0305 14:33:46.853305 140381878482688 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0667211040854454, loss=0.028071174398064613
I0305 14:34:18.884129 140283569538816 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.06084973365068436, loss=0.030137039721012115
I0305 14:34:50.886755 140381878482688 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.07726530730724335, loss=0.02774886041879654
I0305 14:35:23.602326 140283569538816 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.05516152083873749, loss=0.03030272014439106
I0305 14:35:56.056123 140381878482688 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.0625123679637909, loss=0.029689252376556396
I0305 14:36:28.515284 140283569538816 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06276676058769226, loss=0.032212208956480026
I0305 14:36:39.235156 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:38:46.075076 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:38:50.134680 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:38:53.223367 140444430841664 submission_runner.py:411] Time since start: 16058.84s, 	Step: 31034, 	{'train/accuracy': 0.9918270707130432, 'train/loss': 0.02661731466650963, 'train/mean_average_precision': 0.49790427914151714, 'validation/accuracy': 0.9870216250419617, 'validation/loss': 0.04394875094294548, 'validation/mean_average_precision': 0.28877868799504464, 'validation/num_examples': 43793, 'test/accuracy': 0.9862104654312134, 'test/loss': 0.046699363738298416, 'test/mean_average_precision': 0.2768710218968238, 'test/num_examples': 43793, 'score': 10103.851176023483, 'total_duration': 16058.839148044586, 'accumulated_submission_time': 10103.851176023483, 'accumulated_eval_time': 5952.775523900986, 'accumulated_logging_time': 1.2820994853973389}
I0305 14:38:53.246510 140277079156480 logging_writer.py:48] [31034] accumulated_eval_time=5952.775524, accumulated_logging_time=1.282099, accumulated_submission_time=10103.851176, global_step=31034, preemption_count=0, score=10103.851176, test/accuracy=0.986210, test/loss=0.046699, test/mean_average_precision=0.276871, test/num_examples=43793, total_duration=16058.839148, train/accuracy=0.991827, train/loss=0.026617, train/mean_average_precision=0.497904, validation/accuracy=0.987022, validation/loss=0.043949, validation/mean_average_precision=0.288779, validation/num_examples=43793
I0305 14:39:15.219789 140283577931520 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.059433650225400925, loss=0.03087311051785946
I0305 14:39:47.501359 140277079156480 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.053101424127817154, loss=0.02797774225473404
I0305 14:40:20.300920 140283577931520 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.052259963005781174, loss=0.02866814099252224
I0305 14:40:53.008972 140277079156480 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.058642204850912094, loss=0.027674317359924316
I0305 14:41:25.726152 140283577931520 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.05435698479413986, loss=0.027930887416005135
I0305 14:41:58.783468 140277079156480 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.05713387951254845, loss=0.029919086024165154
I0305 14:42:31.970553 140283577931520 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05687238648533821, loss=0.03001183457672596
I0305 14:42:53.413377 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:44:53.803095 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:44:56.853235 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:44:59.893547 140444430841664 submission_runner.py:411] Time since start: 16425.51s, 	Step: 31766, 	{'train/accuracy': 0.9914273023605347, 'train/loss': 0.02756330743432045, 'train/mean_average_precision': 0.47440325071991485, 'validation/accuracy': 0.9870082139968872, 'validation/loss': 0.04413983225822449, 'validation/mean_average_precision': 0.28623787507927667, 'validation/num_examples': 43793, 'test/accuracy': 0.9861763715744019, 'test/loss': 0.04683682695031166, 'test/mean_average_precision': 0.27661051110492435, 'test/num_examples': 43793, 'score': 10343.984036922455, 'total_duration': 16425.5092420578, 'accumulated_submission_time': 10343.984036922455, 'accumulated_eval_time': 6079.2555367946625, 'accumulated_logging_time': 1.3162181377410889}
I0305 14:44:59.916297 140276755326720 logging_writer.py:48] [31766] accumulated_eval_time=6079.255537, accumulated_logging_time=1.316218, accumulated_submission_time=10343.984037, global_step=31766, preemption_count=0, score=10343.984037, test/accuracy=0.986176, test/loss=0.046837, test/mean_average_precision=0.276611, test/num_examples=43793, total_duration=16425.509242, train/accuracy=0.991427, train/loss=0.027563, train/mean_average_precision=0.474403, validation/accuracy=0.987008, validation/loss=0.044140, validation/mean_average_precision=0.286238, validation/num_examples=43793
I0305 14:45:11.408186 140283569538816 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06563806533813477, loss=0.030304497107863426
I0305 14:45:43.914309 140276755326720 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.06204353645443916, loss=0.02973477728664875
I0305 14:46:16.385218 140283569538816 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06803606450557709, loss=0.030236748978495598
I0305 14:46:48.812280 140276755326720 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06523903459310532, loss=0.029649605974555016
I0305 14:47:21.283991 140283569538816 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05775637552142143, loss=0.02510833367705345
I0305 14:47:53.410544 140276755326720 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.06925821304321289, loss=0.028238123282790184
I0305 14:48:26.263913 140283569538816 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.05110345035791397, loss=0.026447558775544167
I0305 14:48:59.200750 140276755326720 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05716855078935623, loss=0.028609950095415115
I0305 14:49:00.206397 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:51:01.216597 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:51:04.446810 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:51:07.451366 140444430841664 submission_runner.py:411] Time since start: 16793.07s, 	Step: 32504, 	{'train/accuracy': 0.9916889071464539, 'train/loss': 0.026959314942359924, 'train/mean_average_precision': 0.4850293929760725, 'validation/accuracy': 0.9868718385696411, 'validation/loss': 0.04421563819050789, 'validation/mean_average_precision': 0.286878314432203, 'validation/num_examples': 43793, 'test/accuracy': 0.9860592484474182, 'test/loss': 0.04674769192934036, 'test/mean_average_precision': 0.27414416958987875, 'test/num_examples': 43793, 'score': 10584.240829467773, 'total_duration': 16793.06715464592, 'accumulated_submission_time': 10584.240829467773, 'accumulated_eval_time': 6206.500435352325, 'accumulated_logging_time': 1.350158452987671}
I0305 14:51:07.473234 140275669853952 logging_writer.py:48] [32504] accumulated_eval_time=6206.500435, accumulated_logging_time=1.350158, accumulated_submission_time=10584.240829, global_step=32504, preemption_count=0, score=10584.240829, test/accuracy=0.986059, test/loss=0.046748, test/mean_average_precision=0.274144, test/num_examples=43793, total_duration=16793.067155, train/accuracy=0.991689, train/loss=0.026959, train/mean_average_precision=0.485029, validation/accuracy=0.986872, validation/loss=0.044216, validation/mean_average_precision=0.286878, validation/num_examples=43793
I0305 14:51:38.853923 140277079156480 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06854026764631271, loss=0.02790742926299572
I0305 14:52:11.235215 140275669853952 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.06504467874765396, loss=0.029110942035913467
I0305 14:52:43.869560 140277079156480 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.06391707807779312, loss=0.02666165865957737
I0305 14:53:16.461265 140275669853952 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.07774432748556137, loss=0.031925540417432785
I0305 14:53:49.145438 140277079156480 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05915215611457825, loss=0.02971704863011837
I0305 14:54:21.947911 140275669853952 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.06600633263587952, loss=0.027678733691573143
I0305 14:54:54.076836 140277079156480 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.06374941021203995, loss=0.026697253808379173
I0305 14:55:07.687693 140444430841664 spec.py:321] Evaluating on the training split.
I0305 14:57:12.036702 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 14:57:15.064334 140444430841664 spec.py:349] Evaluating on the test split.
I0305 14:57:18.075217 140444430841664 submission_runner.py:411] Time since start: 17163.69s, 	Step: 33243, 	{'train/accuracy': 0.9917634129524231, 'train/loss': 0.02650214545428753, 'train/mean_average_precision': 0.5059145808603558, 'validation/accuracy': 0.9871076941490173, 'validation/loss': 0.0442456379532814, 'validation/mean_average_precision': 0.2891483056162617, 'validation/num_examples': 43793, 'test/accuracy': 0.9863191246986389, 'test/loss': 0.046929921954870224, 'test/mean_average_precision': 0.27460019575863404, 'test/num_examples': 43793, 'score': 10824.421991825104, 'total_duration': 17163.691017866135, 'accumulated_submission_time': 10824.421991825104, 'accumulated_eval_time': 6336.8879063129425, 'accumulated_logging_time': 1.3831946849822998}
I0305 14:57:18.098103 140283569538816 logging_writer.py:48] [33243] accumulated_eval_time=6336.887906, accumulated_logging_time=1.383195, accumulated_submission_time=10824.421992, global_step=33243, preemption_count=0, score=10824.421992, test/accuracy=0.986319, test/loss=0.046930, test/mean_average_precision=0.274600, test/num_examples=43793, total_duration=17163.691018, train/accuracy=0.991763, train/loss=0.026502, train/mean_average_precision=0.505915, validation/accuracy=0.987108, validation/loss=0.044246, validation/mean_average_precision=0.289148, validation/num_examples=43793
I0305 14:57:36.637136 140283577931520 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.07029752433300018, loss=0.03109470009803772
I0305 14:58:09.496032 140283569538816 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05922291427850723, loss=0.028169946745038033
I0305 14:58:41.917290 140283577931520 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05166854336857796, loss=0.02794232778251171
I0305 14:59:14.420208 140283569538816 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.07246989011764526, loss=0.027926001697778702
I0305 14:59:46.849289 140283577931520 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.07079577445983887, loss=0.031481143087148666
I0305 15:00:19.509245 140283569538816 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05082739517092705, loss=0.028522243723273277
I0305 15:00:53.046105 140283577931520 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.05745084211230278, loss=0.027754316106438637
I0305 15:01:18.390499 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:03:21.496574 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:03:24.561093 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:03:27.536785 140444430841664 submission_runner.py:411] Time since start: 17533.15s, 	Step: 33978, 	{'train/accuracy': 0.9919225573539734, 'train/loss': 0.026062684133648872, 'train/mean_average_precision': 0.5149891537773273, 'validation/accuracy': 0.9869899749755859, 'validation/loss': 0.043880894780159, 'validation/mean_average_precision': 0.2891141455523475, 'validation/num_examples': 43793, 'test/accuracy': 0.9861114621162415, 'test/loss': 0.04667436331510544, 'test/mean_average_precision': 0.2715926387695952, 'test/num_examples': 43793, 'score': 11064.680742740631, 'total_duration': 17533.152592658997, 'accumulated_submission_time': 11064.680742740631, 'accumulated_eval_time': 6466.034151554108, 'accumulated_logging_time': 1.4176712036132812}
I0305 15:03:27.558720 140276755326720 logging_writer.py:48] [33978] accumulated_eval_time=6466.034152, accumulated_logging_time=1.417671, accumulated_submission_time=11064.680743, global_step=33978, preemption_count=0, score=11064.680743, test/accuracy=0.986111, test/loss=0.046674, test/mean_average_precision=0.271593, test/num_examples=43793, total_duration=17533.152593, train/accuracy=0.991923, train/loss=0.026063, train/mean_average_precision=0.514989, validation/accuracy=0.986990, validation/loss=0.043881, validation/mean_average_precision=0.289114, validation/num_examples=43793
I0305 15:03:35.238859 140277079156480 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0652027428150177, loss=0.026170331984758377
I0305 15:04:07.981298 140276755326720 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06999079138040543, loss=0.029783131554722786
I0305 15:04:40.194689 140277079156480 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.05899123474955559, loss=0.026247242465615273
I0305 15:05:13.161670 140276755326720 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.0639525055885315, loss=0.028357572853565216
I0305 15:05:45.179991 140277079156480 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.055637460201978683, loss=0.025694092735648155
I0305 15:06:17.651450 140276755326720 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06913534551858902, loss=0.026957686990499496
I0305 15:06:50.292757 140277079156480 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06542651355266571, loss=0.03041650913655758
I0305 15:07:22.982599 140276755326720 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06034644693136215, loss=0.026783719658851624
I0305 15:07:27.837187 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:09:30.198920 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:09:33.308968 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:09:36.345879 140444430841664 submission_runner.py:411] Time since start: 17901.96s, 	Step: 34716, 	{'train/accuracy': 0.9920569658279419, 'train/loss': 0.02563641220331192, 'train/mean_average_precision': 0.5100176386259829, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.04440421238541603, 'validation/mean_average_precision': 0.28472198622957406, 'validation/num_examples': 43793, 'test/accuracy': 0.9860807657241821, 'test/loss': 0.047213103622198105, 'test/mean_average_precision': 0.2721279982617621, 'test/num_examples': 43793, 'score': 11304.925557374954, 'total_duration': 17901.961685180664, 'accumulated_submission_time': 11304.925557374954, 'accumulated_eval_time': 6594.542794704437, 'accumulated_logging_time': 1.4511137008666992}
I0305 15:09:36.368160 140275669853952 logging_writer.py:48] [34716] accumulated_eval_time=6594.542795, accumulated_logging_time=1.451114, accumulated_submission_time=11304.925557, global_step=34716, preemption_count=0, score=11304.925557, test/accuracy=0.986081, test/loss=0.047213, test/mean_average_precision=0.272128, test/num_examples=43793, total_duration=17901.961685, train/accuracy=0.992057, train/loss=0.025636, train/mean_average_precision=0.510018, validation/accuracy=0.986974, validation/loss=0.044404, validation/mean_average_precision=0.284722, validation/num_examples=43793
I0305 15:10:04.436041 140283577931520 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06521028280258179, loss=0.026029786095023155
I0305 15:10:37.035930 140275669853952 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06650757789611816, loss=0.027027811855077744
I0305 15:11:09.795843 140283577931520 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.059236954897642136, loss=0.027666078880429268
I0305 15:11:42.525533 140275669853952 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.061971958726644516, loss=0.02930016629397869
I0305 15:12:15.333580 140283577931520 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.06974930316209793, loss=0.03223099187016487
I0305 15:12:48.120511 140275669853952 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05911707878112793, loss=0.024573206901550293
I0305 15:13:20.956866 140283577931520 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05813658982515335, loss=0.028652265667915344
I0305 15:13:36.441344 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:15:40.406908 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:15:43.479584 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:15:46.482200 140444430841664 submission_runner.py:411] Time since start: 18272.10s, 	Step: 35448, 	{'train/accuracy': 0.9924238920211792, 'train/loss': 0.024371912702918053, 'train/mean_average_precision': 0.5482731850412265, 'validation/accuracy': 0.9869936108589172, 'validation/loss': 0.04431796446442604, 'validation/mean_average_precision': 0.2857833668199892, 'validation/num_examples': 43793, 'test/accuracy': 0.9861615896224976, 'test/loss': 0.047021303325891495, 'test/mean_average_precision': 0.2708743413208484, 'test/num_examples': 43793, 'score': 11544.964567899704, 'total_duration': 18272.09800696373, 'accumulated_submission_time': 11544.964567899704, 'accumulated_eval_time': 6724.5836000442505, 'accumulated_logging_time': 1.4851961135864258}
I0305 15:15:46.504479 140276755326720 logging_writer.py:48] [35448] accumulated_eval_time=6724.583600, accumulated_logging_time=1.485196, accumulated_submission_time=11544.964568, global_step=35448, preemption_count=0, score=11544.964568, test/accuracy=0.986162, test/loss=0.047021, test/mean_average_precision=0.270874, test/num_examples=43793, total_duration=18272.098007, train/accuracy=0.992424, train/loss=0.024372, train/mean_average_precision=0.548273, validation/accuracy=0.986994, validation/loss=0.044318, validation/mean_average_precision=0.285783, validation/num_examples=43793
I0305 15:16:03.858509 140283569538816 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.05931800231337547, loss=0.028168799355626106
I0305 15:16:36.447391 140276755326720 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06195753440260887, loss=0.030575091019272804
I0305 15:17:09.160352 140283569538816 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06735210865736008, loss=0.02706841751933098
I0305 15:17:41.557212 140276755326720 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.0667300596833229, loss=0.03245551884174347
I0305 15:18:14.495262 140283569538816 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.054479941725730896, loss=0.02675636112689972
I0305 15:18:46.607691 140276755326720 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.06389108300209045, loss=0.027130136266350746
I0305 15:19:18.820280 140283569538816 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06147265061736107, loss=0.029260210692882538
I0305 15:19:46.550294 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:21:49.189040 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:21:52.224660 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:21:55.475682 140444430841664 submission_runner.py:411] Time since start: 18641.09s, 	Step: 36186, 	{'train/accuracy': 0.9922419190406799, 'train/loss': 0.024927008897066116, 'train/mean_average_precision': 0.5376483588503354, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.04440237209200859, 'validation/mean_average_precision': 0.2826900278886544, 'validation/num_examples': 43793, 'test/accuracy': 0.9862930178642273, 'test/loss': 0.047236256301403046, 'test/mean_average_precision': 0.2747061458046834, 'test/num_examples': 43793, 'score': 11784.975558757782, 'total_duration': 18641.091474056244, 'accumulated_submission_time': 11784.975558757782, 'accumulated_eval_time': 6853.508927345276, 'accumulated_logging_time': 1.5202860832214355}
I0305 15:21:55.499809 140275669853952 logging_writer.py:48] [36186] accumulated_eval_time=6853.508927, accumulated_logging_time=1.520286, accumulated_submission_time=11784.975559, global_step=36186, preemption_count=0, score=11784.975559, test/accuracy=0.986293, test/loss=0.047236, test/mean_average_precision=0.274706, test/num_examples=43793, total_duration=18641.091474, train/accuracy=0.992242, train/loss=0.024927, train/mean_average_precision=0.537648, validation/accuracy=0.987043, validation/loss=0.044402, validation/mean_average_precision=0.282690, validation/num_examples=43793
I0305 15:22:00.509038 140283577931520 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06669425219297409, loss=0.0305576603859663
I0305 15:22:33.612501 140275669853952 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06979302316904068, loss=0.02864271029829979
I0305 15:23:06.467715 140283577931520 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.05825292691588402, loss=0.02556205913424492
I0305 15:23:39.190131 140275669853952 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.07132311910390854, loss=0.0242740660905838
I0305 15:24:11.755407 140283577931520 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06176630035042763, loss=0.030092289671301842
I0305 15:24:43.798605 140275669853952 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.08717095106840134, loss=0.026164086535573006
I0305 15:25:16.276757 140283577931520 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0762578472495079, loss=0.030327122658491135
I0305 15:25:48.621573 140275669853952 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06053757667541504, loss=0.029624879360198975
I0305 15:25:55.644745 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:27:56.625175 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:27:59.690579 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:28:02.869176 140444430841664 submission_runner.py:411] Time since start: 19008.48s, 	Step: 36923, 	{'train/accuracy': 0.9921140670776367, 'train/loss': 0.025393487885594368, 'train/mean_average_precision': 0.519401692550248, 'validation/accuracy': 0.9870488047599792, 'validation/loss': 0.04401364549994469, 'validation/mean_average_precision': 0.28588758446808393, 'validation/num_examples': 43793, 'test/accuracy': 0.9862037301063538, 'test/loss': 0.04686715826392174, 'test/mean_average_precision': 0.2786638609715531, 'test/num_examples': 43793, 'score': 12025.085746765137, 'total_duration': 19008.48497748375, 'accumulated_submission_time': 12025.085746765137, 'accumulated_eval_time': 6980.73330283165, 'accumulated_logging_time': 1.5558314323425293}
I0305 15:28:02.893798 140276755326720 logging_writer.py:48] [36923] accumulated_eval_time=6980.733303, accumulated_logging_time=1.555831, accumulated_submission_time=12025.085747, global_step=36923, preemption_count=0, score=12025.085747, test/accuracy=0.986204, test/loss=0.046867, test/mean_average_precision=0.278664, test/num_examples=43793, total_duration=19008.484977, train/accuracy=0.992114, train/loss=0.025393, train/mean_average_precision=0.519402, validation/accuracy=0.987049, validation/loss=0.044014, validation/mean_average_precision=0.285888, validation/num_examples=43793
I0305 15:28:27.904692 140283569538816 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.056201331317424774, loss=0.02668371982872486
I0305 15:28:59.990347 140276755326720 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.05577996373176575, loss=0.026795687153935432
I0305 15:29:32.239229 140283569538816 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06282191723585129, loss=0.02844434790313244
I0305 15:30:04.634887 140276755326720 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.0679078921675682, loss=0.02680991031229496
I0305 15:30:37.111642 140283569538816 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.07032935321331024, loss=0.02885621041059494
I0305 15:31:09.482690 140276755326720 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0684124007821083, loss=0.028837399557232857
I0305 15:31:41.672244 140283569538816 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06863018870353699, loss=0.026749001815915108
I0305 15:32:03.076106 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:34:05.368789 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:34:08.743747 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:34:12.171356 140444430841664 submission_runner.py:411] Time since start: 19377.79s, 	Step: 37667, 	{'train/accuracy': 0.9920461177825928, 'train/loss': 0.025555802509188652, 'train/mean_average_precision': 0.5183719989880958, 'validation/accuracy': 0.9870699644088745, 'validation/loss': 0.044741440564394, 'validation/mean_average_precision': 0.28286801611528795, 'validation/num_examples': 43793, 'test/accuracy': 0.9862450361251831, 'test/loss': 0.047635842114686966, 'test/mean_average_precision': 0.27484689170873, 'test/num_examples': 43793, 'score': 12265.234405755997, 'total_duration': 19377.787124872208, 'accumulated_submission_time': 12265.234405755997, 'accumulated_eval_time': 7109.82846736908, 'accumulated_logging_time': 1.5917348861694336}
I0305 15:34:12.197889 140275669853952 logging_writer.py:48] [37667] accumulated_eval_time=7109.828467, accumulated_logging_time=1.591735, accumulated_submission_time=12265.234406, global_step=37667, preemption_count=0, score=12265.234406, test/accuracy=0.986245, test/loss=0.047636, test/mean_average_precision=0.274847, test/num_examples=43793, total_duration=19377.787125, train/accuracy=0.992046, train/loss=0.025556, train/mean_average_precision=0.518372, validation/accuracy=0.987070, validation/loss=0.044741, validation/mean_average_precision=0.282868, validation/num_examples=43793
I0305 15:34:23.529847 140283577931520 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06661282479763031, loss=0.028385203331708908
I0305 15:34:56.615453 140275669853952 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.05767669528722763, loss=0.027624865993857384
I0305 15:35:29.168624 140283577931520 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06637178361415863, loss=0.027377016842365265
I0305 15:36:02.325281 140275669853952 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07305621355772018, loss=0.02765887789428234
I0305 15:36:34.794541 140283577931520 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06419569253921509, loss=0.026910768821835518
I0305 15:37:07.230949 140275669853952 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06311846524477005, loss=0.025204356759786606
I0305 15:37:39.259496 140283577931520 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06641064584255219, loss=0.02668888494372368
I0305 15:38:11.576035 140275669853952 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.09261396527290344, loss=0.02750207670032978
I0305 15:38:12.259357 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:40:15.494960 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:40:18.833472 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:40:22.147036 140444430841664 submission_runner.py:411] Time since start: 19747.76s, 	Step: 38403, 	{'train/accuracy': 0.9921436905860901, 'train/loss': 0.025332104414701462, 'train/mean_average_precision': 0.517584729671504, 'validation/accuracy': 0.9869266152381897, 'validation/loss': 0.04449214041233063, 'validation/mean_average_precision': 0.2914535596647501, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.047133319079875946, 'test/mean_average_precision': 0.2745749674200974, 'test/num_examples': 43793, 'score': 12505.260452270508, 'total_duration': 19747.76282644272, 'accumulated_submission_time': 12505.260452270508, 'accumulated_eval_time': 7239.7160794734955, 'accumulated_logging_time': 1.6303064823150635}
I0305 15:40:22.173366 140276755326720 logging_writer.py:48] [38403] accumulated_eval_time=7239.716079, accumulated_logging_time=1.630306, accumulated_submission_time=12505.260452, global_step=38403, preemption_count=0, score=12505.260452, test/accuracy=0.986218, test/loss=0.047133, test/mean_average_precision=0.274575, test/num_examples=43793, total_duration=19747.762826, train/accuracy=0.992144, train/loss=0.025332, train/mean_average_precision=0.517585, validation/accuracy=0.986927, validation/loss=0.044492, validation/mean_average_precision=0.291454, validation/num_examples=43793
I0305 15:40:55.224418 140277079156480 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07281934469938278, loss=0.029265375807881355
I0305 15:41:28.299221 140276755326720 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06076996773481369, loss=0.029486242681741714
I0305 15:42:01.058741 140277079156480 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.05893048644065857, loss=0.027284318581223488
I0305 15:42:34.182019 140276755326720 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06997966766357422, loss=0.027865968644618988
I0305 15:43:07.395694 140277079156480 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06940636783838272, loss=0.02595006301999092
I0305 15:43:39.913041 140276755326720 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.06715716421604156, loss=0.02593333274126053
I0305 15:44:12.704963 140277079156480 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.07136889547109604, loss=0.026188911870121956
I0305 15:44:22.372060 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:46:24.750588 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:46:27.806928 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:46:30.912701 140444430841664 submission_runner.py:411] Time since start: 20116.53s, 	Step: 39131, 	{'train/accuracy': 0.9921480417251587, 'train/loss': 0.024907197803258896, 'train/mean_average_precision': 0.5369891992399025, 'validation/accuracy': 0.9869238138198853, 'validation/loss': 0.044499997049570084, 'validation/mean_average_precision': 0.28554833624418735, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.047154612839221954, 'test/mean_average_precision': 0.27555972277740604, 'test/num_examples': 43793, 'score': 12745.421908378601, 'total_duration': 20116.52850627899, 'accumulated_submission_time': 12745.421908378601, 'accumulated_eval_time': 7368.2566702365875, 'accumulated_logging_time': 1.6696994304656982}
I0305 15:46:30.935726 140275669853952 logging_writer.py:48] [39131] accumulated_eval_time=7368.256670, accumulated_logging_time=1.669699, accumulated_submission_time=12745.421908, global_step=39131, preemption_count=0, score=12745.421908, test/accuracy=0.986159, test/loss=0.047155, test/mean_average_precision=0.275560, test/num_examples=43793, total_duration=20116.528506, train/accuracy=0.992148, train/loss=0.024907, train/mean_average_precision=0.536989, validation/accuracy=0.986924, validation/loss=0.044500, validation/mean_average_precision=0.285548, validation/num_examples=43793
I0305 15:46:54.072578 140283577931520 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.061301302164793015, loss=0.0252072736620903
I0305 15:47:26.915533 140275669853952 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.06857027858495712, loss=0.026034103706479073
I0305 15:47:59.452235 140283577931520 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.07574130594730377, loss=0.027394786477088928
I0305 15:48:32.137305 140275669853952 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.071380615234375, loss=0.025600912049412727
I0305 15:49:04.671440 140283577931520 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.06190527603030205, loss=0.025860745459794998
I0305 15:49:37.354399 140275669853952 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07124397158622742, loss=0.027394119650125504
I0305 15:50:10.115917 140283577931520 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.07613097131252289, loss=0.024224374443292618
I0305 15:50:31.161577 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:52:28.742554 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:52:31.773728 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:52:34.779056 140444430841664 submission_runner.py:411] Time since start: 20480.39s, 	Step: 39866, 	{'train/accuracy': 0.9923309087753296, 'train/loss': 0.024495139718055725, 'train/mean_average_precision': 0.534551486086762, 'validation/accuracy': 0.9868633151054382, 'validation/loss': 0.04431247338652611, 'validation/mean_average_precision': 0.2802457208483463, 'validation/num_examples': 43793, 'test/accuracy': 0.9861182570457458, 'test/loss': 0.046987347304821014, 'test/mean_average_precision': 0.26863042306134033, 'test/num_examples': 43793, 'score': 12985.612907409668, 'total_duration': 20480.394863128662, 'accumulated_submission_time': 12985.612907409668, 'accumulated_eval_time': 7491.874098777771, 'accumulated_logging_time': 1.7054214477539062}
I0305 15:52:34.802326 140276755326720 logging_writer.py:48] [39866] accumulated_eval_time=7491.874099, accumulated_logging_time=1.705421, accumulated_submission_time=12985.612907, global_step=39866, preemption_count=0, score=12985.612907, test/accuracy=0.986118, test/loss=0.046987, test/mean_average_precision=0.268630, test/num_examples=43793, total_duration=20480.394863, train/accuracy=0.992331, train/loss=0.024495, train/mean_average_precision=0.534551, validation/accuracy=0.986863, validation/loss=0.044312, validation/mean_average_precision=0.280246, validation/num_examples=43793
I0305 15:52:46.437939 140283569538816 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.06913229078054428, loss=0.02905752696096897
I0305 15:53:19.224179 140276755326720 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07955408096313477, loss=0.02821698784828186
I0305 15:53:52.060372 140283569538816 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06940323859453201, loss=0.02609969861805439
I0305 15:54:24.666129 140276755326720 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06776529550552368, loss=0.025994066148996353
I0305 15:54:57.267935 140283569538816 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.0804450586438179, loss=0.02585870772600174
I0305 15:55:30.088697 140276755326720 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.0747198536992073, loss=0.027889372780919075
I0305 15:56:02.466040 140283569538816 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0618303045630455, loss=0.024387259036302567
I0305 15:56:34.938951 140444430841664 spec.py:321] Evaluating on the training split.
I0305 15:58:46.265215 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 15:58:49.739102 140444430841664 spec.py:349] Evaluating on the test split.
I0305 15:58:53.148756 140444430841664 submission_runner.py:411] Time since start: 20858.76s, 	Step: 40600, 	{'train/accuracy': 0.9924687743186951, 'train/loss': 0.024013055488467216, 'train/mean_average_precision': 0.5691049705818645, 'validation/accuracy': 0.9870585799217224, 'validation/loss': 0.04448807239532471, 'validation/mean_average_precision': 0.2891847364191049, 'validation/num_examples': 43793, 'test/accuracy': 0.9862959980964661, 'test/loss': 0.04726317524909973, 'test/mean_average_precision': 0.2766268806489722, 'test/num_examples': 43793, 'score': 13225.71517944336, 'total_duration': 20858.764546394348, 'accumulated_submission_time': 13225.71517944336, 'accumulated_eval_time': 7630.08384180069, 'accumulated_logging_time': 1.7412102222442627}
I0305 15:58:53.175691 140275669853952 logging_writer.py:48] [40600] accumulated_eval_time=7630.083842, accumulated_logging_time=1.741210, accumulated_submission_time=13225.715179, global_step=40600, preemption_count=0, score=13225.715179, test/accuracy=0.986296, test/loss=0.047263, test/mean_average_precision=0.276627, test/num_examples=43793, total_duration=20858.764546, train/accuracy=0.992469, train/loss=0.024013, train/mean_average_precision=0.569105, validation/accuracy=0.987059, validation/loss=0.044488, validation/mean_average_precision=0.289185, validation/num_examples=43793
I0305 15:58:53.539723 140283577931520 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.07501963526010513, loss=0.02783416211605072
I0305 15:59:25.966361 140275669853952 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.06969476491212845, loss=0.026666676625609398
I0305 15:59:58.340705 140283577931520 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07910040020942688, loss=0.030018575489521027
I0305 16:00:30.588010 140275669853952 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.08323638141155243, loss=0.028148455545306206
I0305 16:01:03.072039 140283577931520 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0655069425702095, loss=0.028826791793107986
I0305 16:01:35.619890 140275669853952 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07341333478689194, loss=0.03125033527612686
I0305 16:02:08.207476 140283577931520 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.07007932662963867, loss=0.024472467601299286
I0305 16:02:40.054799 140275669853952 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.06534086167812347, loss=0.02670356258749962
I0305 16:02:53.260823 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:04:53.931438 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:04:56.999037 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:05:00.026895 140444430841664 submission_runner.py:411] Time since start: 21225.64s, 	Step: 41341, 	{'train/accuracy': 0.9926599860191345, 'train/loss': 0.02334391139447689, 'train/mean_average_precision': 0.5782135633249319, 'validation/accuracy': 0.9871352910995483, 'validation/loss': 0.04468785226345062, 'validation/mean_average_precision': 0.2936668428755002, 'validation/num_examples': 43793, 'test/accuracy': 0.986275315284729, 'test/loss': 0.04762033000588417, 'test/mean_average_precision': 0.2830297049350724, 'test/num_examples': 43793, 'score': 13465.765576124191, 'total_duration': 21225.642529010773, 'accumulated_submission_time': 13465.765576124191, 'accumulated_eval_time': 7756.8496968746185, 'accumulated_logging_time': 1.7805137634277344}
I0305 16:05:00.050632 140277079156480 logging_writer.py:48] [41341] accumulated_eval_time=7756.849697, accumulated_logging_time=1.780514, accumulated_submission_time=13465.765576, global_step=41341, preemption_count=0, score=13465.765576, test/accuracy=0.986275, test/loss=0.047620, test/mean_average_precision=0.283030, test/num_examples=43793, total_duration=21225.642529, train/accuracy=0.992660, train/loss=0.023344, train/mean_average_precision=0.578214, validation/accuracy=0.987135, validation/loss=0.044688, validation/mean_average_precision=0.293667, validation/num_examples=43793
I0305 16:05:19.406263 140283569538816 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.06531722843647003, loss=0.024808600544929504
I0305 16:05:52.076133 140277079156480 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07522235810756683, loss=0.025515945628285408
I0305 16:06:24.742370 140283569538816 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.0721784383058548, loss=0.024750322103500366
I0305 16:06:57.544946 140277079156480 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.06738723814487457, loss=0.029361478984355927
I0305 16:07:29.740326 140283569538816 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07167265564203262, loss=0.0294995978474617
I0305 16:08:01.742352 140277079156480 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.08603696525096893, loss=0.02881453186273575
I0305 16:08:34.137936 140283569538816 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.06155414134263992, loss=0.025082791224122047
I0305 16:09:00.113953 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:11:00.748295 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:11:03.994619 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:11:07.008214 140444430841664 submission_runner.py:411] Time since start: 21592.62s, 	Step: 42081, 	{'train/accuracy': 0.9928129315376282, 'train/loss': 0.02295875735580921, 'train/mean_average_precision': 0.5814897734017909, 'validation/accuracy': 0.987063467502594, 'validation/loss': 0.04445253312587738, 'validation/mean_average_precision': 0.2931879108949782, 'validation/num_examples': 43793, 'test/accuracy': 0.9861797094345093, 'test/loss': 0.04738381505012512, 'test/mean_average_precision': 0.2803766791096357, 'test/num_examples': 43793, 'score': 13705.795667171478, 'total_duration': 21592.62402153015, 'accumulated_submission_time': 13705.795667171478, 'accumulated_eval_time': 7883.743916749954, 'accumulated_logging_time': 1.8158748149871826}
I0305 16:11:07.031992 140275669853952 logging_writer.py:48] [42081] accumulated_eval_time=7883.743917, accumulated_logging_time=1.815875, accumulated_submission_time=13705.795667, global_step=42081, preemption_count=0, score=13705.795667, test/accuracy=0.986180, test/loss=0.047384, test/mean_average_precision=0.280377, test/num_examples=43793, total_duration=21592.624022, train/accuracy=0.992813, train/loss=0.022959, train/mean_average_precision=0.581490, validation/accuracy=0.987063, validation/loss=0.044453, validation/mean_average_precision=0.293188, validation/num_examples=43793
I0305 16:11:13.735768 140283577931520 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.06127722188830376, loss=0.02270221710205078
I0305 16:11:46.634280 140275669853952 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07475506514310837, loss=0.026535097509622574
I0305 16:12:19.371522 140283577931520 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.06487838178873062, loss=0.024738479405641556
I0305 16:12:51.506378 140275669853952 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.0965244248509407, loss=0.02719283476471901
I0305 16:13:24.082705 140283577931520 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07003982365131378, loss=0.025718431919813156
I0305 16:13:56.418781 140275669853952 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06755434721708298, loss=0.024674415588378906
I0305 16:14:28.785190 140283577931520 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.07048828899860382, loss=0.024273300543427467
I0305 16:15:01.438830 140275669853952 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07778752595186234, loss=0.027831992134451866
I0305 16:15:07.176154 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:17:08.587996 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:17:11.605065 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:17:14.632407 140444430841664 submission_runner.py:411] Time since start: 21960.25s, 	Step: 42818, 	{'train/accuracy': 0.9925578236579895, 'train/loss': 0.02380332164466381, 'train/mean_average_precision': 0.564259724336508, 'validation/accuracy': 0.9869980812072754, 'validation/loss': 0.0447949543595314, 'validation/mean_average_precision': 0.2879867390854697, 'validation/num_examples': 43793, 'test/accuracy': 0.9862193465232849, 'test/loss': 0.04748883843421936, 'test/mean_average_precision': 0.2773864421082346, 'test/num_examples': 43793, 'score': 13945.906858921051, 'total_duration': 21960.248212337494, 'accumulated_submission_time': 13945.906858921051, 'accumulated_eval_time': 8011.200119256973, 'accumulated_logging_time': 1.8510291576385498}
I0305 16:17:14.656210 140276755326720 logging_writer.py:48] [42818] accumulated_eval_time=8011.200119, accumulated_logging_time=1.851029, accumulated_submission_time=13945.906859, global_step=42818, preemption_count=0, score=13945.906859, test/accuracy=0.986219, test/loss=0.047489, test/mean_average_precision=0.277386, test/num_examples=43793, total_duration=21960.248212, train/accuracy=0.992558, train/loss=0.023803, train/mean_average_precision=0.564260, validation/accuracy=0.986998, validation/loss=0.044795, validation/mean_average_precision=0.287987, validation/num_examples=43793
I0305 16:17:42.244344 140277079156480 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08423899859189987, loss=0.02660141885280609
I0305 16:18:15.284922 140276755326720 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0668562799692154, loss=0.02755350060760975
I0305 16:18:48.135195 140277079156480 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.08290913701057434, loss=0.027740247547626495
I0305 16:19:21.166417 140276755326720 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.0671280100941658, loss=0.024216322228312492
I0305 16:19:54.026154 140277079156480 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.08588974922895432, loss=0.02666507102549076
I0305 16:20:26.891642 140276755326720 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.07803671061992645, loss=0.02682868205010891
I0305 16:20:59.320985 140277079156480 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07006119191646576, loss=0.024963917210698128
I0305 16:21:14.767231 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:23:16.274059 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:23:19.357501 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:23:22.441013 140444430841664 submission_runner.py:411] Time since start: 22328.06s, 	Step: 43548, 	{'train/accuracy': 0.9924206733703613, 'train/loss': 0.024289997294545174, 'train/mean_average_precision': 0.5410177292215665, 'validation/accuracy': 0.9870171546936035, 'validation/loss': 0.04486066848039627, 'validation/mean_average_precision': 0.2942300050683708, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.047758396714925766, 'test/mean_average_precision': 0.27616283887442433, 'test/num_examples': 43793, 'score': 14185.98506641388, 'total_duration': 22328.056819438934, 'accumulated_submission_time': 14185.98506641388, 'accumulated_eval_time': 8138.873854398727, 'accumulated_logging_time': 1.8862524032592773}
I0305 16:23:22.464789 140275669853952 logging_writer.py:48] [43548] accumulated_eval_time=8138.873854, accumulated_logging_time=1.886252, accumulated_submission_time=14185.985066, global_step=43548, preemption_count=0, score=14185.985066, test/accuracy=0.986119, test/loss=0.047758, test/mean_average_precision=0.276163, test/num_examples=43793, total_duration=22328.056819, train/accuracy=0.992421, train/loss=0.024290, train/mean_average_precision=0.541018, validation/accuracy=0.987017, validation/loss=0.044861, validation/mean_average_precision=0.294230, validation/num_examples=43793
I0305 16:23:39.914050 140283569538816 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.08429460972547531, loss=0.02887667529284954
I0305 16:24:12.398032 140275669853952 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.08587600290775299, loss=0.02777601219713688
I0305 16:24:44.930223 140283569538816 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.08610561490058899, loss=0.029324891045689583
I0305 16:25:17.935843 140275669853952 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.0867006778717041, loss=0.02835785411298275
I0305 16:25:50.933199 140283569538816 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08137702941894531, loss=0.02827109955251217
I0305 16:26:23.992011 140275669853952 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.08119798451662064, loss=0.02615179866552353
I0305 16:26:56.855929 140283569538816 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07014010101556778, loss=0.02436770126223564
I0305 16:27:22.583274 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:29:24.743683 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:29:27.841986 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:29:30.888542 140444430841664 submission_runner.py:411] Time since start: 22696.50s, 	Step: 44278, 	{'train/accuracy': 0.9924456477165222, 'train/loss': 0.024108177050948143, 'train/mean_average_precision': 0.5539474357205003, 'validation/accuracy': 0.9869996905326843, 'validation/loss': 0.04464448615908623, 'validation/mean_average_precision': 0.2899616419114914, 'validation/num_examples': 43793, 'test/accuracy': 0.9861868619918823, 'test/loss': 0.047412171959877014, 'test/mean_average_precision': 0.2761934727232424, 'test/num_examples': 43793, 'score': 14426.06815457344, 'total_duration': 22696.504336595535, 'accumulated_submission_time': 14426.06815457344, 'accumulated_eval_time': 8267.179068088531, 'accumulated_logging_time': 1.9230847358703613}
I0305 16:29:30.912699 140277079156480 logging_writer.py:48] [44278] accumulated_eval_time=8267.179068, accumulated_logging_time=1.923085, accumulated_submission_time=14426.068155, global_step=44278, preemption_count=0, score=14426.068155, test/accuracy=0.986187, test/loss=0.047412, test/mean_average_precision=0.276193, test/num_examples=43793, total_duration=22696.504337, train/accuracy=0.992446, train/loss=0.024108, train/mean_average_precision=0.553947, validation/accuracy=0.987000, validation/loss=0.044644, validation/mean_average_precision=0.289962, validation/num_examples=43793
I0305 16:29:38.601732 140283577931520 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.08485537767410278, loss=0.027723994106054306
I0305 16:30:12.082436 140277079156480 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.07647214084863663, loss=0.027926025912165642
I0305 16:30:44.520829 140283577931520 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.08251731097698212, loss=0.025436457246541977
I0305 16:31:17.217073 140277079156480 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08372560143470764, loss=0.02415699139237404
I0305 16:31:49.923060 140283577931520 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08067971467971802, loss=0.025766637176275253
I0305 16:32:22.763720 140277079156480 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.0732690840959549, loss=0.021344084292650223
I0305 16:32:55.822421 140283577931520 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07113834470510483, loss=0.025806041434407234
I0305 16:33:28.406568 140277079156480 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07604581862688065, loss=0.027406781911849976
I0305 16:33:31.038873 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:35:29.871049 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:35:32.942077 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:35:35.952555 140444430841664 submission_runner.py:411] Time since start: 23061.57s, 	Step: 45009, 	{'train/accuracy': 0.992694079875946, 'train/loss': 0.023344526067376137, 'train/mean_average_precision': 0.5726172586558297, 'validation/accuracy': 0.9869347810745239, 'validation/loss': 0.04444697126746178, 'validation/mean_average_precision': 0.2926345439056429, 'validation/num_examples': 43793, 'test/accuracy': 0.986127495765686, 'test/loss': 0.04724884405732155, 'test/mean_average_precision': 0.27692497590136844, 'test/num_examples': 43793, 'score': 14666.161107301712, 'total_duration': 23061.568345546722, 'accumulated_submission_time': 14666.161107301712, 'accumulated_eval_time': 8392.092687368393, 'accumulated_logging_time': 1.9581577777862549}
I0305 16:35:35.978049 140275669853952 logging_writer.py:48] [45009] accumulated_eval_time=8392.092687, accumulated_logging_time=1.958158, accumulated_submission_time=14666.161107, global_step=45009, preemption_count=0, score=14666.161107, test/accuracy=0.986127, test/loss=0.047249, test/mean_average_precision=0.276925, test/num_examples=43793, total_duration=23061.568346, train/accuracy=0.992694, train/loss=0.023345, train/mean_average_precision=0.572617, validation/accuracy=0.986935, validation/loss=0.044447, validation/mean_average_precision=0.292635, validation/num_examples=43793
I0305 16:36:05.845534 140283569538816 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.07485440373420715, loss=0.025415411219000816
I0305 16:36:38.436879 140275669853952 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08298823237419128, loss=0.023249812424182892
I0305 16:37:11.402924 140283569538816 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.09898818284273148, loss=0.026765428483486176
I0305 16:37:43.783904 140275669853952 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08452832698822021, loss=0.028950484469532967
I0305 16:38:16.206151 140283569538816 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.07245290279388428, loss=0.022442487999796867
I0305 16:38:47.966463 140275669853952 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.06930115818977356, loss=0.026472339406609535
I0305 16:39:20.378067 140283569538816 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08677792549133301, loss=0.023431476205587387
I0305 16:39:36.155672 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:41:33.556921 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:41:36.639154 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:41:39.639598 140444430841664 submission_runner.py:411] Time since start: 23425.26s, 	Step: 45750, 	{'train/accuracy': 0.9927871227264404, 'train/loss': 0.02289053052663803, 'train/mean_average_precision': 0.5784728400986485, 'validation/accuracy': 0.987106442451477, 'validation/loss': 0.04457703232765198, 'validation/mean_average_precision': 0.28655391589016194, 'validation/num_examples': 43793, 'test/accuracy': 0.9862618446350098, 'test/loss': 0.04730220139026642, 'test/mean_average_precision': 0.27883227992942616, 'test/num_examples': 43793, 'score': 14906.305232286453, 'total_duration': 23425.255395650864, 'accumulated_submission_time': 14906.305232286453, 'accumulated_eval_time': 8515.576565265656, 'accumulated_logging_time': 1.9948420524597168}
I0305 16:41:39.663697 140276755326720 logging_writer.py:48] [45750] accumulated_eval_time=8515.576565, accumulated_logging_time=1.994842, accumulated_submission_time=14906.305232, global_step=45750, preemption_count=0, score=14906.305232, test/accuracy=0.986262, test/loss=0.047302, test/mean_average_precision=0.278832, test/num_examples=43793, total_duration=23425.255396, train/accuracy=0.992787, train/loss=0.022891, train/mean_average_precision=0.578473, validation/accuracy=0.987106, validation/loss=0.044577, validation/mean_average_precision=0.286554, validation/num_examples=43793
I0305 16:41:56.398303 140283577931520 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.08130864799022675, loss=0.028871461749076843
I0305 16:42:28.788119 140276755326720 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08083286881446838, loss=0.02555818483233452
I0305 16:43:01.493040 140283577931520 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.07972712814807892, loss=0.027087485417723656
I0305 16:43:34.068085 140276755326720 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08635210990905762, loss=0.02550528198480606
I0305 16:44:06.386226 140283577931520 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08648505806922913, loss=0.021937737241387367
I0305 16:44:38.858539 140276755326720 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.07627866417169571, loss=0.024970151484012604
I0305 16:45:11.632715 140283577931520 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08533728867769241, loss=0.023916516453027725
I0305 16:45:39.644690 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:47:40.158631 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:47:43.251913 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:47:46.341114 140444430841664 submission_runner.py:411] Time since start: 23791.96s, 	Step: 46486, 	{'train/accuracy': 0.9928334951400757, 'train/loss': 0.022669579833745956, 'train/mean_average_precision': 0.5871963290355688, 'validation/accuracy': 0.9870455861091614, 'validation/loss': 0.04523776099085808, 'validation/mean_average_precision': 0.28493429408904924, 'validation/num_examples': 43793, 'test/accuracy': 0.9862555265426636, 'test/loss': 0.04807151481509209, 'test/mean_average_precision': 0.2768047922216397, 'test/num_examples': 43793, 'score': 15146.25148677826, 'total_duration': 23791.95691871643, 'accumulated_submission_time': 15146.25148677826, 'accumulated_eval_time': 8642.272961139679, 'accumulated_logging_time': 2.0315263271331787}
I0305 16:47:46.365777 140277079156480 logging_writer.py:48] [46486] accumulated_eval_time=8642.272961, accumulated_logging_time=2.031526, accumulated_submission_time=15146.251487, global_step=46486, preemption_count=0, score=15146.251487, test/accuracy=0.986256, test/loss=0.048072, test/mean_average_precision=0.276805, test/num_examples=43793, total_duration=23791.956919, train/accuracy=0.992833, train/loss=0.022670, train/mean_average_precision=0.587196, validation/accuracy=0.987046, validation/loss=0.045238, validation/mean_average_precision=0.284934, validation/num_examples=43793
I0305 16:47:51.376714 140283569538816 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08898913860321045, loss=0.02436640113592148
I0305 16:48:23.897207 140277079156480 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.08270489424467087, loss=0.02638424187898636
I0305 16:48:56.445468 140283569538816 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.0822676345705986, loss=0.0246412456035614
I0305 16:49:28.846564 140277079156480 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07797294855117798, loss=0.024271922186017036
I0305 16:50:01.164458 140283569538816 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.09463731944561005, loss=0.026217026636004448
I0305 16:50:33.787245 140277079156480 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.09408619999885559, loss=0.029926452785730362
I0305 16:51:06.331641 140283569538816 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.07482746243476868, loss=0.0210824366658926
I0305 16:51:38.621750 140277079156480 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.0846048966050148, loss=0.025552289560437202
I0305 16:51:46.363020 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:53:44.850614 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:53:47.896598 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:53:50.940556 140444430841664 submission_runner.py:411] Time since start: 24156.56s, 	Step: 47225, 	{'train/accuracy': 0.9932576417922974, 'train/loss': 0.02126718871295452, 'train/mean_average_precision': 0.6281929712934287, 'validation/accuracy': 0.9870005249977112, 'validation/loss': 0.045025501400232315, 'validation/mean_average_precision': 0.2873283203104504, 'validation/num_examples': 43793, 'test/accuracy': 0.9862689971923828, 'test/loss': 0.04804838076233864, 'test/mean_average_precision': 0.27877165076981725, 'test/num_examples': 43793, 'score': 15386.214908361435, 'total_duration': 24156.556359291077, 'accumulated_submission_time': 15386.214908361435, 'accumulated_eval_time': 8766.850444555283, 'accumulated_logging_time': 2.067582845687866}
I0305 16:53:50.965702 140275669853952 logging_writer.py:48] [47225] accumulated_eval_time=8766.850445, accumulated_logging_time=2.067583, accumulated_submission_time=15386.214908, global_step=47225, preemption_count=0, score=15386.214908, test/accuracy=0.986269, test/loss=0.048048, test/mean_average_precision=0.278772, test/num_examples=43793, total_duration=24156.556359, train/accuracy=0.993258, train/loss=0.021267, train/mean_average_precision=0.628193, validation/accuracy=0.987001, validation/loss=0.045026, validation/mean_average_precision=0.287328, validation/num_examples=43793
I0305 16:54:15.988780 140276755326720 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.08184617012739182, loss=0.024026045575737953
I0305 16:54:47.899516 140275669853952 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.0792970135807991, loss=0.026290087029337883
I0305 16:55:20.130884 140276755326720 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.07265730947256088, loss=0.020697807893157005
I0305 16:55:52.306582 140275669853952 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08253499865531921, loss=0.027294907718896866
I0305 16:56:25.034989 140276755326720 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.07169071584939957, loss=0.02241997979581356
I0305 16:56:58.339345 140275669853952 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.07574635744094849, loss=0.023148121312260628
I0305 16:57:30.786954 140276755326720 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.07346751540899277, loss=0.02437235787510872
I0305 16:57:51.128847 140444430841664 spec.py:321] Evaluating on the training split.
I0305 16:59:50.997622 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 16:59:54.089118 140444430841664 spec.py:349] Evaluating on the test split.
I0305 16:59:57.164519 140444430841664 submission_runner.py:411] Time since start: 24522.78s, 	Step: 47963, 	{'train/accuracy': 0.9932985901832581, 'train/loss': 0.02141503617167473, 'train/mean_average_precision': 0.6106789888766017, 'validation/accuracy': 0.9869879484176636, 'validation/loss': 0.04516957327723503, 'validation/mean_average_precision': 0.2882434698512126, 'validation/num_examples': 43793, 'test/accuracy': 0.9861649870872498, 'test/loss': 0.04811137914657593, 'test/mean_average_precision': 0.27710479908638075, 'test/num_examples': 43793, 'score': 15626.345014333725, 'total_duration': 24522.780300617218, 'accumulated_submission_time': 15626.345014333725, 'accumulated_eval_time': 8892.886050701141, 'accumulated_logging_time': 2.1041386127471924}
I0305 16:59:57.190251 140283569538816 logging_writer.py:48] [47963] accumulated_eval_time=8892.886051, accumulated_logging_time=2.104139, accumulated_submission_time=15626.345014, global_step=47963, preemption_count=0, score=15626.345014, test/accuracy=0.986165, test/loss=0.048111, test/mean_average_precision=0.277105, test/num_examples=43793, total_duration=24522.780301, train/accuracy=0.993299, train/loss=0.021415, train/mean_average_precision=0.610679, validation/accuracy=0.986988, validation/loss=0.045170, validation/mean_average_precision=0.288243, validation/num_examples=43793
I0305 17:00:09.757678 140283577931520 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.07704615592956543, loss=0.024279575794935226
I0305 17:00:41.907089 140283569538816 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.09296596795320511, loss=0.024541154503822327
I0305 17:01:14.414168 140283577931520 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09783768653869629, loss=0.027272989973425865
I0305 17:01:46.568945 140283569538816 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.09277402609586716, loss=0.02846773900091648
I0305 17:02:19.054236 140283577931520 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.07224179804325104, loss=0.02238941751420498
I0305 17:02:51.037746 140283569538816 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.08673878014087677, loss=0.02398647926747799
I0305 17:03:23.488387 140283577931520 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.0926838144659996, loss=0.027491703629493713
I0305 17:03:55.942453 140283569538816 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.0806398093700409, loss=0.023183049634099007
I0305 17:03:57.208551 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:05:57.322033 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:06:00.671124 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:06:04.127923 140444430841664 submission_runner.py:411] Time since start: 24889.74s, 	Step: 48705, 	{'train/accuracy': 0.9930528998374939, 'train/loss': 0.021990438923239708, 'train/mean_average_precision': 0.602626412423219, 'validation/accuracy': 0.987127959728241, 'validation/loss': 0.04522937536239624, 'validation/mean_average_precision': 0.29747820907533434, 'validation/num_examples': 43793, 'test/accuracy': 0.9862959980964661, 'test/loss': 0.04830378666520119, 'test/mean_average_precision': 0.28163037322693724, 'test/num_examples': 43793, 'score': 15866.32997751236, 'total_duration': 24889.743696928024, 'accumulated_submission_time': 15866.32997751236, 'accumulated_eval_time': 9019.805342435837, 'accumulated_logging_time': 2.1411499977111816}
I0305 17:06:04.155369 140275669853952 logging_writer.py:48] [48705] accumulated_eval_time=9019.805342, accumulated_logging_time=2.141150, accumulated_submission_time=15866.329978, global_step=48705, preemption_count=0, score=15866.329978, test/accuracy=0.986296, test/loss=0.048304, test/mean_average_precision=0.281630, test/num_examples=43793, total_duration=24889.743697, train/accuracy=0.993053, train/loss=0.021990, train/mean_average_precision=0.602626, validation/accuracy=0.987128, validation/loss=0.045229, validation/mean_average_precision=0.297478, validation/num_examples=43793
I0305 17:06:35.971994 140277079156480 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.0961655005812645, loss=0.02606884576380253
I0305 17:07:09.042423 140275669853952 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.09454818814992905, loss=0.025817662477493286
I0305 17:07:42.209178 140277079156480 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.09414038062095642, loss=0.021675411611795425
I0305 17:08:15.575828 140275669853952 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.07924854755401611, loss=0.02495085448026657
I0305 17:08:48.235598 140277079156480 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.08594202995300293, loss=0.0226979311555624
I0305 17:09:20.972123 140275669853952 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.10550029575824738, loss=0.026042403653264046
I0305 17:09:53.443031 140277079156480 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.07922270148992538, loss=0.024027138948440552
I0305 17:10:04.297962 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:12:07.986954 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:12:11.080983 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:12:14.076648 140444430841664 submission_runner.py:411] Time since start: 25259.69s, 	Step: 49434, 	{'train/accuracy': 0.9930263161659241, 'train/loss': 0.022090479731559753, 'train/mean_average_precision': 0.5961216643344134, 'validation/accuracy': 0.9870460033416748, 'validation/loss': 0.045253295451402664, 'validation/mean_average_precision': 0.29126033188330824, 'validation/num_examples': 43793, 'test/accuracy': 0.9862993359565735, 'test/loss': 0.04829072207212448, 'test/mean_average_precision': 0.278605704419052, 'test/num_examples': 43793, 'score': 16106.437074422836, 'total_duration': 25259.692447662354, 'accumulated_submission_time': 16106.437074422836, 'accumulated_eval_time': 9149.583971261978, 'accumulated_logging_time': 2.1799392700195312}
I0305 17:12:14.101528 140276755326720 logging_writer.py:48] [49434] accumulated_eval_time=9149.583971, accumulated_logging_time=2.179939, accumulated_submission_time=16106.437074, global_step=49434, preemption_count=0, score=16106.437074, test/accuracy=0.986299, test/loss=0.048291, test/mean_average_precision=0.278606, test/num_examples=43793, total_duration=25259.692448, train/accuracy=0.993026, train/loss=0.022090, train/mean_average_precision=0.596122, validation/accuracy=0.987046, validation/loss=0.045253, validation/mean_average_precision=0.291260, validation/num_examples=43793
I0305 17:12:35.683417 140283569538816 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09044557064771652, loss=0.025999946519732475
I0305 17:13:08.125018 140276755326720 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.08657295256853104, loss=0.02297123894095421
I0305 17:13:40.925554 140283569538816 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09708186984062195, loss=0.026302123442292213
I0305 17:14:13.854525 140276755326720 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.07950443774461746, loss=0.022673096507787704
I0305 17:14:46.598461 140283569538816 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.0915924534201622, loss=0.024036502465605736
I0305 17:15:18.836917 140276755326720 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.07958124577999115, loss=0.023194218054413795
I0305 17:15:51.016540 140283569538816 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.0912875309586525, loss=0.025100141763687134
I0305 17:16:14.187381 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:18:14.322960 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:18:17.503250 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:18:20.642417 140444430841664 submission_runner.py:411] Time since start: 25626.26s, 	Step: 50173, 	{'train/accuracy': 0.9931430816650391, 'train/loss': 0.02186397835612297, 'train/mean_average_precision': 0.5994783641295355, 'validation/accuracy': 0.9869099855422974, 'validation/loss': 0.04548113793134689, 'validation/mean_average_precision': 0.2840237806197731, 'validation/num_examples': 43793, 'test/accuracy': 0.9862332344055176, 'test/loss': 0.04836353287100792, 'test/mean_average_precision': 0.2769694896537659, 'test/num_examples': 43793, 'score': 16346.489510059357, 'total_duration': 25626.25821518898, 'accumulated_submission_time': 16346.489510059357, 'accumulated_eval_time': 9276.038954019547, 'accumulated_logging_time': 2.2161362171173096}
I0305 17:18:20.667979 140275669853952 logging_writer.py:48] [50173] accumulated_eval_time=9276.038954, accumulated_logging_time=2.216136, accumulated_submission_time=16346.489510, global_step=50173, preemption_count=0, score=16346.489510, test/accuracy=0.986233, test/loss=0.048364, test/mean_average_precision=0.276969, test/num_examples=43793, total_duration=25626.258215, train/accuracy=0.993143, train/loss=0.021864, train/mean_average_precision=0.599478, validation/accuracy=0.986910, validation/loss=0.045481, validation/mean_average_precision=0.284024, validation/num_examples=43793
I0305 17:18:29.989032 140277079156480 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09535291790962219, loss=0.026285694912075996
I0305 17:19:03.055457 140275669853952 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.08676721155643463, loss=0.022324666380882263
I0305 17:19:35.565085 140277079156480 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08365652710199356, loss=0.02539142780005932
I0305 17:20:08.122120 140275669853952 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08833786100149155, loss=0.024572335183620453
I0305 17:20:40.787351 140277079156480 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.08377567678689957, loss=0.022137632593512535
I0305 17:21:13.534957 140275669853952 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.0952928215265274, loss=0.02310078591108322
I0305 17:21:46.750985 140277079156480 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.10010117292404175, loss=0.029470423236489296
I0305 17:22:19.942421 140275669853952 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.0927729532122612, loss=0.02464069053530693
I0305 17:22:20.910960 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:24:26.274594 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:24:29.648745 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:24:33.011027 140444430841664 submission_runner.py:411] Time since start: 25998.63s, 	Step: 50904, 	{'train/accuracy': 0.9931342601776123, 'train/loss': 0.021561022847890854, 'train/mean_average_precision': 0.6124295443985085, 'validation/accuracy': 0.9870536923408508, 'validation/loss': 0.045751627534627914, 'validation/mean_average_precision': 0.2861033373103874, 'validation/num_examples': 43793, 'test/accuracy': 0.9863094687461853, 'test/loss': 0.04866671562194824, 'test/mean_average_precision': 0.27798501871573933, 'test/num_examples': 43793, 'score': 16586.699061632156, 'total_duration': 25998.626806259155, 'accumulated_submission_time': 16586.699061632156, 'accumulated_eval_time': 9408.138950824738, 'accumulated_logging_time': 2.25294828414917}
I0305 17:24:33.040930 140283569538816 logging_writer.py:48] [50904] accumulated_eval_time=9408.138951, accumulated_logging_time=2.252948, accumulated_submission_time=16586.699062, global_step=50904, preemption_count=0, score=16586.699062, test/accuracy=0.986309, test/loss=0.048667, test/mean_average_precision=0.277985, test/num_examples=43793, total_duration=25998.626806, train/accuracy=0.993134, train/loss=0.021561, train/mean_average_precision=0.612430, validation/accuracy=0.987054, validation/loss=0.045752, validation/mean_average_precision=0.286103, validation/num_examples=43793
I0305 17:25:05.747210 140283577931520 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.09321106970310211, loss=0.02718655951321125
I0305 17:25:38.636652 140283569538816 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.10210201889276505, loss=0.02784813940525055
I0305 17:26:11.514044 140283577931520 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0861743912100792, loss=0.02271808125078678
I0305 17:26:44.484124 140283569538816 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08867479860782623, loss=0.023075278848409653
I0305 17:27:17.192467 140283577931520 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09606152772903442, loss=0.025295861065387726
I0305 17:27:49.943983 140283569538816 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.11997731775045395, loss=0.027216382324695587
I0305 17:28:22.626271 140283577931520 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.08921206742525101, loss=0.024816911667585373
I0305 17:28:33.229720 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:30:38.492082 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:30:41.888292 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:30:45.203906 140444430841664 submission_runner.py:411] Time since start: 26370.82s, 	Step: 51633, 	{'train/accuracy': 0.9932966232299805, 'train/loss': 0.021137511357665062, 'train/mean_average_precision': 0.6139599699223799, 'validation/accuracy': 0.9870638251304626, 'validation/loss': 0.04578164219856262, 'validation/mean_average_precision': 0.2866013596010617, 'validation/num_examples': 43793, 'test/accuracy': 0.9862887859344482, 'test/loss': 0.048875801265239716, 'test/mean_average_precision': 0.28052262591491767, 'test/num_examples': 43793, 'score': 16826.849710941315, 'total_duration': 26370.819693803787, 'accumulated_submission_time': 16826.849710941315, 'accumulated_eval_time': 9540.113079071045, 'accumulated_logging_time': 2.294236183166504}
I0305 17:30:45.232960 140275669853952 logging_writer.py:48] [51633] accumulated_eval_time=9540.113079, accumulated_logging_time=2.294236, accumulated_submission_time=16826.849711, global_step=51633, preemption_count=0, score=16826.849711, test/accuracy=0.986289, test/loss=0.048876, test/mean_average_precision=0.280523, test/num_examples=43793, total_duration=26370.819694, train/accuracy=0.993297, train/loss=0.021138, train/mean_average_precision=0.613960, validation/accuracy=0.987064, validation/loss=0.045782, validation/mean_average_precision=0.286601, validation/num_examples=43793
I0305 17:31:07.577458 140276755326720 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.08569605648517609, loss=0.023969197645783424
I0305 17:31:40.726467 140275669853952 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.10207783430814743, loss=0.02306104078888893
I0305 17:32:14.051482 140276755326720 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.07821127027273178, loss=0.02100812830030918
I0305 17:32:46.382934 140275669853952 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.09948961436748505, loss=0.023570749908685684
I0305 17:33:18.957481 140276755326720 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.0947912186384201, loss=0.027037134394049644
I0305 17:33:51.377358 140275669853952 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.10238353163003922, loss=0.024856315925717354
I0305 17:34:24.483145 140276755326720 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.10608268529176712, loss=0.02088211663067341
I0305 17:34:45.343692 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:36:42.893541 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:36:45.934603 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:36:48.967670 140444430841664 submission_runner.py:411] Time since start: 26734.58s, 	Step: 52364, 	{'train/accuracy': 0.9933950901031494, 'train/loss': 0.020760487765073776, 'train/mean_average_precision': 0.6385449009632981, 'validation/accuracy': 0.9870825409889221, 'validation/loss': 0.045648619532585144, 'validation/mean_average_precision': 0.29412344441257937, 'validation/num_examples': 43793, 'test/accuracy': 0.986327588558197, 'test/loss': 0.048504531383514404, 'test/mean_average_precision': 0.2836710398323977, 'test/num_examples': 43793, 'score': 17066.924193382263, 'total_duration': 26734.583471298218, 'accumulated_submission_time': 17066.924193382263, 'accumulated_eval_time': 9663.73701095581, 'accumulated_logging_time': 2.3359954357147217}
I0305 17:36:48.994298 140283569538816 logging_writer.py:48] [52364] accumulated_eval_time=9663.737011, accumulated_logging_time=2.335995, accumulated_submission_time=17066.924193, global_step=52364, preemption_count=0, score=17066.924193, test/accuracy=0.986328, test/loss=0.048505, test/mean_average_precision=0.283671, test/num_examples=43793, total_duration=26734.583471, train/accuracy=0.993395, train/loss=0.020760, train/mean_average_precision=0.638545, validation/accuracy=0.987083, validation/loss=0.045649, validation/mean_average_precision=0.294123, validation/num_examples=43793
I0305 17:37:01.281787 140283577931520 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09716346859931946, loss=0.02243995852768421
I0305 17:37:33.869933 140283569538816 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08054446429014206, loss=0.020910700783133507
I0305 17:38:06.506715 140283577931520 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.08598761260509491, loss=0.023192863911390305
I0305 17:38:38.974958 140283569538816 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.09428667277097702, loss=0.024308206513524055
I0305 17:39:11.461707 140283577931520 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.1066003069281578, loss=0.025621529668569565
I0305 17:39:43.880139 140283569538816 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09323086589574814, loss=0.022928176447749138
I0305 17:40:16.602280 140283577931520 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09538186341524124, loss=0.024799274280667305
I0305 17:40:48.590457 140283569538816 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09184309840202332, loss=0.022070884704589844
I0305 17:40:49.246185 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:42:48.871211 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:42:51.947135 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:42:55.001701 140444430841664 submission_runner.py:411] Time since start: 27100.62s, 	Step: 53103, 	{'train/accuracy': 0.9938114285469055, 'train/loss': 0.01955096796154976, 'train/mean_average_precision': 0.6596532125631671, 'validation/accuracy': 0.9870573282241821, 'validation/loss': 0.046172600239515305, 'validation/mean_average_precision': 0.2965265591422013, 'validation/num_examples': 43793, 'test/accuracy': 0.9862959980964661, 'test/loss': 0.0487239696085453, 'test/mean_average_precision': 0.28181534175858186, 'test/num_examples': 43793, 'score': 17307.14094877243, 'total_duration': 27100.617507457733, 'accumulated_submission_time': 17307.14094877243, 'accumulated_eval_time': 9789.492477416992, 'accumulated_logging_time': 2.3756937980651855}
I0305 17:42:55.027558 140276755326720 logging_writer.py:48] [53103] accumulated_eval_time=9789.492477, accumulated_logging_time=2.375694, accumulated_submission_time=17307.140949, global_step=53103, preemption_count=0, score=17307.140949, test/accuracy=0.986296, test/loss=0.048724, test/mean_average_precision=0.281815, test/num_examples=43793, total_duration=27100.617507, train/accuracy=0.993811, train/loss=0.019551, train/mean_average_precision=0.659653, validation/accuracy=0.987057, validation/loss=0.046173, validation/mean_average_precision=0.296527, validation/num_examples=43793
I0305 17:43:27.462931 140277079156480 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.11742755770683289, loss=0.0238681361079216
I0305 17:44:00.320069 140276755326720 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.0998210683465004, loss=0.024886194616556168
I0305 17:44:32.617972 140277079156480 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.08878994733095169, loss=0.0212214644998312
I0305 17:45:05.339003 140276755326720 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.11183582246303558, loss=0.027806617319583893
I0305 17:45:38.034719 140277079156480 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09149019420146942, loss=0.023001814261078835
I0305 17:46:11.246865 140276755326720 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.11631900072097778, loss=0.023626718670129776
I0305 17:46:43.962481 140277079156480 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.09281231462955475, loss=0.02091887965798378
I0305 17:46:55.003813 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:48:49.156114 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:48:52.253166 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:48:55.403445 140444430841664 submission_runner.py:411] Time since start: 27461.02s, 	Step: 53835, 	{'train/accuracy': 0.9938933253288269, 'train/loss': 0.019416574388742447, 'train/mean_average_precision': 0.6604764193160376, 'validation/accuracy': 0.9871150255203247, 'validation/loss': 0.046363137662410736, 'validation/mean_average_precision': 0.2949741394119192, 'validation/num_examples': 43793, 'test/accuracy': 0.9863587617874146, 'test/loss': 0.04895440489053726, 'test/mean_average_precision': 0.28414192060249405, 'test/num_examples': 43793, 'score': 17547.083233356476, 'total_duration': 27461.019245386124, 'accumulated_submission_time': 17547.083233356476, 'accumulated_eval_time': 9909.892055034637, 'accumulated_logging_time': 2.4137353897094727}
I0305 17:48:55.429902 140283569538816 logging_writer.py:48] [53835] accumulated_eval_time=9909.892055, accumulated_logging_time=2.413735, accumulated_submission_time=17547.083233, global_step=53835, preemption_count=0, score=17547.083233, test/accuracy=0.986359, test/loss=0.048954, test/mean_average_precision=0.284142, test/num_examples=43793, total_duration=27461.019245, train/accuracy=0.993893, train/loss=0.019417, train/mean_average_precision=0.660476, validation/accuracy=0.987115, validation/loss=0.046363, validation/mean_average_precision=0.294974, validation/num_examples=43793
I0305 17:49:16.568334 140283577931520 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.1062663272023201, loss=0.025798583403229713
I0305 17:49:49.505396 140283569538816 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.0901622474193573, loss=0.022159786894917488
I0305 17:50:21.735909 140283577931520 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.09831831604242325, loss=0.02344045601785183
I0305 17:50:53.756657 140283569538816 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.09458232671022415, loss=0.022580072283744812
I0305 17:51:25.723514 140283577931520 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.11213099956512451, loss=0.02688385732471943
I0305 17:51:57.914821 140283569538816 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.10475137084722519, loss=0.026751989498734474
I0305 17:52:30.144889 140283577931520 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09743591398000717, loss=0.022253774106502533
I0305 17:52:55.423239 140444430841664 spec.py:321] Evaluating on the training split.
I0305 17:54:57.426349 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 17:55:00.517136 140444430841664 spec.py:349] Evaluating on the test split.
I0305 17:55:05.058797 140444430841664 submission_runner.py:411] Time since start: 27830.67s, 	Step: 54579, 	{'train/accuracy': 0.9937782287597656, 'train/loss': 0.019702868536114693, 'train/mean_average_precision': 0.6423692855399952, 'validation/accuracy': 0.9869229793548584, 'validation/loss': 0.046136025339365005, 'validation/mean_average_precision': 0.293365370342748, 'validation/num_examples': 43793, 'test/accuracy': 0.9861123561859131, 'test/loss': 0.0491110160946846, 'test/mean_average_precision': 0.28095669130614065, 'test/num_examples': 43793, 'score': 17787.043855190277, 'total_duration': 27830.67459988594, 'accumulated_submission_time': 17787.043855190277, 'accumulated_eval_time': 10039.527564287186, 'accumulated_logging_time': 2.451258659362793}
I0305 17:55:05.086442 140275669853952 logging_writer.py:48] [54579] accumulated_eval_time=10039.527564, accumulated_logging_time=2.451259, accumulated_submission_time=17787.043855, global_step=54579, preemption_count=0, score=17787.043855, test/accuracy=0.986112, test/loss=0.049111, test/mean_average_precision=0.280957, test/num_examples=43793, total_duration=27830.674600, train/accuracy=0.993778, train/loss=0.019703, train/mean_average_precision=0.642369, validation/accuracy=0.986923, validation/loss=0.046136, validation/mean_average_precision=0.293365, validation/num_examples=43793
I0305 17:55:12.334132 140277079156480 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.10997700691223145, loss=0.02556557022035122
I0305 17:55:45.020251 140275669853952 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.10410882532596588, loss=0.02386387623846531
I0305 17:56:17.884192 140277079156480 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09801644831895828, loss=0.02007574401795864
I0305 17:56:50.194339 140275669853952 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.1061394140124321, loss=0.023585299029946327
I0305 17:57:22.772364 140277079156480 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.10628686100244522, loss=0.02308301255106926
I0305 17:57:55.529032 140275669853952 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09535831212997437, loss=0.02238127775490284
I0305 17:58:28.091543 140277079156480 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10598792135715485, loss=0.024015266448259354
I0305 17:59:00.849637 140275669853952 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.10375908017158508, loss=0.022237202152609825
I0305 17:59:05.338657 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:01:06.911974 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:01:09.994594 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:01:13.057966 140444430841664 submission_runner.py:411] Time since start: 28198.67s, 	Step: 55314, 	{'train/accuracy': 0.9934359788894653, 'train/loss': 0.020614076405763626, 'train/mean_average_precision': 0.631309266735881, 'validation/accuracy': 0.9870167374610901, 'validation/loss': 0.04639820009469986, 'validation/mean_average_precision': 0.291090867299716, 'validation/num_examples': 43793, 'test/accuracy': 0.9862247705459595, 'test/loss': 0.04937204718589783, 'test/mean_average_precision': 0.28051720596464963, 'test/num_examples': 43793, 'score': 18027.26170873642, 'total_duration': 28198.673761606216, 'accumulated_submission_time': 18027.26170873642, 'accumulated_eval_time': 10167.246821165085, 'accumulated_logging_time': 2.4914052486419678}
I0305 18:01:13.085731 140276755326720 logging_writer.py:48] [55314] accumulated_eval_time=10167.246821, accumulated_logging_time=2.491405, accumulated_submission_time=18027.261709, global_step=55314, preemption_count=0, score=18027.261709, test/accuracy=0.986225, test/loss=0.049372, test/mean_average_precision=0.280517, test/num_examples=43793, total_duration=28198.673762, train/accuracy=0.993436, train/loss=0.020614, train/mean_average_precision=0.631309, validation/accuracy=0.987017, validation/loss=0.046398, validation/mean_average_precision=0.291091, validation/num_examples=43793
I0305 18:01:41.639952 140283569538816 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10257944464683533, loss=0.024888817220926285
I0305 18:02:14.809625 140276755326720 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09647974371910095, loss=0.02113701030611992
I0305 18:02:47.882805 140283569538816 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.09583563357591629, loss=0.020750034600496292
I0305 18:03:20.564095 140276755326720 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.11260713636875153, loss=0.02344069816172123
I0305 18:03:52.671353 140283569538816 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09294521808624268, loss=0.021241091191768646
I0305 18:04:24.954711 140276755326720 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.11356716603040695, loss=0.02259574830532074
I0305 18:04:57.097632 140283569538816 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.10848340392112732, loss=0.023899205029010773
I0305 18:05:13.127954 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:07:13.920432 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:07:16.975980 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:07:20.024194 140444430841664 submission_runner.py:411] Time since start: 28565.64s, 	Step: 56050, 	{'train/accuracy': 0.9934125542640686, 'train/loss': 0.02054968848824501, 'train/mean_average_precision': 0.6211181522600252, 'validation/accuracy': 0.9869664311408997, 'validation/loss': 0.046516112983226776, 'validation/mean_average_precision': 0.291339921317668, 'validation/num_examples': 43793, 'test/accuracy': 0.9862374067306519, 'test/loss': 0.04930568486452103, 'test/mean_average_precision': 0.28589856450714607, 'test/num_examples': 43793, 'score': 18267.270840644836, 'total_duration': 28565.63999724388, 'accumulated_submission_time': 18267.270840644836, 'accumulated_eval_time': 10294.143009662628, 'accumulated_logging_time': 2.530449390411377}
I0305 18:07:20.051374 140275669853952 logging_writer.py:48] [56050] accumulated_eval_time=10294.143010, accumulated_logging_time=2.530449, accumulated_submission_time=18267.270841, global_step=56050, preemption_count=0, score=18267.270841, test/accuracy=0.986237, test/loss=0.049306, test/mean_average_precision=0.285899, test/num_examples=43793, total_duration=28565.639997, train/accuracy=0.993413, train/loss=0.020550, train/mean_average_precision=0.621118, validation/accuracy=0.986966, validation/loss=0.046516, validation/mean_average_precision=0.291340, validation/num_examples=43793
I0305 18:07:36.654023 140277079156480 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.10462313890457153, loss=0.023144109174609184
I0305 18:08:09.125941 140275669853952 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.09996374696493149, loss=0.023734640330076218
I0305 18:08:41.861338 140277079156480 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.1257331371307373, loss=0.02121218666434288
I0305 18:09:14.212102 140275669853952 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10217539221048355, loss=0.0208648182451725
I0305 18:09:46.245873 140277079156480 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.11700397729873657, loss=0.024226011708378792
I0305 18:10:18.817969 140275669853952 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.11811003088951111, loss=0.0248421598225832
I0305 18:10:51.603821 140277079156480 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.10154254734516144, loss=0.022534463554620743
I0305 18:11:20.232124 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:13:20.296361 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:13:23.394791 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:13:26.490200 140444430841664 submission_runner.py:411] Time since start: 28932.11s, 	Step: 56785, 	{'train/accuracy': 0.9937226176261902, 'train/loss': 0.01963372901082039, 'train/mean_average_precision': 0.6670870496531485, 'validation/accuracy': 0.9869379997253418, 'validation/loss': 0.04662768542766571, 'validation/mean_average_precision': 0.2882335202718856, 'validation/num_examples': 43793, 'test/accuracy': 0.9862247705459595, 'test/loss': 0.04947953298687935, 'test/mean_average_precision': 0.27947302206076435, 'test/num_examples': 43793, 'score': 18507.41755247116, 'total_duration': 28932.106006860733, 'accumulated_submission_time': 18507.41755247116, 'accumulated_eval_time': 10420.40105509758, 'accumulated_logging_time': 2.5692050457000732}
I0305 18:13:26.516837 140276755326720 logging_writer.py:48] [56785] accumulated_eval_time=10420.401055, accumulated_logging_time=2.569205, accumulated_submission_time=18507.417552, global_step=56785, preemption_count=0, score=18507.417552, test/accuracy=0.986225, test/loss=0.049480, test/mean_average_precision=0.279473, test/num_examples=43793, total_duration=28932.106007, train/accuracy=0.993723, train/loss=0.019634, train/mean_average_precision=0.667087, validation/accuracy=0.986938, validation/loss=0.046628, validation/mean_average_precision=0.288234, validation/num_examples=43793
I0305 18:13:31.976939 140283577931520 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.10243584960699081, loss=0.020831789821386337
I0305 18:14:04.936856 140276755326720 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.10948161035776138, loss=0.02236051857471466
I0305 18:14:38.649245 140283577931520 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.09395267069339752, loss=0.0202693659812212
I0305 18:15:11.806241 140276755326720 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.09825102239847183, loss=0.020020654425024986
I0305 18:15:45.473244 140283577931520 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.11158744990825653, loss=0.02103257365524769
I0305 18:16:18.564866 140276755326720 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.09590701013803482, loss=0.022205175831913948
I0305 18:16:51.425725 140283577931520 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.11375320702791214, loss=0.02472122386097908
I0305 18:17:24.432778 140276755326720 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10343349725008011, loss=0.022230880334973335
I0305 18:17:26.786296 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:19:29.406824 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:19:32.784240 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:19:36.156708 140444430841664 submission_runner.py:411] Time since start: 29301.77s, 	Step: 57508, 	{'train/accuracy': 0.9938977956771851, 'train/loss': 0.019118938595056534, 'train/mean_average_precision': 0.6645415284070957, 'validation/accuracy': 0.9869725108146667, 'validation/loss': 0.04692523926496506, 'validation/mean_average_precision': 0.287619898859582, 'validation/num_examples': 43793, 'test/accuracy': 0.9861935973167419, 'test/loss': 0.04980267584323883, 'test/mean_average_precision': 0.2824516621344256, 'test/num_examples': 43793, 'score': 18747.650901317596, 'total_duration': 29301.772493600845, 'accumulated_submission_time': 18747.650901317596, 'accumulated_eval_time': 10549.771411418915, 'accumulated_logging_time': 2.607391595840454}
I0305 18:19:36.187536 140277079156480 logging_writer.py:48] [57508] accumulated_eval_time=10549.771411, accumulated_logging_time=2.607392, accumulated_submission_time=18747.650901, global_step=57508, preemption_count=0, score=18747.650901, test/accuracy=0.986194, test/loss=0.049803, test/mean_average_precision=0.282452, test/num_examples=43793, total_duration=29301.772494, train/accuracy=0.993898, train/loss=0.019119, train/mean_average_precision=0.664542, validation/accuracy=0.986973, validation/loss=0.046925, validation/mean_average_precision=0.287620, validation/num_examples=43793
I0305 18:20:07.031050 140283569538816 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.11903500556945801, loss=0.02620324306190014
I0305 18:20:40.041074 140277079156480 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.11154268682003021, loss=0.022080115973949432
I0305 18:21:13.073816 140283569538816 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.11482349783182144, loss=0.021346110850572586
I0305 18:21:45.913180 140277079156480 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10949265211820602, loss=0.02105819806456566
I0305 18:22:19.116137 140283569538816 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.11734668165445328, loss=0.024790503084659576
I0305 18:22:52.189977 140277079156480 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.10816475749015808, loss=0.019069667905569077
I0305 18:23:25.047121 140283569538816 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.11299926787614822, loss=0.022213371470570564
I0305 18:23:36.193804 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:25:33.762186 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:25:36.785662 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:25:39.817532 140444430841664 submission_runner.py:411] Time since start: 29665.43s, 	Step: 58235, 	{'train/accuracy': 0.9940282702445984, 'train/loss': 0.01871313713490963, 'train/mean_average_precision': 0.6765143704229382, 'validation/accuracy': 0.986976146697998, 'validation/loss': 0.047310762107372284, 'validation/mean_average_precision': 0.2894394066239375, 'validation/num_examples': 43793, 'test/accuracy': 0.9862008094787598, 'test/loss': 0.050183627754449844, 'test/mean_average_precision': 0.28089953646608246, 'test/num_examples': 43793, 'score': 18987.619652748108, 'total_duration': 29665.43332839012, 'accumulated_submission_time': 18987.619652748108, 'accumulated_eval_time': 10673.395092010498, 'accumulated_logging_time': 2.6499857902526855}
I0305 18:25:39.846210 140275669853952 logging_writer.py:48] [58235] accumulated_eval_time=10673.395092, accumulated_logging_time=2.649986, accumulated_submission_time=18987.619653, global_step=58235, preemption_count=0, score=18987.619653, test/accuracy=0.986201, test/loss=0.050184, test/mean_average_precision=0.280900, test/num_examples=43793, total_duration=29665.433328, train/accuracy=0.994028, train/loss=0.018713, train/mean_average_precision=0.676514, validation/accuracy=0.986976, validation/loss=0.047311, validation/mean_average_precision=0.289439, validation/num_examples=43793
I0305 18:26:01.764321 140283577931520 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.11735209077596664, loss=0.02267143875360489
I0305 18:26:34.978598 140275669853952 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.12217050790786743, loss=0.023425856605172157
I0305 18:27:07.951317 140283577931520 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.09958724677562714, loss=0.021126864477992058
I0305 18:27:40.442510 140275669853952 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.12052097916603088, loss=0.01992967166006565
I0305 18:28:13.253903 140283577931520 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.1009574830532074, loss=0.02115028351545334
I0305 18:28:45.673208 140275669853952 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.10317917168140411, loss=0.02059725485742092
I0305 18:29:18.253451 140283577931520 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.129525825381279, loss=0.022247791290283203
I0305 18:29:39.971201 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:31:35.132133 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:31:38.207126 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:31:42.665954 140444430841664 submission_runner.py:411] Time since start: 30028.28s, 	Step: 58968, 	{'train/accuracy': 0.9943478107452393, 'train/loss': 0.017651693895459175, 'train/mean_average_precision': 0.698494736500919, 'validation/accuracy': 0.9870277047157288, 'validation/loss': 0.04756660386919975, 'validation/mean_average_precision': 0.28924621983951043, 'validation/num_examples': 43793, 'test/accuracy': 0.9862332344055176, 'test/loss': 0.05058598518371582, 'test/mean_average_precision': 0.2774700604654444, 'test/num_examples': 43793, 'score': 19227.710742235184, 'total_duration': 30028.281749010086, 'accumulated_submission_time': 19227.710742235184, 'accumulated_eval_time': 10796.089787244797, 'accumulated_logging_time': 2.6901214122772217}
I0305 18:31:42.693555 140276755326720 logging_writer.py:48] [58968] accumulated_eval_time=10796.089787, accumulated_logging_time=2.690121, accumulated_submission_time=19227.710742, global_step=58968, preemption_count=0, score=19227.710742, test/accuracy=0.986233, test/loss=0.050586, test/mean_average_precision=0.277470, test/num_examples=43793, total_duration=30028.281749, train/accuracy=0.994348, train/loss=0.017652, train/mean_average_precision=0.698495, validation/accuracy=0.987028, validation/loss=0.047567, validation/mean_average_precision=0.289246, validation/num_examples=43793
I0305 18:31:53.456053 140277079156480 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.12262491881847382, loss=0.02289535664021969
I0305 18:32:25.784716 140276755326720 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.11741042882204056, loss=0.022237906232476234
I0305 18:32:58.525337 140277079156480 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.12362735718488693, loss=0.023299751803278923
I0305 18:33:31.208866 140276755326720 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.11333684623241425, loss=0.02185858227312565
I0305 18:34:03.885257 140277079156480 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.10633070766925812, loss=0.02168942056596279
I0305 18:34:36.129884 140276755326720 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.1128590852022171, loss=0.0208380538970232
I0305 18:35:08.584916 140277079156480 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.11027548462152481, loss=0.020784398540854454
I0305 18:35:41.093780 140276755326720 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.1285349577665329, loss=0.023084819316864014
I0305 18:35:42.716590 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:37:39.151595 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:37:42.205244 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:37:45.228456 140444430841664 submission_runner.py:411] Time since start: 30390.84s, 	Step: 59706, 	{'train/accuracy': 0.9944453835487366, 'train/loss': 0.01746751368045807, 'train/mean_average_precision': 0.7007133967198509, 'validation/accuracy': 0.9869737029075623, 'validation/loss': 0.04749451205134392, 'validation/mean_average_precision': 0.2934684995339162, 'validation/num_examples': 43793, 'test/accuracy': 0.9862951040267944, 'test/loss': 0.05038211867213249, 'test/mean_average_precision': 0.2815228197907312, 'test/num_examples': 43793, 'score': 19467.699901103973, 'total_duration': 30390.844262599945, 'accumulated_submission_time': 19467.699901103973, 'accumulated_eval_time': 10918.601605176926, 'accumulated_logging_time': 2.7292253971099854}
I0305 18:37:45.255427 140275669853952 logging_writer.py:48] [59706] accumulated_eval_time=10918.601605, accumulated_logging_time=2.729225, accumulated_submission_time=19467.699901, global_step=59706, preemption_count=0, score=19467.699901, test/accuracy=0.986295, test/loss=0.050382, test/mean_average_precision=0.281523, test/num_examples=43793, total_duration=30390.844263, train/accuracy=0.994445, train/loss=0.017468, train/mean_average_precision=0.700713, validation/accuracy=0.986974, validation/loss=0.047495, validation/mean_average_precision=0.293468, validation/num_examples=43793
I0305 18:38:16.350687 140283569538816 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.11960296332836151, loss=0.023027945309877396
I0305 18:38:48.888052 140275669853952 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.11959002912044525, loss=0.02143878862261772
I0305 18:39:21.286994 140283569538816 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.10306566953659058, loss=0.020178478211164474
I0305 18:39:53.538985 140275669853952 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.11554952710866928, loss=0.02042553946375847
I0305 18:40:26.300840 140283569538816 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.11592452973127365, loss=0.025206465274095535
I0305 18:40:58.494875 140275669853952 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.12534357607364655, loss=0.019310137256979942
I0305 18:41:31.036760 140283569538816 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.1358739286661148, loss=0.022280268371105194
I0305 18:41:45.472528 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:43:41.501237 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:43:44.548043 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:43:47.553267 140444430841664 submission_runner.py:411] Time since start: 30753.17s, 	Step: 60443, 	{'train/accuracy': 0.9942583441734314, 'train/loss': 0.01808089017868042, 'train/mean_average_precision': 0.6905366072922718, 'validation/accuracy': 0.9869469404220581, 'validation/loss': 0.04756121337413788, 'validation/mean_average_precision': 0.2924110143159727, 'validation/num_examples': 43793, 'test/accuracy': 0.9862176179885864, 'test/loss': 0.05072605982422829, 'test/mean_average_precision': 0.28308896856454646, 'test/num_examples': 43793, 'score': 19707.881704092026, 'total_duration': 30753.16907453537, 'accumulated_submission_time': 19707.881704092026, 'accumulated_eval_time': 11040.682296514511, 'accumulated_logging_time': 2.767962694168091}
I0305 18:43:47.580951 140277079156480 logging_writer.py:48] [60443] accumulated_eval_time=11040.682297, accumulated_logging_time=2.767963, accumulated_submission_time=19707.881704, global_step=60443, preemption_count=0, score=19707.881704, test/accuracy=0.986218, test/loss=0.050726, test/mean_average_precision=0.283089, test/num_examples=43793, total_duration=30753.169075, train/accuracy=0.994258, train/loss=0.018081, train/mean_average_precision=0.690537, validation/accuracy=0.986947, validation/loss=0.047561, validation/mean_average_precision=0.292411, validation/num_examples=43793
I0305 18:44:06.856673 140283577931520 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.11295740306377411, loss=0.019096987321972847
I0305 18:44:39.596103 140277079156480 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.11702670902013779, loss=0.021708546206355095
I0305 18:45:12.491209 140283577931520 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.12658512592315674, loss=0.021925127133727074
I0305 18:45:44.736880 140277079156480 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.11155645549297333, loss=0.02182386815547943
I0305 18:46:17.493488 140283577931520 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.12661738693714142, loss=0.02374795265495777
I0305 18:46:49.921045 140277079156480 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.11802291125059128, loss=0.019871115684509277
I0305 18:47:22.846856 140283577931520 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.11547306925058365, loss=0.020483553409576416
I0305 18:47:47.630671 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:49:47.253962 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:49:50.343480 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:49:53.440633 140444430841664 submission_runner.py:411] Time since start: 31119.06s, 	Step: 61177, 	{'train/accuracy': 0.9941892027854919, 'train/loss': 0.01809295080602169, 'train/mean_average_precision': 0.6996259855388032, 'validation/accuracy': 0.9869688749313354, 'validation/loss': 0.04775715991854668, 'validation/mean_average_precision': 0.2939358251755667, 'validation/num_examples': 43793, 'test/accuracy': 0.9861498475074768, 'test/loss': 0.05087883025407791, 'test/mean_average_precision': 0.28106135242629554, 'test/num_examples': 43793, 'score': 19947.89863538742, 'total_duration': 31119.0564391613, 'accumulated_submission_time': 19947.89863538742, 'accumulated_eval_time': 11166.49221110344, 'accumulated_logging_time': 2.8065388202667236}
I0305 18:49:53.469842 140275669853952 logging_writer.py:48] [61177] accumulated_eval_time=11166.492211, accumulated_logging_time=2.806539, accumulated_submission_time=19947.898635, global_step=61177, preemption_count=0, score=19947.898635, test/accuracy=0.986150, test/loss=0.050879, test/mean_average_precision=0.281061, test/num_examples=43793, total_duration=31119.056439, train/accuracy=0.994189, train/loss=0.018093, train/mean_average_precision=0.699626, validation/accuracy=0.986969, validation/loss=0.047757, validation/mean_average_precision=0.293936, validation/num_examples=43793
I0305 18:50:01.717073 140276755326720 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.12755058705806732, loss=0.022282933816313744
I0305 18:50:35.357585 140275669853952 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.12304722517728806, loss=0.02092006430029869
I0305 18:51:09.313003 140276755326720 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.11738613247871399, loss=0.020963849499821663
I0305 18:51:42.851469 140275669853952 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.11744849383831024, loss=0.021650295704603195
I0305 18:52:16.272867 140276755326720 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.1335093080997467, loss=0.02476244978606701
I0305 18:52:49.819070 140275669853952 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.1098191887140274, loss=0.02081909030675888
I0305 18:53:22.881846 140276755326720 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.12475480884313583, loss=0.020967844873666763
I0305 18:53:53.550587 140444430841664 spec.py:321] Evaluating on the training split.
I0305 18:55:50.185454 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 18:55:53.301586 140444430841664 spec.py:349] Evaluating on the test split.
I0305 18:55:56.290391 140444430841664 submission_runner.py:411] Time since start: 31481.91s, 	Step: 61894, 	{'train/accuracy': 0.9942132234573364, 'train/loss': 0.0179898701608181, 'train/mean_average_precision': 0.6911047828460279, 'validation/accuracy': 0.9869351387023926, 'validation/loss': 0.04810238629579544, 'validation/mean_average_precision': 0.28834254794728287, 'validation/num_examples': 43793, 'test/accuracy': 0.9862441420555115, 'test/loss': 0.05120840668678284, 'test/mean_average_precision': 0.27970269187566327, 'test/num_examples': 43793, 'score': 20187.945281505585, 'total_duration': 31481.90607857704, 'accumulated_submission_time': 20187.945281505585, 'accumulated_eval_time': 11289.23188996315, 'accumulated_logging_time': 2.847842216491699}
I0305 18:55:56.317601 140277079156480 logging_writer.py:48] [61894] accumulated_eval_time=11289.231890, accumulated_logging_time=2.847842, accumulated_submission_time=20187.945282, global_step=61894, preemption_count=0, score=20187.945282, test/accuracy=0.986244, test/loss=0.051208, test/mean_average_precision=0.279703, test/num_examples=43793, total_duration=31481.906079, train/accuracy=0.994213, train/loss=0.017990, train/mean_average_precision=0.691105, validation/accuracy=0.986935, validation/loss=0.048102, validation/mean_average_precision=0.288343, validation/num_examples=43793
I0305 18:55:58.605612 140283569538816 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.13073930144309998, loss=0.022089608013629913
I0305 18:56:31.222780 140277079156480 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.114900603890419, loss=0.018505223095417023
I0305 18:57:03.965126 140283569538816 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.11984775215387344, loss=0.018726352602243423
I0305 18:57:36.874071 140277079156480 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.12777961790561676, loss=0.018819840624928474
I0305 18:58:09.937666 140283569538816 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.1336093246936798, loss=0.02405717223882675
I0305 18:58:42.826251 140277079156480 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.1418333500623703, loss=0.020644471049308777
I0305 18:59:15.679962 140283569538816 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.1174929141998291, loss=0.01890784502029419
I0305 18:59:48.399997 140277079156480 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.13236984610557556, loss=0.021055804565548897
I0305 18:59:56.343400 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:01:53.207842 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:01:56.265126 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:01:59.366838 140444430841664 submission_runner.py:411] Time since start: 31844.98s, 	Step: 62626, 	{'train/accuracy': 0.9943777322769165, 'train/loss': 0.017447976395487785, 'train/mean_average_precision': 0.6899870812187572, 'validation/accuracy': 0.9869586825370789, 'validation/loss': 0.04833875969052315, 'validation/mean_average_precision': 0.2845790269248508, 'validation/num_examples': 43793, 'test/accuracy': 0.9862605929374695, 'test/loss': 0.051550161093473434, 'test/mean_average_precision': 0.2782914654132266, 'test/num_examples': 43793, 'score': 20427.938327550888, 'total_duration': 31844.98264527321, 'accumulated_submission_time': 20427.938327550888, 'accumulated_eval_time': 11412.255279064178, 'accumulated_logging_time': 2.88604474067688}
I0305 19:01:59.394430 140275669853952 logging_writer.py:48] [62626] accumulated_eval_time=11412.255279, accumulated_logging_time=2.886045, accumulated_submission_time=20427.938328, global_step=62626, preemption_count=0, score=20427.938328, test/accuracy=0.986261, test/loss=0.051550, test/mean_average_precision=0.278291, test/num_examples=43793, total_duration=31844.982645, train/accuracy=0.994378, train/loss=0.017448, train/mean_average_precision=0.689987, validation/accuracy=0.986959, validation/loss=0.048339, validation/mean_average_precision=0.284579, validation/num_examples=43793
I0305 19:02:24.402908 140276755326720 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.11882108449935913, loss=0.020241079851984978
I0305 19:02:56.939812 140275669853952 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.11289329081773758, loss=0.020029079169034958
I0305 19:03:29.572237 140276755326720 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.12127544730901718, loss=0.02027132920920849
I0305 19:04:01.819503 140275669853952 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.1088559478521347, loss=0.017836198210716248
I0305 19:04:34.843658 140276755326720 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.11397603154182434, loss=0.01953938603401184
I0305 19:05:07.664335 140275669853952 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.11657058447599411, loss=0.018105169758200645
I0305 19:05:40.377775 140276755326720 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.1405249536037445, loss=0.021046336740255356
I0305 19:05:59.397671 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:07:49.705623 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:07:52.814654 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:07:55.857116 140444430841664 submission_runner.py:411] Time since start: 32201.47s, 	Step: 63360, 	{'train/accuracy': 0.9945623874664307, 'train/loss': 0.01691831462085247, 'train/mean_average_precision': 0.7184558394793336, 'validation/accuracy': 0.986990749835968, 'validation/loss': 0.04825434461236, 'validation/mean_average_precision': 0.2925786355656576, 'validation/num_examples': 43793, 'test/accuracy': 0.9862703084945679, 'test/loss': 0.05126902461051941, 'test/mean_average_precision': 0.280880005569525, 'test/num_examples': 43793, 'score': 20667.90618634224, 'total_duration': 32201.472920179367, 'accumulated_submission_time': 20667.90618634224, 'accumulated_eval_time': 11528.714675426483, 'accumulated_logging_time': 2.926175355911255}
I0305 19:07:55.885269 140277079156480 logging_writer.py:48] [63360] accumulated_eval_time=11528.714675, accumulated_logging_time=2.926175, accumulated_submission_time=20667.906186, global_step=63360, preemption_count=0, score=20667.906186, test/accuracy=0.986270, test/loss=0.051269, test/mean_average_precision=0.280880, test/num_examples=43793, total_duration=32201.472920, train/accuracy=0.994562, train/loss=0.016918, train/mean_average_precision=0.718456, validation/accuracy=0.986991, validation/loss=0.048254, validation/mean_average_precision=0.292579, validation/num_examples=43793
I0305 19:08:09.341655 140283577931520 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.11279021948575974, loss=0.020052926614880562
I0305 19:08:41.347973 140277079156480 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.12040284276008606, loss=0.021025437861680984
I0305 19:09:13.736926 140283577931520 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.11815830320119858, loss=0.018666261807084084
I0305 19:09:45.709167 140277079156480 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.1296529918909073, loss=0.020889177918434143
I0305 19:10:18.070242 140283577931520 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.13717611134052277, loss=0.0172467902302742
I0305 19:10:50.436085 140277079156480 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.10773128271102905, loss=0.018119411543011665
I0305 19:11:22.701626 140283577931520 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.1267973929643631, loss=0.019815724343061447
I0305 19:11:54.522277 140277079156480 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.13021612167358398, loss=0.019738702103495598
I0305 19:11:56.106081 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:13:52.698533 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:13:55.758892 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:13:58.741319 140444430841664 submission_runner.py:411] Time since start: 32564.36s, 	Step: 64106, 	{'train/accuracy': 0.9946512579917908, 'train/loss': 0.016558779403567314, 'train/mean_average_precision': 0.724740173698647, 'validation/accuracy': 0.9870366454124451, 'validation/loss': 0.048616185784339905, 'validation/mean_average_precision': 0.28946556430698595, 'validation/num_examples': 43793, 'test/accuracy': 0.9863195419311523, 'test/loss': 0.051568761467933655, 'test/mean_average_precision': 0.28404523532838727, 'test/num_examples': 43793, 'score': 20908.093871831894, 'total_duration': 32564.357125520706, 'accumulated_submission_time': 20908.093871831894, 'accumulated_eval_time': 11651.349862098694, 'accumulated_logging_time': 2.965777635574341}
I0305 19:13:58.769572 140276755326720 logging_writer.py:48] [64106] accumulated_eval_time=11651.349862, accumulated_logging_time=2.965778, accumulated_submission_time=20908.093872, global_step=64106, preemption_count=0, score=20908.093872, test/accuracy=0.986320, test/loss=0.051569, test/mean_average_precision=0.284045, test/num_examples=43793, total_duration=32564.357126, train/accuracy=0.994651, train/loss=0.016559, train/mean_average_precision=0.724740, validation/accuracy=0.987037, validation/loss=0.048616, validation/mean_average_precision=0.289466, validation/num_examples=43793
I0305 19:14:29.456932 140283569538816 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.13092269003391266, loss=0.019336408004164696
I0305 19:15:01.350534 140276755326720 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.12909665703773499, loss=0.02085067890584469
I0305 19:15:33.577608 140283569538816 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.13531717658042908, loss=0.02048124372959137
I0305 19:16:05.906701 140276755326720 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.11275426298379898, loss=0.018843181431293488
I0305 19:16:38.357595 140283569538816 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.12176721543073654, loss=0.0183754563331604
I0305 19:17:10.804686 140276755326720 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.1457667499780655, loss=0.020575111731886864
I0305 19:17:44.450329 140283569538816 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.1403980404138565, loss=0.020370597019791603
I0305 19:17:58.909399 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:19:57.001338 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:20:00.035496 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:20:03.279138 140444430841664 submission_runner.py:411] Time since start: 32928.89s, 	Step: 64844, 	{'train/accuracy': 0.995011568069458, 'train/loss': 0.015779579058289528, 'train/mean_average_precision': 0.7456424173940478, 'validation/accuracy': 0.9868746995925903, 'validation/loss': 0.04860717058181763, 'validation/mean_average_precision': 0.2935998478437701, 'validation/num_examples': 43793, 'test/accuracy': 0.9861548542976379, 'test/loss': 0.05154639482498169, 'test/mean_average_precision': 0.2841144505434902, 'test/num_examples': 43793, 'score': 21148.1995844841, 'total_duration': 32928.89493846893, 'accumulated_submission_time': 21148.1995844841, 'accumulated_eval_time': 11775.719557523727, 'accumulated_logging_time': 3.005262613296509}
I0305 19:20:03.308375 140277079156480 logging_writer.py:48] [64844] accumulated_eval_time=11775.719558, accumulated_logging_time=3.005263, accumulated_submission_time=21148.199584, global_step=64844, preemption_count=0, score=21148.199584, test/accuracy=0.986155, test/loss=0.051546, test/mean_average_precision=0.284114, test/num_examples=43793, total_duration=32928.894938, train/accuracy=0.995012, train/loss=0.015780, train/mean_average_precision=0.745642, validation/accuracy=0.986875, validation/loss=0.048607, validation/mean_average_precision=0.293600, validation/num_examples=43793
I0305 19:20:21.591692 140283577931520 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.12995992600917816, loss=0.01949235051870346
I0305 19:20:53.502016 140277079156480 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.14447258412837982, loss=0.01972218044102192
I0305 19:21:25.897594 140283577931520 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1083415076136589, loss=0.01972910389304161
I0305 19:21:58.187416 140277079156480 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.1302206665277481, loss=0.01901422068476677
I0305 19:22:30.988128 140283577931520 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.12372030317783356, loss=0.019040146842598915
I0305 19:23:03.554723 140277079156480 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.15504054725170135, loss=0.018487336114048958
I0305 19:23:35.694893 140283577931520 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.13461868464946747, loss=0.019756335765123367
I0305 19:24:03.323644 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:25:54.339037 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:25:57.407452 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:26:00.423927 140444430841664 submission_runner.py:411] Time since start: 33286.04s, 	Step: 65585, 	{'train/accuracy': 0.9950687885284424, 'train/loss': 0.015537439845502377, 'train/mean_average_precision': 0.744460006485492, 'validation/accuracy': 0.9869189262390137, 'validation/loss': 0.048930082470178604, 'validation/mean_average_precision': 0.2965670504331467, 'validation/num_examples': 43793, 'test/accuracy': 0.9861262440681458, 'test/loss': 0.051987893879413605, 'test/mean_average_precision': 0.2851634495635804, 'test/num_examples': 43793, 'score': 21388.181638002396, 'total_duration': 33286.03973245621, 'accumulated_submission_time': 21388.181638002396, 'accumulated_eval_time': 11892.819792985916, 'accumulated_logging_time': 3.0460572242736816}
I0305 19:26:00.452433 140275669853952 logging_writer.py:48] [65585] accumulated_eval_time=11892.819793, accumulated_logging_time=3.046057, accumulated_submission_time=21388.181638, global_step=65585, preemption_count=0, score=21388.181638, test/accuracy=0.986126, test/loss=0.051988, test/mean_average_precision=0.285163, test/num_examples=43793, total_duration=33286.039732, train/accuracy=0.995069, train/loss=0.015537, train/mean_average_precision=0.744460, validation/accuracy=0.986919, validation/loss=0.048930, validation/mean_average_precision=0.296567, validation/num_examples=43793
I0305 19:26:05.750121 140276755326720 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.12415565550327301, loss=0.01722601242363453
I0305 19:26:38.016525 140275669853952 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.137812539935112, loss=0.017675861716270447
I0305 19:27:10.241804 140276755326720 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.13196469843387604, loss=0.02013767696917057
I0305 19:27:42.222035 140275669853952 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.14243845641613007, loss=0.020676691085100174
I0305 19:28:14.440662 140276755326720 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.12940214574337006, loss=0.020708542317152023
I0305 19:28:46.459822 140275669853952 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.13647840917110443, loss=0.017760278657078743
I0305 19:29:18.575394 140276755326720 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.13745930790901184, loss=0.019103433936834335
I0305 19:29:50.534277 140275669853952 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.13502569496631622, loss=0.020273687317967415
I0305 19:30:00.484504 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:31:49.932616 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:31:53.031115 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:31:56.061722 140444430841664 submission_runner.py:411] Time since start: 33641.68s, 	Step: 66332, 	{'train/accuracy': 0.9950166940689087, 'train/loss': 0.015686750411987305, 'train/mean_average_precision': 0.7417331948355668, 'validation/accuracy': 0.9868669509887695, 'validation/loss': 0.048815615475177765, 'validation/mean_average_precision': 0.29481865949704666, 'validation/num_examples': 43793, 'test/accuracy': 0.986139714717865, 'test/loss': 0.052001871168613434, 'test/mean_average_precision': 0.2864536287657171, 'test/num_examples': 43793, 'score': 21628.180775880814, 'total_duration': 33641.677525281906, 'accumulated_submission_time': 21628.180775880814, 'accumulated_eval_time': 12008.396959066391, 'accumulated_logging_time': 3.0855839252471924}
I0305 19:31:56.090634 140277079156480 logging_writer.py:48] [66332] accumulated_eval_time=12008.396959, accumulated_logging_time=3.085584, accumulated_submission_time=21628.180776, global_step=66332, preemption_count=0, score=21628.180776, test/accuracy=0.986140, test/loss=0.052002, test/mean_average_precision=0.286454, test/num_examples=43793, total_duration=33641.677525, train/accuracy=0.995017, train/loss=0.015687, train/mean_average_precision=0.741733, validation/accuracy=0.986867, validation/loss=0.048816, validation/mean_average_precision=0.294819, validation/num_examples=43793
I0305 19:32:18.921382 140283569538816 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.11873222142457962, loss=0.0200581643730402
I0305 19:32:51.672738 140277079156480 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.14058971405029297, loss=0.020158743485808372
I0305 19:33:24.684165 140283569538816 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.14322833716869354, loss=0.019087638705968857
I0305 19:33:57.338425 140277079156480 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.128194198012352, loss=0.01688118651509285
I0305 19:34:29.990406 140283569538816 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.13339009881019592, loss=0.02226002886891365
I0305 19:35:02.851919 140277079156480 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.14082427322864532, loss=0.01944398321211338
I0305 19:35:35.609933 140283569538816 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.13909003138542175, loss=0.020594824105501175
I0305 19:35:56.118895 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:37:51.447613 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:37:54.544037 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:37:57.554423 140444430841664 submission_runner.py:411] Time since start: 34003.17s, 	Step: 67063, 	{'train/accuracy': 0.9946566820144653, 'train/loss': 0.016492776572704315, 'train/mean_average_precision': 0.7092785997806119, 'validation/accuracy': 0.9869843125343323, 'validation/loss': 0.04931102693080902, 'validation/mean_average_precision': 0.29387932542911893, 'validation/num_examples': 43793, 'test/accuracy': 0.9862896800041199, 'test/loss': 0.05232629552483559, 'test/mean_average_precision': 0.2879645668159487, 'test/num_examples': 43793, 'score': 21868.17398071289, 'total_duration': 34003.17022848129, 'accumulated_submission_time': 21868.17398071289, 'accumulated_eval_time': 12129.832447767258, 'accumulated_logging_time': 3.125771999359131}
I0305 19:37:57.583251 140275669853952 logging_writer.py:48] [67063] accumulated_eval_time=12129.832448, accumulated_logging_time=3.125772, accumulated_submission_time=21868.173981, global_step=67063, preemption_count=0, score=21868.173981, test/accuracy=0.986290, test/loss=0.052326, test/mean_average_precision=0.287965, test/num_examples=43793, total_duration=34003.170228, train/accuracy=0.994657, train/loss=0.016493, train/mean_average_precision=0.709279, validation/accuracy=0.986984, validation/loss=0.049311, validation/mean_average_precision=0.293879, validation/num_examples=43793
I0305 19:38:10.099672 140276755326720 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.10710097849369049, loss=0.0168096125125885
I0305 19:38:42.418557 140275669853952 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1437659114599228, loss=0.018820205703377724
I0305 19:39:14.697345 140276755326720 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.13742418587207794, loss=0.01989809423685074
I0305 19:39:46.998144 140275669853952 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.15510991215705872, loss=0.021263249218463898
I0305 19:40:19.173622 140276755326720 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.15113352239131927, loss=0.02093280665576458
I0305 19:40:51.511396 140275669853952 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.13931982219219208, loss=0.02080763503909111
I0305 19:41:24.050406 140276755326720 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.13369804620742798, loss=0.01803845912218094
I0305 19:41:56.058091 140275669853952 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.1581067591905594, loss=0.020744891837239265
I0305 19:41:57.706929 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:43:47.817003 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:43:50.885237 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:43:53.916050 140444430841664 submission_runner.py:411] Time since start: 34359.53s, 	Step: 67806, 	{'train/accuracy': 0.9946697354316711, 'train/loss': 0.01659359410405159, 'train/mean_average_precision': 0.7233143226581569, 'validation/accuracy': 0.9868795275688171, 'validation/loss': 0.0493571013212204, 'validation/mean_average_precision': 0.2930838377370317, 'validation/num_examples': 43793, 'test/accuracy': 0.9861851930618286, 'test/loss': 0.0525597482919693, 'test/mean_average_precision': 0.2859367983010787, 'test/num_examples': 43793, 'score': 22108.26478767395, 'total_duration': 34359.531853199005, 'accumulated_submission_time': 22108.26478767395, 'accumulated_eval_time': 12246.04151725769, 'accumulated_logging_time': 3.166090250015259}
I0305 19:43:53.945275 140283569538816 logging_writer.py:48] [67806] accumulated_eval_time=12246.041517, accumulated_logging_time=3.166090, accumulated_submission_time=22108.264788, global_step=67806, preemption_count=0, score=22108.264788, test/accuracy=0.986185, test/loss=0.052560, test/mean_average_precision=0.285937, test/num_examples=43793, total_duration=34359.531853, train/accuracy=0.994670, train/loss=0.016594, train/mean_average_precision=0.723314, validation/accuracy=0.986880, validation/loss=0.049357, validation/mean_average_precision=0.293084, validation/num_examples=43793
I0305 19:44:24.717289 140283577931520 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.13261878490447998, loss=0.01938328519463539
I0305 19:44:56.483661 140283569538816 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.1513126790523529, loss=0.021718665957450867
I0305 19:45:28.473024 140283577931520 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.13308517634868622, loss=0.018757227808237076
I0305 19:46:00.445155 140283569538816 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.12907923758029938, loss=0.01860143430531025
I0305 19:46:32.958674 140283577931520 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.14818212389945984, loss=0.02128753252327442
I0305 19:47:05.118859 140283569538816 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.13477863371372223, loss=0.02019413933157921
I0305 19:47:37.122613 140283577931520 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.12051352113485336, loss=0.018389951437711716
I0305 19:47:53.986845 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:49:43.133202 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:49:46.174913 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:49:49.208074 140444430841664 submission_runner.py:411] Time since start: 34714.82s, 	Step: 68554, 	{'train/accuracy': 0.9949089884757996, 'train/loss': 0.015775755047798157, 'train/mean_average_precision': 0.7360908466647706, 'validation/accuracy': 0.9869266152381897, 'validation/loss': 0.04924175143241882, 'validation/mean_average_precision': 0.2951497027189781, 'validation/num_examples': 43793, 'test/accuracy': 0.9861515164375305, 'test/loss': 0.05223073810338974, 'test/mean_average_precision': 0.28887261517120655, 'test/num_examples': 43793, 'score': 22348.272294044495, 'total_duration': 34714.82387185097, 'accumulated_submission_time': 22348.272294044495, 'accumulated_eval_time': 12361.262688159943, 'accumulated_logging_time': 3.2080864906311035}
I0305 19:49:49.236612 140275669853952 logging_writer.py:48] [68554] accumulated_eval_time=12361.262688, accumulated_logging_time=3.208086, accumulated_submission_time=22348.272294, global_step=68554, preemption_count=0, score=22348.272294, test/accuracy=0.986152, test/loss=0.052231, test/mean_average_precision=0.288873, test/num_examples=43793, total_duration=34714.823872, train/accuracy=0.994909, train/loss=0.015776, train/mean_average_precision=0.736091, validation/accuracy=0.986927, validation/loss=0.049242, validation/mean_average_precision=0.295150, validation/num_examples=43793
I0305 19:50:05.089743 140277079156480 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.13396568596363068, loss=0.01762685552239418
I0305 19:50:38.111242 140275669853952 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.14431697130203247, loss=0.018496742472052574
I0305 19:51:10.998102 140277079156480 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.13668622076511383, loss=0.02058923803269863
I0305 19:51:43.566562 140275669853952 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.1449693739414215, loss=0.019047923386096954
I0305 19:52:16.381217 140277079156480 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.14202621579170227, loss=0.018909666687250137
I0305 19:52:49.145765 140275669853952 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.14012382924556732, loss=0.01819964312016964
I0305 19:53:22.065022 140277079156480 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.1260186731815338, loss=0.018621617928147316
I0305 19:53:49.226924 140444430841664 spec.py:321] Evaluating on the training split.
I0305 19:55:41.630915 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 19:55:44.850052 140444430841664 spec.py:349] Evaluating on the test split.
I0305 19:55:47.920621 140444430841664 submission_runner.py:411] Time since start: 35073.54s, 	Step: 69283, 	{'train/accuracy': 0.9949740171432495, 'train/loss': 0.015598345547914505, 'train/mean_average_precision': 0.7509368956064921, 'validation/accuracy': 0.986963152885437, 'validation/loss': 0.049518078565597534, 'validation/mean_average_precision': 0.2943744306236122, 'validation/num_examples': 43793, 'test/accuracy': 0.986163318157196, 'test/loss': 0.05272916331887245, 'test/mean_average_precision': 0.2851350869481603, 'test/num_examples': 43793, 'score': 22588.225852251053, 'total_duration': 35073.536291360855, 'accumulated_submission_time': 22588.225852251053, 'accumulated_eval_time': 12479.956215381622, 'accumulated_logging_time': 3.2476272583007812}
I0305 19:55:47.950436 140276755326720 logging_writer.py:48] [69283] accumulated_eval_time=12479.956215, accumulated_logging_time=3.247627, accumulated_submission_time=22588.225852, global_step=69283, preemption_count=0, score=22588.225852, test/accuracy=0.986163, test/loss=0.052729, test/mean_average_precision=0.285135, test/num_examples=43793, total_duration=35073.536291, train/accuracy=0.994974, train/loss=0.015598, train/mean_average_precision=0.750937, validation/accuracy=0.986963, validation/loss=0.049518, validation/mean_average_precision=0.294374, validation/num_examples=43793
I0305 19:55:53.820662 140283577931520 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.130177304148674, loss=0.019233468919992447
I0305 19:56:26.417132 140276755326720 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.15032507479190826, loss=0.022421790286898613
I0305 19:56:58.967191 140283577931520 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.13805878162384033, loss=0.01954713463783264
I0305 19:57:31.190765 140276755326720 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.14848878979682922, loss=0.018930304795503616
I0305 19:58:03.435683 140283577931520 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.140405535697937, loss=0.018304795026779175
I0305 19:58:35.403610 140276755326720 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.1171695664525032, loss=0.018464552238583565
I0305 19:59:07.787099 140283577931520 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.1470782458782196, loss=0.018799064680933952
I0305 19:59:40.315196 140276755326720 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.13900712132453918, loss=0.019186487421393394
I0305 19:59:47.958863 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:01:41.229763 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:01:44.383264 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:01:47.471643 140444430841664 submission_runner.py:411] Time since start: 35433.09s, 	Step: 70025, 	{'train/accuracy': 0.9950136542320251, 'train/loss': 0.015369239263236523, 'train/mean_average_precision': 0.7516144347200444, 'validation/accuracy': 0.9869911670684814, 'validation/loss': 0.04963357746601105, 'validation/mean_average_precision': 0.2945740366852919, 'validation/num_examples': 43793, 'test/accuracy': 0.9862056374549866, 'test/loss': 0.052915312349796295, 'test/mean_average_precision': 0.28510508019760306, 'test/num_examples': 43793, 'score': 22828.19718813896, 'total_duration': 35433.087446689606, 'accumulated_submission_time': 22828.19718813896, 'accumulated_eval_time': 12599.468941926956, 'accumulated_logging_time': 3.289933919906616}
I0305 20:01:47.501855 140277079156480 logging_writer.py:48] [70025] accumulated_eval_time=12599.468942, accumulated_logging_time=3.289934, accumulated_submission_time=22828.197188, global_step=70025, preemption_count=0, score=22828.197188, test/accuracy=0.986206, test/loss=0.052915, test/mean_average_precision=0.285105, test/num_examples=43793, total_duration=35433.087447, train/accuracy=0.995014, train/loss=0.015369, train/mean_average_precision=0.751614, validation/accuracy=0.986991, validation/loss=0.049634, validation/mean_average_precision=0.294574, validation/num_examples=43793
I0305 20:02:12.723280 140283569538816 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.14260387420654297, loss=0.016594232991337776
I0305 20:02:45.223715 140277079156480 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.13890329003334045, loss=0.02025514282286167
I0305 20:03:17.791470 140283569538816 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.14131712913513184, loss=0.019188493490219116
I0305 20:03:49.986065 140277079156480 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1394326388835907, loss=0.018868334591388702
I0305 20:04:22.632360 140283569538816 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.14354676008224487, loss=0.020536979660391808
I0305 20:04:55.230062 140277079156480 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.15825021266937256, loss=0.018944216892123222
I0305 20:05:27.621042 140283569538816 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.1202801987528801, loss=0.016767043620347977
I0305 20:05:47.534348 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:07:42.820839 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:07:46.174363 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:07:49.471606 140444430841664 submission_runner.py:411] Time since start: 35795.09s, 	Step: 70763, 	{'train/accuracy': 0.9954207539558411, 'train/loss': 0.014500892721116543, 'train/mean_average_precision': 0.7669779487185071, 'validation/accuracy': 0.9868775010108948, 'validation/loss': 0.04957318305969238, 'validation/mean_average_precision': 0.29364509992147486, 'validation/num_examples': 43793, 'test/accuracy': 0.9860811829566956, 'test/loss': 0.05280185863375664, 'test/mean_average_precision': 0.2839924188185285, 'test/num_examples': 43793, 'score': 23068.196282863617, 'total_duration': 35795.0873939991, 'accumulated_submission_time': 23068.196282863617, 'accumulated_eval_time': 12721.406133413315, 'accumulated_logging_time': 3.3317060470581055}
I0305 20:07:49.502194 140275669853952 logging_writer.py:48] [70763] accumulated_eval_time=12721.406133, accumulated_logging_time=3.331706, accumulated_submission_time=23068.196283, global_step=70763, preemption_count=0, score=23068.196283, test/accuracy=0.986081, test/loss=0.052802, test/mean_average_precision=0.283992, test/num_examples=43793, total_duration=35795.087394, train/accuracy=0.995421, train/loss=0.014501, train/mean_average_precision=0.766978, validation/accuracy=0.986878, validation/loss=0.049573, validation/mean_average_precision=0.293645, validation/num_examples=43793
I0305 20:08:01.943254 140283577931520 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.15198196470737457, loss=0.01863005757331848
I0305 20:08:34.451365 140275669853952 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.15348628163337708, loss=0.02030472829937935
I0305 20:09:07.299864 140283577931520 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.14836996793746948, loss=0.01794123835861683
I0305 20:09:39.874608 140275669853952 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.13743974268436432, loss=0.01759764365851879
I0305 20:10:12.616814 140283577931520 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.13783568143844604, loss=0.01769498735666275
I0305 20:10:44.960125 140275669853952 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.15169166028499603, loss=0.020025338977575302
I0305 20:11:17.573223 140283577931520 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.14291828870773315, loss=0.018632346764206886
I0305 20:11:49.640361 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:13:46.204232 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:13:49.292853 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:13:52.325359 140444430841664 submission_runner.py:411] Time since start: 36157.94s, 	Step: 71500, 	{'train/accuracy': 0.9954916834831238, 'train/loss': 0.014190438203513622, 'train/mean_average_precision': 0.7654607218142788, 'validation/accuracy': 0.9869996905326843, 'validation/loss': 0.049864936619997025, 'validation/mean_average_precision': 0.2918474328552706, 'validation/num_examples': 43793, 'test/accuracy': 0.9862028956413269, 'test/loss': 0.05319730564951897, 'test/mean_average_precision': 0.2830510085013739, 'test/num_examples': 43793, 'score': 23308.296140670776, 'total_duration': 36157.941160440445, 'accumulated_submission_time': 23308.296140670776, 'accumulated_eval_time': 12844.09109044075, 'accumulated_logging_time': 3.37443470954895}
I0305 20:13:52.355871 140277079156480 logging_writer.py:48] [71500] accumulated_eval_time=12844.091090, accumulated_logging_time=3.374435, accumulated_submission_time=23308.296141, global_step=71500, preemption_count=0, score=23308.296141, test/accuracy=0.986203, test/loss=0.053197, test/mean_average_precision=0.283051, test/num_examples=43793, total_duration=36157.941160, train/accuracy=0.995492, train/loss=0.014190, train/mean_average_precision=0.765461, validation/accuracy=0.987000, validation/loss=0.049865, validation/mean_average_precision=0.291847, validation/num_examples=43793
I0305 20:13:52.739723 140283569538816 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.14054511487483978, loss=0.017160052433609962
I0305 20:14:25.061995 140277079156480 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.14013558626174927, loss=0.017184622585773468
I0305 20:14:57.217595 140283569538816 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.1329011768102646, loss=0.01652158796787262
I0305 20:15:29.895503 140277079156480 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.1349756270647049, loss=0.018148580566048622
I0305 20:16:02.560789 140283569538816 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.13396398723125458, loss=0.019353924319148064
I0305 20:16:35.060043 140277079156480 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.14845220744609833, loss=0.016700508072972298
I0305 20:17:06.986560 140283569538816 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.13784264028072357, loss=0.017207659780979156
I0305 20:17:39.112195 140277079156480 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.12971720099449158, loss=0.016437385231256485
I0305 20:17:52.384215 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:19:42.063524 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:19:45.072502 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:19:48.133819 140444430841664 submission_runner.py:411] Time since start: 36513.75s, 	Step: 72242, 	{'train/accuracy': 0.9953582286834717, 'train/loss': 0.014519170857965946, 'train/mean_average_precision': 0.7666464923592602, 'validation/accuracy': 0.9869623780250549, 'validation/loss': 0.05003394931554794, 'validation/mean_average_precision': 0.29206171128001307, 'validation/num_examples': 43793, 'test/accuracy': 0.9862096309661865, 'test/loss': 0.05341162532567978, 'test/mean_average_precision': 0.28410793966182907, 'test/num_examples': 43793, 'score': 23548.290924072266, 'total_duration': 36513.74961900711, 'accumulated_submission_time': 23548.290924072266, 'accumulated_eval_time': 12959.840638399124, 'accumulated_logging_time': 3.4160678386688232}
I0305 20:19:48.162900 140275669853952 logging_writer.py:48] [72242] accumulated_eval_time=12959.840638, accumulated_logging_time=3.416068, accumulated_submission_time=23548.290924, global_step=72242, preemption_count=0, score=23548.290924, test/accuracy=0.986210, test/loss=0.053412, test/mean_average_precision=0.284108, test/num_examples=43793, total_duration=36513.749619, train/accuracy=0.995358, train/loss=0.014519, train/mean_average_precision=0.766646, validation/accuracy=0.986962, validation/loss=0.050034, validation/mean_average_precision=0.292062, validation/num_examples=43793
I0305 20:20:07.263510 140283577931520 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.14640657603740692, loss=0.020127372816205025
I0305 20:20:39.255952 140275669853952 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1494974046945572, loss=0.01913278177380562
I0305 20:21:10.937767 140283577931520 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.1332816630601883, loss=0.01719016209244728
I0305 20:21:42.970813 140275669853952 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.13309739530086517, loss=0.017090892419219017
I0305 20:22:15.107375 140283577931520 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.17912842333316803, loss=0.019987083971500397
I0305 20:22:46.998471 140275669853952 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.13046416640281677, loss=0.01746203564107418
I0305 20:23:19.309984 140283577931520 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.13446472585201263, loss=0.016336608678102493
I0305 20:23:48.257263 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:25:42.766435 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:25:45.777431 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:25:48.747015 140444430841664 submission_runner.py:411] Time since start: 36874.36s, 	Step: 72992, 	{'train/accuracy': 0.9953705072402954, 'train/loss': 0.014444353990256786, 'train/mean_average_precision': 0.7663247210353313, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.050080012530088425, 'validation/mean_average_precision': 0.2917908410573102, 'validation/num_examples': 43793, 'test/accuracy': 0.9861733913421631, 'test/loss': 0.05351288244128227, 'test/mean_average_precision': 0.283005966453115, 'test/num_examples': 43793, 'score': 23788.352286815643, 'total_duration': 36874.36281085014, 'accumulated_submission_time': 23788.352286815643, 'accumulated_eval_time': 13080.330332517624, 'accumulated_logging_time': 3.4564311504364014}
I0305 20:25:48.776302 140277079156480 logging_writer.py:48] [72992] accumulated_eval_time=13080.330333, accumulated_logging_time=3.456431, accumulated_submission_time=23788.352287, global_step=72992, preemption_count=0, score=23788.352287, test/accuracy=0.986173, test/loss=0.053513, test/mean_average_precision=0.283006, test/num_examples=43793, total_duration=36874.362811, train/accuracy=0.995371, train/loss=0.014444, train/mean_average_precision=0.766325, validation/accuracy=0.986956, validation/loss=0.050080, validation/mean_average_precision=0.291791, validation/num_examples=43793
I0305 20:25:51.694037 140283569538816 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.14592525362968445, loss=0.01823895052075386
I0305 20:26:24.002388 140277079156480 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.14900235831737518, loss=0.01843733713030815
I0305 20:26:56.277652 140283569538816 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.13198761641979218, loss=0.01817604899406433
I0305 20:27:28.536702 140277079156480 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.1521662473678589, loss=0.01890820637345314
I0305 20:28:01.019350 140283569538816 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.14282231032848358, loss=0.018452905118465424
I0305 20:28:33.001703 140277079156480 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1317286491394043, loss=0.01650978997349739
I0305 20:29:04.732169 140283569538816 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.15140020847320557, loss=0.0184154212474823
I0305 20:29:36.372190 140277079156480 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.1510990858078003, loss=0.0193478986620903
I0305 20:29:48.793355 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:31:39.736485 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:31:44.329256 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:31:47.330345 140444430841664 submission_runner.py:411] Time since start: 37232.95s, 	Step: 73740, 	{'train/accuracy': 0.9952463507652283, 'train/loss': 0.014853943139314651, 'train/mean_average_precision': 0.772484291095771, 'validation/accuracy': 0.9869952201843262, 'validation/loss': 0.050183337181806564, 'validation/mean_average_precision': 0.29287669063169997, 'validation/num_examples': 43793, 'test/accuracy': 0.9862037301063538, 'test/loss': 0.05349535867571831, 'test/mean_average_precision': 0.2853542408933036, 'test/num_examples': 43793, 'score': 24028.334567308426, 'total_duration': 37232.946142435074, 'accumulated_submission_time': 24028.334567308426, 'accumulated_eval_time': 13198.867261886597, 'accumulated_logging_time': 3.4985146522521973}
I0305 20:31:47.360553 140276755326720 logging_writer.py:48] [73740] accumulated_eval_time=13198.867262, accumulated_logging_time=3.498515, accumulated_submission_time=24028.334567, global_step=73740, preemption_count=0, score=24028.334567, test/accuracy=0.986204, test/loss=0.053495, test/mean_average_precision=0.285354, test/num_examples=43793, total_duration=37232.946142, train/accuracy=0.995246, train/loss=0.014854, train/mean_average_precision=0.772484, validation/accuracy=0.986995, validation/loss=0.050183, validation/mean_average_precision=0.292877, validation/num_examples=43793
I0305 20:32:07.348093 140283577931520 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.14945103228092194, loss=0.018415119498968124
I0305 20:32:39.827949 140276755326720 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.1541306972503662, loss=0.018460391089320183
I0305 20:33:12.282672 140283577931520 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.13184376060962677, loss=0.017258066684007645
I0305 20:33:44.179076 140276755326720 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.16165465116500854, loss=0.018730133771896362
I0305 20:34:16.496839 140283577931520 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.1296747922897339, loss=0.01688035950064659
I0305 20:34:49.006309 140276755326720 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.14722824096679688, loss=0.017247283831238747
I0305 20:35:21.809687 140283577931520 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.14588136970996857, loss=0.015571613796055317
I0305 20:35:47.629433 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:37:39.011166 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:37:42.096436 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:37:45.144101 140444430841664 submission_runner.py:411] Time since start: 37590.76s, 	Step: 74480, 	{'train/accuracy': 0.9953171610832214, 'train/loss': 0.014523771591484547, 'train/mean_average_precision': 0.7563114128114683, 'validation/accuracy': 0.9869773983955383, 'validation/loss': 0.05015420913696289, 'validation/mean_average_precision': 0.29368117677057287, 'validation/num_examples': 43793, 'test/accuracy': 0.9862239360809326, 'test/loss': 0.05351460725069046, 'test/mean_average_precision': 0.28635000612234157, 'test/num_examples': 43793, 'score': 24268.569067955017, 'total_duration': 37590.75980067253, 'accumulated_submission_time': 24268.569067955017, 'accumulated_eval_time': 13316.381780862808, 'accumulated_logging_time': 3.541311264038086}
I0305 20:37:45.173533 140275669853952 logging_writer.py:48] [74480] accumulated_eval_time=13316.381781, accumulated_logging_time=3.541311, accumulated_submission_time=24268.569068, global_step=74480, preemption_count=0, score=24268.569068, test/accuracy=0.986224, test/loss=0.053515, test/mean_average_precision=0.286350, test/num_examples=43793, total_duration=37590.759801, train/accuracy=0.995317, train/loss=0.014524, train/mean_average_precision=0.756311, validation/accuracy=0.986977, validation/loss=0.050154, validation/mean_average_precision=0.293681, validation/num_examples=43793
I0305 20:37:52.164776 140277079156480 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.13120849430561066, loss=0.016617296263575554
I0305 20:38:24.958787 140275669853952 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.15002059936523438, loss=0.01783347688615322
I0305 20:38:57.562272 140277079156480 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.1562204211950302, loss=0.017824918031692505
I0305 20:39:30.114203 140275669853952 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.14937804639339447, loss=0.01797289401292801
I0305 20:40:02.743906 140277079156480 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.13727368414402008, loss=0.018483376130461693
I0305 20:40:35.260565 140275669853952 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.11819464713335037, loss=0.015783023089170456
I0305 20:41:08.265048 140277079156480 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.15486976504325867, loss=0.01910959929227829
I0305 20:41:41.097735 140275669853952 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.1578918844461441, loss=0.01873694732785225
I0305 20:41:45.382979 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:43:38.599407 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:43:41.675493 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:43:44.757139 140444430841664 submission_runner.py:411] Time since start: 37950.37s, 	Step: 75214, 	{'train/accuracy': 0.9954608678817749, 'train/loss': 0.014255638234317303, 'train/mean_average_precision': 0.7678145129889065, 'validation/accuracy': 0.9869444966316223, 'validation/loss': 0.05010787025094032, 'validation/mean_average_precision': 0.29452733556286476, 'validation/num_examples': 43793, 'test/accuracy': 0.9862167835235596, 'test/loss': 0.05344220623373985, 'test/mean_average_precision': 0.28588109973052184, 'test/num_examples': 43793, 'score': 24508.743983983994, 'total_duration': 37950.37283325195, 'accumulated_submission_time': 24508.743983983994, 'accumulated_eval_time': 13435.755778074265, 'accumulated_logging_time': 3.583484649658203}
I0305 20:43:44.788102 140276755326720 logging_writer.py:48] [75214] accumulated_eval_time=13435.755778, accumulated_logging_time=3.583485, accumulated_submission_time=24508.743984, global_step=75214, preemption_count=0, score=24508.743984, test/accuracy=0.986217, test/loss=0.053442, test/mean_average_precision=0.285881, test/num_examples=43793, total_duration=37950.372833, train/accuracy=0.995461, train/loss=0.014256, train/mean_average_precision=0.767815, validation/accuracy=0.986944, validation/loss=0.050108, validation/mean_average_precision=0.294527, validation/num_examples=43793
I0305 20:44:13.428294 140283577931520 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.12591248750686646, loss=0.016777006909251213
I0305 20:44:45.827778 140276755326720 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.14203286170959473, loss=0.016170833259820938
I0305 20:45:18.315378 140283577931520 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.15901510417461395, loss=0.01920279860496521
I0305 20:45:50.427695 140276755326720 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.14337435364723206, loss=0.017773887142539024
I0305 20:46:22.995337 140283577931520 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.13946115970611572, loss=0.01986420899629593
I0305 20:46:55.239854 140276755326720 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.13100695610046387, loss=0.0164084080606699
I0305 20:47:27.886734 140283577931520 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.14612729847431183, loss=0.018585342913866043
I0305 20:47:44.786707 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:49:33.138882 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:49:36.251616 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:49:39.291248 140444430841664 submission_runner.py:411] Time since start: 38304.91s, 	Step: 75953, 	{'train/accuracy': 0.9954636096954346, 'train/loss': 0.014144206419587135, 'train/mean_average_precision': 0.7713807343557014, 'validation/accuracy': 0.9870074391365051, 'validation/loss': 0.050181545317173004, 'validation/mean_average_precision': 0.29517386035988996, 'validation/num_examples': 43793, 'test/accuracy': 0.9862155318260193, 'test/loss': 0.05356040224432945, 'test/mean_average_precision': 0.28565870781791536, 'test/num_examples': 43793, 'score': 24748.70914363861, 'total_duration': 38304.90705013275, 'accumulated_submission_time': 24748.70914363861, 'accumulated_eval_time': 13550.260266304016, 'accumulated_logging_time': 3.625715970993042}
I0305 20:49:39.322077 140275669853952 logging_writer.py:48] [75953] accumulated_eval_time=13550.260266, accumulated_logging_time=3.625716, accumulated_submission_time=24748.709144, global_step=75953, preemption_count=0, score=24748.709144, test/accuracy=0.986216, test/loss=0.053560, test/mean_average_precision=0.285659, test/num_examples=43793, total_duration=38304.907050, train/accuracy=0.995464, train/loss=0.014144, train/mean_average_precision=0.771381, validation/accuracy=0.987007, validation/loss=0.050182, validation/mean_average_precision=0.295174, validation/num_examples=43793
I0305 20:49:54.870247 140283569538816 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.15630652010440826, loss=0.018632613122463226
I0305 20:50:27.269289 140275669853952 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.14250820875167847, loss=0.01696990244090557
I0305 20:50:59.033577 140283569538816 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.15200455486774445, loss=0.020003579556941986
I0305 20:51:31.312666 140275669853952 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.13670185208320618, loss=0.016797391697764397
I0305 20:52:03.766442 140283569538816 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.13755670189857483, loss=0.017538772895932198
I0305 20:52:35.891791 140275669853952 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.15799522399902344, loss=0.01679925061762333
I0305 20:53:08.034445 140283569538816 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.1403711438179016, loss=0.017735153436660767
I0305 20:53:39.350104 140444430841664 spec.py:321] Evaluating on the training split.
I0305 20:55:32.941113 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 20:55:36.345090 140444430841664 spec.py:349] Evaluating on the test split.
I0305 20:55:39.692352 140444430841664 submission_runner.py:411] Time since start: 38665.31s, 	Step: 76699, 	{'train/accuracy': 0.995585560798645, 'train/loss': 0.013901792466640472, 'train/mean_average_precision': 0.7811432594628471, 'validation/accuracy': 0.9869680404663086, 'validation/loss': 0.05020562559366226, 'validation/mean_average_precision': 0.29388701770989656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861986637115479, 'test/loss': 0.053518351167440414, 'test/mean_average_precision': 0.28523059295665065, 'test/num_examples': 43793, 'score': 24988.704931735992, 'total_duration': 38665.30813074112, 'accumulated_submission_time': 24988.704931735992, 'accumulated_eval_time': 13670.602442026138, 'accumulated_logging_time': 3.6674931049346924}
I0305 20:55:39.724173 140276755326720 logging_writer.py:48] [76699] accumulated_eval_time=13670.602442, accumulated_logging_time=3.667493, accumulated_submission_time=24988.704932, global_step=76699, preemption_count=0, score=24988.704932, test/accuracy=0.986199, test/loss=0.053518, test/mean_average_precision=0.285231, test/num_examples=43793, total_duration=38665.308131, train/accuracy=0.995586, train/loss=0.013902, train/mean_average_precision=0.781143, validation/accuracy=0.986968, validation/loss=0.050206, validation/mean_average_precision=0.293887, validation/num_examples=43793
I0305 20:55:40.430193 140277079156480 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.1374395191669464, loss=0.018094174563884735
I0305 20:56:13.487452 140276755326720 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.13111934065818787, loss=0.01722831092774868
I0305 20:56:46.341746 140277079156480 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.14040979743003845, loss=0.017472194507718086
I0305 20:57:19.469516 140276755326720 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.14103898406028748, loss=0.017334917560219765
I0305 20:57:51.526378 140277079156480 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.1537512093782425, loss=0.01935667358338833
I0305 20:58:23.808761 140276755326720 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.14179028570652008, loss=0.01692318543791771
I0305 20:58:55.489773 140277079156480 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.1489369124174118, loss=0.017032090574502945
I0305 20:59:27.432814 140276755326720 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.15840263664722443, loss=0.01866699941456318
I0305 20:59:39.865516 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:01:35.586475 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:01:38.709515 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:01:41.728803 140444430841664 submission_runner.py:411] Time since start: 39027.34s, 	Step: 77440, 	{'train/accuracy': 0.9955874085426331, 'train/loss': 0.013882093131542206, 'train/mean_average_precision': 0.7873145623554283, 'validation/accuracy': 0.9869798421859741, 'validation/loss': 0.05025235563516617, 'validation/mean_average_precision': 0.29456755738555024, 'validation/num_examples': 43793, 'test/accuracy': 0.9862180352210999, 'test/loss': 0.053591467440128326, 'test/mean_average_precision': 0.28577016603080413, 'test/num_examples': 43793, 'score': 25228.811174869537, 'total_duration': 39027.34461021423, 'accumulated_submission_time': 25228.811174869537, 'accumulated_eval_time': 13792.465679645538, 'accumulated_logging_time': 3.7109150886535645}
I0305 21:01:41.759384 140275669853952 logging_writer.py:48] [77440] accumulated_eval_time=13792.465680, accumulated_logging_time=3.710915, accumulated_submission_time=25228.811175, global_step=77440, preemption_count=0, score=25228.811175, test/accuracy=0.986218, test/loss=0.053591, test/mean_average_precision=0.285770, test/num_examples=43793, total_duration=39027.344610, train/accuracy=0.995587, train/loss=0.013882, train/mean_average_precision=0.787315, validation/accuracy=0.986980, validation/loss=0.050252, validation/mean_average_precision=0.294568, validation/num_examples=43793
I0305 21:02:01.114485 140283577931520 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.13621000945568085, loss=0.01637854054570198
I0305 21:02:33.304784 140275669853952 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.1334255337715149, loss=0.017452849075198174
I0305 21:03:05.230046 140283577931520 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.13435140252113342, loss=0.017788926139473915
I0305 21:03:37.190832 140275669853952 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.1314680427312851, loss=0.01565968617796898
I0305 21:04:09.425713 140283577931520 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.14563609659671783, loss=0.019432974979281425
I0305 21:04:41.530910 140275669853952 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.14231158792972565, loss=0.018824942409992218
I0305 21:05:13.892302 140283577931520 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.12766984105110168, loss=0.016130337491631508
I0305 21:05:42.004705 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:07:29.778829 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:07:32.956527 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:07:36.037106 140444430841664 submission_runner.py:411] Time since start: 39381.65s, 	Step: 78188, 	{'train/accuracy': 0.9955474734306335, 'train/loss': 0.01395756471902132, 'train/mean_average_precision': 0.7654514914396318, 'validation/accuracy': 0.986976146697998, 'validation/loss': 0.050253089517354965, 'validation/mean_average_precision': 0.2955371902344945, 'validation/num_examples': 43793, 'test/accuracy': 0.9862058162689209, 'test/loss': 0.05359548702836037, 'test/mean_average_precision': 0.28537590114608347, 'test/num_examples': 43793, 'score': 25469.02388048172, 'total_duration': 39381.652913331985, 'accumulated_submission_time': 25469.02388048172, 'accumulated_eval_time': 13906.49803853035, 'accumulated_logging_time': 3.752807378768921}
I0305 21:07:36.067517 140276755326720 logging_writer.py:48] [78188] accumulated_eval_time=13906.498039, accumulated_logging_time=3.752807, accumulated_submission_time=25469.023880, global_step=78188, preemption_count=0, score=25469.023880, test/accuracy=0.986206, test/loss=0.053595, test/mean_average_precision=0.285376, test/num_examples=43793, total_duration=39381.652913, train/accuracy=0.995547, train/loss=0.013958, train/mean_average_precision=0.765451, validation/accuracy=0.986976, validation/loss=0.050253, validation/mean_average_precision=0.295537, validation/num_examples=43793
I0305 21:07:40.303275 140283569538816 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.13558481633663177, loss=0.015130255371332169
I0305 21:08:13.188583 140276755326720 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.1568671315908432, loss=0.018816517665982246
I0305 21:08:45.752200 140283569538816 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.14811448752880096, loss=0.02076132595539093
I0305 21:09:18.690059 140276755326720 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.13148146867752075, loss=0.015861602500081062
I0305 21:09:51.186253 140283569538816 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.15338510274887085, loss=0.02126668021082878
I0305 21:10:23.630322 140276755326720 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.1382879912853241, loss=0.018636299297213554
I0305 21:10:55.980630 140283569538816 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.14683304727077484, loss=0.01839795522391796
I0305 21:11:28.664946 140276755326720 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.13282063603401184, loss=0.01549350842833519
I0305 21:11:36.253383 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:13:27.048507 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:13:30.123779 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:13:33.260322 140444430841664 submission_runner.py:411] Time since start: 39738.88s, 	Step: 78924, 	{'train/accuracy': 0.9955233931541443, 'train/loss': 0.01403582189232111, 'train/mean_average_precision': 0.7749583686005181, 'validation/accuracy': 0.9869713187217712, 'validation/loss': 0.05021810531616211, 'validation/mean_average_precision': 0.29578712873191515, 'validation/num_examples': 43793, 'test/accuracy': 0.9861990809440613, 'test/loss': 0.053563736379146576, 'test/mean_average_precision': 0.2856132740395028, 'test/num_examples': 43793, 'score': 25709.17568540573, 'total_duration': 39738.876128435135, 'accumulated_submission_time': 25709.17568540573, 'accumulated_eval_time': 14023.504931926727, 'accumulated_logging_time': 3.79575777053833}
I0305 21:13:33.290723 140275669853952 logging_writer.py:48] [78924] accumulated_eval_time=14023.504932, accumulated_logging_time=3.795758, accumulated_submission_time=25709.175685, global_step=78924, preemption_count=0, score=25709.175685, test/accuracy=0.986199, test/loss=0.053564, test/mean_average_precision=0.285613, test/num_examples=43793, total_duration=39738.876128, train/accuracy=0.995523, train/loss=0.014036, train/mean_average_precision=0.774958, validation/accuracy=0.986971, validation/loss=0.050218, validation/mean_average_precision=0.295787, validation/num_examples=43793
I0305 21:13:58.147361 140277079156480 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.14518959820270538, loss=0.01918884553015232
I0305 21:14:30.497447 140275669853952 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.14429858326911926, loss=0.016425281763076782
I0305 21:15:02.754656 140277079156480 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.1387956440448761, loss=0.0170113667845726
I0305 21:15:34.861441 140275669853952 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.14230197668075562, loss=0.018169915303587914
I0305 21:16:06.964732 140277079156480 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.14567583799362183, loss=0.017372416332364082
I0305 21:16:39.109066 140275669853952 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.15016739070415497, loss=0.01763828657567501
I0305 21:17:12.042692 140277079156480 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.14535075426101685, loss=0.01651625707745552
I0305 21:17:33.297814 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:19:21.820567 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:19:24.906316 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:19:27.939353 140444430841664 submission_runner.py:411] Time since start: 40093.56s, 	Step: 79667, 	{'train/accuracy': 0.9955575466156006, 'train/loss': 0.014005761593580246, 'train/mean_average_precision': 0.7686545535718703, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.05021429806947708, 'validation/mean_average_precision': 0.29529583902026846, 'validation/num_examples': 43793, 'test/accuracy': 0.9861965775489807, 'test/loss': 0.053561218082904816, 'test/mean_average_precision': 0.2857440594827082, 'test/num_examples': 43793, 'score': 25949.150153398514, 'total_duration': 40093.55513715744, 'accumulated_submission_time': 25949.150153398514, 'accumulated_eval_time': 14138.14640378952, 'accumulated_logging_time': 3.837240219116211}
I0305 21:19:27.971276 140276755326720 logging_writer.py:48] [79667] accumulated_eval_time=14138.146404, accumulated_logging_time=3.837240, accumulated_submission_time=25949.150153, global_step=79667, preemption_count=0, score=25949.150153, test/accuracy=0.986197, test/loss=0.053561, test/mean_average_precision=0.285744, test/num_examples=43793, total_duration=40093.555137, train/accuracy=0.995558, train/loss=0.014006, train/mean_average_precision=0.768655, validation/accuracy=0.986974, validation/loss=0.050214, validation/mean_average_precision=0.295296, validation/num_examples=43793
I0305 21:19:39.539330 140283577931520 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.1476573944091797, loss=0.017093665897846222
I0305 21:20:12.525773 140276755326720 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.14075393974781036, loss=0.01769869588315487
I0305 21:20:45.686908 140283577931520 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.14836473762989044, loss=0.01825997792184353
I0305 21:21:18.551276 140276755326720 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.14798301458358765, loss=0.01957450993359089
I0305 21:21:50.836225 140283577931520 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.15050816535949707, loss=0.01933390647172928
I0305 21:22:23.686684 140276755326720 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.1405777782201767, loss=0.01695401221513748
I0305 21:22:56.863746 140283577931520 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.1497499644756317, loss=0.016115328297019005
I0305 21:23:28.035498 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:25:23.257948 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:25:26.326472 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:25:29.312454 140444430841664 submission_runner.py:411] Time since start: 40454.93s, 	Step: 80396, 	{'train/accuracy': 0.9955423474311829, 'train/loss': 0.013979917392134666, 'train/mean_average_precision': 0.7752668434431129, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29449261448258673, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28566935144235855, 'test/num_examples': 43793, 'score': 26189.18081855774, 'total_duration': 40454.92825961113, 'accumulated_submission_time': 26189.18081855774, 'accumulated_eval_time': 14259.4233148098, 'accumulated_logging_time': 3.880352020263672}
I0305 21:25:29.343159 140252715751168 logging_writer.py:48] [80396] accumulated_eval_time=14259.423315, accumulated_logging_time=3.880352, accumulated_submission_time=26189.180819, global_step=80396, preemption_count=0, score=26189.180819, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285669, test/num_examples=43793, total_duration=40454.928260, train/accuracy=0.995542, train/loss=0.013980, train/mean_average_precision=0.775267, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294493, validation/num_examples=43793
I0305 21:25:31.042522 140277079156480 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.14583231508731842, loss=0.017587020993232727
I0305 21:26:03.978100 140252715751168 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.13773596286773682, loss=0.017431365326046944
I0305 21:26:35.849255 140277079156480 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.1431114375591278, loss=0.01791265606880188
I0305 21:27:07.920175 140252715751168 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.1668085902929306, loss=0.01975247822701931
I0305 21:27:40.260273 140277079156480 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.15807460248470306, loss=0.021158959716558456
I0305 21:28:12.405586 140252715751168 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.14647968113422394, loss=0.018254350870847702
I0305 21:28:45.344634 140277079156480 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.15228813886642456, loss=0.016912467777729034
I0305 21:29:17.959381 140252715751168 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.16179442405700684, loss=0.01886325515806675
I0305 21:29:29.385182 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:31:24.859455 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:31:27.949149 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:31:30.959098 140444430841664 submission_runner.py:411] Time since start: 40816.57s, 	Step: 81136, 	{'train/accuracy': 0.9955633878707886, 'train/loss': 0.013961724005639553, 'train/mean_average_precision': 0.7864327946995338, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29454856305988186, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857074698180062, 'test/num_examples': 43793, 'score': 26429.1895840168, 'total_duration': 40816.574889183044, 'accumulated_submission_time': 26429.1895840168, 'accumulated_eval_time': 14380.997165203094, 'accumulated_logging_time': 3.922349452972412}
I0305 21:31:30.990889 140275669853952 logging_writer.py:48] [81136] accumulated_eval_time=14380.997165, accumulated_logging_time=3.922349, accumulated_submission_time=26429.189584, global_step=81136, preemption_count=0, score=26429.189584, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285707, test/num_examples=43793, total_duration=40816.574889, train/accuracy=0.995563, train/loss=0.013962, train/mean_average_precision=0.786433, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294549, validation/num_examples=43793
I0305 21:31:52.322885 140283577931520 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.1377243846654892, loss=0.01610979624092579
I0305 21:32:25.288629 140275669853952 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.1321200430393219, loss=0.017049195244908333
I0305 21:32:57.905513 140283577931520 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.1263873428106308, loss=0.01624084636569023
I0305 21:33:30.693289 140275669853952 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.14400723576545715, loss=0.0169985294342041
I0305 21:34:03.530201 140283577931520 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.11729857325553894, loss=0.014967352151870728
I0305 21:34:35.566359 140275669853952 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.15596124529838562, loss=0.019915742799639702
I0305 21:35:08.377308 140283577931520 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.15404868125915527, loss=0.019361987709999084
I0305 21:35:30.962379 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:37:26.756176 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:37:29.977333 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:37:33.062901 140444430841664 submission_runner.py:411] Time since start: 41178.68s, 	Step: 81870, 	{'train/accuracy': 0.995542049407959, 'train/loss': 0.014020097441971302, 'train/mean_average_precision': 0.7701309672792755, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29446535383578554, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28577349413466374, 'test/num_examples': 43793, 'score': 26669.127532482147, 'total_duration': 41178.67870616913, 'accumulated_submission_time': 26669.127532482147, 'accumulated_eval_time': 14503.09766292572, 'accumulated_logging_time': 3.96537709236145}
I0305 21:37:33.096930 140252715751168 logging_writer.py:48] [81870] accumulated_eval_time=14503.097663, accumulated_logging_time=3.965377, accumulated_submission_time=26669.127532, global_step=81870, preemption_count=0, score=26669.127532, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285773, test/num_examples=43793, total_duration=41178.678706, train/accuracy=0.995542, train/loss=0.014020, train/mean_average_precision=0.770131, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294465, validation/num_examples=43793
I0305 21:37:43.330472 140277079156480 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.14657184481620789, loss=0.019003530964255333
I0305 21:38:16.276339 140252715751168 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.12874586880207062, loss=0.017200885340571404
I0305 21:38:48.646668 140277079156480 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.14325185120105743, loss=0.01729741133749485
I0305 21:39:20.976596 140252715751168 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.14652051031589508, loss=0.020104829221963882
I0305 21:39:53.370988 140277079156480 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.11880214512348175, loss=0.014365147799253464
I0305 21:40:26.083949 140252715751168 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.14252890646457672, loss=0.01826571859419346
I0305 21:40:58.368806 140277079156480 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.13945533335208893, loss=0.016992934048175812
I0305 21:41:31.210747 140252715751168 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.14356598258018494, loss=0.018183238804340363
I0305 21:41:33.207677 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:43:25.955769 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:43:29.054758 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:43:32.049713 140444430841664 submission_runner.py:411] Time since start: 41537.67s, 	Step: 82607, 	{'train/accuracy': 0.9955546259880066, 'train/loss': 0.013937359675765038, 'train/mean_average_precision': 0.7792414263929057, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29447000547318997, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28567468422972897, 'test/num_examples': 43793, 'score': 26909.205247163773, 'total_duration': 41537.66540026665, 'accumulated_submission_time': 26909.205247163773, 'accumulated_eval_time': 14621.939532279968, 'accumulated_logging_time': 4.011141300201416}
I0305 21:43:32.080800 140275669853952 logging_writer.py:48] [82607] accumulated_eval_time=14621.939532, accumulated_logging_time=4.011141, accumulated_submission_time=26909.205247, global_step=82607, preemption_count=0, score=26909.205247, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285675, test/num_examples=43793, total_duration=41537.665400, train/accuracy=0.995555, train/loss=0.013937, train/mean_average_precision=0.779241, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294470, validation/num_examples=43793
I0305 21:44:02.342377 140283577931520 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.1368398815393448, loss=0.015533246099948883
I0305 21:44:34.957430 140275669853952 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.13180679082870483, loss=0.017712468281388283
I0305 21:45:07.392709 140283577931520 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.15622662007808685, loss=0.018430951982736588
I0305 21:45:39.828735 140275669853952 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.13172051310539246, loss=0.01730225421488285
I0305 21:46:12.271070 140283577931520 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.16217473149299622, loss=0.019174546003341675
I0305 21:46:44.458000 140275669853952 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.14208413660526276, loss=0.017773427069187164
I0305 21:47:16.706219 140283577931520 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.15998315811157227, loss=0.020075375214219093
I0305 21:47:32.190701 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:49:26.326552 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:49:29.435611 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:49:32.545158 140444430841664 submission_runner.py:411] Time since start: 41898.16s, 	Step: 83349, 	{'train/accuracy': 0.9955734610557556, 'train/loss': 0.013991492800414562, 'train/mean_average_precision': 0.7649956167611187, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.294621958718315, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857537976988816, 'test/num_examples': 43793, 'score': 27149.281101465225, 'total_duration': 41898.16096329689, 'accumulated_submission_time': 27149.281101465225, 'accumulated_eval_time': 14742.293938875198, 'accumulated_logging_time': 4.054401874542236}
I0305 21:49:32.577456 140252715751168 logging_writer.py:48] [83349] accumulated_eval_time=14742.293939, accumulated_logging_time=4.054402, accumulated_submission_time=27149.281101, global_step=83349, preemption_count=0, score=27149.281101, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285754, test/num_examples=43793, total_duration=41898.160963, train/accuracy=0.995573, train/loss=0.013991, train/mean_average_precision=0.764996, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294622, validation/num_examples=43793
I0305 21:49:49.950349 140277079156480 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.1311655044555664, loss=0.01550616417080164
I0305 21:50:22.926142 140252715751168 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.1683325469493866, loss=0.01859087310731411
I0305 21:50:56.055209 140277079156480 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.13919557631015778, loss=0.018619291484355927
I0305 21:51:29.371222 140252715751168 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.14425824582576752, loss=0.018200576305389404
I0305 21:52:02.732855 140277079156480 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.1576690971851349, loss=0.017588939517736435
I0305 21:52:35.966812 140252715751168 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.1452319473028183, loss=0.01869950257241726
I0305 21:53:09.263709 140277079156480 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.1568688601255417, loss=0.01919332519173622
I0305 21:53:32.774385 140444430841664 spec.py:321] Evaluating on the training split.
I0305 21:55:25.649782 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 21:55:28.810043 140444430841664 spec.py:349] Evaluating on the test split.
I0305 21:55:31.939052 140444430841664 submission_runner.py:411] Time since start: 42257.55s, 	Step: 84073, 	{'train/accuracy': 0.9955219030380249, 'train/loss': 0.01402114238590002, 'train/mean_average_precision': 0.7767582004172939, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29451951564285994, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28579426425299964, 'test/num_examples': 43793, 'score': 27389.44238615036, 'total_duration': 42257.554856061935, 'accumulated_submission_time': 27389.44238615036, 'accumulated_eval_time': 14861.458558797836, 'accumulated_logging_time': 4.097753524780273}
I0305 21:55:31.971498 140251254044416 logging_writer.py:48] [84073] accumulated_eval_time=14861.458559, accumulated_logging_time=4.097754, accumulated_submission_time=27389.442386, global_step=84073, preemption_count=0, score=27389.442386, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285794, test/num_examples=43793, total_duration=42257.554856, train/accuracy=0.995522, train/loss=0.014021, train/mean_average_precision=0.776758, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294520, validation/num_examples=43793
I0305 21:55:41.534854 140276755326720 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.13438613712787628, loss=0.01729459874331951
I0305 21:56:14.085092 140251254044416 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.14521275460720062, loss=0.02026454545557499
I0305 21:56:47.004177 140276755326720 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.15598776936531067, loss=0.01882329024374485
I0305 21:57:20.267127 140251254044416 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.13535305857658386, loss=0.01671735942363739
I0305 21:57:53.472491 140276755326720 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.15140244364738464, loss=0.01993144117295742
I0305 21:58:26.973962 140251254044416 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.14362163841724396, loss=0.01794048212468624
I0305 21:59:00.244170 140276755326720 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.14856141805648804, loss=0.018386444076895714
I0305 21:59:32.242404 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:01:22.553108 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:01:25.646279 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:01:28.711710 140444430841664 submission_runner.py:411] Time since start: 42614.33s, 	Step: 84796, 	{'train/accuracy': 0.9955556392669678, 'train/loss': 0.013966081663966179, 'train/mean_average_precision': 0.7768871909447967, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29442127283345476, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857006953799908, 'test/num_examples': 43793, 'score': 27629.67704296112, 'total_duration': 42614.32751560211, 'accumulated_submission_time': 27629.67704296112, 'accumulated_eval_time': 14977.927819490433, 'accumulated_logging_time': 4.1424174308776855}
I0305 22:01:28.743391 140252715751168 logging_writer.py:48] [84796] accumulated_eval_time=14977.927819, accumulated_logging_time=4.142417, accumulated_submission_time=27629.677043, global_step=84796, preemption_count=0, score=27629.677043, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285701, test/num_examples=43793, total_duration=42614.327516, train/accuracy=0.995556, train/loss=0.013966, train/mean_average_precision=0.776887, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294421, validation/num_examples=43793
I0305 22:01:30.590391 140275669853952 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.150056853890419, loss=0.017708629369735718
I0305 22:02:03.844612 140252715751168 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.13564397394657135, loss=0.01778770238161087
I0305 22:02:36.882202 140275669853952 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.1453782469034195, loss=0.018334969878196716
I0305 22:03:10.092203 140252715751168 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.1323547512292862, loss=0.01769254542887211
I0305 22:03:42.977020 140275669853952 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.16629905998706818, loss=0.01983162760734558
I0305 22:04:15.839547 140252715751168 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.1340985894203186, loss=0.015928225591778755
I0305 22:04:48.152636 140275669853952 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.13670599460601807, loss=0.017647581174969673
I0305 22:05:21.147855 140252715751168 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.14449885487556458, loss=0.017681008204817772
I0305 22:05:28.998715 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:07:19.567079 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:07:22.995480 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:07:26.352047 140444430841664 submission_runner.py:411] Time since start: 42971.97s, 	Step: 85525, 	{'train/accuracy': 0.9955314993858337, 'train/loss': 0.014036653563380241, 'train/mean_average_precision': 0.7768274028723495, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.294535595189946, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285688490661133, 'test/num_examples': 43793, 'score': 27869.899612665176, 'total_duration': 42971.96783590317, 'accumulated_submission_time': 27869.899612665176, 'accumulated_eval_time': 15095.281085968018, 'accumulated_logging_time': 4.185272216796875}
I0305 22:07:26.386517 140276755326720 logging_writer.py:48] [85525] accumulated_eval_time=15095.281086, accumulated_logging_time=4.185272, accumulated_submission_time=27869.899613, global_step=85525, preemption_count=0, score=27869.899613, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285688, test/num_examples=43793, total_duration=42971.967836, train/accuracy=0.995531, train/loss=0.014037, train/mean_average_precision=0.776827, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294536, validation/num_examples=43793
I0305 22:07:51.439465 140277079156480 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.1537104994058609, loss=0.020117303356528282
I0305 22:08:24.508401 140276755326720 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.14625594019889832, loss=0.018392622470855713
I0305 22:08:57.445767 140277079156480 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.11950179934501648, loss=0.016149017959833145
I0305 22:09:30.351077 140276755326720 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.15963593125343323, loss=0.01897462084889412
I0305 22:10:03.180341 140277079156480 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.14114263653755188, loss=0.017556143924593925
I0305 22:10:36.163895 140276755326720 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.16074609756469727, loss=0.020575422793626785
I0305 22:11:09.099645 140277079156480 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.13870683312416077, loss=0.016742635518312454
I0305 22:11:26.422841 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:13:19.281624 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:13:22.365065 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:13:25.469822 140444430841664 submission_runner.py:411] Time since start: 43331.09s, 	Step: 86254, 	{'train/accuracy': 0.9955706000328064, 'train/loss': 0.013937825337052345, 'train/mean_average_precision': 0.7793773087752416, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29471781791010904, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28576501369190144, 'test/num_examples': 43793, 'score': 28109.899499177933, 'total_duration': 43331.085626363754, 'accumulated_submission_time': 28109.899499177933, 'accumulated_eval_time': 15214.328017950058, 'accumulated_logging_time': 4.231759786605835}
I0305 22:13:25.501756 140251254044416 logging_writer.py:48] [86254] accumulated_eval_time=15214.328018, accumulated_logging_time=4.231760, accumulated_submission_time=28109.899499, global_step=86254, preemption_count=0, score=28109.899499, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285765, test/num_examples=43793, total_duration=43331.085626, train/accuracy=0.995571, train/loss=0.013938, train/mean_average_precision=0.779377, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294718, validation/num_examples=43793
I0305 22:13:40.888054 140252715751168 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.1458439975976944, loss=0.017584078013896942
I0305 22:14:14.181011 140251254044416 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.1564919501543045, loss=0.01844119280576706
I0305 22:14:47.392196 140252715751168 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.15150856971740723, loss=0.019191280007362366
I0305 22:15:20.377817 140251254044416 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.13507528603076935, loss=0.017491571605205536
I0305 22:15:53.279341 140252715751168 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.15201953053474426, loss=0.01758105866611004
I0305 22:16:26.884489 140251254044416 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.15121282637119293, loss=0.017323024570941925
I0305 22:17:00.028811 140252715751168 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.14249004423618317, loss=0.018584420904517174
I0305 22:17:25.555586 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:19:20.095295 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:19:23.195660 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:19:26.252413 140444430841664 submission_runner.py:411] Time since start: 43691.87s, 	Step: 86979, 	{'train/accuracy': 0.9955781102180481, 'train/loss': 0.013913175091147423, 'train/mean_average_precision': 0.7689155617309109, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29451130820266197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2858110313416989, 'test/num_examples': 43793, 'score': 28349.91640353203, 'total_duration': 43691.868216753006, 'accumulated_submission_time': 28349.91640353203, 'accumulated_eval_time': 15335.024807214737, 'accumulated_logging_time': 4.2755584716796875}
I0305 22:19:26.286138 140276755326720 logging_writer.py:48] [86979] accumulated_eval_time=15335.024807, accumulated_logging_time=4.275558, accumulated_submission_time=28349.916404, global_step=86979, preemption_count=0, score=28349.916404, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285811, test/num_examples=43793, total_duration=43691.868217, train/accuracy=0.995578, train/loss=0.013913, train/mean_average_precision=0.768916, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294511, validation/num_examples=43793
I0305 22:19:33.579908 140277079156480 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.14009849727153778, loss=0.01761566288769245
I0305 22:20:07.039865 140276755326720 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.1374230533838272, loss=0.018382802605628967
I0305 22:20:39.916693 140277079156480 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.14880743622779846, loss=0.018567346036434174
I0305 22:21:12.792000 140276755326720 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.13926902413368225, loss=0.017814919352531433
I0305 22:21:45.359181 140277079156480 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.1448042094707489, loss=0.018565773963928223
I0305 22:22:18.165748 140276755326720 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.14123617112636566, loss=0.017282795161008835
I0305 22:22:50.978928 140277079156480 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.13665656745433807, loss=0.015367751009762287
I0305 22:23:23.806591 140276755326720 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.16647128760814667, loss=0.01954078860580921
I0305 22:23:26.423578 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:25:18.701795 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:25:21.788717 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:25:24.889059 140444430841664 submission_runner.py:411] Time since start: 44050.50s, 	Step: 87709, 	{'train/accuracy': 0.9954716563224792, 'train/loss': 0.014210577122867107, 'train/mean_average_precision': 0.7767942679630444, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29453535640063394, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28583382212025543, 'test/num_examples': 43793, 'score': 28590.017910003662, 'total_duration': 44050.504854917526, 'accumulated_submission_time': 28590.017910003662, 'accumulated_eval_time': 15453.490227460861, 'accumulated_logging_time': 4.321666479110718}
I0305 22:25:24.921580 140251254044416 logging_writer.py:48] [87709] accumulated_eval_time=15453.490227, accumulated_logging_time=4.321666, accumulated_submission_time=28590.017910, global_step=87709, preemption_count=0, score=28590.017910, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285834, test/num_examples=43793, total_duration=44050.504855, train/accuracy=0.995472, train/loss=0.014211, train/mean_average_precision=0.776794, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294535, validation/num_examples=43793
I0305 22:25:56.305351 140252715751168 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.15539731085300446, loss=0.018635401502251625
I0305 22:26:29.754970 140251254044416 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.1380634754896164, loss=0.01681656762957573
I0305 22:27:02.238875 140252715751168 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.1509457528591156, loss=0.017733396962285042
I0305 22:27:35.583357 140251254044416 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.13653822243213654, loss=0.016425030305981636
I0305 22:28:08.408910 140252715751168 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.15519586205482483, loss=0.018826734274625778
I0305 22:28:41.468410 140251254044416 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.14170224964618683, loss=0.017564697191119194
I0305 22:29:14.713859 140252715751168 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.15324993431568146, loss=0.018964827060699463
I0305 22:29:25.109466 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:31:21.652335 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:31:24.793908 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:31:27.823247 140444430841664 submission_runner.py:411] Time since start: 44413.44s, 	Step: 88433, 	{'train/accuracy': 0.9956030249595642, 'train/loss': 0.01383145246654749, 'train/mean_average_precision': 0.7797903015231427, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29460803129407204, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857710486792785, 'test/num_examples': 43793, 'score': 28830.172302246094, 'total_duration': 44413.43903660774, 'accumulated_submission_time': 28830.172302246094, 'accumulated_eval_time': 15576.203959941864, 'accumulated_logging_time': 4.365557670593262}
I0305 22:31:27.856129 140275669853952 logging_writer.py:48] [88433] accumulated_eval_time=15576.203960, accumulated_logging_time=4.365558, accumulated_submission_time=28830.172302, global_step=88433, preemption_count=0, score=28830.172302, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285771, test/num_examples=43793, total_duration=44413.439037, train/accuracy=0.995603, train/loss=0.013831, train/mean_average_precision=0.779790, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294608, validation/num_examples=43793
I0305 22:31:50.177975 140277079156480 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.1260254979133606, loss=0.017083557322621346
I0305 22:32:22.733330 140275669853952 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.14110533893108368, loss=0.0166561771184206
I0305 22:32:55.308447 140277079156480 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.14187030494213104, loss=0.015583311207592487
I0305 22:33:28.293150 140275669853952 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.1378960907459259, loss=0.018062777817249298
I0305 22:34:01.044040 140277079156480 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.1412631720304489, loss=0.01744413562119007
I0305 22:34:34.059853 140275669853952 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.14587673544883728, loss=0.01840801350772381
I0305 22:35:06.832149 140277079156480 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.14148086309432983, loss=0.018468102440238
I0305 22:35:27.841075 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:37:18.551629 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:37:21.717663 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:37:24.838808 140444430841664 submission_runner.py:411] Time since start: 44770.45s, 	Step: 89165, 	{'train/accuracy': 0.9955229163169861, 'train/loss': 0.01405814103782177, 'train/mean_average_precision': 0.7773444141831463, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29457742830258793, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28570714385952645, 'test/num_examples': 43793, 'score': 29070.12420630455, 'total_duration': 44770.4546122551, 'accumulated_submission_time': 29070.12420630455, 'accumulated_eval_time': 15693.201646327972, 'accumulated_logging_time': 4.409271240234375}
I0305 22:37:24.872186 140251254044416 logging_writer.py:48] [89165] accumulated_eval_time=15693.201646, accumulated_logging_time=4.409271, accumulated_submission_time=29070.124206, global_step=89165, preemption_count=0, score=29070.124206, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285707, test/num_examples=43793, total_duration=44770.454612, train/accuracy=0.995523, train/loss=0.014058, train/mean_average_precision=0.777344, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294577, validation/num_examples=43793
I0305 22:37:36.841317 140276755326720 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.14909617602825165, loss=0.01794343814253807
I0305 22:38:09.857990 140251254044416 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.1373835951089859, loss=0.01856505498290062
I0305 22:38:42.359494 140276755326720 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.13259896636009216, loss=0.0166945643723011
I0305 22:39:15.008047 140251254044416 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.12393110245466232, loss=0.017164550721645355
I0305 22:39:48.195269 140276755326720 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.13787683844566345, loss=0.016406849026679993
I0305 22:40:21.141198 140251254044416 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.150149405002594, loss=0.017501266673207283
I0305 22:40:53.274073 140276755326720 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.14316324889659882, loss=0.018411580473184586
I0305 22:41:24.974634 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:43:21.198519 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:43:24.613436 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:43:27.921154 140444430841664 submission_runner.py:411] Time since start: 45133.54s, 	Step: 89897, 	{'train/accuracy': 0.9955548048019409, 'train/loss': 0.01399694848805666, 'train/mean_average_precision': 0.7708878094406291, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944496519986352, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2859636076934878, 'test/num_examples': 43793, 'score': 29310.193900823593, 'total_duration': 45133.53694319725, 'accumulated_submission_time': 29310.193900823593, 'accumulated_eval_time': 15816.148104906082, 'accumulated_logging_time': 4.453778028488159}
I0305 22:43:27.956842 140252715751168 logging_writer.py:48] [89897] accumulated_eval_time=15816.148105, accumulated_logging_time=4.453778, accumulated_submission_time=29310.193901, global_step=89897, preemption_count=0, score=29310.193901, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285964, test/num_examples=43793, total_duration=45133.536943, train/accuracy=0.995555, train/loss=0.013997, train/mean_average_precision=0.770888, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294450, validation/num_examples=43793
I0305 22:43:29.270772 140275669853952 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.14314034581184387, loss=0.01739400438964367
I0305 22:44:02.015285 140252715751168 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.13905681669712067, loss=0.017882876098155975
I0305 22:44:34.667832 140275669853952 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.13895867764949799, loss=0.016624096781015396
I0305 22:45:07.500701 140252715751168 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.14722080528736115, loss=0.019031548872590065
I0305 22:45:39.778981 140275669853952 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.13928136229515076, loss=0.01791592314839363
I0305 22:46:12.138707 140252715751168 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.1416240781545639, loss=0.01703791134059429
I0305 22:46:44.781697 140275669853952 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.15085235238075256, loss=0.01641722582280636
I0305 22:47:17.908335 140252715751168 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.15293669700622559, loss=0.018685754388570786
I0305 22:47:28.190369 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:49:19.804623 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:49:22.957342 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:49:25.988854 140444430841664 submission_runner.py:411] Time since start: 45491.60s, 	Step: 90633, 	{'train/accuracy': 0.9955064058303833, 'train/loss': 0.014086688868701458, 'train/mean_average_precision': 0.7742396649486707, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945673298006516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857039443897824, 'test/num_examples': 43793, 'score': 29550.39145565033, 'total_duration': 45491.60465455055, 'accumulated_submission_time': 29550.39145565033, 'accumulated_eval_time': 15933.94653224945, 'accumulated_logging_time': 4.502529859542847}
I0305 22:49:26.022037 140251254044416 logging_writer.py:48] [90633] accumulated_eval_time=15933.946532, accumulated_logging_time=4.502530, accumulated_submission_time=29550.391456, global_step=90633, preemption_count=0, score=29550.391456, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285704, test/num_examples=43793, total_duration=45491.604655, train/accuracy=0.995506, train/loss=0.014087, train/mean_average_precision=0.774240, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294567, validation/num_examples=43793
I0305 22:49:47.842266 140277079156480 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.12908117473125458, loss=0.01687525399029255
I0305 22:50:20.549091 140251254044416 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.1416785567998886, loss=0.017600221559405327
I0305 22:50:52.955892 140277079156480 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.14517168700695038, loss=0.018393920734524727
I0305 22:51:25.727601 140251254044416 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.16338086128234863, loss=0.0176046434789896
I0305 22:51:58.135793 140277079156480 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.14432765543460846, loss=0.018153168261051178
I0305 22:52:30.884731 140251254044416 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.13502101600170135, loss=0.01671265810728073
I0305 22:53:03.873332 140277079156480 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.1535065472126007, loss=0.019764160737395287
I0305 22:53:26.026668 140444430841664 spec.py:321] Evaluating on the training split.
I0305 22:55:14.783389 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 22:55:17.878142 140444430841664 spec.py:349] Evaluating on the test split.
I0305 22:55:20.864782 140444430841664 submission_runner.py:411] Time since start: 45846.48s, 	Step: 91370, 	{'train/accuracy': 0.9955812096595764, 'train/loss': 0.013929544948041439, 'train/mean_average_precision': 0.7740129386936662, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.294608725721259, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28566857478682434, 'test/num_examples': 43793, 'score': 29790.36308145523, 'total_duration': 45846.48057794571, 'accumulated_submission_time': 29790.36308145523, 'accumulated_eval_time': 16048.78459572792, 'accumulated_logging_time': 4.5468220710754395}
I0305 22:55:20.898306 140252715751168 logging_writer.py:48] [91370] accumulated_eval_time=16048.784596, accumulated_logging_time=4.546822, accumulated_submission_time=29790.363081, global_step=91370, preemption_count=0, score=29790.363081, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285669, test/num_examples=43793, total_duration=45846.480578, train/accuracy=0.995581, train/loss=0.013930, train/mean_average_precision=0.774013, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294609, validation/num_examples=43793
I0305 22:55:31.035903 140276755326720 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.162404403090477, loss=0.01784091256558895
I0305 22:56:03.566837 140252715751168 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.17155134677886963, loss=0.019217481836676598
I0305 22:56:35.930723 140276755326720 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.14225488901138306, loss=0.018508370965719223
I0305 22:57:08.570234 140252715751168 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.15682150423526764, loss=0.015327036380767822
I0305 22:57:41.225725 140276755326720 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.14253094792366028, loss=0.016701607033610344
I0305 22:58:14.155603 140252715751168 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.13944575190544128, loss=0.016080697998404503
I0305 22:58:47.044036 140276755326720 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.153697669506073, loss=0.019362810999155045
I0305 22:59:19.900585 140252715751168 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.1375395953655243, loss=0.016872135922312737
I0305 22:59:20.880659 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:01:06.606603 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:01:11.991153 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:01:15.236264 140444430841664 submission_runner.py:411] Time since start: 46200.85s, 	Step: 92104, 	{'train/accuracy': 0.9955791234970093, 'train/loss': 0.01389493327587843, 'train/mean_average_precision': 0.7753812512261653, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945517697474837, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2859116347293922, 'test/num_examples': 43793, 'score': 30030.31191968918, 'total_duration': 46200.85204720497, 'accumulated_submission_time': 30030.31191968918, 'accumulated_eval_time': 16163.140138626099, 'accumulated_logging_time': 4.591657400131226}
I0305 23:01:15.273369 140251254044416 logging_writer.py:48] [92104] accumulated_eval_time=16163.140139, accumulated_logging_time=4.591657, accumulated_submission_time=30030.311920, global_step=92104, preemption_count=0, score=30030.311920, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285912, test/num_examples=43793, total_duration=46200.852047, train/accuracy=0.995579, train/loss=0.013895, train/mean_average_precision=0.775381, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294552, validation/num_examples=43793
I0305 23:01:46.878324 140275669853952 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.1422334611415863, loss=0.017003227025270462
I0305 23:02:19.632398 140251254044416 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.1457914412021637, loss=0.018494602292776108
I0305 23:02:52.347320 140275669853952 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.14507223665714264, loss=0.01719881407916546
I0305 23:03:25.016624 140251254044416 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.137441486120224, loss=0.017482716590166092
I0305 23:03:57.681252 140275669853952 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.15527383983135223, loss=0.018458331003785133
I0305 23:04:30.159747 140251254044416 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.1394995003938675, loss=0.015987243503332138
I0305 23:05:02.806442 140275669853952 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.12884297966957092, loss=0.015778664499521255
I0305 23:05:15.426241 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:07:04.155350 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:07:07.269919 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:07:10.318536 140444430841664 submission_runner.py:411] Time since start: 46555.93s, 	Step: 92839, 	{'train/accuracy': 0.9955458641052246, 'train/loss': 0.014004378579556942, 'train/mean_average_precision': 0.7818500732533857, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29445386560848574, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857860262767945, 'test/num_examples': 43793, 'score': 30270.42789888382, 'total_duration': 46555.93433070183, 'accumulated_submission_time': 30270.42789888382, 'accumulated_eval_time': 16278.03237748146, 'accumulated_logging_time': 4.639986991882324}
I0305 23:07:10.352316 140252715751168 logging_writer.py:48] [92839] accumulated_eval_time=16278.032377, accumulated_logging_time=4.639987, accumulated_submission_time=30270.427899, global_step=92839, preemption_count=0, score=30270.427899, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285786, test/num_examples=43793, total_duration=46555.934331, train/accuracy=0.995546, train/loss=0.014004, train/mean_average_precision=0.781850, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294454, validation/num_examples=43793
I0305 23:07:30.738586 140276755326720 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.14212027192115784, loss=0.01817668229341507
I0305 23:08:03.744197 140252715751168 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.12945058941841125, loss=0.018950190395116806
I0305 23:08:36.560694 140276755326720 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.15295691788196564, loss=0.017993370071053505
I0305 23:09:09.321393 140252715751168 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.14463622868061066, loss=0.0191557127982378
I0305 23:09:41.996144 140276755326720 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.14283223450183868, loss=0.015900230035185814
I0305 23:10:14.656782 140252715751168 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.14594507217407227, loss=0.016171583905816078
I0305 23:10:47.588760 140276755326720 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.15488901734352112, loss=0.017418578267097473
I0305 23:11:10.619204 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:13:09.016994 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:13:12.225905 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:13:15.257312 140444430841664 submission_runner.py:411] Time since start: 46920.87s, 	Step: 93570, 	{'train/accuracy': 0.9955337047576904, 'train/loss': 0.013993624597787857, 'train/mean_average_precision': 0.7719343342136304, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945561478444909, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858384018367464, 'test/num_examples': 43793, 'score': 30510.660259962082, 'total_duration': 46920.87311458588, 'accumulated_submission_time': 30510.660259962082, 'accumulated_eval_time': 16402.670434951782, 'accumulated_logging_time': 4.685810327529907}
I0305 23:13:15.291482 140251254044416 logging_writer.py:48] [93570] accumulated_eval_time=16402.670435, accumulated_logging_time=4.685810, accumulated_submission_time=30510.660260, global_step=93570, preemption_count=0, score=30510.660260, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285838, test/num_examples=43793, total_duration=46920.873115, train/accuracy=0.995534, train/loss=0.013994, train/mean_average_precision=0.771934, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294556, validation/num_examples=43793
I0305 23:13:25.750960 140277079156480 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.13109511137008667, loss=0.017460275441408157
I0305 23:13:58.482977 140251254044416 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.13999992609024048, loss=0.0168308075517416
I0305 23:14:30.803190 140277079156480 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.15392638742923737, loss=0.018518691882491112
I0305 23:15:03.545248 140251254044416 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.13073496520519257, loss=0.016617748886346817
I0305 23:15:36.219088 140277079156480 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.1312015950679779, loss=0.017125103622674942
I0305 23:16:09.177859 140251254044416 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.14847666025161743, loss=0.017388785257935524
I0305 23:16:41.332707 140277079156480 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.1420271098613739, loss=0.020031630992889404
I0305 23:17:13.593413 140251254044416 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.14054012298583984, loss=0.016836201772093773
I0305 23:17:15.522748 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:19:02.459516 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:19:05.500148 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:19:08.502236 140444430841664 submission_runner.py:411] Time since start: 47274.12s, 	Step: 94307, 	{'train/accuracy': 0.9955654740333557, 'train/loss': 0.013947048224508762, 'train/mean_average_precision': 0.7784315567150579, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29455860251784405, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857131922781328, 'test/num_examples': 43793, 'score': 30750.85684776306, 'total_duration': 47274.11804151535, 'accumulated_submission_time': 30750.85684776306, 'accumulated_eval_time': 16515.649873495102, 'accumulated_logging_time': 4.73239541053772}
I0305 23:19:08.535853 140252715751168 logging_writer.py:48] [94307] accumulated_eval_time=16515.649873, accumulated_logging_time=4.732395, accumulated_submission_time=30750.856848, global_step=94307, preemption_count=0, score=30750.856848, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285713, test/num_examples=43793, total_duration=47274.118042, train/accuracy=0.995565, train/loss=0.013947, train/mean_average_precision=0.778432, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294559, validation/num_examples=43793
I0305 23:19:38.992023 140275669853952 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.14206092059612274, loss=0.018390456214547157
I0305 23:20:11.604987 140252715751168 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.156098410487175, loss=0.019489983096718788
I0305 23:20:44.154651 140275669853952 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.14552323520183563, loss=0.016669243574142456
I0305 23:21:16.439140 140252715751168 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.15260151028633118, loss=0.01834515482187271
I0305 23:21:48.743938 140275669853952 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.14714743196964264, loss=0.01941951923072338
I0305 23:22:21.020213 140252715751168 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.13541346788406372, loss=0.017634591087698936
I0305 23:22:53.252973 140275669853952 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.14975272119045258, loss=0.01806645281612873
I0305 23:23:08.692479 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:24:56.389982 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:24:59.500083 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:25:02.683648 140444430841664 submission_runner.py:411] Time since start: 47628.30s, 	Step: 95048, 	{'train/accuracy': 0.9955480098724365, 'train/loss': 0.014032140374183655, 'train/mean_average_precision': 0.7664942574340686, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29451657928961034, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858839892161938, 'test/num_examples': 43793, 'score': 30990.980556488037, 'total_duration': 47628.299444913864, 'accumulated_submission_time': 30990.980556488037, 'accumulated_eval_time': 16629.640988588333, 'accumulated_logging_time': 4.777262449264526}
I0305 23:25:02.716764 140251254044416 logging_writer.py:48] [95048] accumulated_eval_time=16629.640989, accumulated_logging_time=4.777262, accumulated_submission_time=30990.980556, global_step=95048, preemption_count=0, score=30990.980556, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285884, test/num_examples=43793, total_duration=47628.299445, train/accuracy=0.995548, train/loss=0.014032, train/mean_average_precision=0.766494, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294517, validation/num_examples=43793
I0305 23:25:20.460330 140277079156480 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.14378365874290466, loss=0.02040213532745838
I0305 23:25:53.554441 140251254044416 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.1283537596464157, loss=0.0171611700206995
I0305 23:26:26.414174 140277079156480 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.13495570421218872, loss=0.017713718116283417
I0305 23:26:58.939338 140251254044416 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.15125791728496552, loss=0.02053711749613285
I0305 23:27:31.280121 140277079156480 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.13723023235797882, loss=0.017587793990969658
I0305 23:28:03.967056 140251254044416 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.14594949781894684, loss=0.018650075420737267
I0305 23:28:36.509946 140277079156480 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.13085895776748657, loss=0.016742313280701637
I0305 23:29:02.938995 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:30:52.351053 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:30:55.381489 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:30:58.406567 140444430841664 submission_runner.py:411] Time since start: 47984.02s, 	Step: 95782, 	{'train/accuracy': 0.9955334663391113, 'train/loss': 0.014043392613530159, 'train/mean_average_precision': 0.7828414672130524, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.294550277044121, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28587481913823376, 'test/num_examples': 43793, 'score': 31231.168965101242, 'total_duration': 47984.022364616394, 'accumulated_submission_time': 31231.168965101242, 'accumulated_eval_time': 16745.10850763321, 'accumulated_logging_time': 4.8226141929626465}
I0305 23:30:58.441828 140275669853952 logging_writer.py:48] [95782] accumulated_eval_time=16745.108508, accumulated_logging_time=4.822614, accumulated_submission_time=31231.168965, global_step=95782, preemption_count=0, score=31231.168965, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285875, test/num_examples=43793, total_duration=47984.022365, train/accuracy=0.995533, train/loss=0.014043, train/mean_average_precision=0.782841, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294550, validation/num_examples=43793
I0305 23:31:04.695523 140276755326720 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.14881233870983124, loss=0.018829168751835823
I0305 23:31:37.389196 140275669853952 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.14267441630363464, loss=0.016129139810800552
I0305 23:32:10.059922 140276755326720 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.14770007133483887, loss=0.01897239498794079
I0305 23:32:42.222782 140275669853952 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.13697712123394012, loss=0.017781049013137817
I0305 23:33:14.803321 140276755326720 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.14665181934833527, loss=0.018364015966653824
I0305 23:33:46.984945 140275669853952 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.14849595725536346, loss=0.019175615161657333
I0305 23:34:19.617127 140276755326720 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.15685974061489105, loss=0.020692475140094757
I0305 23:34:52.133682 140275669853952 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.1673044115304947, loss=0.02033369429409504
I0305 23:34:58.676775 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:36:53.580606 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:36:56.965039 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:37:00.255192 140444430841664 submission_runner.py:411] Time since start: 48345.87s, 	Step: 96521, 	{'train/accuracy': 0.9955517649650574, 'train/loss': 0.013922733254730701, 'train/mean_average_precision': 0.7761237102647274, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29454301885101214, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28579258614151537, 'test/num_examples': 43793, 'score': 31471.37131333351, 'total_duration': 48345.87097764015, 'accumulated_submission_time': 31471.37131333351, 'accumulated_eval_time': 16866.686855316162, 'accumulated_logging_time': 4.869102954864502}
I0305 23:37:00.293820 140252715751168 logging_writer.py:48] [96521] accumulated_eval_time=16866.686855, accumulated_logging_time=4.869103, accumulated_submission_time=31471.371313, global_step=96521, preemption_count=0, score=31471.371313, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285793, test/num_examples=43793, total_duration=48345.870978, train/accuracy=0.995552, train/loss=0.013923, train/mean_average_precision=0.776124, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294543, validation/num_examples=43793
I0305 23:37:26.574457 140277079156480 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.14679521322250366, loss=0.018156016245484352
I0305 23:37:58.742956 140252715751168 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.12939134240150452, loss=0.017217252403497696
I0305 23:38:31.835372 140277079156480 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.14795441925525665, loss=0.019014893099665642
I0305 23:39:04.901465 140252715751168 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.15581673383712769, loss=0.01667422242462635
I0305 23:39:37.689281 140277079156480 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.16157543659210205, loss=0.01759638451039791
I0305 23:40:10.078696 140252715751168 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.13961023092269897, loss=0.018161607906222343
I0305 23:40:43.435644 140277079156480 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.13874386250972748, loss=0.017317116260528564
I0305 23:41:00.470813 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:42:47.602164 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:42:50.699864 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:42:53.820010 140444430841664 submission_runner.py:411] Time since start: 48699.44s, 	Step: 97252, 	{'train/accuracy': 0.9955731630325317, 'train/loss': 0.01397616695612669, 'train/mean_average_precision': 0.7844379211332362, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2947258707045029, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856464496291582, 'test/num_examples': 43793, 'score': 31711.512481689453, 'total_duration': 48699.4358150959, 'accumulated_submission_time': 31711.512481689453, 'accumulated_eval_time': 16980.036007642746, 'accumulated_logging_time': 4.920137643814087}
I0305 23:42:53.854007 140275669853952 logging_writer.py:48] [97252] accumulated_eval_time=16980.036008, accumulated_logging_time=4.920138, accumulated_submission_time=31711.512482, global_step=97252, preemption_count=0, score=31711.512482, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285646, test/num_examples=43793, total_duration=48699.435815, train/accuracy=0.995573, train/loss=0.013976, train/mean_average_precision=0.784438, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294726, validation/num_examples=43793
I0305 23:43:10.350836 140276755326720 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.12736979126930237, loss=0.016792191192507744
I0305 23:43:42.511597 140275669853952 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.1456626057624817, loss=0.0187700092792511
I0305 23:44:14.998588 140276755326720 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.13804297149181366, loss=0.018882034346461296
I0305 23:44:47.578129 140275669853952 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.16365601122379303, loss=0.017603948712348938
I0305 23:45:20.204306 140276755326720 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.15426316857337952, loss=0.019262053072452545
I0305 23:45:52.497050 140275669853952 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.1320389211177826, loss=0.016993317753076553
I0305 23:46:25.160116 140276755326720 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.1507805585861206, loss=0.019216591492295265
I0305 23:46:53.897123 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:48:41.543687 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:48:44.539872 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:48:47.530147 140444430841664 submission_runner.py:411] Time since start: 49053.15s, 	Step: 97989, 	{'train/accuracy': 0.9955433011054993, 'train/loss': 0.01398606039583683, 'train/mean_average_precision': 0.7620940939206702, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29456396965753767, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857639257450596, 'test/num_examples': 43793, 'score': 31951.521282196045, 'total_duration': 49053.14595079422, 'accumulated_submission_time': 31951.521282196045, 'accumulated_eval_time': 17093.668981790543, 'accumulated_logging_time': 4.96671462059021}
I0305 23:48:47.564113 140252715751168 logging_writer.py:48] [97989] accumulated_eval_time=17093.668982, accumulated_logging_time=4.966715, accumulated_submission_time=31951.521282, global_step=97989, preemption_count=0, score=31951.521282, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285764, test/num_examples=43793, total_duration=49053.145951, train/accuracy=0.995543, train/loss=0.013986, train/mean_average_precision=0.762094, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294564, validation/num_examples=43793
I0305 23:48:51.413138 140277079156480 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.12598368525505066, loss=0.015687141567468643
I0305 23:49:23.740527 140252715751168 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.14456774294376373, loss=0.01595829427242279
I0305 23:49:55.938915 140277079156480 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.14228229224681854, loss=0.016781959682703018
I0305 23:50:28.214469 140252715751168 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.15245629847049713, loss=0.019904939457774162
I0305 23:50:59.710327 140277079156480 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.1448301076889038, loss=0.01733040250837803
I0305 23:51:32.157065 140252715751168 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.13794463872909546, loss=0.018699321895837784
I0305 23:52:04.444837 140277079156480 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.14944148063659668, loss=0.019263697788119316
I0305 23:52:37.112599 140252715751168 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.15647552907466888, loss=0.01897283084690571
I0305 23:52:47.750720 140444430841664 spec.py:321] Evaluating on the training split.
I0305 23:54:37.318064 140444430841664 spec.py:333] Evaluating on the validation split.
I0305 23:54:40.409802 140444430841664 spec.py:349] Evaluating on the test split.
I0305 23:54:43.468881 140444430841664 submission_runner.py:411] Time since start: 49409.08s, 	Step: 98734, 	{'train/accuracy': 0.9955700635910034, 'train/loss': 0.013995001092553139, 'train/mean_average_precision': 0.7774484806299122, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945178067125195, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28569234507930347, 'test/num_examples': 43793, 'score': 32191.674822330475, 'total_duration': 49409.084685087204, 'accumulated_submission_time': 32191.674822330475, 'accumulated_eval_time': 17209.38709139824, 'accumulated_logging_time': 5.011778116226196}
I0305 23:54:43.503198 140251254044416 logging_writer.py:48] [98734] accumulated_eval_time=17209.387091, accumulated_logging_time=5.011778, accumulated_submission_time=32191.674822, global_step=98734, preemption_count=0, score=32191.674822, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285692, test/num_examples=43793, total_duration=49409.084685, train/accuracy=0.995570, train/loss=0.013995, train/mean_average_precision=0.777448, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294518, validation/num_examples=43793
I0305 23:55:05.552265 140275669853952 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.140604168176651, loss=0.018716828897595406
I0305 23:55:38.358927 140251254044416 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.13334159553050995, loss=0.01522078737616539
I0305 23:56:10.727453 140275669853952 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.15379315614700317, loss=0.017379766330122948
I0305 23:56:42.949805 140251254044416 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.13949383795261383, loss=0.016277221962809563
I0305 23:57:15.459041 140275669853952 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.13880853354930878, loss=0.019034450873732567
I0305 23:57:48.188756 140251254044416 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.1457468867301941, loss=0.016558466479182243
I0305 23:58:20.847282 140275669853952 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.12272079288959503, loss=0.01570272445678711
I0305 23:58:43.656459 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:00:31.696482 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:00:35.018312 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:00:38.325162 140444430841664 submission_runner.py:411] Time since start: 49763.94s, 	Step: 99472, 	{'train/accuracy': 0.9954909086227417, 'train/loss': 0.014085124246776104, 'train/mean_average_precision': 0.7770188283552418, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29452930231062674, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858091196595225, 'test/num_examples': 43793, 'score': 32431.794410705566, 'total_duration': 49763.940938949585, 'accumulated_submission_time': 32431.794410705566, 'accumulated_eval_time': 17324.055718421936, 'accumulated_logging_time': 5.057526350021362}
I0306 00:00:38.363664 140252715751168 logging_writer.py:48] [99472] accumulated_eval_time=17324.055718, accumulated_logging_time=5.057526, accumulated_submission_time=32431.794411, global_step=99472, preemption_count=0, score=32431.794411, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285809, test/num_examples=43793, total_duration=49763.940939, train/accuracy=0.995491, train/loss=0.014085, train/mean_average_precision=0.777019, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294529, validation/num_examples=43793
I0306 00:00:47.913949 140276755326720 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.17545880377292633, loss=0.018330490216612816
I0306 00:01:20.253347 140252715751168 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.149929940700531, loss=0.01817399635910988
I0306 00:01:52.548467 140276755326720 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.14017747342586517, loss=0.015469441190361977
I0306 00:02:25.105165 140252715751168 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.14005757868289948, loss=0.01650061458349228
I0306 00:02:57.420161 140276755326720 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.1452540010213852, loss=0.01769956760108471
I0306 00:03:30.120573 140252715751168 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.15319079160690308, loss=0.017473537474870682
I0306 00:04:02.236102 140276755326720 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.14009912312030792, loss=0.015441859140992165
I0306 00:04:34.869768 140252715751168 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.1387246698141098, loss=0.017294688150286674
I0306 00:04:38.439221 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:06:33.257511 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:06:36.349035 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:06:39.408205 140444430841664 submission_runner.py:411] Time since start: 50125.02s, 	Step: 100212, 	{'train/accuracy': 0.9955756664276123, 'train/loss': 0.01393657736480236, 'train/mean_average_precision': 0.7749262622194402, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944989993784632, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857367479087632, 'test/num_examples': 43793, 'score': 32671.83607316017, 'total_duration': 50125.02400946617, 'accumulated_submission_time': 32671.83607316017, 'accumulated_eval_time': 17445.024650096893, 'accumulated_logging_time': 5.107828140258789}
I0306 00:06:39.442956 140251254044416 logging_writer.py:48] [100212] accumulated_eval_time=17445.024650, accumulated_logging_time=5.107828, accumulated_submission_time=32671.836073, global_step=100212, preemption_count=0, score=32671.836073, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285737, test/num_examples=43793, total_duration=50125.024009, train/accuracy=0.995576, train/loss=0.013937, train/mean_average_precision=0.774926, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294499, validation/num_examples=43793
I0306 00:07:08.366476 140275669853952 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.14809082448482513, loss=0.019456077367067337
I0306 00:07:40.901263 140251254044416 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.16963627934455872, loss=0.020043885335326195
I0306 00:08:13.335553 140275669853952 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.13519597053527832, loss=0.016959941014647484
I0306 00:08:45.575671 140251254044416 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.14833073318004608, loss=0.018491925671696663
I0306 00:09:17.886713 140275669853952 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.18981343507766724, loss=0.01721869222819805
I0306 00:09:50.417665 140251254044416 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.14224903285503387, loss=0.016561752185225487
I0306 00:10:23.405896 140275669853952 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.1459033489227295, loss=0.017638396471738815
I0306 00:10:39.433000 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:12:27.016827 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:12:30.405532 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:12:33.707116 140444430841664 submission_runner.py:411] Time since start: 50479.32s, 	Step: 100950, 	{'train/accuracy': 0.99558025598526, 'train/loss': 0.013915282674133778, 'train/mean_average_precision': 0.7891355115019367, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29461311606019663, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2858639553246325, 'test/num_examples': 43793, 'score': 32911.79198694229, 'total_duration': 50479.322907447815, 'accumulated_submission_time': 32911.79198694229, 'accumulated_eval_time': 17559.29871249199, 'accumulated_logging_time': 5.153444766998291}
I0306 00:12:33.746744 140276755326720 logging_writer.py:48] [100950] accumulated_eval_time=17559.298712, accumulated_logging_time=5.153445, accumulated_submission_time=32911.791987, global_step=100950, preemption_count=0, score=32911.791987, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285864, test/num_examples=43793, total_duration=50479.322907, train/accuracy=0.995580, train/loss=0.013915, train/mean_average_precision=0.789136, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294613, validation/num_examples=43793
I0306 00:12:51.176814 140277079156480 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.15250977873802185, loss=0.01946842670440674
I0306 00:13:24.181444 140276755326720 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.1368773877620697, loss=0.0176437608897686
I0306 00:13:56.860391 140277079156480 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.1241578459739685, loss=0.015767326578497887
I0306 00:14:29.535891 140276755326720 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.15137571096420288, loss=0.018319474533200264
I0306 00:15:01.645155 140277079156480 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.14284181594848633, loss=0.016759512946009636
I0306 00:15:34.291434 140276755326720 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.14511260390281677, loss=0.017014402896165848
I0306 00:16:06.784148 140277079156480 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.14237886667251587, loss=0.01872868649661541
I0306 00:16:33.736479 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:18:27.808090 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:18:30.886746 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:18:33.935354 140444430841664 submission_runner.py:411] Time since start: 50839.55s, 	Step: 101684, 	{'train/accuracy': 0.9955013990402222, 'train/loss': 0.014096292667090893, 'train/mean_average_precision': 0.7691005217213778, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2944360238053917, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857212204469937, 'test/num_examples': 43793, 'score': 33151.745717048645, 'total_duration': 50839.55113840103, 'accumulated_submission_time': 33151.745717048645, 'accumulated_eval_time': 17679.49751996994, 'accumulated_logging_time': 5.205646276473999}
I0306 00:18:33.971138 140251254044416 logging_writer.py:48] [101684] accumulated_eval_time=17679.497520, accumulated_logging_time=5.205646, accumulated_submission_time=33151.745717, global_step=101684, preemption_count=0, score=33151.745717, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285721, test/num_examples=43793, total_duration=50839.551138, train/accuracy=0.995501, train/loss=0.014096, train/mean_average_precision=0.769101, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294436, validation/num_examples=43793
I0306 00:18:39.703433 140252715751168 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.13960082828998566, loss=0.017166662961244583
I0306 00:19:11.991983 140251254044416 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.13777197897434235, loss=0.016576450318098068
I0306 00:19:44.477356 140252715751168 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.14943405985832214, loss=0.01917712204158306
I0306 00:20:17.449712 140251254044416 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.14773572981357574, loss=0.019855940714478493
I0306 00:20:51.344461 140252715751168 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.14023558795452118, loss=0.01632002368569374
I0306 00:21:24.662342 140251254044416 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.14110204577445984, loss=0.018134092912077904
I0306 00:21:57.606765 140252715751168 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.14513680338859558, loss=0.019184593111276627
I0306 00:22:30.894111 140251254044416 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.15224993228912354, loss=0.019358249381184578
I0306 00:22:34.249349 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:24:21.767993 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:24:25.250160 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:24:28.713715 140444430841664 submission_runner.py:411] Time since start: 51194.33s, 	Step: 102411, 	{'train/accuracy': 0.9955734014511108, 'train/loss': 0.01391941960901022, 'train/mean_average_precision': 0.7724062579147245, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945845273702825, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857138954593107, 'test/num_examples': 43793, 'score': 33391.99066519737, 'total_duration': 51194.329501628876, 'accumulated_submission_time': 33391.99066519737, 'accumulated_eval_time': 17793.961818933487, 'accumulated_logging_time': 5.252429485321045}
I0306 00:24:28.752395 140275669853952 logging_writer.py:48] [102411] accumulated_eval_time=17793.961819, accumulated_logging_time=5.252429, accumulated_submission_time=33391.990665, global_step=102411, preemption_count=0, score=33391.990665, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285714, test/num_examples=43793, total_duration=51194.329502, train/accuracy=0.995573, train/loss=0.013919, train/mean_average_precision=0.772406, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294585, validation/num_examples=43793
I0306 00:24:58.331246 140276755326720 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.15027450025081635, loss=0.01926349103450775
I0306 00:25:31.134430 140275669853952 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.13567733764648438, loss=0.017489131540060043
I0306 00:26:03.969728 140276755326720 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.1627846360206604, loss=0.020663227885961533
I0306 00:26:37.029680 140275669853952 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.143379807472229, loss=0.019684694707393646
I0306 00:27:09.479083 140276755326720 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.1365336924791336, loss=0.019016528502106667
I0306 00:27:42.044208 140275669853952 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.15090346336364746, loss=0.019279154017567635
I0306 00:28:14.495987 140276755326720 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.1745244562625885, loss=0.019025500863790512
I0306 00:28:28.985913 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:30:16.998221 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:30:20.073257 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:30:23.083039 140444430841664 submission_runner.py:411] Time since start: 51548.70s, 	Step: 103146, 	{'train/accuracy': 0.9955319166183472, 'train/loss': 0.014067105017602444, 'train/mean_average_precision': 0.7735482801034659, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29456109763857424, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856564410760763, 'test/num_examples': 43793, 'score': 33632.18942737579, 'total_duration': 51548.698831796646, 'accumulated_submission_time': 33632.18942737579, 'accumulated_eval_time': 17908.058881998062, 'accumulated_logging_time': 5.303457260131836}
I0306 00:30:23.117886 140252715751168 logging_writer.py:48] [103146] accumulated_eval_time=17908.058882, accumulated_logging_time=5.303457, accumulated_submission_time=33632.189427, global_step=103146, preemption_count=0, score=33632.189427, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285656, test/num_examples=43793, total_duration=51548.698832, train/accuracy=0.995532, train/loss=0.014067, train/mean_average_precision=0.773548, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294561, validation/num_examples=43793
I0306 00:30:40.726618 140277079156480 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.14888358116149902, loss=0.019725164398550987
I0306 00:31:12.687167 140252715751168 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.13708028197288513, loss=0.017182106152176857
I0306 00:31:44.850284 140277079156480 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.1452200561761856, loss=0.01904568448662758
I0306 00:32:16.786187 140252715751168 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.15666694939136505, loss=0.01866253651678562
I0306 00:32:48.965967 140277079156480 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.14746713638305664, loss=0.01773861236870289
I0306 00:33:21.610931 140252715751168 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.13426005840301514, loss=0.017798975110054016
I0306 00:33:53.635626 140277079156480 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.16110239923000336, loss=0.02021610550582409
I0306 00:34:23.173010 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:36:06.404864 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:36:09.438968 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:36:12.475163 140444430841664 submission_runner.py:411] Time since start: 51898.09s, 	Step: 103893, 	{'train/accuracy': 0.9955902695655823, 'train/loss': 0.013891680166125298, 'train/mean_average_precision': 0.7762755546021376, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945026049765142, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856985312715242, 'test/num_examples': 43793, 'score': 33872.21159863472, 'total_duration': 51898.090837955475, 'accumulated_submission_time': 33872.21159863472, 'accumulated_eval_time': 18017.360858678818, 'accumulated_logging_time': 5.349218845367432}
I0306 00:36:12.509805 140275669853952 logging_writer.py:48] [103893] accumulated_eval_time=18017.360859, accumulated_logging_time=5.349219, accumulated_submission_time=33872.211599, global_step=103893, preemption_count=0, score=33872.211599, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285699, test/num_examples=43793, total_duration=51898.090838, train/accuracy=0.995590, train/loss=0.013892, train/mean_average_precision=0.776276, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294503, validation/num_examples=43793
I0306 00:36:15.226744 140276755326720 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.14941546320915222, loss=0.01911657303571701
I0306 00:36:47.229128 140275669853952 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.13817794620990753, loss=0.017505789175629616
I0306 00:37:19.218043 140276755326720 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.14180859923362732, loss=0.017709244042634964
I0306 00:37:51.254042 140275669853952 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.13999156653881073, loss=0.017776526510715485
I0306 00:38:23.407262 140276755326720 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.1444496363401413, loss=0.01634533517062664
I0306 00:38:56.012026 140275669853952 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.14695268869400024, loss=0.017301904037594795
I0306 00:39:28.190226 140276755326720 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.15647973120212555, loss=0.0201948881149292
I0306 00:40:00.017559 140275669853952 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.13779044151306152, loss=0.01781627908349037
I0306 00:40:12.647391 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:42:04.642239 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:42:07.688624 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:42:10.717722 140444430841664 submission_runner.py:411] Time since start: 52256.33s, 	Step: 104640, 	{'train/accuracy': 0.9955270886421204, 'train/loss': 0.01399421039968729, 'train/mean_average_precision': 0.7827216048150842, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945389773860817, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858397657044168, 'test/num_examples': 43793, 'score': 34112.31607079506, 'total_duration': 52256.33351922035, 'accumulated_submission_time': 34112.31607079506, 'accumulated_eval_time': 18135.431131839752, 'accumulated_logging_time': 5.395155668258667}
I0306 00:42:10.754231 140252715751168 logging_writer.py:48] [104640] accumulated_eval_time=18135.431132, accumulated_logging_time=5.395156, accumulated_submission_time=34112.316071, global_step=104640, preemption_count=0, score=34112.316071, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285840, test/num_examples=43793, total_duration=52256.333519, train/accuracy=0.995527, train/loss=0.013994, train/mean_average_precision=0.782722, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294539, validation/num_examples=43793
I0306 00:42:30.472592 140277079156480 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.15134654939174652, loss=0.01882622018456459
I0306 00:43:03.325564 140252715751168 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.14656218886375427, loss=0.01810380630195141
I0306 00:43:35.724206 140277079156480 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.14620614051818848, loss=0.01735289953649044
I0306 00:44:08.149385 140252715751168 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.15671613812446594, loss=0.01998453587293625
I0306 00:44:40.146635 140277079156480 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.16658414900302887, loss=0.021855724975466728
I0306 00:45:12.431727 140252715751168 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.13962046802043915, loss=0.016618138179183006
I0306 00:45:44.503719 140277079156480 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.1491275280714035, loss=0.01958995871245861
I0306 00:46:10.901965 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:48:03.065482 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:48:06.458136 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:48:09.848730 140444430841664 submission_runner.py:411] Time since start: 52615.46s, 	Step: 105384, 	{'train/accuracy': 0.9955507516860962, 'train/loss': 0.01404513232409954, 'train/mean_average_precision': 0.7694572761821414, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945578506109468, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285678615368202, 'test/num_examples': 43793, 'score': 34352.42887854576, 'total_duration': 52615.46451854706, 'accumulated_submission_time': 34352.42887854576, 'accumulated_eval_time': 18254.377835989, 'accumulated_logging_time': 5.444571256637573}
I0306 00:48:09.888268 140275669853952 logging_writer.py:48] [105384] accumulated_eval_time=18254.377836, accumulated_logging_time=5.444571, accumulated_submission_time=34352.428879, global_step=105384, preemption_count=0, score=34352.428879, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285679, test/num_examples=43793, total_duration=52615.464519, train/accuracy=0.995551, train/loss=0.014045, train/mean_average_precision=0.769457, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294558, validation/num_examples=43793
I0306 00:48:15.482332 140276755326720 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.14285999536514282, loss=0.018374817445874214
I0306 00:48:48.102055 140275669853952 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.14953802525997162, loss=0.01999060995876789
I0306 00:49:20.807572 140276755326720 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.13065630197525024, loss=0.017384612932801247
I0306 00:49:53.130913 140275669853952 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.12679539620876312, loss=0.015220319852232933
I0306 00:50:26.130082 140276755326720 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.1470738649368286, loss=0.018749192357063293
I0306 00:50:59.109914 140275669853952 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.14037592709064484, loss=0.016980884596705437
I0306 00:51:31.991487 140276755326720 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.16533979773521423, loss=0.01766577921807766
I0306 00:52:04.894395 140275669853952 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.14208747446537018, loss=0.01731819286942482
I0306 00:52:10.137895 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:53:59.142925 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:54:02.466896 140444430841664 spec.py:349] Evaluating on the test split.
I0306 00:54:05.537998 140444430841664 submission_runner.py:411] Time since start: 52971.15s, 	Step: 106117, 	{'train/accuracy': 0.995536208152771, 'train/loss': 0.013979118317365646, 'train/mean_average_precision': 0.7786668022825228, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29454642503252687, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28569164248118945, 'test/num_examples': 43793, 'score': 34592.64105916023, 'total_duration': 52971.15380167961, 'accumulated_submission_time': 34592.64105916023, 'accumulated_eval_time': 18369.77789402008, 'accumulated_logging_time': 5.496541738510132}
I0306 00:54:05.574161 140251254044416 logging_writer.py:48] [106117] accumulated_eval_time=18369.777894, accumulated_logging_time=5.496542, accumulated_submission_time=34592.641059, global_step=106117, preemption_count=0, score=34592.641059, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285692, test/num_examples=43793, total_duration=52971.153802, train/accuracy=0.995536, train/loss=0.013979, train/mean_average_precision=0.778667, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294546, validation/num_examples=43793
I0306 00:54:33.013669 140252715751168 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.14418178796768188, loss=0.01767587661743164
I0306 00:55:05.790220 140251254044416 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.13915827870368958, loss=0.017886342480778694
I0306 00:55:38.558578 140252715751168 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.14593057334423065, loss=0.018863830715417862
I0306 00:56:11.325778 140251254044416 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.16071435809135437, loss=0.019421694800257683
I0306 00:56:43.926392 140252715751168 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.16754910349845886, loss=0.018110016360878944
I0306 00:57:16.853080 140251254044416 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.13875123858451843, loss=0.01569821499288082
I0306 00:57:49.150507 140252715751168 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.14879576861858368, loss=0.019005972892045975
I0306 00:58:05.578207 140444430841664 spec.py:321] Evaluating on the training split.
I0306 00:59:55.934254 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 00:59:58.985129 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:00:02.061016 140444430841664 submission_runner.py:411] Time since start: 53327.68s, 	Step: 106851, 	{'train/accuracy': 0.9955457448959351, 'train/loss': 0.014003886841237545, 'train/mean_average_precision': 0.7663039849163852, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29453653140461783, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28566669891559426, 'test/num_examples': 43793, 'score': 34832.609432935715, 'total_duration': 53327.67676925659, 'accumulated_submission_time': 34832.609432935715, 'accumulated_eval_time': 18486.26060438156, 'accumulated_logging_time': 5.544113397598267}
I0306 01:00:02.109404 140275669853952 logging_writer.py:48] [106851] accumulated_eval_time=18486.260604, accumulated_logging_time=5.544113, accumulated_submission_time=34832.609433, global_step=106851, preemption_count=0, score=34832.609433, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285667, test/num_examples=43793, total_duration=53327.676769, train/accuracy=0.995546, train/loss=0.014004, train/mean_average_precision=0.766304, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294537, validation/num_examples=43793
I0306 01:00:18.296373 140277079156480 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.14734232425689697, loss=0.016351399943232536
I0306 01:00:50.067353 140275669853952 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.13884781301021576, loss=0.01906931772828102
I0306 01:01:22.124305 140277079156480 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.1359570324420929, loss=0.017410961911082268
I0306 01:01:54.171857 140275669853952 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.14657467603683472, loss=0.018520036712288857
I0306 01:02:26.590358 140277079156480 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.14704188704490662, loss=0.018729910254478455
I0306 01:02:58.993649 140275669853952 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.15531378984451294, loss=0.019088786095380783
I0306 01:03:31.243233 140277079156480 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.13962580263614655, loss=0.015279118902981281
I0306 01:04:02.183231 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:05:48.199424 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:05:51.298139 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:05:54.321793 140444430841664 submission_runner.py:411] Time since start: 53679.94s, 	Step: 107596, 	{'train/accuracy': 0.9955405592918396, 'train/loss': 0.013997292146086693, 'train/mean_average_precision': 0.7824891126363962, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945553946251877, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28584856625576105, 'test/num_examples': 43793, 'score': 35072.64724302292, 'total_duration': 53679.93759918213, 'accumulated_submission_time': 35072.64724302292, 'accumulated_eval_time': 18598.39912390709, 'accumulated_logging_time': 5.606086492538452}
I0306 01:05:54.357588 140251254044416 logging_writer.py:48] [107596] accumulated_eval_time=18598.399124, accumulated_logging_time=5.606086, accumulated_submission_time=35072.647243, global_step=107596, preemption_count=0, score=35072.647243, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285849, test/num_examples=43793, total_duration=53679.937599, train/accuracy=0.995541, train/loss=0.013997, train/mean_average_precision=0.782489, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294555, validation/num_examples=43793
I0306 01:05:56.049433 140276755326720 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.1525764912366867, loss=0.020021025091409683
I0306 01:06:29.001376 140251254044416 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.14890269935131073, loss=0.019988788291811943
I0306 01:07:01.891259 140276755326720 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.1499130129814148, loss=0.019881466403603554
I0306 01:07:34.335371 140251254044416 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.15964925289154053, loss=0.019356917589902878
I0306 01:08:07.273171 140276755326720 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.13646797835826874, loss=0.017265979200601578
I0306 01:08:39.507247 140251254044416 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.17057166993618011, loss=0.01723235845565796
I0306 01:09:12.576122 140276755326720 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.16055206954479218, loss=0.020375793799757957
I0306 01:09:45.319319 140251254044416 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.1372419148683548, loss=0.017527520656585693
I0306 01:09:54.547823 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:11:39.716938 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:11:42.737238 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:11:45.776292 140444430841664 submission_runner.py:411] Time since start: 54031.39s, 	Step: 108330, 	{'train/accuracy': 0.9955700039863586, 'train/loss': 0.01395201962441206, 'train/mean_average_precision': 0.780402457785562, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.294552374588472, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856731132602263, 'test/num_examples': 43793, 'score': 35312.800602436066, 'total_duration': 54031.39209771156, 'accumulated_submission_time': 35312.800602436066, 'accumulated_eval_time': 18709.627541542053, 'accumulated_logging_time': 5.654670476913452}
I0306 01:11:45.812151 140252715751168 logging_writer.py:48] [108330] accumulated_eval_time=18709.627542, accumulated_logging_time=5.654670, accumulated_submission_time=35312.800602, global_step=108330, preemption_count=0, score=35312.800602, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285673, test/num_examples=43793, total_duration=54031.392098, train/accuracy=0.995570, train/loss=0.013952, train/mean_average_precision=0.780402, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294552, validation/num_examples=43793
I0306 01:12:08.808837 140277079156480 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.15918080508708954, loss=0.01833241619169712
I0306 01:12:40.991451 140252715751168 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.14394763112068176, loss=0.018782323226332664
I0306 01:13:13.248712 140277079156480 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.140385702252388, loss=0.0160539448261261
I0306 01:13:45.658766 140252715751168 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.13957402110099792, loss=0.018548915162682533
I0306 01:14:18.095501 140277079156480 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.15228371322155, loss=0.018368547782301903
I0306 01:14:50.545360 140252715751168 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.14832249283790588, loss=0.020892981439828873
I0306 01:15:22.725714 140277079156480 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.14384213089942932, loss=0.018415160477161407
I0306 01:15:45.784082 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:17:29.769264 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:17:32.895144 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:17:35.944945 140444430841664 submission_runner.py:411] Time since start: 54381.56s, 	Step: 109073, 	{'train/accuracy': 0.9955811500549316, 'train/loss': 0.013888290151953697, 'train/mean_average_precision': 0.7799591160022632, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448188556445337, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28571065923166294, 'test/num_examples': 43793, 'score': 35552.739119291306, 'total_duration': 54381.56074023247, 'accumulated_submission_time': 35552.739119291306, 'accumulated_eval_time': 18819.788346767426, 'accumulated_logging_time': 5.7019219398498535}
I0306 01:17:35.981890 140251254044416 logging_writer.py:48] [109073] accumulated_eval_time=18819.788347, accumulated_logging_time=5.701922, accumulated_submission_time=35552.739119, global_step=109073, preemption_count=0, score=35552.739119, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285711, test/num_examples=43793, total_duration=54381.560740, train/accuracy=0.995581, train/loss=0.013888, train/mean_average_precision=0.779959, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294482, validation/num_examples=43793
I0306 01:17:45.320584 140276755326720 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.13996991515159607, loss=0.017081335186958313
I0306 01:18:18.012553 140251254044416 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.13820889592170715, loss=0.017348637804389
I0306 01:18:50.200751 140276755326720 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.14230281114578247, loss=0.0182612594217062
I0306 01:19:22.594513 140251254044416 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.14793895184993744, loss=0.017995888367295265
I0306 01:19:54.735431 140276755326720 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.14538221061229706, loss=0.01718158647418022
I0306 01:20:27.130604 140251254044416 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.129301518201828, loss=0.016984140500426292
I0306 01:20:58.984382 140276755326720 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.1615665704011917, loss=0.0196799598634243
I0306 01:21:31.143549 140251254044416 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.14484940469264984, loss=0.017743200063705444
I0306 01:21:36.010290 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:23:25.023368 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:23:28.089071 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:23:31.098714 140444430841664 submission_runner.py:411] Time since start: 54736.71s, 	Step: 109816, 	{'train/accuracy': 0.9955028891563416, 'train/loss': 0.01416180282831192, 'train/mean_average_precision': 0.7691131012284711, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29458288524693865, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28574578542323764, 'test/num_examples': 43793, 'score': 35792.734656095505, 'total_duration': 54736.71452140808, 'accumulated_submission_time': 35792.734656095505, 'accumulated_eval_time': 18934.87672638893, 'accumulated_logging_time': 5.749660015106201}
I0306 01:23:31.135237 140275669853952 logging_writer.py:48] [109816] accumulated_eval_time=18934.876726, accumulated_logging_time=5.749660, accumulated_submission_time=35792.734656, global_step=109816, preemption_count=0, score=35792.734656, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285746, test/num_examples=43793, total_duration=54736.714521, train/accuracy=0.995503, train/loss=0.014162, train/mean_average_precision=0.769113, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294583, validation/num_examples=43793
I0306 01:23:58.900294 140277079156480 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.1474638283252716, loss=0.018125906586647034
I0306 01:24:31.309705 140275669853952 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.1393660008907318, loss=0.019211899489164352
I0306 01:25:03.570771 140277079156480 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.1445123553276062, loss=0.017782554030418396
I0306 01:25:35.913608 140275669853952 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.13832800090312958, loss=0.016629675403237343
I0306 01:26:08.298439 140277079156480 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.16349731385707855, loss=0.019313741475343704
I0306 01:26:40.472171 140275669853952 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.14538531005382538, loss=0.016771163791418076
I0306 01:27:12.895723 140277079156480 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.15573711693286896, loss=0.019568707793951035
I0306 01:27:31.141045 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:29:16.853954 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:29:19.897771 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:29:22.937381 140444430841664 submission_runner.py:411] Time since start: 55088.55s, 	Step: 110558, 	{'train/accuracy': 0.995578408241272, 'train/loss': 0.013893427327275276, 'train/mean_average_precision': 0.7705730961172595, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.294555251483837, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856821056185651, 'test/num_examples': 43793, 'score': 36032.70607757568, 'total_duration': 55088.55318880081, 'accumulated_submission_time': 36032.70607757568, 'accumulated_eval_time': 19046.673018455505, 'accumulated_logging_time': 5.798440217971802}
I0306 01:29:22.974469 140251254044416 logging_writer.py:48] [110558] accumulated_eval_time=19046.673018, accumulated_logging_time=5.798440, accumulated_submission_time=36032.706078, global_step=110558, preemption_count=0, score=36032.706078, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285682, test/num_examples=43793, total_duration=55088.553189, train/accuracy=0.995578, train/loss=0.013893, train/mean_average_precision=0.770573, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294555, validation/num_examples=43793
I0306 01:29:37.096737 140276755326720 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.14741629362106323, loss=0.017187567427754402
I0306 01:30:09.677398 140251254044416 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.14412343502044678, loss=0.016649773344397545
I0306 01:30:42.504217 140276755326720 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.1543753445148468, loss=0.01990819163620472
I0306 01:31:15.205167 140251254044416 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.13553091883659363, loss=0.016209570690989494
I0306 01:31:47.515804 140276755326720 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.16388623416423798, loss=0.020000241696834564
I0306 01:32:20.220941 140251254044416 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.14728763699531555, loss=0.016624407842755318
I0306 01:32:52.995682 140276755326720 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.14690810441970825, loss=0.016825292259454727
I0306 01:33:23.148521 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:35:11.832819 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:35:14.888351 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:35:17.920452 140444430841664 submission_runner.py:411] Time since start: 55443.54s, 	Step: 111293, 	{'train/accuracy': 0.9955031871795654, 'train/loss': 0.014136292971670628, 'train/mean_average_precision': 0.7735100883177448, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448824049981526, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857235851747701, 'test/num_examples': 43793, 'score': 36272.84723806381, 'total_duration': 55443.536256074905, 'accumulated_submission_time': 36272.84723806381, 'accumulated_eval_time': 19161.44490623474, 'accumulated_logging_time': 5.846911668777466}
I0306 01:35:17.957293 140252715751168 logging_writer.py:48] [111293] accumulated_eval_time=19161.444906, accumulated_logging_time=5.846912, accumulated_submission_time=36272.847238, global_step=111293, preemption_count=0, score=36272.847238, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285724, test/num_examples=43793, total_duration=55443.536256, train/accuracy=0.995503, train/loss=0.014136, train/mean_average_precision=0.773510, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294488, validation/num_examples=43793
I0306 01:35:20.577844 140277079156480 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.14162540435791016, loss=0.01912401244044304
I0306 01:35:53.289766 140252715751168 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.1410577893257141, loss=0.016848497092723846
I0306 01:36:26.139083 140277079156480 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.1549203246831894, loss=0.01844863034784794
I0306 01:36:58.685634 140252715751168 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.13393788039684296, loss=0.017480598762631416
I0306 01:37:30.992812 140277079156480 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.14316426217556, loss=0.01703205332159996
I0306 01:38:03.822259 140252715751168 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.152888685464859, loss=0.018905768170952797
I0306 01:38:36.051388 140277079156480 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.14179371297359467, loss=0.017633525654673576
I0306 01:39:08.252550 140252715751168 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.1467808485031128, loss=0.01951935887336731
I0306 01:39:18.179243 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:41:00.235848 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:41:03.517153 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:41:06.507065 140444430841664 submission_runner.py:411] Time since start: 55792.12s, 	Step: 112032, 	{'train/accuracy': 0.9955770969390869, 'train/loss': 0.013891604728996754, 'train/mean_average_precision': 0.7804736794685526, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945283101312911, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856990657290821, 'test/num_examples': 43793, 'score': 36513.03610134125, 'total_duration': 55792.122868299484, 'accumulated_submission_time': 36513.03610134125, 'accumulated_eval_time': 19269.772682905197, 'accumulated_logging_time': 5.894815921783447}
I0306 01:41:06.543406 140275669853952 logging_writer.py:48] [112032] accumulated_eval_time=19269.772683, accumulated_logging_time=5.894816, accumulated_submission_time=36513.036101, global_step=112032, preemption_count=0, score=36513.036101, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285699, test/num_examples=43793, total_duration=55792.122868, train/accuracy=0.995577, train/loss=0.013892, train/mean_average_precision=0.780474, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294528, validation/num_examples=43793
I0306 01:41:28.756028 140276755326720 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.14066839218139648, loss=0.019270986318588257
I0306 01:42:00.772485 140275669853952 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.14029856026172638, loss=0.017589300870895386
I0306 01:42:32.584876 140276755326720 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.16887417435646057, loss=0.019101977348327637
I0306 01:43:04.841657 140275669853952 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.13766971230506897, loss=0.019005898386240005
I0306 01:43:36.688651 140276755326720 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.13384100794792175, loss=0.01756639964878559
I0306 01:44:08.833778 140275669853952 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.12953390181064606, loss=0.017042603343725204
I0306 01:44:40.968218 140276755326720 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.1548970490694046, loss=0.020423561334609985
I0306 01:45:06.592483 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:46:55.909699 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:46:59.349138 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:47:02.846172 140444430841664 submission_runner.py:411] Time since start: 56148.46s, 	Step: 112781, 	{'train/accuracy': 0.9955218434333801, 'train/loss': 0.014051965437829494, 'train/mean_average_precision': 0.7811979990159192, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29461045910436456, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857793929004372, 'test/num_examples': 43793, 'score': 36753.05175638199, 'total_duration': 56148.46195721626, 'accumulated_submission_time': 36753.05175638199, 'accumulated_eval_time': 19386.026316165924, 'accumulated_logging_time': 5.942633867263794}
I0306 01:47:02.887920 140251254044416 logging_writer.py:48] [112781] accumulated_eval_time=19386.026316, accumulated_logging_time=5.942634, accumulated_submission_time=36753.051756, global_step=112781, preemption_count=0, score=36753.051756, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285779, test/num_examples=43793, total_duration=56148.461957, train/accuracy=0.995522, train/loss=0.014052, train/mean_average_precision=0.781198, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294610, validation/num_examples=43793
I0306 01:47:09.673696 140277079156480 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.1464017927646637, loss=0.016223298385739326
I0306 01:47:42.941598 140251254044416 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.16851207613945007, loss=0.021090563386678696
I0306 01:48:15.833061 140277079156480 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.14109836518764496, loss=0.01664162054657936
I0306 01:48:47.870622 140251254044416 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.16333650052547455, loss=0.017642391845583916
I0306 01:49:19.947566 140277079156480 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.13264644145965576, loss=0.015722783282399178
I0306 01:49:52.150252 140251254044416 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.15225374698638916, loss=0.01974393241107464
I0306 01:50:24.169225 140277079156480 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.15308865904808044, loss=0.018366914242506027
I0306 01:50:56.551505 140251254044416 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.14992481470108032, loss=0.018647054210305214
I0306 01:51:02.921224 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:52:51.698109 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:52:57.452273 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:53:00.835561 140444430841664 submission_runner.py:411] Time since start: 56506.45s, 	Step: 113520, 	{'train/accuracy': 0.9955678582191467, 'train/loss': 0.013942718505859375, 'train/mean_average_precision': 0.7712682210840969, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29466195087899233, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856983237546496, 'test/num_examples': 43793, 'score': 36993.05020284653, 'total_duration': 56506.45134592056, 'accumulated_submission_time': 36993.05020284653, 'accumulated_eval_time': 19503.940582990646, 'accumulated_logging_time': 5.9964470863342285}
I0306 01:53:00.877006 140252715751168 logging_writer.py:48] [113520] accumulated_eval_time=19503.940583, accumulated_logging_time=5.996447, accumulated_submission_time=36993.050203, global_step=113520, preemption_count=0, score=36993.050203, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285698, test/num_examples=43793, total_duration=56506.451346, train/accuracy=0.995568, train/loss=0.013943, train/mean_average_precision=0.771268, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294662, validation/num_examples=43793
I0306 01:53:27.245748 140275669853952 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.15792043507099152, loss=0.018563708290457726
I0306 01:54:00.326332 140252715751168 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.15322768688201904, loss=0.01895884796977043
I0306 01:54:33.391061 140275669853952 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.15314729511737823, loss=0.018561305478215218
I0306 01:55:06.221125 140252715751168 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.16720066964626312, loss=0.018846025690436363
I0306 01:55:39.327029 140275669853952 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.14627304673194885, loss=0.018081732094287872
I0306 01:56:12.697153 140252715751168 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.14404626190662384, loss=0.017725622281432152
I0306 01:56:45.844719 140275669853952 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.1402921825647354, loss=0.01672661304473877
I0306 01:57:00.964426 140444430841664 spec.py:321] Evaluating on the training split.
I0306 01:58:44.802180 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 01:58:47.879261 140444430841664 spec.py:349] Evaluating on the test split.
I0306 01:58:50.872701 140444430841664 submission_runner.py:411] Time since start: 56856.49s, 	Step: 114247, 	{'train/accuracy': 0.9955666661262512, 'train/loss': 0.013914069160819054, 'train/mean_average_precision': 0.7792822577149576, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29451015641427747, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857388313861314, 'test/num_examples': 43793, 'score': 37233.100048303604, 'total_duration': 56856.48850417137, 'accumulated_submission_time': 37233.100048303604, 'accumulated_eval_time': 19613.848816156387, 'accumulated_logging_time': 6.050333261489868}
I0306 01:58:50.914339 140251254044416 logging_writer.py:48] [114247] accumulated_eval_time=19613.848816, accumulated_logging_time=6.050333, accumulated_submission_time=37233.100048, global_step=114247, preemption_count=0, score=37233.100048, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285739, test/num_examples=43793, total_duration=56856.488504, train/accuracy=0.995567, train/loss=0.013914, train/mean_average_precision=0.779282, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294510, validation/num_examples=43793
I0306 01:59:08.450955 140276755326720 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.14730304479599, loss=0.017572395503520966
I0306 01:59:40.714847 140251254044416 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.13551606237888336, loss=0.017305511981248856
I0306 02:00:12.778863 140276755326720 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.15383440256118774, loss=0.019208939746022224
I0306 02:00:45.229662 140251254044416 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.13593176007270813, loss=0.016674503684043884
I0306 02:01:17.731668 140276755326720 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.13456784188747406, loss=0.01703336276113987
I0306 02:01:50.132209 140251254044416 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.1704859882593155, loss=0.01921885274350643
I0306 02:02:22.571791 140276755326720 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.13805720210075378, loss=0.0188258308917284
I0306 02:02:50.940952 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:04:36.036425 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:04:39.197849 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:04:44.196367 140444430841664 submission_runner.py:411] Time since start: 57209.81s, 	Step: 114989, 	{'train/accuracy': 0.9955500960350037, 'train/loss': 0.014085116796195507, 'train/mean_average_precision': 0.7752664714391566, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29454608431402185, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858201532068772, 'test/num_examples': 43793, 'score': 37473.09302973747, 'total_duration': 57209.81217265129, 'accumulated_submission_time': 37473.09302973747, 'accumulated_eval_time': 19727.10418367386, 'accumulated_logging_time': 6.103281021118164}
I0306 02:04:44.235043 140275669853952 logging_writer.py:48] [114989] accumulated_eval_time=19727.104184, accumulated_logging_time=6.103281, accumulated_submission_time=37473.093030, global_step=114989, preemption_count=0, score=37473.093030, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285820, test/num_examples=43793, total_duration=57209.812173, train/accuracy=0.995550, train/loss=0.014085, train/mean_average_precision=0.775266, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294546, validation/num_examples=43793
I0306 02:04:48.303089 140277079156480 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.13781103491783142, loss=0.015568862669169903
I0306 02:05:20.820574 140275669853952 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.12234598398208618, loss=0.016433777287602425
I0306 02:05:53.438827 140277079156480 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.1340557187795639, loss=0.0154013866558671
I0306 02:06:26.234697 140275669853952 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.14850658178329468, loss=0.01818312518298626
I0306 02:06:58.864558 140277079156480 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.1586180031299591, loss=0.017443593591451645
I0306 02:07:31.949673 140275669853952 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.14972005784511566, loss=0.017642933875322342
I0306 02:08:04.762922 140277079156480 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.15474240481853485, loss=0.01880050264298916
I0306 02:08:37.125282 140275669853952 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.15370696783065796, loss=0.019886048510670662
I0306 02:08:44.328939 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:10:31.924827 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:10:35.343376 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:10:38.614436 140444430841664 submission_runner.py:411] Time since start: 57564.23s, 	Step: 115723, 	{'train/accuracy': 0.9955061078071594, 'train/loss': 0.014068830758333206, 'train/mean_average_precision': 0.7755653651667929, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29455401907605383, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28570222160773406, 'test/num_examples': 43793, 'score': 37713.152237176895, 'total_duration': 57564.230226278305, 'accumulated_submission_time': 37713.152237176895, 'accumulated_eval_time': 19841.389622211456, 'accumulated_logging_time': 6.152727365493774}
I0306 02:10:38.655978 140251254044416 logging_writer.py:48] [115723] accumulated_eval_time=19841.389622, accumulated_logging_time=6.152727, accumulated_submission_time=37713.152237, global_step=115723, preemption_count=0, score=37713.152237, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285702, test/num_examples=43793, total_duration=57564.230226, train/accuracy=0.995506, train/loss=0.014069, train/mean_average_precision=0.775565, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294554, validation/num_examples=43793
I0306 02:11:04.481075 140252715751168 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.12987494468688965, loss=0.016094688326120377
I0306 02:11:36.867908 140251254044416 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.13602493703365326, loss=0.015711849555373192
I0306 02:12:09.374309 140252715751168 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.13935880362987518, loss=0.018834766000509262
I0306 02:12:41.893364 140251254044416 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.13234852254390717, loss=0.01644650474190712
I0306 02:13:14.476382 140252715751168 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.1441468894481659, loss=0.019204920157790184
I0306 02:13:46.709194 140251254044416 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.14024659991264343, loss=0.017712844535708427
I0306 02:14:19.210190 140252715751168 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.15321937203407288, loss=0.017076382413506508
I0306 02:14:38.911928 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:16:25.632805 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:16:28.680651 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:16:31.998519 140444430841664 submission_runner.py:411] Time since start: 57917.61s, 	Step: 116462, 	{'train/accuracy': 0.9955783486366272, 'train/loss': 0.013889076188206673, 'train/mean_average_precision': 0.7852735330342986, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944726678951031, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856991776358954, 'test/num_examples': 43793, 'score': 37953.37311530113, 'total_duration': 57917.61430335045, 'accumulated_submission_time': 37953.37311530113, 'accumulated_eval_time': 19954.476145982742, 'accumulated_logging_time': 6.206709146499634}
I0306 02:16:32.041697 140276755326720 logging_writer.py:48] [116462] accumulated_eval_time=19954.476146, accumulated_logging_time=6.206709, accumulated_submission_time=37953.373115, global_step=116462, preemption_count=0, score=37953.373115, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285699, test/num_examples=43793, total_duration=57917.614303, train/accuracy=0.995578, train/loss=0.013889, train/mean_average_precision=0.785274, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294473, validation/num_examples=43793
I0306 02:16:46.081471 140277079156480 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.14973847568035126, loss=0.019115721806883812
I0306 02:17:20.290069 140276755326720 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.17447802424430847, loss=0.018386203795671463
I0306 02:17:52.838968 140277079156480 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.13273009657859802, loss=0.017142441123723984
I0306 02:18:25.410256 140276755326720 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.13715235888957977, loss=0.01606190763413906
I0306 02:18:57.787693 140277079156480 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.14488689601421356, loss=0.020179318264126778
I0306 02:19:29.906478 140276755326720 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.14939889311790466, loss=0.017998643219470978
I0306 02:20:02.323990 140277079156480 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.13664154708385468, loss=0.015738224610686302
I0306 02:20:32.031615 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:22:18.020514 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:22:21.387055 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:22:24.717316 140444430841664 submission_runner.py:411] Time since start: 58270.33s, 	Step: 117193, 	{'train/accuracy': 0.9955693483352661, 'train/loss': 0.013938511721789837, 'train/mean_average_precision': 0.7685402304600731, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944315252732025, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857992774557656, 'test/num_examples': 43793, 'score': 38193.32923769951, 'total_duration': 58270.33309483528, 'accumulated_submission_time': 38193.32923769951, 'accumulated_eval_time': 20067.16177225113, 'accumulated_logging_time': 6.261343479156494}
I0306 02:22:24.759009 140251254044416 logging_writer.py:48] [117193] accumulated_eval_time=20067.161772, accumulated_logging_time=6.261343, accumulated_submission_time=38193.329238, global_step=117193, preemption_count=0, score=38193.329238, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285799, test/num_examples=43793, total_duration=58270.333095, train/accuracy=0.995569, train/loss=0.013939, train/mean_average_precision=0.768540, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294432, validation/num_examples=43793
I0306 02:22:27.473642 140275669853952 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.14349664747714996, loss=0.020136544480919838
I0306 02:23:00.613673 140251254044416 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.13861103355884552, loss=0.016744447872042656
I0306 02:23:32.762856 140275669853952 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.1538935899734497, loss=0.020386304706335068
I0306 02:24:05.205819 140251254044416 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.17305780947208405, loss=0.019189750775694847
I0306 02:24:37.458501 140275669853952 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.1413225382566452, loss=0.017694171518087387
I0306 02:25:09.822946 140251254044416 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.14425131678581238, loss=0.017204660922288895
I0306 02:25:41.844666 140275669853952 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.14939585328102112, loss=0.01792055368423462
I0306 02:26:14.029356 140251254044416 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.12702949345111847, loss=0.015888072550296783
I0306 02:26:25.029947 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:28:10.926081 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:28:13.936025 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:28:16.963738 140444430841664 submission_runner.py:411] Time since start: 58622.58s, 	Step: 117935, 	{'train/accuracy': 0.9955466389656067, 'train/loss': 0.014009775593876839, 'train/mean_average_precision': 0.778482909552098, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945424307965914, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2858286062184439, 'test/num_examples': 43793, 'score': 38433.5650575161, 'total_duration': 58622.579538583755, 'accumulated_submission_time': 38433.5650575161, 'accumulated_eval_time': 20179.09550833702, 'accumulated_logging_time': 6.315385818481445}
I0306 02:28:17.002583 140252715751168 logging_writer.py:48] [117935] accumulated_eval_time=20179.095508, accumulated_logging_time=6.315386, accumulated_submission_time=38433.565058, global_step=117935, preemption_count=0, score=38433.565058, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285829, test/num_examples=43793, total_duration=58622.579539, train/accuracy=0.995547, train/loss=0.014010, train/mean_average_precision=0.778483, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294542, validation/num_examples=43793
I0306 02:28:38.404021 140276755326720 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.16121500730514526, loss=0.020353028550744057
I0306 02:29:11.019118 140252715751168 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.13552334904670715, loss=0.018099341541528702
I0306 02:29:43.181824 140276755326720 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.13763661682605743, loss=0.01833566650748253
I0306 02:30:15.674534 140252715751168 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.15325313806533813, loss=0.01793590746819973
I0306 02:30:47.677469 140276755326720 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.131778746843338, loss=0.017961425706744194
I0306 02:31:19.848892 140252715751168 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.1341988742351532, loss=0.018295757472515106
I0306 02:31:51.842539 140276755326720 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.13957102596759796, loss=0.01575491577386856
I0306 02:32:17.291135 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:33:59.614613 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:34:02.840667 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:34:05.833937 140444430841664 submission_runner.py:411] Time since start: 58971.45s, 	Step: 118679, 	{'train/accuracy': 0.9955329895019531, 'train/loss': 0.014011407271027565, 'train/mean_average_precision': 0.76781053698901, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944648560792302, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28569030539985224, 'test/num_examples': 43793, 'score': 38673.81978392601, 'total_duration': 58971.44974565506, 'accumulated_submission_time': 38673.81978392601, 'accumulated_eval_time': 20287.63827586174, 'accumulated_logging_time': 6.3654186725616455}
I0306 02:34:05.872172 140251254044416 logging_writer.py:48] [118679] accumulated_eval_time=20287.638276, accumulated_logging_time=6.365419, accumulated_submission_time=38673.819784, global_step=118679, preemption_count=0, score=38673.819784, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285690, test/num_examples=43793, total_duration=58971.449746, train/accuracy=0.995533, train/loss=0.014011, train/mean_average_precision=0.767811, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294465, validation/num_examples=43793
I0306 02:34:13.171418 140275669853952 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.1439124196767807, loss=0.01796652004122734
I0306 02:34:46.412252 140251254044416 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.12690377235412598, loss=0.016629379242658615
I0306 02:35:19.041463 140275669853952 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.1539389044046402, loss=0.017340263351798058
I0306 02:35:51.867508 140251254044416 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.14110004901885986, loss=0.017156336456537247
I0306 02:36:24.370130 140275669853952 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.1404130905866623, loss=0.016860036179423332
I0306 02:36:56.986755 140251254044416 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.14125405251979828, loss=0.016975993290543556
I0306 02:37:30.188086 140275669853952 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.15221959352493286, loss=0.02081291936337948
I0306 02:38:03.313511 140251254044416 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.15345251560211182, loss=0.017600033432245255
I0306 02:38:05.922696 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:39:53.607864 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:39:56.663538 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:39:59.647334 140444430841664 submission_runner.py:411] Time since start: 59325.26s, 	Step: 119409, 	{'train/accuracy': 0.9955406188964844, 'train/loss': 0.013977606780827045, 'train/mean_average_precision': 0.7782823868286457, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29451752748201476, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28571934184225706, 'test/num_examples': 43793, 'score': 38913.83612418175, 'total_duration': 59325.26311969757, 'accumulated_submission_time': 38913.83612418175, 'accumulated_eval_time': 20401.36284303665, 'accumulated_logging_time': 6.414927959442139}
I0306 02:39:59.687574 140252715751168 logging_writer.py:48] [119409] accumulated_eval_time=20401.362843, accumulated_logging_time=6.414928, accumulated_submission_time=38913.836124, global_step=119409, preemption_count=0, score=38913.836124, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285719, test/num_examples=43793, total_duration=59325.263120, train/accuracy=0.995541, train/loss=0.013978, train/mean_average_precision=0.778282, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294518, validation/num_examples=43793
I0306 02:40:29.795653 140276755326720 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.128048375248909, loss=0.015732694417238235
I0306 02:41:02.854068 140252715751168 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.15555866062641144, loss=0.020538857206702232
I0306 02:41:36.335248 140276755326720 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.13019061088562012, loss=0.01526043750345707
I0306 02:42:09.166662 140252715751168 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.1410561203956604, loss=0.01632985845208168
I0306 02:42:42.046732 140276755326720 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.1419990360736847, loss=0.016274094581604004
I0306 02:43:14.808470 140252715751168 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.14695444703102112, loss=0.017953651025891304
I0306 02:43:47.140588 140276755326720 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.13644813001155853, loss=0.017803195863962173
I0306 02:43:59.706236 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:45:47.476343 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:45:50.866541 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:45:54.224594 140444430841664 submission_runner.py:411] Time since start: 59679.84s, 	Step: 120140, 	{'train/accuracy': 0.9955416321754456, 'train/loss': 0.013997571542859077, 'train/mean_average_precision': 0.7779783333646086, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944633500683524, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28573928942482363, 'test/num_examples': 43793, 'score': 39153.818912267685, 'total_duration': 59679.84038281441, 'accumulated_submission_time': 39153.818912267685, 'accumulated_eval_time': 20515.88113284111, 'accumulated_logging_time': 6.468688011169434}
I0306 02:45:54.266401 140251254044416 logging_writer.py:48] [120140] accumulated_eval_time=20515.881133, accumulated_logging_time=6.468688, accumulated_submission_time=39153.818912, global_step=120140, preemption_count=0, score=39153.818912, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285739, test/num_examples=43793, total_duration=59679.840383, train/accuracy=0.995542, train/loss=0.013998, train/mean_average_precision=0.777978, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294463, validation/num_examples=43793
I0306 02:46:14.203994 140277079156480 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.15081943571567535, loss=0.01855356991291046
I0306 02:46:47.143379 140251254044416 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.14212222397327423, loss=0.015977496281266212
I0306 02:47:19.776796 140277079156480 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.14511075615882874, loss=0.01914217323064804
I0306 02:47:52.605726 140251254044416 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.15517008304595947, loss=0.01826336793601513
I0306 02:48:25.444025 140277079156480 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.1375795304775238, loss=0.017752189189195633
I0306 02:48:57.883076 140251254044416 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.14011473953723907, loss=0.017790546640753746
I0306 02:49:30.749790 140277079156480 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.14185066521167755, loss=0.01806160993874073
I0306 02:49:54.542703 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:51:42.820915 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:51:45.887852 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:51:48.920761 140444430841664 submission_runner.py:411] Time since start: 60034.54s, 	Step: 120873, 	{'train/accuracy': 0.9955613017082214, 'train/loss': 0.014009865932166576, 'train/mean_average_precision': 0.7866055579445876, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2944610430847229, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28582400135593145, 'test/num_examples': 43793, 'score': 39394.06165885925, 'total_duration': 60034.536561727524, 'accumulated_submission_time': 39394.06165885925, 'accumulated_eval_time': 20630.259138822556, 'accumulated_logging_time': 6.522387504577637}
I0306 02:51:48.960654 140275669853952 logging_writer.py:48] [120873] accumulated_eval_time=20630.259139, accumulated_logging_time=6.522388, accumulated_submission_time=39394.061659, global_step=120873, preemption_count=0, score=39394.061659, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285824, test/num_examples=43793, total_duration=60034.536562, train/accuracy=0.995561, train/loss=0.014010, train/mean_average_precision=0.786606, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294461, validation/num_examples=43793
I0306 02:51:58.102581 140276755326720 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.15918715298175812, loss=0.017357470467686653
I0306 02:52:30.725149 140275669853952 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.14279036223888397, loss=0.018272077664732933
I0306 02:53:03.671881 140276755326720 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.1591457575559616, loss=0.01595858484506607
I0306 02:53:35.847196 140275669853952 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.15996360778808594, loss=0.018705913797020912
I0306 02:54:08.170287 140276755326720 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.17259497940540314, loss=0.019826913252472878
I0306 02:54:40.312452 140275669853952 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.13656212389469147, loss=0.015138436108827591
I0306 02:55:12.831601 140276755326720 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.14551781117916107, loss=0.01714613102376461
I0306 02:55:45.030385 140275669853952 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.13173964619636536, loss=0.016545966267585754
I0306 02:55:49.216658 140444430841664 spec.py:321] Evaluating on the training split.
I0306 02:57:33.208354 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 02:57:36.606876 140444430841664 spec.py:349] Evaluating on the test split.
I0306 02:57:39.937095 140444430841664 submission_runner.py:411] Time since start: 60385.55s, 	Step: 121614, 	{'train/accuracy': 0.9955511689186096, 'train/loss': 0.013948217034339905, 'train/mean_average_precision': 0.7662657186104772, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945873331954825, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28572045614722924, 'test/num_examples': 43793, 'score': 39634.28438973427, 'total_duration': 60385.552882909775, 'accumulated_submission_time': 39634.28438973427, 'accumulated_eval_time': 20740.979505062103, 'accumulated_logging_time': 6.573132276535034}
I0306 02:57:39.981590 140252715751168 logging_writer.py:48] [121614] accumulated_eval_time=20740.979505, accumulated_logging_time=6.573132, accumulated_submission_time=39634.284390, global_step=121614, preemption_count=0, score=39634.284390, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285720, test/num_examples=43793, total_duration=60385.552883, train/accuracy=0.995551, train/loss=0.013948, train/mean_average_precision=0.766266, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294587, validation/num_examples=43793
I0306 02:58:08.668381 140277079156480 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.14966140687465668, loss=0.018386078998446465
I0306 02:58:41.262753 140252715751168 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.13340787589550018, loss=0.01757221296429634
I0306 02:59:13.658501 140277079156480 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.1613771766424179, loss=0.01993558183312416
I0306 02:59:45.484867 140252715751168 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.16607514023780823, loss=0.017081979662179947
I0306 03:00:17.975966 140277079156480 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.16296520829200745, loss=0.01967492699623108
I0306 03:00:50.438046 140252715751168 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.14683659374713898, loss=0.019336331635713577
I0306 03:01:22.882244 140277079156480 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.13810931146144867, loss=0.01721041090786457
I0306 03:01:40.214358 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:03:31.050494 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:03:34.372927 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:03:37.662494 140444430841664 submission_runner.py:411] Time since start: 60743.28s, 	Step: 122354, 	{'train/accuracy': 0.9955592155456543, 'train/loss': 0.014011980034410954, 'train/mean_average_precision': 0.7753787037792779, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944693613267529, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858334033911579, 'test/num_examples': 43793, 'score': 39874.48172211647, 'total_duration': 60743.27828383446, 'accumulated_submission_time': 39874.48172211647, 'accumulated_eval_time': 20858.42757821083, 'accumulated_logging_time': 6.62934947013855}
I0306 03:03:37.703834 140275669853952 logging_writer.py:48] [122354] accumulated_eval_time=20858.427578, accumulated_logging_time=6.629349, accumulated_submission_time=39874.481722, global_step=122354, preemption_count=0, score=39874.481722, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285833, test/num_examples=43793, total_duration=60743.278284, train/accuracy=0.995559, train/loss=0.014012, train/mean_average_precision=0.775379, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294469, validation/num_examples=43793
I0306 03:03:53.014029 140276755326720 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.14126001298427582, loss=0.01865283027291298
I0306 03:04:25.843735 140275669853952 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.12426938861608505, loss=0.01600596122443676
I0306 03:04:58.260913 140276755326720 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.14234234392642975, loss=0.01838616468012333
I0306 03:05:30.546595 140275669853952 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.14036251604557037, loss=0.018600642681121826
I0306 03:06:02.924774 140276755326720 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.1380237638950348, loss=0.016906028613448143
I0306 03:06:35.373097 140275669853952 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.13135124742984772, loss=0.017345482483506203
I0306 03:07:07.749257 140276755326720 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.14128310978412628, loss=0.020506663247942924
I0306 03:07:37.824987 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:09:20.183378 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:09:23.208444 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:09:26.220190 140444430841664 submission_runner.py:411] Time since start: 61091.84s, 	Step: 123095, 	{'train/accuracy': 0.9955024123191833, 'train/loss': 0.014059833250939846, 'train/mean_average_precision': 0.770876647929712, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29447345274161363, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28570196675879445, 'test/num_examples': 43793, 'score': 40114.56664967537, 'total_duration': 61091.83598303795, 'accumulated_submission_time': 40114.56664967537, 'accumulated_eval_time': 20966.82272863388, 'accumulated_logging_time': 6.682506799697876}
I0306 03:09:26.265044 140251254044416 logging_writer.py:48] [123095] accumulated_eval_time=20966.822729, accumulated_logging_time=6.682507, accumulated_submission_time=40114.566650, global_step=123095, preemption_count=0, score=40114.566650, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285702, test/num_examples=43793, total_duration=61091.835983, train/accuracy=0.995502, train/loss=0.014060, train/mean_average_precision=0.770877, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294473, validation/num_examples=43793
I0306 03:09:28.334807 140277079156480 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.1497942954301834, loss=0.020341405645012856
I0306 03:10:01.195437 140251254044416 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.14150552451610565, loss=0.017585016787052155
I0306 03:10:33.718180 140277079156480 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.1511124074459076, loss=0.018506618216633797
I0306 03:11:06.603213 140251254044416 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.14391940832138062, loss=0.01777578704059124
I0306 03:11:39.129033 140277079156480 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.12282373011112213, loss=0.01692172884941101
I0306 03:12:11.728707 140251254044416 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.15559694170951843, loss=0.0177010428160429
I0306 03:12:44.108722 140277079156480 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.1477755755186081, loss=0.017873406410217285
I0306 03:13:17.030013 140251254044416 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.13666966557502747, loss=0.01796894706785679
I0306 03:13:26.468171 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:15:10.863758 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:15:13.950868 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:15:16.997277 140444430841664 submission_runner.py:411] Time since start: 61442.61s, 	Step: 123830, 	{'train/accuracy': 0.9956189393997192, 'train/loss': 0.013855394907295704, 'train/mean_average_precision': 0.7783691267641425, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944677065948684, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2858282936163239, 'test/num_examples': 43793, 'score': 40354.73410964012, 'total_duration': 61442.61308217049, 'accumulated_submission_time': 40354.73410964012, 'accumulated_eval_time': 21077.351784706116, 'accumulated_logging_time': 6.739075183868408}
I0306 03:15:17.036637 140252715751168 logging_writer.py:48] [123830] accumulated_eval_time=21077.351785, accumulated_logging_time=6.739075, accumulated_submission_time=40354.734110, global_step=123830, preemption_count=0, score=40354.734110, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285828, test/num_examples=43793, total_duration=61442.613082, train/accuracy=0.995619, train/loss=0.013855, train/mean_average_precision=0.778369, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294468, validation/num_examples=43793
I0306 03:15:40.633782 140275669853952 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.13702762126922607, loss=0.016314029693603516
I0306 03:16:13.239981 140252715751168 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.13522721827030182, loss=0.01538913045078516
I0306 03:16:45.679085 140275669853952 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.14713576436042786, loss=0.0185282863676548
I0306 03:17:18.197073 140252715751168 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.14176638424396515, loss=0.01771572045981884
I0306 03:17:50.854810 140275669853952 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.16000299155712128, loss=0.018393171951174736
I0306 03:18:23.592670 140252715751168 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.11326318234205246, loss=0.015084649436175823
I0306 03:18:57.446931 140275669853952 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.1639062464237213, loss=0.017729582265019417
I0306 03:19:17.170322 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:21:02.557361 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:21:05.556105 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:21:08.600093 140444430841664 submission_runner.py:411] Time since start: 61794.22s, 	Step: 124560, 	{'train/accuracy': 0.995537519454956, 'train/loss': 0.014022862538695335, 'train/mean_average_precision': 0.7846410551431506, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2946003063371421, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28590111751146347, 'test/num_examples': 43793, 'score': 40594.8329679966, 'total_duration': 61794.2158946991, 'accumulated_submission_time': 40594.8329679966, 'accumulated_eval_time': 21188.7815053463, 'accumulated_logging_time': 6.7893126010894775}
I0306 03:21:08.640960 140251254044416 logging_writer.py:48] [124560] accumulated_eval_time=21188.781505, accumulated_logging_time=6.789313, accumulated_submission_time=40594.832968, global_step=124560, preemption_count=0, score=40594.832968, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285901, test/num_examples=43793, total_duration=61794.215895, train/accuracy=0.995538, train/loss=0.014023, train/mean_average_precision=0.784641, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294600, validation/num_examples=43793
I0306 03:21:22.135636 140277079156480 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.13382510840892792, loss=0.016790011897683144
I0306 03:21:54.931770 140251254044416 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.15523181855678558, loss=0.020174941048026085
I0306 03:22:27.778982 140277079156480 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.13729840517044067, loss=0.016818810254335403
I0306 03:23:00.178529 140251254044416 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.12498484551906586, loss=0.014853880740702152
I0306 03:23:32.840515 140277079156480 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.15024389326572418, loss=0.01869806833565235
I0306 03:24:05.594223 140251254044416 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.14385084807872772, loss=0.019254814833402634
I0306 03:24:38.375475 140277079156480 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.15172111988067627, loss=0.01954071968793869
I0306 03:25:08.779541 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:26:55.192298 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:26:58.260738 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:27:01.281845 140444430841664 submission_runner.py:411] Time since start: 62146.90s, 	Step: 125294, 	{'train/accuracy': 0.9955279231071472, 'train/loss': 0.014042826369404793, 'train/mean_average_precision': 0.7715511151186719, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945159621426331, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28566680540037065, 'test/num_examples': 43793, 'score': 40834.938383340836, 'total_duration': 62146.897647857666, 'accumulated_submission_time': 40834.938383340836, 'accumulated_eval_time': 21301.28376197815, 'accumulated_logging_time': 6.841134786605835}
I0306 03:27:01.322663 140252715751168 logging_writer.py:48] [125294] accumulated_eval_time=21301.283762, accumulated_logging_time=6.841135, accumulated_submission_time=40834.938383, global_step=125294, preemption_count=0, score=40834.938383, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285667, test/num_examples=43793, total_duration=62146.897648, train/accuracy=0.995528, train/loss=0.014043, train/mean_average_precision=0.771551, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294516, validation/num_examples=43793
I0306 03:27:03.780176 140275669853952 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.14947718381881714, loss=0.020135676488280296
I0306 03:27:36.506233 140252715751168 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.13454222679138184, loss=0.01694304868578911
I0306 03:28:08.840881 140275669853952 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.15157145261764526, loss=0.019745584577322006
I0306 03:28:40.738465 140252715751168 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.1549197882413864, loss=0.01745113544166088
I0306 03:29:13.468358 140275669853952 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.13738985359668732, loss=0.01725097745656967
I0306 03:29:46.268823 140252715751168 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.15131069719791412, loss=0.017246032133698463
I0306 03:30:18.883641 140275669853952 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.15274585783481598, loss=0.01797454245388508
I0306 03:30:51.383506 140252715751168 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.14443673193454742, loss=0.01868768036365509
I0306 03:31:01.637506 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:32:47.728056 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:32:50.749132 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:32:53.737250 140444430841664 submission_runner.py:411] Time since start: 62499.35s, 	Step: 126032, 	{'train/accuracy': 0.9955497980117798, 'train/loss': 0.01397396344691515, 'train/mean_average_precision': 0.7776480587757924, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29452895124001954, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2859947860418702, 'test/num_examples': 43793, 'score': 41075.216349601746, 'total_duration': 62499.35304760933, 'accumulated_submission_time': 41075.216349601746, 'accumulated_eval_time': 21413.383462429047, 'accumulated_logging_time': 6.894498348236084}
I0306 03:32:53.778867 140251254044416 logging_writer.py:48] [126032] accumulated_eval_time=21413.383462, accumulated_logging_time=6.894498, accumulated_submission_time=41075.216350, global_step=126032, preemption_count=0, score=41075.216350, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285995, test/num_examples=43793, total_duration=62499.353048, train/accuracy=0.995550, train/loss=0.013974, train/mean_average_precision=0.777648, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294529, validation/num_examples=43793
I0306 03:33:16.262021 140277079156480 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.14298607409000397, loss=0.01814296469092369
I0306 03:33:48.626619 140251254044416 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.15826120972633362, loss=0.018040485680103302
I0306 03:34:21.047215 140277079156480 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.14475779235363007, loss=0.017237955704331398
I0306 03:34:53.346692 140251254044416 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.14038482308387756, loss=0.01781752146780491
I0306 03:35:25.513402 140277079156480 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.13981182873249054, loss=0.01726539433002472
I0306 03:35:57.712912 140251254044416 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.15167565643787384, loss=0.019680602476000786
I0306 03:36:30.437281 140277079156480 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.1382032185792923, loss=0.01706925965845585
I0306 03:36:53.893319 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:38:39.021232 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:38:42.086169 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:38:45.075256 140444430841664 submission_runner.py:411] Time since start: 62850.69s, 	Step: 126774, 	{'train/accuracy': 0.9955505132675171, 'train/loss': 0.013991804793477058, 'train/mean_average_precision': 0.7683404890570733, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945988444665576, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857411098029996, 'test/num_examples': 43793, 'score': 41315.295952796936, 'total_duration': 62850.69105839729, 'accumulated_submission_time': 41315.295952796936, 'accumulated_eval_time': 21524.56535053253, 'accumulated_logging_time': 6.948744535446167}
I0306 03:38:45.115363 140252715751168 logging_writer.py:48] [126774] accumulated_eval_time=21524.565351, accumulated_logging_time=6.948745, accumulated_submission_time=41315.295953, global_step=126774, preemption_count=0, score=41315.295953, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285741, test/num_examples=43793, total_duration=62850.691058, train/accuracy=0.995551, train/loss=0.013992, train/mean_average_precision=0.768340, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294599, validation/num_examples=43793
I0306 03:38:54.068572 140275669853952 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.1384621262550354, loss=0.017140695825219154
I0306 03:39:26.881286 140252715751168 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.16671708226203918, loss=0.019124580547213554
I0306 03:39:59.430058 140275669853952 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.13922105729579926, loss=0.01840229332447052
I0306 03:40:32.001250 140252715751168 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.1542597860097885, loss=0.020632481202483177
I0306 03:41:04.507762 140275669853952 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.14162147045135498, loss=0.016203831881284714
I0306 03:41:36.635013 140252715751168 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.15329408645629883, loss=0.018602732568979263
I0306 03:42:09.382157 140275669853952 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.1597311943769455, loss=0.017924081534147263
I0306 03:42:41.638966 140252715751168 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.1400483101606369, loss=0.017881471663713455
I0306 03:42:45.179887 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:44:26.603100 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:44:29.704054 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:44:32.728986 140444430841664 submission_runner.py:411] Time since start: 63198.34s, 	Step: 127512, 	{'train/accuracy': 0.995507538318634, 'train/loss': 0.014115703292191029, 'train/mean_average_precision': 0.780440909566623, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.294529530609065, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285670475255235, 'test/num_examples': 43793, 'score': 41555.32707571983, 'total_duration': 63198.34479141235, 'accumulated_submission_time': 41555.32707571983, 'accumulated_eval_time': 21632.114396095276, 'accumulated_logging_time': 6.999794960021973}
I0306 03:44:32.770292 140276755326720 logging_writer.py:48] [127512] accumulated_eval_time=21632.114396, accumulated_logging_time=6.999795, accumulated_submission_time=41555.327076, global_step=127512, preemption_count=0, score=41555.327076, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285670, test/num_examples=43793, total_duration=63198.344791, train/accuracy=0.995508, train/loss=0.014116, train/mean_average_precision=0.780441, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294530, validation/num_examples=43793
I0306 03:45:01.893842 140277079156480 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.13606038689613342, loss=0.01772136427462101
I0306 03:45:34.448506 140276755326720 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.16522160172462463, loss=0.019352173432707787
I0306 03:46:07.137295 140277079156480 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.12970460951328278, loss=0.016464542597532272
I0306 03:46:39.648739 140276755326720 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.13917599618434906, loss=0.0185611080378294
I0306 03:47:12.381926 140277079156480 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.14362700283527374, loss=0.018382474780082703
I0306 03:47:45.051517 140276755326720 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.16440409421920776, loss=0.019175071269273758
I0306 03:48:17.820489 140277079156480 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.12547047436237335, loss=0.016067970544099808
I0306 03:48:32.946898 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:50:18.278514 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:50:21.418603 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:50:24.484119 140444430841664 submission_runner.py:411] Time since start: 63550.10s, 	Step: 128248, 	{'train/accuracy': 0.9955496191978455, 'train/loss': 0.01396853942424059, 'train/mean_average_precision': 0.7795067563109996, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29452388556903725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28569272211545155, 'test/num_examples': 43793, 'score': 41795.47059297562, 'total_duration': 63550.09992480278, 'accumulated_submission_time': 41795.47059297562, 'accumulated_eval_time': 21743.651569128036, 'accumulated_logging_time': 7.051799297332764}
I0306 03:50:24.524059 140252715751168 logging_writer.py:48] [128248] accumulated_eval_time=21743.651569, accumulated_logging_time=7.051799, accumulated_submission_time=41795.470593, global_step=128248, preemption_count=0, score=41795.470593, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285693, test/num_examples=43793, total_duration=63550.099925, train/accuracy=0.995550, train/loss=0.013969, train/mean_average_precision=0.779507, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294524, validation/num_examples=43793
I0306 03:50:41.845804 140275669853952 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.14834804832935333, loss=0.019429512321949005
I0306 03:51:14.255605 140252715751168 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.1393732875585556, loss=0.01868310198187828
I0306 03:51:46.451272 140275669853952 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.13803185522556305, loss=0.017782632261514664
I0306 03:52:18.945864 140252715751168 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.1335296481847763, loss=0.01730867475271225
I0306 03:52:51.447508 140275669853952 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.13912895321846008, loss=0.01709185726940632
I0306 03:53:23.961111 140252715751168 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.14889734983444214, loss=0.019094591960310936
I0306 03:53:56.226705 140275669853952 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.14205355942249298, loss=0.01647249422967434
I0306 03:54:24.781140 140444430841664 spec.py:321] Evaluating on the training split.
I0306 03:56:12.477228 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 03:56:15.579518 140444430841664 spec.py:349] Evaluating on the test split.
I0306 03:56:18.652186 140444430841664 submission_runner.py:411] Time since start: 63904.27s, 	Step: 128988, 	{'train/accuracy': 0.9955980181694031, 'train/loss': 0.013859663158655167, 'train/mean_average_precision': 0.78344074625687, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29455038029554326, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857917035503163, 'test/num_examples': 43793, 'score': 42035.69354510307, 'total_duration': 63904.267990112305, 'accumulated_submission_time': 42035.69354510307, 'accumulated_eval_time': 21857.522568941116, 'accumulated_logging_time': 7.103084325790405}
I0306 03:56:18.692191 140251254044416 logging_writer.py:48] [128988] accumulated_eval_time=21857.522569, accumulated_logging_time=7.103084, accumulated_submission_time=42035.693545, global_step=128988, preemption_count=0, score=42035.693545, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285792, test/num_examples=43793, total_duration=63904.267990, train/accuracy=0.995598, train/loss=0.013860, train/mean_average_precision=0.783441, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294550, validation/num_examples=43793
I0306 03:56:22.967276 140277079156480 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.14581240713596344, loss=0.017605654895305634
I0306 03:56:55.504397 140251254044416 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.1252650022506714, loss=0.016837574541568756
I0306 03:57:28.078135 140277079156480 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.1342659294605255, loss=0.016181834042072296
I0306 03:58:00.789493 140251254044416 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.16103872656822205, loss=0.01951931044459343
I0306 03:58:33.111116 140277079156480 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.14267250895500183, loss=0.01653376594185829
I0306 03:59:05.224071 140251254044416 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.15546350181102753, loss=0.019704267382621765
I0306 03:59:37.580504 140277079156480 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.12762032449245453, loss=0.016669388860464096
I0306 04:00:10.525446 140251254044416 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.13982874155044556, loss=0.017278948798775673
I0306 04:00:18.813191 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:02:06.267559 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:02:09.407665 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:02:12.467014 140444430841664 submission_runner.py:411] Time since start: 64258.08s, 	Step: 129726, 	{'train/accuracy': 0.9955507516860962, 'train/loss': 0.013985131867229939, 'train/mean_average_precision': 0.7687265903200935, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29463061554172393, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28569051306232546, 'test/num_examples': 43793, 'score': 42275.78005862236, 'total_duration': 64258.08281850815, 'accumulated_submission_time': 42275.78005862236, 'accumulated_eval_time': 21971.176341056824, 'accumulated_logging_time': 7.15567946434021}
I0306 04:02:12.507401 140275669853952 logging_writer.py:48] [129726] accumulated_eval_time=21971.176341, accumulated_logging_time=7.155679, accumulated_submission_time=42275.780059, global_step=129726, preemption_count=0, score=42275.780059, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285691, test/num_examples=43793, total_duration=64258.082819, train/accuracy=0.995551, train/loss=0.013985, train/mean_average_precision=0.768727, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294631, validation/num_examples=43793
I0306 04:02:37.988435 140276755326720 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.16034802794456482, loss=0.0172649584710598
I0306 04:03:11.262068 140275669853952 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.13649512827396393, loss=0.018284514546394348
I0306 04:03:44.598940 140276755326720 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.14338041841983795, loss=0.01767905056476593
I0306 04:04:17.622069 140275669853952 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.14278630912303925, loss=0.019042879343032837
I0306 04:04:50.084095 140276755326720 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.1489139199256897, loss=0.019182410091161728
I0306 04:05:22.255086 140275669853952 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.15608097612857819, loss=0.019307445734739304
I0306 04:05:54.902402 140276755326720 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.14070965349674225, loss=0.016783667728304863
I0306 04:06:12.689079 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:07:55.442843 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:08:00.524935 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:08:03.782285 140444430841664 submission_runner.py:411] Time since start: 64609.40s, 	Step: 130455, 	{'train/accuracy': 0.9955714344978333, 'train/loss': 0.013960082083940506, 'train/mean_average_precision': 0.7693036488054488, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945816972273906, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857761141941721, 'test/num_examples': 43793, 'score': 42515.92790675163, 'total_duration': 64609.398089408875, 'accumulated_submission_time': 42515.92790675163, 'accumulated_eval_time': 22082.269499063492, 'accumulated_logging_time': 7.207721710205078}
I0306 04:08:03.823891 140252715751168 logging_writer.py:48] [130455] accumulated_eval_time=22082.269499, accumulated_logging_time=7.207722, accumulated_submission_time=42515.927907, global_step=130455, preemption_count=0, score=42515.927907, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285776, test/num_examples=43793, total_duration=64609.398089, train/accuracy=0.995571, train/loss=0.013960, train/mean_average_precision=0.769304, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294582, validation/num_examples=43793
I0306 04:08:19.268807 140277079156480 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.1333433985710144, loss=0.01734881103038788
I0306 04:08:51.977423 140252715751168 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.14103084802627563, loss=0.017364174127578735
I0306 04:09:24.826841 140277079156480 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.14318105578422546, loss=0.01783813163638115
I0306 04:09:57.468096 140252715751168 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.13237909972667694, loss=0.014805499464273453
I0306 04:10:30.244559 140277079156480 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.1549878865480423, loss=0.018862470984458923
I0306 04:11:02.921515 140252715751168 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.1400468796491623, loss=0.019311923533678055
I0306 04:11:35.151788 140277079156480 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.14530979096889496, loss=0.018763894215226173
I0306 04:12:03.879066 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:13:50.104868 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:13:53.273996 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:13:56.273151 140444430841664 submission_runner.py:411] Time since start: 64961.89s, 	Step: 131190, 	{'train/accuracy': 0.9954754114151001, 'train/loss': 0.014152027666568756, 'train/mean_average_precision': 0.7745251563418538, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448708529000645, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857930735203836, 'test/num_examples': 43793, 'score': 42755.94807124138, 'total_duration': 64961.888946056366, 'accumulated_submission_time': 42755.94807124138, 'accumulated_eval_time': 22194.663531780243, 'accumulated_logging_time': 7.26179051399231}
I0306 04:13:56.314000 140251254044416 logging_writer.py:48] [131190] accumulated_eval_time=22194.663532, accumulated_logging_time=7.261791, accumulated_submission_time=42755.948071, global_step=131190, preemption_count=0, score=42755.948071, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285793, test/num_examples=43793, total_duration=64961.888946, train/accuracy=0.995475, train/loss=0.014152, train/mean_average_precision=0.774525, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294487, validation/num_examples=43793
I0306 04:14:00.087772 140276755326720 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.13515350222587585, loss=0.01824680157005787
I0306 04:14:32.371837 140251254044416 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.15082859992980957, loss=0.01753404177725315
I0306 04:15:04.849424 140276755326720 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.14977778494358063, loss=0.01694760099053383
I0306 04:15:37.179732 140251254044416 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.16576075553894043, loss=0.016968069598078728
I0306 04:16:09.650939 140276755326720 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.1304554045200348, loss=0.01684776321053505
I0306 04:16:42.037307 140251254044416 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.14377661049365997, loss=0.017748789861798286
I0306 04:17:14.612326 140276755326720 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.14577819406986237, loss=0.019301874563097954
I0306 04:17:47.173298 140251254044416 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.1590292900800705, loss=0.01699143648147583
I0306 04:17:56.292202 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:19:38.656291 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:19:41.784162 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:19:46.830032 140444430841664 submission_runner.py:411] Time since start: 65312.45s, 	Step: 131929, 	{'train/accuracy': 0.9955991506576538, 'train/loss': 0.013889210298657417, 'train/mean_average_precision': 0.7770598738446075, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2946191638573021, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856906678947874, 'test/num_examples': 43793, 'score': 42995.89313149452, 'total_duration': 65312.44583821297, 'accumulated_submission_time': 42995.89313149452, 'accumulated_eval_time': 22305.20131421089, 'accumulated_logging_time': 7.314098358154297}
I0306 04:19:46.873052 140275669853952 logging_writer.py:48] [131929] accumulated_eval_time=22305.201314, accumulated_logging_time=7.314098, accumulated_submission_time=42995.893131, global_step=131929, preemption_count=0, score=42995.893131, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285691, test/num_examples=43793, total_duration=65312.445838, train/accuracy=0.995599, train/loss=0.013889, train/mean_average_precision=0.777060, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294619, validation/num_examples=43793
I0306 04:20:10.344425 140277079156480 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.13954105973243713, loss=0.017883751541376114
I0306 04:20:42.926979 140275669853952 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.1465611308813095, loss=0.018468961119651794
I0306 04:21:16.154704 140277079156480 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.1393899917602539, loss=0.017170382663607597
I0306 04:21:48.460693 140275669853952 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.1468479037284851, loss=0.01874569244682789
I0306 04:22:20.892909 140277079156480 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.12387041002511978, loss=0.013929760083556175
I0306 04:22:53.226698 140275669853952 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.13821914792060852, loss=0.01755567640066147
I0306 04:23:25.908364 140277079156480 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.1380760371685028, loss=0.018623720854520798
I0306 04:23:47.026211 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:25:27.319519 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:25:30.462413 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:25:33.485921 140444430841664 submission_runner.py:411] Time since start: 65659.10s, 	Step: 132666, 	{'train/accuracy': 0.9955676794052124, 'train/loss': 0.013970430940389633, 'train/mean_average_precision': 0.784020367065633, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944997865847881, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.285701665999011, 'test/num_examples': 43793, 'score': 43236.01311707497, 'total_duration': 65659.10172605515, 'accumulated_submission_time': 43236.01311707497, 'accumulated_eval_time': 22411.660974740982, 'accumulated_logging_time': 7.368078708648682}
I0306 04:25:33.527347 140252715751168 logging_writer.py:48] [132666] accumulated_eval_time=22411.660975, accumulated_logging_time=7.368079, accumulated_submission_time=43236.013117, global_step=132666, preemption_count=0, score=43236.013117, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285702, test/num_examples=43793, total_duration=65659.101726, train/accuracy=0.995568, train/loss=0.013970, train/mean_average_precision=0.784020, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294500, validation/num_examples=43793
I0306 04:25:44.837843 140276755326720 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.1392410695552826, loss=0.0183528121560812
I0306 04:26:17.593078 140252715751168 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.12938879430294037, loss=0.016988448798656464
I0306 04:26:50.354433 140276755326720 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.12805068492889404, loss=0.016595900058746338
I0306 04:27:23.253623 140252715751168 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.144767627120018, loss=0.01877807080745697
I0306 04:27:55.952025 140276755326720 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.15846797823905945, loss=0.01922944374382496
I0306 04:28:28.497994 140252715751168 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.1405518800020218, loss=0.018514633178710938
I0306 04:29:00.910242 140276755326720 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.14421002566814423, loss=0.016589868813753128
I0306 04:29:33.245374 140252715751168 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.1418537050485611, loss=0.018418237566947937
I0306 04:29:33.568389 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:31:18.273244 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:31:21.540719 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:31:24.819514 140444430841664 submission_runner.py:411] Time since start: 66010.44s, 	Step: 133402, 	{'train/accuracy': 0.9955294132232666, 'train/loss': 0.013977526687085629, 'train/mean_average_precision': 0.7719156447251221, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944672347701114, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2859014656423316, 'test/num_examples': 43793, 'score': 43476.01622104645, 'total_duration': 66010.43513774872, 'accumulated_submission_time': 43476.01622104645, 'accumulated_eval_time': 22522.911881923676, 'accumulated_logging_time': 7.421867370605469}
I0306 04:31:24.864124 140275669853952 logging_writer.py:48] [133402] accumulated_eval_time=22522.911882, accumulated_logging_time=7.421867, accumulated_submission_time=43476.016221, global_step=133402, preemption_count=0, score=43476.016221, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285901, test/num_examples=43793, total_duration=66010.435138, train/accuracy=0.995529, train/loss=0.013978, train/mean_average_precision=0.771916, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294467, validation/num_examples=43793
I0306 04:31:57.108249 140277079156480 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.1430797427892685, loss=0.017191633582115173
I0306 04:32:29.685155 140275669853952 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.1416095346212387, loss=0.017382781952619553
I0306 04:33:02.531159 140277079156480 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.14853954315185547, loss=0.018184270709753036
I0306 04:33:34.780839 140275669853952 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.14648258686065674, loss=0.02124386467039585
I0306 04:34:07.292001 140277079156480 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.14381304383277893, loss=0.01733403466641903
I0306 04:34:39.815860 140275669853952 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.13956503570079803, loss=0.018103113397955894
I0306 04:35:12.374663 140277079156480 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.1269209086894989, loss=0.016924504190683365
I0306 04:35:24.951758 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:37:11.267087 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:37:14.457550 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:37:17.563717 140444430841664 submission_runner.py:411] Time since start: 66363.18s, 	Step: 134140, 	{'train/accuracy': 0.995570719242096, 'train/loss': 0.013936983421444893, 'train/mean_average_precision': 0.7698331831434866, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29455821997862136, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857146601910127, 'test/num_examples': 43793, 'score': 43716.06605672836, 'total_duration': 66363.17952227592, 'accumulated_submission_time': 43716.06605672836, 'accumulated_eval_time': 22635.523801088333, 'accumulated_logging_time': 7.4783735275268555}
I0306 04:37:17.605295 140252715751168 logging_writer.py:48] [134140] accumulated_eval_time=22635.523801, accumulated_logging_time=7.478374, accumulated_submission_time=43716.066057, global_step=134140, preemption_count=0, score=43716.066057, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285715, test/num_examples=43793, total_duration=66363.179522, train/accuracy=0.995571, train/loss=0.013937, train/mean_average_precision=0.769833, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294558, validation/num_examples=43793
I0306 04:37:38.053898 140276755326720 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.13297848403453827, loss=0.01903742179274559
I0306 04:38:11.333407 140252715751168 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.14986054599285126, loss=0.018671352416276932
I0306 04:38:44.390208 140276755326720 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.15681728720664978, loss=0.01948319561779499
I0306 04:39:16.973914 140252715751168 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.1431538462638855, loss=0.01662832871079445
I0306 04:39:49.695508 140276755326720 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.1480955332517624, loss=0.01647489331662655
I0306 04:40:22.612755 140252715751168 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.16353489458560944, loss=0.019668491557240486
I0306 04:40:54.937881 140276755326720 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.1435687094926834, loss=0.016294702887535095
I0306 04:41:17.593791 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:42:59.545811 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:43:02.897460 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:43:08.653663 140444430841664 submission_runner.py:411] Time since start: 66714.27s, 	Step: 134871, 	{'train/accuracy': 0.995523989200592, 'train/loss': 0.014071997255086899, 'train/mean_average_precision': 0.772939178937591, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29458259860451064, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2858309724099989, 'test/num_examples': 43793, 'score': 43956.02018260956, 'total_duration': 66714.26945352554, 'accumulated_submission_time': 43956.02018260956, 'accumulated_eval_time': 22746.583614349365, 'accumulated_logging_time': 7.531560182571411}
I0306 04:43:08.701710 140275669853952 logging_writer.py:48] [134871] accumulated_eval_time=22746.583614, accumulated_logging_time=7.531560, accumulated_submission_time=43956.020183, global_step=134871, preemption_count=0, score=43956.020183, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285831, test/num_examples=43793, total_duration=66714.269454, train/accuracy=0.995524, train/loss=0.014072, train/mean_average_precision=0.772939, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294583, validation/num_examples=43793
I0306 04:43:18.495945 140277079156480 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.14484678208827972, loss=0.019611071795225143
I0306 04:43:50.971797 140275669853952 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.1548793911933899, loss=0.019006231799721718
I0306 04:44:23.949740 140277079156480 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.13205701112747192, loss=0.016264675185084343
I0306 04:44:56.565328 140275669853952 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.1421353816986084, loss=0.01871706172823906
I0306 04:45:28.998135 140277079156480 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.17122989892959595, loss=0.01994304731488228
I0306 04:46:01.441387 140275669853952 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.14025582373142242, loss=0.018451549112796783
I0306 04:46:34.272812 140277079156480 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.17022734880447388, loss=0.01869027130305767
I0306 04:47:07.160144 140275669853952 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.1553521454334259, loss=0.019541053101420403
I0306 04:47:08.773340 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:48:50.278203 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:48:53.468175 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:48:56.548937 140444430841664 submission_runner.py:411] Time since start: 67062.16s, 	Step: 135606, 	{'train/accuracy': 0.9955481886863708, 'train/loss': 0.013948153704404831, 'train/mean_average_precision': 0.7806334622776318, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945880907545196, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857107305887505, 'test/num_examples': 43793, 'score': 44196.05651593208, 'total_duration': 67062.164737463, 'accumulated_submission_time': 44196.05651593208, 'accumulated_eval_time': 22854.359155654907, 'accumulated_logging_time': 7.5914692878723145}
I0306 04:48:56.591412 140251254044416 logging_writer.py:48] [135606] accumulated_eval_time=22854.359156, accumulated_logging_time=7.591469, accumulated_submission_time=44196.056516, global_step=135606, preemption_count=0, score=44196.056516, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285711, test/num_examples=43793, total_duration=67062.164737, train/accuracy=0.995548, train/loss=0.013948, train/mean_average_precision=0.780633, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294588, validation/num_examples=43793
I0306 04:49:27.915245 140276755326720 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.1518612504005432, loss=0.019614199176430702
I0306 04:50:00.696354 140251254044416 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.13215453922748566, loss=0.017315784469246864
I0306 04:50:33.660193 140276755326720 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.1438588947057724, loss=0.016344930976629257
I0306 04:51:06.421535 140251254044416 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.12891334295272827, loss=0.01738050766289234
I0306 04:51:38.754156 140276755326720 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.14358137547969818, loss=0.017794154584407806
I0306 04:52:11.797888 140251254044416 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.1428070217370987, loss=0.017755359411239624
I0306 04:52:44.399749 140276755326720 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.15082798898220062, loss=0.01773376576602459
I0306 04:52:56.703226 140444430841664 spec.py:321] Evaluating on the training split.
I0306 04:54:36.557236 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 04:54:39.616366 140444430841664 spec.py:349] Evaluating on the test split.
I0306 04:54:42.681784 140444430841664 submission_runner.py:411] Time since start: 67408.30s, 	Step: 136339, 	{'train/accuracy': 0.9955635070800781, 'train/loss': 0.013981434516608715, 'train/mean_average_precision': 0.7787603347264325, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2946117411358875, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28570032637718984, 'test/num_examples': 43793, 'score': 44436.13486599922, 'total_duration': 67408.29759216309, 'accumulated_submission_time': 44436.13486599922, 'accumulated_eval_time': 22960.337666273117, 'accumulated_logging_time': 7.645104646682739}
I0306 04:54:42.723218 140275669853952 logging_writer.py:48] [136339] accumulated_eval_time=22960.337666, accumulated_logging_time=7.645105, accumulated_submission_time=44436.134866, global_step=136339, preemption_count=0, score=44436.134866, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285700, test/num_examples=43793, total_duration=67408.297592, train/accuracy=0.995564, train/loss=0.013981, train/mean_average_precision=0.778760, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294612, validation/num_examples=43793
I0306 04:55:03.277846 140277079156480 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.13739854097366333, loss=0.016263451427221298
I0306 04:55:35.398003 140275669853952 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.1553879976272583, loss=0.01957133412361145
I0306 04:56:07.880897 140277079156480 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.13804014027118683, loss=0.01778753474354744
I0306 04:56:40.406000 140275669853952 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.14213836193084717, loss=0.01671203039586544
I0306 04:57:13.061024 140277079156480 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.1449538618326187, loss=0.018521029502153397
I0306 04:57:45.426571 140275669853952 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.15315169095993042, loss=0.018673811107873917
I0306 04:58:18.092786 140277079156480 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.1593032330274582, loss=0.020005881786346436
I0306 04:58:42.852070 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:00:25.363250 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:00:28.503724 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:00:31.644268 140444430841664 submission_runner.py:411] Time since start: 67757.26s, 	Step: 137078, 	{'train/accuracy': 0.9955287575721741, 'train/loss': 0.014043471775949001, 'train/mean_average_precision': 0.773235530751952, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945560652848558, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856534864633366, 'test/num_examples': 43793, 'score': 44676.229766607285, 'total_duration': 67757.26006889343, 'accumulated_submission_time': 44676.229766607285, 'accumulated_eval_time': 23069.12981247902, 'accumulated_logging_time': 7.6977643966674805}
I0306 05:00:31.688191 140251254044416 logging_writer.py:48] [137078] accumulated_eval_time=23069.129812, accumulated_logging_time=7.697764, accumulated_submission_time=44676.229767, global_step=137078, preemption_count=0, score=44676.229767, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285653, test/num_examples=43793, total_duration=67757.260069, train/accuracy=0.995529, train/loss=0.014043, train/mean_average_precision=0.773236, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294556, validation/num_examples=43793
I0306 05:00:39.239024 140252715751168 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.15520203113555908, loss=0.019015880301594734
I0306 05:01:12.182792 140251254044416 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.13333086669445038, loss=0.017357954755425453
I0306 05:01:44.864845 140252715751168 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.16287833452224731, loss=0.020052989944815636
I0306 05:02:17.671440 140251254044416 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.13434311747550964, loss=0.017039787024259567
I0306 05:02:50.218749 140252715751168 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.12567313015460968, loss=0.016633208841085434
I0306 05:03:23.289834 140251254044416 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.12967130541801453, loss=0.017732592299580574
I0306 05:03:55.545869 140252715751168 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.14758270978927612, loss=0.019840853288769722
I0306 05:04:28.236458 140251254044416 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.1289006918668747, loss=0.01577066071331501
I0306 05:04:31.784425 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:06:13.639468 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:06:16.707601 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:06:21.774222 140444430841664 submission_runner.py:411] Time since start: 68107.39s, 	Step: 137812, 	{'train/accuracy': 0.9955397248268127, 'train/loss': 0.01404982153326273, 'train/mean_average_precision': 0.7716928762320991, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945598515327954, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856929229490918, 'test/num_examples': 43793, 'score': 44916.29040455818, 'total_duration': 68107.390021801, 'accumulated_submission_time': 44916.29040455818, 'accumulated_eval_time': 23179.11955356598, 'accumulated_logging_time': 7.7549028396606445}
I0306 05:06:21.819072 140275669853952 logging_writer.py:48] [137812] accumulated_eval_time=23179.119554, accumulated_logging_time=7.754903, accumulated_submission_time=44916.290405, global_step=137812, preemption_count=0, score=44916.290405, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285693, test/num_examples=43793, total_duration=68107.390022, train/accuracy=0.995540, train/loss=0.014050, train/mean_average_precision=0.771693, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294560, validation/num_examples=43793
I0306 05:06:50.715063 140277079156480 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.14410454034805298, loss=0.017531953752040863
I0306 05:07:23.208506 140275669853952 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.15461330115795135, loss=0.02089526690542698
I0306 05:07:55.986933 140277079156480 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.15875621140003204, loss=0.017200957983732224
I0306 05:08:28.695792 140275669853952 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.1507081538438797, loss=0.017891084775328636
I0306 05:09:01.476974 140277079156480 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.13956980407238007, loss=0.018803855404257774
I0306 05:09:33.912678 140275669853952 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.14709505438804626, loss=0.01984572224318981
I0306 05:10:06.267399 140277079156480 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.15423955023288727, loss=0.019257718697190285
I0306 05:10:21.894517 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:12:03.805296 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:12:06.888162 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:12:09.969015 140444430841664 submission_runner.py:411] Time since start: 68455.58s, 	Step: 138549, 	{'train/accuracy': 0.9955530166625977, 'train/loss': 0.013956384733319283, 'train/mean_average_precision': 0.7666460856938745, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945474761017078, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28565505018762904, 'test/num_examples': 43793, 'score': 45156.33108019829, 'total_duration': 68455.58481907845, 'accumulated_submission_time': 45156.33108019829, 'accumulated_eval_time': 23287.194014549255, 'accumulated_logging_time': 7.812101364135742}
I0306 05:12:10.010812 140251254044416 logging_writer.py:48] [138549] accumulated_eval_time=23287.194015, accumulated_logging_time=7.812101, accumulated_submission_time=45156.331080, global_step=138549, preemption_count=0, score=45156.331080, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285655, test/num_examples=43793, total_duration=68455.584819, train/accuracy=0.995553, train/loss=0.013956, train/mean_average_precision=0.766646, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294547, validation/num_examples=43793
I0306 05:12:27.034973 140276755326720 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.13795854151248932, loss=0.0181276798248291
I0306 05:12:59.660201 140251254044416 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.1361476182937622, loss=0.01736346259713173
I0306 05:13:32.345088 140276755326720 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.1496739685535431, loss=0.019844304770231247
I0306 05:14:04.996407 140251254044416 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.157725527882576, loss=0.020267004147171974
I0306 05:14:38.234718 140276755326720 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.1536046266555786, loss=0.018063873052597046
I0306 05:15:12.055943 140251254044416 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.13750644028186798, loss=0.01699148677289486
I0306 05:15:44.274517 140276755326720 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.13809852302074432, loss=0.016748549416661263
I0306 05:16:10.179416 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:17:53.411890 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:17:56.501234 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:17:59.587483 140444430841664 submission_runner.py:411] Time since start: 68805.20s, 	Step: 139280, 	{'train/accuracy': 0.9955349564552307, 'train/loss': 0.013997852802276611, 'train/mean_average_precision': 0.7760984157232222, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448928960317605, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2856873922115879, 'test/num_examples': 43793, 'score': 45396.46606326103, 'total_duration': 68805.20327782631, 'accumulated_submission_time': 45396.46606326103, 'accumulated_eval_time': 23396.602025985718, 'accumulated_logging_time': 7.865617513656616}
I0306 05:17:59.630475 140252715751168 logging_writer.py:48] [139280] accumulated_eval_time=23396.602026, accumulated_logging_time=7.865618, accumulated_submission_time=45396.466063, global_step=139280, preemption_count=0, score=45396.466063, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285687, test/num_examples=43793, total_duration=68805.203278, train/accuracy=0.995535, train/loss=0.013998, train/mean_average_precision=0.776098, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294489, validation/num_examples=43793
I0306 05:18:06.648062 140275669853952 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.1442706137895584, loss=0.01724623143672943
I0306 05:18:39.050291 140252715751168 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.1425980180501938, loss=0.019018277525901794
I0306 05:19:11.354233 140275669853952 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.14786522090435028, loss=0.017579633742570877
I0306 05:19:43.418041 140252715751168 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.1426497995853424, loss=0.018559642136096954
I0306 05:20:15.580792 140275669853952 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.15537913143634796, loss=0.017777591943740845
I0306 05:20:47.772982 140252715751168 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.15711675584316254, loss=0.01812383346259594
I0306 05:21:19.884613 140275669853952 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.14865517616271973, loss=0.01901119202375412
I0306 05:21:52.187878 140252715751168 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.13465002179145813, loss=0.01903373748064041
I0306 05:21:59.606215 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:23:42.274836 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:23:45.334920 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:23:48.356534 140444430841664 submission_runner.py:411] Time since start: 69153.97s, 	Step: 140024, 	{'train/accuracy': 0.9955670833587646, 'train/loss': 0.01396939903497696, 'train/mean_average_precision': 0.7815567202019054, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945874760881046, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28589442628899386, 'test/num_examples': 43793, 'score': 45636.407398462296, 'total_duration': 69153.9723265171, 'accumulated_submission_time': 45636.407398462296, 'accumulated_eval_time': 23505.352279663086, 'accumulated_logging_time': 7.920868873596191}
I0306 05:23:48.399202 140276755326720 logging_writer.py:48] [140024] accumulated_eval_time=23505.352280, accumulated_logging_time=7.920869, accumulated_submission_time=45636.407398, global_step=140024, preemption_count=0, score=45636.407398, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285894, test/num_examples=43793, total_duration=69153.972327, train/accuracy=0.995567, train/loss=0.013969, train/mean_average_precision=0.781557, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294587, validation/num_examples=43793
I0306 05:24:13.535738 140277079156480 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.15792644023895264, loss=0.019398769363760948
I0306 05:24:45.833636 140276755326720 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.15374687314033508, loss=0.018072931095957756
I0306 05:25:18.301187 140277079156480 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.1440580189228058, loss=0.017394326627254486
I0306 05:25:50.503520 140276755326720 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.14686159789562225, loss=0.017517419531941414
I0306 05:26:22.815240 140277079156480 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.15275494754314423, loss=0.017962593585252762
I0306 05:26:55.012012 140276755326720 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.13952726125717163, loss=0.017177168279886246
I0306 05:27:27.404394 140277079156480 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.14934688806533813, loss=0.016965201124548912
I0306 05:27:48.621443 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:29:30.417499 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:29:33.639620 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:29:38.700643 140444430841664 submission_runner.py:411] Time since start: 69504.32s, 	Step: 140767, 	{'train/accuracy': 0.995562732219696, 'train/loss': 0.013971544802188873, 'train/mean_average_precision': 0.7835845042765484, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29469528732973843, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856803090264078, 'test/num_examples': 43793, 'score': 45876.594853162766, 'total_duration': 69504.31644678116, 'accumulated_submission_time': 45876.594853162766, 'accumulated_eval_time': 23615.431432724, 'accumulated_logging_time': 7.97593355178833}
I0306 05:29:38.744771 140252715751168 logging_writer.py:48] [140767] accumulated_eval_time=23615.431433, accumulated_logging_time=7.975934, accumulated_submission_time=45876.594853, global_step=140767, preemption_count=0, score=45876.594853, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285680, test/num_examples=43793, total_duration=69504.316447, train/accuracy=0.995563, train/loss=0.013972, train/mean_average_precision=0.783585, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294695, validation/num_examples=43793
I0306 05:29:49.877595 140275669853952 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.14742019772529602, loss=0.017972785979509354
I0306 05:30:22.010949 140252715751168 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.14897455275058746, loss=0.016854513436555862
I0306 05:30:53.921028 140275669853952 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.1479136049747467, loss=0.01762557215988636
I0306 05:31:26.150954 140252715751168 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.14416801929473877, loss=0.0173126719892025
I0306 05:31:58.077634 140275669853952 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.1498473584651947, loss=0.01974693313241005
I0306 05:32:30.029050 140252715751168 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.152301624417305, loss=0.01770462468266487
I0306 05:33:02.236850 140275669853952 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.14115414023399353, loss=0.017560843378305435
I0306 05:33:34.676080 140252715751168 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.12311138957738876, loss=0.01582944206893444
I0306 05:33:38.953209 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:35:21.820294 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:35:24.894832 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:35:27.915493 140444430841664 submission_runner.py:411] Time since start: 69853.53s, 	Step: 141514, 	{'train/accuracy': 0.9955607056617737, 'train/loss': 0.013952791690826416, 'train/mean_average_precision': 0.7693302481492503, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29450973986243745, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857075935707077, 'test/num_examples': 43793, 'score': 46116.76946258545, 'total_duration': 69853.53129506111, 'accumulated_submission_time': 46116.76946258545, 'accumulated_eval_time': 23724.393659591675, 'accumulated_logging_time': 8.031497716903687}
I0306 05:35:27.957969 140276755326720 logging_writer.py:48] [141514] accumulated_eval_time=23724.393660, accumulated_logging_time=8.031498, accumulated_submission_time=46116.769463, global_step=141514, preemption_count=0, score=46116.769463, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285708, test/num_examples=43793, total_duration=69853.531295, train/accuracy=0.995561, train/loss=0.013953, train/mean_average_precision=0.769330, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294510, validation/num_examples=43793
I0306 05:35:55.927592 140277079156480 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.13536761701107025, loss=0.016834847629070282
I0306 05:36:28.378568 140276755326720 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.1340731680393219, loss=0.0165998674929142
I0306 05:37:01.034161 140277079156480 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.14784076809883118, loss=0.017139676958322525
I0306 05:37:33.722588 140276755326720 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.13059820234775543, loss=0.016802595928311348
I0306 05:38:06.101023 140277079156480 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.12682373821735382, loss=0.017709292471408844
I0306 05:38:38.221753 140276755326720 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.1409265697002411, loss=0.018065784126520157
I0306 05:39:10.426794 140277079156480 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.12854455411434174, loss=0.016773857176303864
I0306 05:39:28.212713 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:41:11.884524 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:41:14.908456 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:41:20.123181 140444430841664 submission_runner.py:411] Time since start: 70205.74s, 	Step: 142256, 	{'train/accuracy': 0.995522677898407, 'train/loss': 0.01407346036285162, 'train/mean_average_precision': 0.7720646022590836, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945275033973265, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28570754165905193, 'test/num_examples': 43793, 'score': 46356.98973464966, 'total_duration': 70205.73898673058, 'accumulated_submission_time': 46356.98973464966, 'accumulated_eval_time': 23836.304080724716, 'accumulated_logging_time': 8.086358785629272}
I0306 05:41:20.167458 140251254044416 logging_writer.py:48] [142256] accumulated_eval_time=23836.304081, accumulated_logging_time=8.086359, accumulated_submission_time=46356.989735, global_step=142256, preemption_count=0, score=46356.989735, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285708, test/num_examples=43793, total_duration=70205.738987, train/accuracy=0.995523, train/loss=0.014073, train/mean_average_precision=0.772065, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294528, validation/num_examples=43793
I0306 05:41:35.011764 140275669853952 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.13584811985492706, loss=0.0169322919100523
I0306 05:42:07.384843 140251254044416 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.1376708596944809, loss=0.017765022814273834
I0306 05:42:39.435977 140275669853952 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.14400100708007812, loss=0.018435928970575333
I0306 05:43:11.778585 140251254044416 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.1366438865661621, loss=0.016995834186673164
I0306 05:43:44.656874 140275669853952 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.14226292073726654, loss=0.016919933259487152
I0306 05:44:17.869755 140251254044416 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.12951645255088806, loss=0.015607615932822227
I0306 05:44:49.832313 140275669853952 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.16407252848148346, loss=0.019548628479242325
I0306 05:45:20.445164 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:47:01.134072 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:47:04.395678 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:47:07.407082 140444430841664 submission_runner.py:411] Time since start: 70553.02s, 	Step: 142996, 	{'train/accuracy': 0.9955246448516846, 'train/loss': 0.014024957083165646, 'train/mean_average_precision': 0.7742682707114532, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945714240781431, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857092445829384, 'test/num_examples': 43793, 'score': 46597.234624147415, 'total_duration': 70553.02288079262, 'accumulated_submission_time': 46597.234624147415, 'accumulated_eval_time': 23943.265949487686, 'accumulated_logging_time': 8.141406297683716}
I0306 05:47:07.451357 140276755326720 logging_writer.py:48] [142996] accumulated_eval_time=23943.265949, accumulated_logging_time=8.141406, accumulated_submission_time=46597.234624, global_step=142996, preemption_count=0, score=46597.234624, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285709, test/num_examples=43793, total_duration=70553.022881, train/accuracy=0.995525, train/loss=0.014025, train/mean_average_precision=0.774268, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294571, validation/num_examples=43793
I0306 05:47:09.123049 140277079156480 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.15838821232318878, loss=0.019921381026506424
I0306 05:47:41.369010 140276755326720 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.12417474389076233, loss=0.017719263210892677
I0306 05:48:13.543714 140277079156480 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.16026893258094788, loss=0.017452996224164963
I0306 05:48:46.059078 140276755326720 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.1318999081850052, loss=0.017048651352524757
I0306 05:49:18.541884 140277079156480 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.13750691711902618, loss=0.017100874334573746
I0306 05:49:50.571519 140276755326720 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.14019067585468292, loss=0.018020866438746452
I0306 05:50:22.777218 140277079156480 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.13832582533359528, loss=0.0179488193243742
I0306 05:50:54.980432 140276755326720 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.14665240049362183, loss=0.018727527931332588
I0306 05:51:07.512496 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:52:50.060561 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:52:53.108285 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:52:56.071641 140444430841664 submission_runner.py:411] Time since start: 70901.69s, 	Step: 143740, 	{'train/accuracy': 0.9955927133560181, 'train/loss': 0.013905912637710571, 'train/mean_average_precision': 0.7787287733952317, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2947369056523955, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285687980017125, 'test/num_examples': 43793, 'score': 46837.26299023628, 'total_duration': 70901.68744587898, 'accumulated_submission_time': 46837.26299023628, 'accumulated_eval_time': 24051.82504749298, 'accumulated_logging_time': 8.196717262268066}
I0306 05:52:56.114940 140252715751168 logging_writer.py:48] [143740] accumulated_eval_time=24051.825047, accumulated_logging_time=8.196717, accumulated_submission_time=46837.262990, global_step=143740, preemption_count=0, score=46837.262990, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285688, test/num_examples=43793, total_duration=70901.687446, train/accuracy=0.995593, train/loss=0.013906, train/mean_average_precision=0.778729, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294737, validation/num_examples=43793
I0306 05:53:15.857971 140275669853952 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.1482936441898346, loss=0.01719331368803978
I0306 05:53:47.986099 140252715751168 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.15119317173957825, loss=0.01836719550192356
I0306 05:54:20.437439 140275669853952 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.16505403816699982, loss=0.01778772473335266
I0306 05:54:52.385909 140252715751168 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.1353827565908432, loss=0.016551418229937553
I0306 05:55:24.376489 140275669853952 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.14057840406894684, loss=0.017761463299393654
I0306 05:55:56.191137 140252715751168 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.15249384939670563, loss=0.019864700734615326
I0306 05:56:28.443956 140275669853952 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.14454947412014008, loss=0.01867532730102539
I0306 05:56:56.083837 140444430841664 spec.py:321] Evaluating on the training split.
I0306 05:58:36.347898 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 05:58:39.336096 140444430841664 spec.py:349] Evaluating on the test split.
I0306 05:58:42.333914 140444430841664 submission_runner.py:411] Time since start: 71247.95s, 	Step: 144487, 	{'train/accuracy': 0.9955635070800781, 'train/loss': 0.014012456871569157, 'train/mean_average_precision': 0.7812311607734994, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945227253288941, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28566755639923, 'test/num_examples': 43793, 'score': 47077.19783067703, 'total_duration': 71247.94971942902, 'accumulated_submission_time': 47077.19783067703, 'accumulated_eval_time': 24158.07507967949, 'accumulated_logging_time': 8.25191354751587}
I0306 05:58:42.376826 140251254044416 logging_writer.py:48] [144487] accumulated_eval_time=24158.075080, accumulated_logging_time=8.251914, accumulated_submission_time=47077.197831, global_step=144487, preemption_count=0, score=47077.197831, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285668, test/num_examples=43793, total_duration=71247.949719, train/accuracy=0.995564, train/loss=0.014012, train/mean_average_precision=0.781231, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294523, validation/num_examples=43793
I0306 05:58:47.076690 140277079156480 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.1441766619682312, loss=0.0178952906280756
I0306 05:59:19.811984 140251254044416 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.16941988468170166, loss=0.020150750875473022
I0306 05:59:52.166025 140277079156480 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.13949467241764069, loss=0.017824644222855568
I0306 06:00:24.594758 140251254044416 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.1590384542942047, loss=0.01856403611600399
I0306 06:00:57.031837 140277079156480 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.15301845967769623, loss=0.021104587242007256
I0306 06:01:29.443505 140251254044416 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.1424458622932434, loss=0.017298931255936623
I0306 06:02:02.013519 140277079156480 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.1243278980255127, loss=0.01630605198442936
I0306 06:02:34.541081 140251254044416 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.15061524510383606, loss=0.018775124102830887
I0306 06:02:42.366865 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:04:24.320716 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:04:27.380947 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:04:30.422299 140444430841664 submission_runner.py:411] Time since start: 71596.04s, 	Step: 145225, 	{'train/accuracy': 0.995533287525177, 'train/loss': 0.01394967082887888, 'train/mean_average_precision': 0.7758802733144703, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448656949201785, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285769349252281, 'test/num_examples': 43793, 'score': 47317.14991569519, 'total_duration': 71596.03808999062, 'accumulated_submission_time': 47317.14991569519, 'accumulated_eval_time': 24266.130458831787, 'accumulated_logging_time': 8.306992053985596}
I0306 06:04:30.470922 140252715751168 logging_writer.py:48] [145225] accumulated_eval_time=24266.130459, accumulated_logging_time=8.306992, accumulated_submission_time=47317.149916, global_step=145225, preemption_count=0, score=47317.149916, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285769, test/num_examples=43793, total_duration=71596.038090, train/accuracy=0.995533, train/loss=0.013950, train/mean_average_precision=0.775880, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294487, validation/num_examples=43793
I0306 06:04:55.465580 140275669853952 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.1568075716495514, loss=0.019104022532701492
I0306 06:05:27.797688 140252715751168 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.1422477513551712, loss=0.01696666330099106
I0306 06:05:59.677727 140275669853952 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.15808214247226715, loss=0.018496336415410042
I0306 06:06:32.245247 140252715751168 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.1455824375152588, loss=0.018967987969517708
I0306 06:07:04.847007 140275669853952 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.16735263168811798, loss=0.01807866245508194
I0306 06:07:37.463338 140252715751168 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.1420532763004303, loss=0.0175181794911623
I0306 06:08:09.798391 140275669853952 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.15034663677215576, loss=0.0175032839179039
I0306 06:08:30.542172 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:10:15.049402 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:10:18.039673 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:10:20.998364 140444430841664 submission_runner.py:411] Time since start: 71946.61s, 	Step: 145966, 	{'train/accuracy': 0.995587170124054, 'train/loss': 0.013897706754505634, 'train/mean_average_precision': 0.773782049719947, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2946646276835428, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28571729271441937, 'test/num_examples': 43793, 'score': 47557.18814301491, 'total_duration': 71946.61416625977, 'accumulated_submission_time': 47557.18814301491, 'accumulated_eval_time': 24376.586597919464, 'accumulated_logging_time': 8.366672277450562}
I0306 06:10:21.042884 140276755326720 logging_writer.py:48] [145966] accumulated_eval_time=24376.586598, accumulated_logging_time=8.366672, accumulated_submission_time=47557.188143, global_step=145966, preemption_count=0, score=47557.188143, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285717, test/num_examples=43793, total_duration=71946.614166, train/accuracy=0.995587, train/loss=0.013898, train/mean_average_precision=0.773782, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294665, validation/num_examples=43793
I0306 06:10:32.595935 140277079156480 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.1549646556377411, loss=0.01975780725479126
I0306 06:11:04.338739 140276755326720 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.13776923716068268, loss=0.01841346174478531
I0306 06:11:36.097500 140277079156480 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.14136411249637604, loss=0.01707983948290348
I0306 06:12:08.017005 140276755326720 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.13795626163482666, loss=0.01884106546640396
I0306 06:12:39.819355 140277079156480 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.14174777269363403, loss=0.017846042290329933
I0306 06:13:11.998548 140276755326720 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.13902628421783447, loss=0.017832661047577858
I0306 06:13:43.883512 140277079156480 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.14813178777694702, loss=0.018048755824565887
I0306 06:14:16.484384 140276755326720 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.14750877022743225, loss=0.019203493371605873
I0306 06:14:21.128268 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:16:05.455795 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:16:08.454998 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:16:11.497732 140444430841664 submission_runner.py:411] Time since start: 72297.11s, 	Step: 146715, 	{'train/accuracy': 0.9954803586006165, 'train/loss': 0.014134406112134457, 'train/mean_average_precision': 0.7739438319568628, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945254938863308, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28572009514614927, 'test/num_examples': 43793, 'score': 47797.23997068405, 'total_duration': 72297.11353874207, 'accumulated_submission_time': 47797.23997068405, 'accumulated_eval_time': 24486.95604276657, 'accumulated_logging_time': 8.422574520111084}
I0306 06:16:11.542192 140251254044416 logging_writer.py:48] [146715] accumulated_eval_time=24486.956043, accumulated_logging_time=8.422575, accumulated_submission_time=47797.239971, global_step=146715, preemption_count=0, score=47797.239971, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285720, test/num_examples=43793, total_duration=72297.113539, train/accuracy=0.995480, train/loss=0.014134, train/mean_average_precision=0.773944, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294525, validation/num_examples=43793
I0306 06:16:39.027131 140275669853952 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.13764744997024536, loss=0.018140293657779694
I0306 06:17:11.147238 140251254044416 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.15527251362800598, loss=0.01879112794995308
I0306 06:17:43.273295 140275669853952 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.13577409088611603, loss=0.01704753376543522
I0306 06:18:15.463544 140251254044416 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.14960144460201263, loss=0.01805632747709751
I0306 06:18:47.336252 140275669853952 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.13112697005271912, loss=0.017031121999025345
I0306 06:19:19.444197 140251254044416 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.13286252319812775, loss=0.016680950298905373
I0306 06:19:51.883657 140275669853952 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.15160512924194336, loss=0.019726544618606567
I0306 06:20:11.556278 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:21:52.195250 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:21:55.198446 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:21:58.202906 140444430841664 submission_runner.py:411] Time since start: 72643.82s, 	Step: 147461, 	{'train/accuracy': 0.9955896735191345, 'train/loss': 0.013877772726118565, 'train/mean_average_precision': 0.7762476049009219, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2944843963867238, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28575850335549846, 'test/num_examples': 43793, 'score': 48037.220688819885, 'total_duration': 72643.81870794296, 'accumulated_submission_time': 48037.220688819885, 'accumulated_eval_time': 24593.602620601654, 'accumulated_logging_time': 8.478218793869019}
I0306 06:21:58.247359 140252715751168 logging_writer.py:48] [147461] accumulated_eval_time=24593.602621, accumulated_logging_time=8.478219, accumulated_submission_time=48037.220689, global_step=147461, preemption_count=0, score=48037.220689, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285759, test/num_examples=43793, total_duration=72643.818708, train/accuracy=0.995590, train/loss=0.013878, train/mean_average_precision=0.776248, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294484, validation/num_examples=43793
I0306 06:22:11.747968 140276755326720 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.14237423241138458, loss=0.01909892074763775
I0306 06:22:44.464566 140252715751168 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.12363110482692719, loss=0.015878863632678986
I0306 06:23:17.299015 140276755326720 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.14466556906700134, loss=0.019198015332221985
I0306 06:23:49.967340 140252715751168 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.13315673172473907, loss=0.016409320756793022
I0306 06:24:22.604380 140276755326720 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.13901519775390625, loss=0.01776212453842163
I0306 06:24:54.936777 140252715751168 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.1703665852546692, loss=0.019734783098101616
I0306 06:25:27.890043 140276755326720 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.1357506662607193, loss=0.017398668453097343
I0306 06:25:58.332521 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:27:37.234664 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:27:40.238006 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:27:43.204266 140444430841664 submission_runner.py:411] Time since start: 72988.82s, 	Step: 148195, 	{'train/accuracy': 0.9955464005470276, 'train/loss': 0.014040638692677021, 'train/mean_average_precision': 0.7835602272200127, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29461198184363807, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857212119481352, 'test/num_examples': 43793, 'score': 48277.27271056175, 'total_duration': 72988.82006072998, 'accumulated_submission_time': 48277.27271056175, 'accumulated_eval_time': 24698.474319934845, 'accumulated_logging_time': 8.533739566802979}
I0306 06:27:43.247940 140251254044416 logging_writer.py:48] [148195] accumulated_eval_time=24698.474320, accumulated_logging_time=8.533740, accumulated_submission_time=48277.272711, global_step=148195, preemption_count=0, score=48277.272711, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285721, test/num_examples=43793, total_duration=72988.820061, train/accuracy=0.995546, train/loss=0.014041, train/mean_average_precision=0.783560, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294612, validation/num_examples=43793
I0306 06:27:45.410686 140275669853952 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.1335754543542862, loss=0.016326433047652245
I0306 06:28:18.045945 140251254044416 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.15077362954616547, loss=0.01786652021110058
I0306 06:28:50.165990 140275669853952 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.15209481120109558, loss=0.019163012504577637
I0306 06:29:22.938340 140251254044416 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.13449636101722717, loss=0.01720208115875721
I0306 06:29:55.293907 140275669853952 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.1524834930896759, loss=0.018475396558642387
I0306 06:30:27.653402 140251254044416 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.14208397269248962, loss=0.018422316759824753
I0306 06:30:59.694102 140275669853952 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.13009494543075562, loss=0.016813209280371666
I0306 06:31:32.018748 140251254044416 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.13870279490947723, loss=0.018195800483226776
I0306 06:31:43.282223 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:33:29.951958 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:33:32.982997 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:33:36.068451 140444430841664 submission_runner.py:411] Time since start: 73341.68s, 	Step: 148936, 	{'train/accuracy': 0.9955344200134277, 'train/loss': 0.01405887771397829, 'train/mean_average_precision': 0.7736069100158747, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945416036507968, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2856568886471392, 'test/num_examples': 43793, 'score': 48517.27303361893, 'total_duration': 73341.68425559998, 'accumulated_submission_time': 48517.27303361893, 'accumulated_eval_time': 24811.260496854782, 'accumulated_logging_time': 8.589645624160767}
I0306 06:33:36.113030 140252715751168 logging_writer.py:48] [148936] accumulated_eval_time=24811.260497, accumulated_logging_time=8.589646, accumulated_submission_time=48517.273034, global_step=148936, preemption_count=0, score=48517.273034, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285657, test/num_examples=43793, total_duration=73341.684256, train/accuracy=0.995534, train/loss=0.014059, train/mean_average_precision=0.773607, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294542, validation/num_examples=43793
I0306 06:33:57.223913 140277079156480 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.1512395292520523, loss=0.01821337454020977
I0306 06:34:29.623276 140252715751168 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.1585587114095688, loss=0.019980885088443756
I0306 06:35:01.982322 140277079156480 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.1397945135831833, loss=0.01776803843677044
I0306 06:35:34.218163 140252715751168 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.13918635249137878, loss=0.016619889065623283
I0306 06:36:06.603035 140277079156480 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.15234699845314026, loss=0.01942886784672737
I0306 06:36:38.766441 140252715751168 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.15906578302383423, loss=0.0182948000729084
I0306 06:37:11.092018 140277079156480 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.13799922168254852, loss=0.01900937221944332
I0306 06:37:36.224154 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:39:21.731447 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:39:24.830133 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:39:27.890016 140444430841664 submission_runner.py:411] Time since start: 73693.51s, 	Step: 149679, 	{'train/accuracy': 0.995566189289093, 'train/loss': 0.013927438296377659, 'train/mean_average_precision': 0.7797421371283534, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2947302321803623, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28578043059003444, 'test/num_examples': 43793, 'score': 48757.350826501846, 'total_duration': 73693.50581741333, 'accumulated_submission_time': 48757.350826501846, 'accumulated_eval_time': 24922.926308870316, 'accumulated_logging_time': 8.645307302474976}
I0306 06:39:27.944242 140251254044416 logging_writer.py:48] [149679] accumulated_eval_time=24922.926309, accumulated_logging_time=8.645307, accumulated_submission_time=48757.350827, global_step=149679, preemption_count=0, score=48757.350827, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285780, test/num_examples=43793, total_duration=73693.505817, train/accuracy=0.995566, train/loss=0.013927, train/mean_average_precision=0.779742, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294730, validation/num_examples=43793
I0306 06:39:35.289448 140276755326720 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.14937929809093475, loss=0.019219540059566498
I0306 06:40:07.843564 140251254044416 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.13493649661540985, loss=0.016127778217196465
I0306 06:40:40.027102 140276755326720 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.12115304917097092, loss=0.014665740542113781
I0306 06:41:12.061442 140251254044416 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.1421436071395874, loss=0.018366718664765358
I0306 06:41:44.189706 140276755326720 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.14335863292217255, loss=0.017314912751317024
I0306 06:42:16.282488 140251254044416 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.15285798907279968, loss=0.01866408810019493
I0306 06:42:48.382916 140276755326720 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.155915766954422, loss=0.01889997161924839
I0306 06:43:20.600287 140251254044416 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.1672290563583374, loss=0.018229752779006958
I0306 06:43:27.970998 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:45:12.601598 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:45:15.632008 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:45:18.663685 140444430841664 submission_runner.py:411] Time since start: 74044.28s, 	Step: 150424, 	{'train/accuracy': 0.9955343008041382, 'train/loss': 0.014041313901543617, 'train/mean_average_precision': 0.7644432246613764, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29455362998971557, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856523374013939, 'test/num_examples': 43793, 'score': 48997.344621658325, 'total_duration': 74044.27948951721, 'accumulated_submission_time': 48997.344621658325, 'accumulated_eval_time': 25033.61894416809, 'accumulated_logging_time': 8.710498809814453}
I0306 06:45:18.709226 140252715751168 logging_writer.py:48] [150424] accumulated_eval_time=25033.618944, accumulated_logging_time=8.710499, accumulated_submission_time=48997.344622, global_step=150424, preemption_count=0, score=48997.344622, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285652, test/num_examples=43793, total_duration=74044.279490, train/accuracy=0.995534, train/loss=0.014041, train/mean_average_precision=0.764443, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294554, validation/num_examples=43793
I0306 06:45:44.479334 140277079156480 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.14528629183769226, loss=0.017862223088741302
I0306 06:46:17.530287 140252715751168 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.15239334106445312, loss=0.018871070817112923
I0306 06:46:49.794328 140277079156480 logging_writer.py:48] [150700] global_step=150700, grad_norm=0.13383381068706512, loss=0.018236808478832245
I0306 06:47:21.931146 140252715751168 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.1602458506822586, loss=0.01812930218875408
I0306 06:47:54.026335 140277079156480 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.1468527466058731, loss=0.019083401188254356
I0306 06:48:26.606167 140252715751168 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.1468917578458786, loss=0.018602371215820312
I0306 06:48:58.361973 140277079156480 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.15462751686573029, loss=0.017175616696476936
I0306 06:49:18.724836 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:51:02.681139 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:51:05.943705 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:51:09.076466 140444430841664 submission_runner.py:411] Time since start: 74394.69s, 	Step: 151164, 	{'train/accuracy': 0.9955853223800659, 'train/loss': 0.013883808627724648, 'train/mean_average_precision': 0.7787436689713627, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945482853631458, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28570420564577215, 'test/num_examples': 43793, 'score': 49236.9952814579, 'total_duration': 74394.6922750473, 'accumulated_submission_time': 49236.9952814579, 'accumulated_eval_time': 25143.97053194046, 'accumulated_logging_time': 9.098667860031128}
I0306 06:51:09.120862 140251254044416 logging_writer.py:48] [151164] accumulated_eval_time=25143.970532, accumulated_logging_time=9.098668, accumulated_submission_time=49236.995281, global_step=151164, preemption_count=0, score=49236.995281, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285704, test/num_examples=43793, total_duration=74394.692275, train/accuracy=0.995585, train/loss=0.013884, train/mean_average_precision=0.778744, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294548, validation/num_examples=43793
I0306 06:51:20.965176 140276755326720 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.14571499824523926, loss=0.017903389409184456
I0306 06:51:53.352739 140251254044416 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.1345803290605545, loss=0.016762759536504745
I0306 06:52:25.582419 140276755326720 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.16159822046756744, loss=0.019651981070637703
I0306 06:52:57.712338 140251254044416 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.13531501591205597, loss=0.015301343984901905
I0306 06:53:30.153239 140276755326720 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.14956147968769073, loss=0.019014233723282814
I0306 06:54:02.805963 140251254044416 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.1375935673713684, loss=0.016472842544317245
I0306 06:54:35.259006 140276755326720 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.14514972269535065, loss=0.01749126985669136
I0306 06:55:07.441596 140251254044416 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.13268229365348816, loss=0.01718137040734291
I0306 06:55:09.392501 140444430841664 spec.py:321] Evaluating on the training split.
I0306 06:56:55.750061 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 06:56:59.194517 140444430841664 spec.py:349] Evaluating on the test split.
I0306 06:57:02.620102 140444430841664 submission_runner.py:411] Time since start: 74748.24s, 	Step: 151907, 	{'train/accuracy': 0.9954984784126282, 'train/loss': 0.014124805107712746, 'train/mean_average_precision': 0.7813303133972982, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945451697094664, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28592636977561464, 'test/num_examples': 43793, 'score': 49477.232617139816, 'total_duration': 74748.23588776588, 'accumulated_submission_time': 49477.232617139816, 'accumulated_eval_time': 25257.198073625565, 'accumulated_logging_time': 9.155184268951416}
I0306 06:57:02.669462 140275669853952 logging_writer.py:48] [151907] accumulated_eval_time=25257.198074, accumulated_logging_time=9.155184, accumulated_submission_time=49477.232617, global_step=151907, preemption_count=0, score=49477.232617, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285926, test/num_examples=43793, total_duration=74748.235888, train/accuracy=0.995498, train/loss=0.014125, train/mean_average_precision=0.781330, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294545, validation/num_examples=43793
I0306 06:57:33.271033 140277079156480 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.1455020010471344, loss=0.01793166995048523
I0306 06:58:05.428560 140275669853952 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.1647689789533615, loss=0.016852885484695435
I0306 06:58:37.803707 140277079156480 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.15840040147304535, loss=0.019255448132753372
I0306 06:59:10.307804 140275669853952 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.13277249038219452, loss=0.01499849371612072
I0306 06:59:42.576248 140277079156480 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.15083815157413483, loss=0.017146095633506775
I0306 07:00:14.963055 140275669853952 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.15969376266002655, loss=0.020503411069512367
I0306 07:00:47.006770 140277079156480 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.16500075161457062, loss=0.0183895081281662
I0306 07:01:02.715852 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:02:42.925277 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:02:46.076660 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:02:49.129798 140444430841664 submission_runner.py:411] Time since start: 75094.75s, 	Step: 152649, 	{'train/accuracy': 0.99554842710495, 'train/loss': 0.013996561989188194, 'train/mean_average_precision': 0.7735667334688098, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29459917341767783, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857255714175824, 'test/num_examples': 43793, 'score': 49717.24135637283, 'total_duration': 75094.74559378624, 'accumulated_submission_time': 49717.24135637283, 'accumulated_eval_time': 25363.61196255684, 'accumulated_logging_time': 9.216427087783813}
I0306 07:02:49.174702 140251254044416 logging_writer.py:48] [152649] accumulated_eval_time=25363.611963, accumulated_logging_time=9.216427, accumulated_submission_time=49717.241356, global_step=152649, preemption_count=0, score=49717.241356, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285726, test/num_examples=43793, total_duration=75094.745594, train/accuracy=0.995548, train/loss=0.013997, train/mean_average_precision=0.773567, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294599, validation/num_examples=43793
I0306 07:03:06.495791 140276755326720 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.1480339616537094, loss=0.01702181249856949
I0306 07:03:39.078325 140251254044416 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.12826025485992432, loss=0.01627608761191368
I0306 07:04:11.445428 140276755326720 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.14510168135166168, loss=0.018873397260904312
I0306 07:04:43.870500 140251254044416 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.13153661787509918, loss=0.017480235546827316
I0306 07:05:16.299906 140276755326720 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.14966699481010437, loss=0.018099011853337288
I0306 07:05:48.992068 140251254044416 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.14547011256217957, loss=0.0182296484708786
I0306 07:06:21.690056 140276755326720 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.1521061509847641, loss=0.016359947621822357
I0306 07:06:49.140266 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:08:31.610844 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:08:34.682656 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:08:37.721923 140444430841664 submission_runner.py:411] Time since start: 75443.34s, 	Step: 153386, 	{'train/accuracy': 0.9955598711967468, 'train/loss': 0.013950332999229431, 'train/mean_average_precision': 0.7727688636510217, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945004272365443, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2856822054806806, 'test/num_examples': 43793, 'score': 49957.173169374466, 'total_duration': 75443.33772158623, 'accumulated_submission_time': 49957.173169374466, 'accumulated_eval_time': 25472.193568706512, 'accumulated_logging_time': 9.272395849227905}
I0306 07:08:37.766774 140275669853952 logging_writer.py:48] [153386] accumulated_eval_time=25472.193569, accumulated_logging_time=9.272396, accumulated_submission_time=49957.173169, global_step=153386, preemption_count=0, score=49957.173169, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285682, test/num_examples=43793, total_duration=75443.337722, train/accuracy=0.995560, train/loss=0.013950, train/mean_average_precision=0.772769, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294500, validation/num_examples=43793
I0306 07:08:42.626956 140277079156480 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.13728022575378418, loss=0.01739237830042839
I0306 07:09:15.634458 140275669853952 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.14412662386894226, loss=0.020590122789144516
I0306 07:09:48.271740 140277079156480 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.13042569160461426, loss=0.014913873746991158
I0306 07:10:20.645049 140275669853952 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.14823317527770996, loss=0.01884552277624607
I0306 07:10:53.213648 140277079156480 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.1329115331172943, loss=0.01774578168988228
I0306 07:11:25.627241 140275669853952 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.14955690503120422, loss=0.018218686804175377
I0306 07:11:57.627107 140277079156480 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.1437535136938095, loss=0.018636107444763184
I0306 07:12:29.942886 140275669853952 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.1344507336616516, loss=0.01682441495358944
I0306 07:12:37.750424 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:14:20.707556 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:14:23.802282 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:14:26.795553 140444430841664 submission_runner.py:411] Time since start: 75792.41s, 	Step: 154125, 	{'train/accuracy': 0.99556964635849, 'train/loss': 0.013967925682663918, 'train/mean_average_precision': 0.7743029819107554, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29449224033764076, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857150265415266, 'test/num_examples': 43793, 'score': 50197.12275767326, 'total_duration': 75792.4113547802, 'accumulated_submission_time': 50197.12275767326, 'accumulated_eval_time': 25581.238644123077, 'accumulated_logging_time': 9.328509330749512}
I0306 07:14:26.840383 140252715751168 logging_writer.py:48] [154125] accumulated_eval_time=25581.238644, accumulated_logging_time=9.328509, accumulated_submission_time=50197.122758, global_step=154125, preemption_count=0, score=50197.122758, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285715, test/num_examples=43793, total_duration=75792.411355, train/accuracy=0.995570, train/loss=0.013968, train/mean_average_precision=0.774303, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294492, validation/num_examples=43793
I0306 07:14:51.323918 140276755326720 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.15015806257724762, loss=0.01874186471104622
I0306 07:15:24.279756 140252715751168 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.13307331502437592, loss=0.016234450042247772
I0306 07:15:56.913905 140276755326720 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.15661346912384033, loss=0.017972884699702263
I0306 07:16:30.178879 140252715751168 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.13956671953201294, loss=0.017806004732847214
I0306 07:17:03.340573 140276755326720 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.13455480337142944, loss=0.017767740413546562
I0306 07:17:35.809795 140252715751168 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.13263103365898132, loss=0.016828911378979683
I0306 07:18:08.333554 140276755326720 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.1360272914171219, loss=0.017917020246386528
I0306 07:18:27.058864 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:20:10.856883 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:20:13.915008 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:20:16.990319 140444430841664 submission_runner.py:411] Time since start: 76142.61s, 	Step: 154859, 	{'train/accuracy': 0.995524525642395, 'train/loss': 0.014022833667695522, 'train/mean_average_precision': 0.7734727200254108, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29454796117468324, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28574015684993587, 'test/num_examples': 43793, 'score': 50437.306334257126, 'total_duration': 76142.60612154007, 'accumulated_submission_time': 50437.306334257126, 'accumulated_eval_time': 25691.170045137405, 'accumulated_logging_time': 9.385692358016968}
I0306 07:20:17.037011 140251254044416 logging_writer.py:48] [154859] accumulated_eval_time=25691.170045, accumulated_logging_time=9.385692, accumulated_submission_time=50437.306334, global_step=154859, preemption_count=0, score=50437.306334, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285740, test/num_examples=43793, total_duration=76142.606122, train/accuracy=0.995525, train/loss=0.014023, train/mean_average_precision=0.773473, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294548, validation/num_examples=43793
I0306 07:20:30.772855 140277079156480 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.13104283809661865, loss=0.015451802872121334
I0306 07:21:03.162195 140251254044416 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.14399202167987823, loss=0.0170192439109087
I0306 07:21:35.609213 140277079156480 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.14127127826213837, loss=0.01817459613084793
I0306 07:22:08.547122 140251254044416 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.14175784587860107, loss=0.01847575418651104
I0306 07:22:40.972363 140277079156480 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.16808415949344635, loss=0.019721711054444313
I0306 07:23:13.172591 140251254044416 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.14663460850715637, loss=0.017942937090992928
I0306 07:23:44.839034 140277079156480 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.13855762779712677, loss=0.017650632187724113
I0306 07:24:17.078904 140251254044416 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.14639292657375336, loss=0.019960764795541763
I0306 07:24:17.084209 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:25:58.801554 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:26:02.089913 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:26:05.132984 140444430841664 submission_runner.py:411] Time since start: 76490.75s, 	Step: 155601, 	{'train/accuracy': 0.9955626726150513, 'train/loss': 0.013949127867817879, 'train/mean_average_precision': 0.7778168183954786, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29447238453238495, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.285683389303403, 'test/num_examples': 43793, 'score': 50677.317564725876, 'total_duration': 76490.74878931046, 'accumulated_submission_time': 50677.317564725876, 'accumulated_eval_time': 25799.218749046326, 'accumulated_logging_time': 9.444878578186035}
I0306 07:26:05.177976 140275669853952 logging_writer.py:48] [155601] accumulated_eval_time=25799.218749, accumulated_logging_time=9.444879, accumulated_submission_time=50677.317565, global_step=155601, preemption_count=0, score=50677.317565, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285683, test/num_examples=43793, total_duration=76490.748789, train/accuracy=0.995563, train/loss=0.013949, train/mean_average_precision=0.777817, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294472, validation/num_examples=43793
I0306 07:26:37.947729 140276755326720 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.15150651335716248, loss=0.017554476857185364
I0306 07:27:10.800039 140275669853952 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.12268050760030746, loss=0.01575659029185772
I0306 07:27:43.895340 140276755326720 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.14521396160125732, loss=0.017084475606679916
I0306 07:28:16.450052 140275669853952 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.13440823554992676, loss=0.01713608205318451
I0306 07:28:48.648936 140276755326720 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.1374674290418625, loss=0.019363578408956528
I0306 07:29:21.312200 140275669853952 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.127344012260437, loss=0.01526669505983591
I0306 07:29:53.945559 140276755326720 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.14661529660224915, loss=0.019482797011733055
I0306 07:30:05.454448 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:31:47.205329 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:31:50.299537 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:31:53.349622 140444430841664 submission_runner.py:411] Time since start: 76838.97s, 	Step: 156336, 	{'train/accuracy': 0.995536208152771, 'train/loss': 0.014027833007276058, 'train/mean_average_precision': 0.7825999920584917, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2946293654606327, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2861027977699629, 'test/num_examples': 43793, 'score': 50917.56117296219, 'total_duration': 76838.96542525291, 'accumulated_submission_time': 50917.56117296219, 'accumulated_eval_time': 25907.113879442215, 'accumulated_logging_time': 9.500680446624756}
I0306 07:31:53.397001 140251254044416 logging_writer.py:48] [156336] accumulated_eval_time=25907.113879, accumulated_logging_time=9.500680, accumulated_submission_time=50917.561173, global_step=156336, preemption_count=0, score=50917.561173, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.286103, test/num_examples=43793, total_duration=76838.965425, train/accuracy=0.995536, train/loss=0.014028, train/mean_average_precision=0.782600, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294629, validation/num_examples=43793
I0306 07:32:14.709430 140252715751168 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.15644006431102753, loss=0.01739620417356491
I0306 07:32:47.222989 140251254044416 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.15273484587669373, loss=0.017576036974787712
I0306 07:33:19.893342 140252715751168 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.15034286677837372, loss=0.017670972272753716
I0306 07:33:52.155333 140251254044416 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.1287132054567337, loss=0.016113294288516045
I0306 07:34:24.594235 140252715751168 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.14623425900936127, loss=0.017219742760062218
I0306 07:34:57.042717 140251254044416 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.14600138366222382, loss=0.019193029031157494
I0306 07:35:29.276329 140252715751168 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.16229283809661865, loss=0.01977924443781376
I0306 07:35:53.404606 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:37:31.495168 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:37:34.713053 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:37:37.800847 140444430841664 submission_runner.py:411] Time since start: 77183.42s, 	Step: 157076, 	{'train/accuracy': 0.9955697655677795, 'train/loss': 0.013960791751742363, 'train/mean_average_precision': 0.7700218074027575, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2946568657246006, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856674154938795, 'test/num_examples': 43793, 'score': 51157.53374195099, 'total_duration': 77183.41665196419, 'accumulated_submission_time': 51157.53374195099, 'accumulated_eval_time': 26011.510071754456, 'accumulated_logging_time': 9.560796022415161}
I0306 07:37:37.846362 140275669853952 logging_writer.py:48] [157076] accumulated_eval_time=26011.510072, accumulated_logging_time=9.560796, accumulated_submission_time=51157.533742, global_step=157076, preemption_count=0, score=51157.533742, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285667, test/num_examples=43793, total_duration=77183.416652, train/accuracy=0.995570, train/loss=0.013961, train/mean_average_precision=0.770022, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294657, validation/num_examples=43793
I0306 07:37:45.927136 140277079156480 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.13872016966342926, loss=0.01698942296206951
I0306 07:38:18.250348 140275669853952 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.16514073312282562, loss=0.020964009687304497
I0306 07:38:50.203353 140277079156480 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.13213540613651276, loss=0.01708473451435566
I0306 07:39:22.475422 140275669853952 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.129674032330513, loss=0.016554435715079308
I0306 07:39:55.220221 140277079156480 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.13676908612251282, loss=0.018128711730241776
I0306 07:40:27.910339 140275669853952 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.12592457234859467, loss=0.016439666971564293
I0306 07:41:00.326330 140277079156480 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.14729751646518707, loss=0.01877562329173088
I0306 07:41:32.765212 140275669853952 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.1381262093782425, loss=0.01729217916727066
I0306 07:41:37.958220 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:43:20.547380 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:43:23.952658 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:43:27.248568 140444430841664 submission_runner.py:411] Time since start: 77532.86s, 	Step: 157817, 	{'train/accuracy': 0.9955599308013916, 'train/loss': 0.013939803466200829, 'train/mean_average_precision': 0.7757960907398831, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945389451791146, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856970875343083, 'test/num_examples': 43793, 'score': 51397.612380981445, 'total_duration': 77532.86435222626, 'accumulated_submission_time': 51397.612380981445, 'accumulated_eval_time': 26120.800348758698, 'accumulated_logging_time': 9.617570638656616}
I0306 07:43:27.298732 140251254044416 logging_writer.py:48] [157817] accumulated_eval_time=26120.800349, accumulated_logging_time=9.617571, accumulated_submission_time=51397.612381, global_step=157817, preemption_count=0, score=51397.612381, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285697, test/num_examples=43793, total_duration=77532.864352, train/accuracy=0.995560, train/loss=0.013940, train/mean_average_precision=0.775796, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294539, validation/num_examples=43793
I0306 07:43:54.615132 140252715751168 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.1516868621110916, loss=0.017150897532701492
I0306 07:44:27.242716 140251254044416 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.14276859164237976, loss=0.019404031336307526
I0306 07:44:59.861682 140252715751168 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.14762306213378906, loss=0.01758698932826519
I0306 07:45:32.198208 140251254044416 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.12561212480068207, loss=0.016733508557081223
I0306 07:46:04.533532 140252715751168 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.14354053139686584, loss=0.01586143486201763
I0306 07:46:37.142011 140251254044416 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.15055112540721893, loss=0.019618384540081024
I0306 07:47:09.686767 140252715751168 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.1287430077791214, loss=0.01631990633904934
I0306 07:47:27.310915 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:49:04.460391 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:49:07.508727 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:49:10.568015 140444430841664 submission_runner.py:411] Time since start: 77876.18s, 	Step: 158556, 	{'train/accuracy': 0.9955105185508728, 'train/loss': 0.014083905145525932, 'train/mean_average_precision': 0.7653077269785241, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29469835920800364, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28583807091210867, 'test/num_examples': 43793, 'score': 51637.58630156517, 'total_duration': 77876.18381977081, 'accumulated_submission_time': 51637.58630156517, 'accumulated_eval_time': 26224.05740213394, 'accumulated_logging_time': 9.681266069412231}
I0306 07:49:10.616471 140276755326720 logging_writer.py:48] [158556] accumulated_eval_time=26224.057402, accumulated_logging_time=9.681266, accumulated_submission_time=51637.586302, global_step=158556, preemption_count=0, score=51637.586302, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285838, test/num_examples=43793, total_duration=77876.183820, train/accuracy=0.995511, train/loss=0.014084, train/mean_average_precision=0.765308, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294698, validation/num_examples=43793
I0306 07:49:25.187544 140277079156480 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.16402113437652588, loss=0.018609659746289253
I0306 07:49:57.296253 140276755326720 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.1687615066766739, loss=0.01778257079422474
I0306 07:50:29.640612 140277079156480 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.13168832659721375, loss=0.01760842837393284
I0306 07:51:01.550680 140276755326720 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.14308084547519684, loss=0.017293810844421387
I0306 07:51:33.536679 140277079156480 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.1476290374994278, loss=0.01865793950855732
I0306 07:52:05.583779 140276755326720 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.1487502157688141, loss=0.019304342567920685
I0306 07:52:37.799726 140277079156480 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.13984687626361847, loss=0.01774388924241066
I0306 07:53:10.375481 140276755326720 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.16231262683868408, loss=0.019678430631756783
I0306 07:53:10.703261 140444430841664 spec.py:321] Evaluating on the training split.
I0306 07:54:57.479216 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 07:55:00.550346 140444430841664 spec.py:349] Evaluating on the test split.
I0306 07:55:03.751953 140444430841664 submission_runner.py:411] Time since start: 78229.37s, 	Step: 159302, 	{'train/accuracy': 0.995564877986908, 'train/loss': 0.013986615464091301, 'train/mean_average_precision': 0.7803369469318409, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945265167472148, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857684194818551, 'test/num_examples': 43793, 'score': 51877.640649318695, 'total_duration': 78229.36774969101, 'accumulated_submission_time': 51877.640649318695, 'accumulated_eval_time': 26337.106037139893, 'accumulated_logging_time': 9.740336656570435}
I0306 07:55:03.800054 140251254044416 logging_writer.py:48] [159302] accumulated_eval_time=26337.106037, accumulated_logging_time=9.740337, accumulated_submission_time=51877.640649, global_step=159302, preemption_count=0, score=51877.640649, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285768, test/num_examples=43793, total_duration=78229.367750, train/accuracy=0.995565, train/loss=0.013987, train/mean_average_precision=0.780337, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294527, validation/num_examples=43793
I0306 07:55:35.732094 140252715751168 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.13758976757526398, loss=0.01823655515909195
I0306 07:56:08.070115 140251254044416 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.13316790759563446, loss=0.018660996109247208
I0306 07:56:40.675729 140252715751168 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.13489104807376862, loss=0.01656763255596161
I0306 07:57:13.087701 140251254044416 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.13085800409317017, loss=0.016595030203461647
I0306 07:57:45.548480 140252715751168 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.15262778103351593, loss=0.01800244115293026
I0306 07:58:18.218590 140251254044416 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.14464722573757172, loss=0.019440673291683197
I0306 07:58:50.448221 140252715751168 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.13325121998786926, loss=0.01399759016931057
I0306 07:59:04.055462 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:00:43.682098 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:00:46.727457 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:00:49.732944 140444430841664 submission_runner.py:411] Time since start: 78575.35s, 	Step: 160043, 	{'train/accuracy': 0.995552122592926, 'train/loss': 0.013912917114794254, 'train/mean_average_precision': 0.7836719891879151, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2946214386872627, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28565282162487143, 'test/num_examples': 43793, 'score': 52117.862928152084, 'total_duration': 78575.34875035286, 'accumulated_submission_time': 52117.862928152084, 'accumulated_eval_time': 26442.78346991539, 'accumulated_logging_time': 9.7996826171875}
I0306 08:00:49.779350 140275669853952 logging_writer.py:48] [160043] accumulated_eval_time=26442.783470, accumulated_logging_time=9.799683, accumulated_submission_time=52117.862928, global_step=160043, preemption_count=0, score=52117.862928, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285653, test/num_examples=43793, total_duration=78575.348750, train/accuracy=0.995552, train/loss=0.013913, train/mean_average_precision=0.783672, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294621, validation/num_examples=43793
I0306 08:01:08.939503 140276755326720 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.13297727704048157, loss=0.01586322858929634
I0306 08:01:41.396301 140275669853952 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.14641571044921875, loss=0.01681213639676571
I0306 08:02:13.524866 140276755326720 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.16539162397384644, loss=0.019609933719038963
I0306 08:02:46.016430 140275669853952 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.16654036939144135, loss=0.02100456692278385
I0306 08:03:18.316293 140276755326720 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.15234214067459106, loss=0.019117897376418114
I0306 08:03:50.544605 140275669853952 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.16489353775978088, loss=0.01868255063891411
I0306 08:04:22.845987 140276755326720 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.14113102853298187, loss=0.019847480580210686
I0306 08:04:49.998441 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:06:34.796717 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:06:38.189616 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:06:41.580689 140444430841664 submission_runner.py:411] Time since start: 78927.20s, 	Step: 160786, 	{'train/accuracy': 0.9955381751060486, 'train/loss': 0.014034404419362545, 'train/mean_average_precision': 0.7719031206961338, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29461628311979815, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857982278245645, 'test/num_examples': 43793, 'score': 52358.04911708832, 'total_duration': 78927.19647717476, 'accumulated_submission_time': 52358.04911708832, 'accumulated_eval_time': 26554.365653038025, 'accumulated_logging_time': 9.857231855392456}
I0306 08:06:41.634753 140251254044416 logging_writer.py:48] [160786] accumulated_eval_time=26554.365653, accumulated_logging_time=9.857232, accumulated_submission_time=52358.049117, global_step=160786, preemption_count=0, score=52358.049117, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285798, test/num_examples=43793, total_duration=78927.196477, train/accuracy=0.995538, train/loss=0.014034, train/mean_average_precision=0.771903, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294616, validation/num_examples=43793
I0306 08:06:46.881436 140277079156480 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.14186161756515503, loss=0.016849122941493988
I0306 08:07:19.928861 140251254044416 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.1408209204673767, loss=0.016278017312288284
I0306 08:07:52.982544 140277079156480 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.13153663277626038, loss=0.018375873565673828
I0306 08:08:26.102202 140251254044416 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.14569051563739777, loss=0.01803852990269661
I0306 08:08:59.098320 140277079156480 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.13816937804222107, loss=0.017902256920933723
I0306 08:09:31.360136 140251254044416 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.16148504614830017, loss=0.01947503723204136
I0306 08:10:05.071410 140277079156480 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.1453375220298767, loss=0.01794021762907505
I0306 08:10:37.550937 140251254044416 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.1347113996744156, loss=0.017138997092843056
I0306 08:10:41.875387 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:12:20.353195 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:12:23.433677 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:12:26.384584 140444430841664 submission_runner.py:411] Time since start: 79272.00s, 	Step: 161514, 	{'train/accuracy': 0.9955840706825256, 'train/loss': 0.013921987265348434, 'train/mean_average_precision': 0.7775919723837796, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29460460441457387, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857322342018337, 'test/num_examples': 43793, 'score': 52598.251499414444, 'total_duration': 79272.00039196014, 'accumulated_submission_time': 52598.251499414444, 'accumulated_eval_time': 26658.87480187416, 'accumulated_logging_time': 9.923945665359497}
I0306 08:12:26.431261 140252715751168 logging_writer.py:48] [161514] accumulated_eval_time=26658.874802, accumulated_logging_time=9.923946, accumulated_submission_time=52598.251499, global_step=161514, preemption_count=0, score=52598.251499, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285732, test/num_examples=43793, total_duration=79272.000392, train/accuracy=0.995584, train/loss=0.013922, train/mean_average_precision=0.777592, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294605, validation/num_examples=43793
I0306 08:12:54.209905 140276755326720 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.13141374289989471, loss=0.018043670803308487
I0306 08:13:26.515968 140252715751168 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.12415673583745956, loss=0.016974903643131256
I0306 08:13:58.749975 140276755326720 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.1564541906118393, loss=0.018927346915006638
I0306 08:14:30.936645 140252715751168 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.14318518340587616, loss=0.018391015008091927
I0306 08:15:03.203372 140276755326720 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.13565553724765778, loss=0.01757867820560932
I0306 08:15:35.664624 140252715751168 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.15286126732826233, loss=0.020285187289118767
I0306 08:16:07.893659 140276755326720 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.14596909284591675, loss=0.02034224569797516
I0306 08:16:26.413124 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:18:10.748271 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:18:13.787901 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:18:16.782341 140444430841664 submission_runner.py:411] Time since start: 79622.40s, 	Step: 162258, 	{'train/accuracy': 0.99552983045578, 'train/loss': 0.014056675136089325, 'train/mean_average_precision': 0.7652208045052136, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945383467173924, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.28576396478810984, 'test/num_examples': 43793, 'score': 52838.20043492317, 'total_duration': 79622.39813017845, 'accumulated_submission_time': 52838.20043492317, 'accumulated_eval_time': 26769.24395275116, 'accumulated_logging_time': 9.981608629226685}
I0306 08:18:16.827825 140251254044416 logging_writer.py:48] [162258] accumulated_eval_time=26769.243953, accumulated_logging_time=9.981609, accumulated_submission_time=52838.200435, global_step=162258, preemption_count=0, score=52838.200435, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285764, test/num_examples=43793, total_duration=79622.398130, train/accuracy=0.995530, train/loss=0.014057, train/mean_average_precision=0.765221, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294538, validation/num_examples=43793
I0306 08:18:30.615084 140277079156480 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.13680686056613922, loss=0.01820206642150879
I0306 08:19:03.521687 140251254044416 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.13173268735408783, loss=0.0177165437489748
I0306 08:19:35.455220 140277079156480 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.1375369429588318, loss=0.017258692532777786
I0306 08:20:07.443500 140251254044416 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.14520977437496185, loss=0.019102640450000763
I0306 08:20:38.980546 140277079156480 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.1475992649793625, loss=0.01863761991262436
I0306 08:21:10.834529 140251254044416 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.13945205509662628, loss=0.01727576181292534
I0306 08:21:42.537204 140277079156480 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.1550854742527008, loss=0.019006328657269478
I0306 08:22:14.569999 140251254044416 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.13144342601299286, loss=0.017520712688565254
I0306 08:22:16.784916 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:23:56.771613 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:23:59.834347 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:24:03.043349 140444430841664 submission_runner.py:411] Time since start: 79968.66s, 	Step: 163008, 	{'train/accuracy': 0.9955242872238159, 'train/loss': 0.014006540179252625, 'train/mean_average_precision': 0.7796286507900729, 'validation/accuracy': 0.9869741201400757, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2945911879708596, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.285866172733904, 'test/num_examples': 43793, 'score': 53078.12421751022, 'total_duration': 79968.65912795067, 'accumulated_submission_time': 53078.12421751022, 'accumulated_eval_time': 26875.5023086071, 'accumulated_logging_time': 10.038224220275879}
I0306 08:24:03.091634 140252715751168 logging_writer.py:48] [163008] accumulated_eval_time=26875.502309, accumulated_logging_time=10.038224, accumulated_submission_time=53078.124218, global_step=163008, preemption_count=0, score=53078.124218, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285866, test/num_examples=43793, total_duration=79968.659128, train/accuracy=0.995524, train/loss=0.014007, train/mean_average_precision=0.779629, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294591, validation/num_examples=43793
I0306 08:24:33.375516 140276755326720 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.1322910040616989, loss=0.018842918798327446
I0306 08:25:05.626663 140252715751168 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.13411419093608856, loss=0.01820903643965721
I0306 08:25:37.853878 140276755326720 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.15195401012897491, loss=0.018211983144283295
I0306 08:26:09.838638 140252715751168 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.14041239023208618, loss=0.017741598188877106
I0306 08:26:42.038090 140276755326720 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.15764182806015015, loss=0.01845376379787922
I0306 08:27:14.215625 140252715751168 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.12734894454479218, loss=0.017425231635570526
I0306 08:27:46.151005 140276755326720 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.1441953033208847, loss=0.01737101748585701
I0306 08:28:03.331571 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:29:41.195673 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:29:44.226372 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:29:49.395529 140444430841664 submission_runner.py:411] Time since start: 80315.01s, 	Step: 163755, 	{'train/accuracy': 0.9955673217773438, 'train/loss': 0.013943160884082317, 'train/mean_average_precision': 0.7821653090591817, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.29449875595906827, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.2857193972505057, 'test/num_examples': 43793, 'score': 53318.33121609688, 'total_duration': 80315.01131868362, 'accumulated_submission_time': 53318.33121609688, 'accumulated_eval_time': 26981.566202640533, 'accumulated_logging_time': 10.09718656539917}
I0306 08:29:49.444507 140251254044416 logging_writer.py:48] [163755] accumulated_eval_time=26981.566203, accumulated_logging_time=10.097187, accumulated_submission_time=53318.331216, global_step=163755, preemption_count=0, score=53318.331216, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285719, test/num_examples=43793, total_duration=80315.011319, train/accuracy=0.995567, train/loss=0.013943, train/mean_average_precision=0.782165, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294499, validation/num_examples=43793
I0306 08:30:04.676626 140277079156480 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.12270522862672806, loss=0.016433322802186012
I0306 08:30:37.068365 140251254044416 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.15763533115386963, loss=0.01872793585062027
I0306 08:31:09.551461 140277079156480 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.14443500339984894, loss=0.01816573180258274
I0306 08:31:42.433040 140251254044416 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.14666229486465454, loss=0.016439182683825493
I0306 08:32:15.192025 140277079156480 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.14135025441646576, loss=0.01736190728843212
I0306 08:32:47.619800 140251254044416 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.13998135924339294, loss=0.016992395743727684
I0306 08:33:19.974719 140277079156480 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.14158757030963898, loss=0.018685899674892426
I0306 08:33:49.537639 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:35:25.200903 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:35:28.254975 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:35:31.261354 140444430841664 submission_runner.py:411] Time since start: 80656.88s, 	Step: 164493, 	{'train/accuracy': 0.9955205917358398, 'train/loss': 0.014089780859649181, 'train/mean_average_precision': 0.7764114194363383, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945493493391814, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28564400258294054, 'test/num_examples': 43793, 'score': 53558.39138150215, 'total_duration': 80656.87715959549, 'accumulated_submission_time': 53558.39138150215, 'accumulated_eval_time': 27083.28986978531, 'accumulated_logging_time': 10.157109260559082}
I0306 08:35:31.308386 140275669853952 logging_writer.py:48] [164493] accumulated_eval_time=27083.289870, accumulated_logging_time=10.157109, accumulated_submission_time=53558.391382, global_step=164493, preemption_count=0, score=53558.391382, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285644, test/num_examples=43793, total_duration=80656.877160, train/accuracy=0.995521, train/loss=0.014090, train/mean_average_precision=0.776411, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294549, validation/num_examples=43793
I0306 08:35:34.034203 140276755326720 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.1459157019853592, loss=0.01577518880367279
I0306 08:36:06.091154 140275669853952 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.14525695145130157, loss=0.01761613041162491
I0306 08:36:38.779928 140276755326720 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.16086558997631073, loss=0.018728548660874367
I0306 08:37:10.718191 140275669853952 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.14478643238544464, loss=0.018646299839019775
I0306 08:37:42.830249 140276755326720 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.1572140008211136, loss=0.020834866911172867
I0306 08:38:14.841173 140275669853952 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.13949427008628845, loss=0.01737084425985813
I0306 08:38:46.713619 140276755326720 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.16769753396511078, loss=0.01853308267891407
I0306 08:39:19.055851 140275669853952 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.12684625387191772, loss=0.0166617501527071
I0306 08:39:31.319544 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:41:07.733662 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:41:11.230366 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:41:14.644902 140444430841664 submission_runner.py:411] Time since start: 81000.26s, 	Step: 165239, 	{'train/accuracy': 0.9955911040306091, 'train/loss': 0.013928915373980999, 'train/mean_average_precision': 0.7734966359252158, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29448491213507644, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28578934118285765, 'test/num_examples': 43793, 'score': 53798.36969470978, 'total_duration': 81000.26069259644, 'accumulated_submission_time': 53798.36969470978, 'accumulated_eval_time': 27186.615166664124, 'accumulated_logging_time': 10.215405464172363}
I0306 08:41:14.697100 140251254044416 logging_writer.py:48] [165239] accumulated_eval_time=27186.615167, accumulated_logging_time=10.215405, accumulated_submission_time=53798.369695, global_step=165239, preemption_count=0, score=53798.369695, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285789, test/num_examples=43793, total_duration=81000.260693, train/accuracy=0.995591, train/loss=0.013929, train/mean_average_precision=0.773497, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294485, validation/num_examples=43793
I0306 08:41:35.031092 140252715751168 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.13812576234340668, loss=0.01778094656765461
I0306 08:42:07.864731 140251254044416 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.1604839712381363, loss=0.019247721880674362
I0306 08:42:40.233811 140252715751168 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.12552031874656677, loss=0.01668493263423443
I0306 08:43:12.387448 140251254044416 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.13022460043430328, loss=0.016953932121396065
I0306 08:43:43.781943 140252715751168 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.13730621337890625, loss=0.018345052376389503
I0306 08:44:15.717458 140251254044416 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.14435206353664398, loss=0.018612917512655258
I0306 08:44:47.667860 140252715751168 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.1391947865486145, loss=0.01816524565219879
I0306 08:45:14.708055 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:46:58.064672 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:47:01.312304 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:47:04.405970 140444430841664 submission_runner.py:411] Time since start: 81350.02s, 	Step: 165985, 	{'train/accuracy': 0.9955465197563171, 'train/loss': 0.013940013013780117, 'train/mean_average_precision': 0.7729779314410089, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2944748477438155, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857110507380125, 'test/num_examples': 43793, 'score': 54038.34677696228, 'total_duration': 81350.02177357674, 'accumulated_submission_time': 54038.34677696228, 'accumulated_eval_time': 27296.31303691864, 'accumulated_logging_time': 10.27896499633789}
I0306 08:47:04.452528 140275669853952 logging_writer.py:48] [165985] accumulated_eval_time=27296.313037, accumulated_logging_time=10.278965, accumulated_submission_time=54038.346777, global_step=165985, preemption_count=0, score=54038.346777, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285711, test/num_examples=43793, total_duration=81350.021774, train/accuracy=0.995547, train/loss=0.013940, train/mean_average_precision=0.772978, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294475, validation/num_examples=43793
I0306 08:47:09.628980 140277079156480 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.15475070476531982, loss=0.018888549879193306
I0306 08:47:41.615753 140275669853952 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.16371455788612366, loss=0.019370798021554947
I0306 08:48:13.853788 140277079156480 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.13825346529483795, loss=0.016394048929214478
I0306 08:48:45.816540 140275669853952 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.14639075100421906, loss=0.019092146307229996
I0306 08:49:18.046501 140277079156480 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.13542115688323975, loss=0.01671062409877777
I0306 08:49:50.276966 140275669853952 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.160623237490654, loss=0.019122594967484474
I0306 08:50:22.995398 140277079156480 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.145240917801857, loss=0.017936881631612778
I0306 08:50:55.082602 140275669853952 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.154876247048378, loss=0.01794150285422802
I0306 08:51:04.665546 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:52:48.268706 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:52:51.430181 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:52:54.534881 140444430841664 submission_runner.py:411] Time since start: 81700.15s, 	Step: 166730, 	{'train/accuracy': 0.9955199956893921, 'train/loss': 0.014083348214626312, 'train/mean_average_precision': 0.7805190367217666, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29456748494114104, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28572118170643146, 'test/num_examples': 43793, 'score': 54278.52591109276, 'total_duration': 81700.15067434311, 'accumulated_submission_time': 54278.52591109276, 'accumulated_eval_time': 27406.182319402695, 'accumulated_logging_time': 10.336283922195435}
I0306 08:52:54.583081 140251254044416 logging_writer.py:48] [166730] accumulated_eval_time=27406.182319, accumulated_logging_time=10.336284, accumulated_submission_time=54278.525911, global_step=166730, preemption_count=0, score=54278.525911, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285721, test/num_examples=43793, total_duration=81700.150674, train/accuracy=0.995520, train/loss=0.014083, train/mean_average_precision=0.780519, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294567, validation/num_examples=43793
I0306 08:53:17.752005 140252715751168 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.13579656183719635, loss=0.019047236070036888
I0306 08:53:49.754381 140251254044416 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.1567731499671936, loss=0.018493372946977615
I0306 08:54:22.087390 140252715751168 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.1496375948190689, loss=0.018754947930574417
I0306 08:54:54.128968 140251254044416 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.15526072680950165, loss=0.018660850822925568
I0306 08:55:26.557369 140252715751168 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.1285046488046646, loss=0.01839825138449669
I0306 08:55:58.419134 140251254044416 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.14227589964866638, loss=0.017295056954026222
I0306 08:56:30.538453 140252715751168 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.12791019678115845, loss=0.01799851842224598
I0306 08:56:54.677440 140444430841664 spec.py:321] Evaluating on the training split.
I0306 08:58:37.125865 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 08:58:40.493891 140444430841664 spec.py:349] Evaluating on the test split.
I0306 08:58:43.772672 140444430841664 submission_runner.py:411] Time since start: 82049.39s, 	Step: 167476, 	{'train/accuracy': 0.9955546855926514, 'train/loss': 0.013964761979877949, 'train/mean_average_precision': 0.7759126234046201, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945054412881348, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2857042919315566, 'test/num_examples': 43793, 'score': 54518.587480545044, 'total_duration': 82049.38845849037, 'accumulated_submission_time': 54518.587480545044, 'accumulated_eval_time': 27515.27748608589, 'accumulated_logging_time': 10.39532470703125}
I0306 08:58:43.828146 140275669853952 logging_writer.py:48] [167476] accumulated_eval_time=27515.277486, accumulated_logging_time=10.395325, accumulated_submission_time=54518.587481, global_step=167476, preemption_count=0, score=54518.587481, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285704, test/num_examples=43793, total_duration=82049.388458, train/accuracy=0.995555, train/loss=0.013965, train/mean_average_precision=0.775913, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294505, validation/num_examples=43793
I0306 08:58:52.014027 140277079156480 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.14773224294185638, loss=0.016635680571198463
I0306 08:59:24.825024 140275669853952 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.15296317636966705, loss=0.019239764660596848
I0306 08:59:57.216472 140277079156480 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.1333596408367157, loss=0.017240474000573158
I0306 09:00:29.692292 140275669853952 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.14777125418186188, loss=0.01571916788816452
I0306 09:01:01.730334 140277079156480 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.1437578946352005, loss=0.01824929006397724
I0306 09:01:33.920750 140275669853952 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.15333884954452515, loss=0.018066756427288055
I0306 09:02:06.200088 140277079156480 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.14762619137763977, loss=0.018108634278178215
I0306 09:02:38.286457 140275669853952 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.14408865571022034, loss=0.016420813277363777
I0306 09:02:43.989602 140444430841664 spec.py:321] Evaluating on the training split.
I0306 09:04:23.794819 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 09:04:27.046819 140444430841664 spec.py:349] Evaluating on the test split.
I0306 09:04:30.138548 140444430841664 submission_runner.py:411] Time since start: 82395.75s, 	Step: 168219, 	{'train/accuracy': 0.9955841302871704, 'train/loss': 0.01392018049955368, 'train/mean_average_precision': 0.7750355602140353, 'validation/accuracy': 0.9869743585586548, 'validation/loss': 0.050212517380714417, 'validation/mean_average_precision': 0.2946438301036884, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.2856700453934852, 'test/num_examples': 43793, 'score': 54758.71322131157, 'total_duration': 82395.75434350967, 'accumulated_submission_time': 54758.71322131157, 'accumulated_eval_time': 27621.426381587982, 'accumulated_logging_time': 10.46361517906189}
I0306 09:04:30.187577 140251254044416 logging_writer.py:48] [168219] accumulated_eval_time=27621.426382, accumulated_logging_time=10.463615, accumulated_submission_time=54758.713221, global_step=168219, preemption_count=0, score=54758.713221, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285670, test/num_examples=43793, total_duration=82395.754344, train/accuracy=0.995584, train/loss=0.013920, train/mean_average_precision=0.775036, validation/accuracy=0.986974, validation/loss=0.050213, validation/mean_average_precision=0.294644, validation/num_examples=43793
I0306 09:04:56.727301 140252715751168 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.13351348042488098, loss=0.01724030077457428
I0306 09:05:29.346822 140251254044416 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.14083874225616455, loss=0.01848878338932991
I0306 09:06:01.513847 140252715751168 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.1355312466621399, loss=0.016563955694437027
I0306 09:06:33.614929 140251254044416 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.1411401480436325, loss=0.016466787084937096
I0306 09:07:05.822205 140252715751168 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.13124980032444, loss=0.01698371209204197
I0306 09:07:38.152531 140251254044416 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.1354907602071762, loss=0.017532959580421448
I0306 09:08:10.477515 140252715751168 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.15696118772029877, loss=0.019556667655706406
I0306 09:08:30.321477 140444430841664 spec.py:321] Evaluating on the training split.
I0306 09:10:10.495102 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 09:10:13.553107 140444430841664 spec.py:349] Evaluating on the test split.
I0306 09:10:16.602985 140444430841664 submission_runner.py:411] Time since start: 82742.22s, 	Step: 168963, 	{'train/accuracy': 0.9955199360847473, 'train/loss': 0.014063313603401184, 'train/mean_average_precision': 0.7707108704904337, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.29449983863407897, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355957895517349, 'test/mean_average_precision': 0.28570182528690735, 'test/num_examples': 43793, 'score': 54998.81431245804, 'total_duration': 82742.2187845707, 'accumulated_submission_time': 54998.81431245804, 'accumulated_eval_time': 27727.707834243774, 'accumulated_logging_time': 10.523662567138672}
I0306 09:10:16.650699 140275669853952 logging_writer.py:48] [168963] accumulated_eval_time=27727.707834, accumulated_logging_time=10.523663, accumulated_submission_time=54998.814312, global_step=168963, preemption_count=0, score=54998.814312, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285702, test/num_examples=43793, total_duration=82742.218785, train/accuracy=0.995520, train/loss=0.014063, train/mean_average_precision=0.770711, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294500, validation/num_examples=43793
I0306 09:10:28.917321 140276755326720 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.13758797943592072, loss=0.018030118197202682
I0306 09:11:01.048629 140275669853952 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.14595115184783936, loss=0.017356090247631073
I0306 09:11:33.187344 140276755326720 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.1371575891971588, loss=0.01721305586397648
I0306 09:12:04.946753 140275669853952 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.13539594411849976, loss=0.01746390573680401
I0306 09:12:37.207816 140276755326720 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.13291031122207642, loss=0.016381321474909782
I0306 09:13:09.779092 140275669853952 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.16708654165267944, loss=0.019572561606764793
I0306 09:13:41.941288 140276755326720 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.15226054191589355, loss=0.01751651056110859
I0306 09:14:14.425818 140275669853952 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.14242465794086456, loss=0.01801864244043827
I0306 09:14:16.627235 140444430841664 spec.py:321] Evaluating on the training split.
I0306 09:15:53.159640 140444430841664 spec.py:333] Evaluating on the validation split.
I0306 09:15:56.178776 140444430841664 spec.py:349] Evaluating on the test split.
I0306 09:15:59.203619 140444430841664 submission_runner.py:411] Time since start: 83084.82s, 	Step: 169708, 	{'train/accuracy': 0.9955832958221436, 'train/loss': 0.013892631977796555, 'train/mean_average_precision': 0.7774776130313928, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.050212521106004715, 'validation/mean_average_precision': 0.2945532663143946, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.05355958268046379, 'test/mean_average_precision': 0.285909555773725, 'test/num_examples': 43793, 'score': 55238.7580242157, 'total_duration': 83084.81942462921, 'accumulated_submission_time': 55238.7580242157, 'accumulated_eval_time': 27830.284168481827, 'accumulated_logging_time': 10.582506656646729}
I0306 09:15:59.250810 140252715751168 logging_writer.py:48] [169708] accumulated_eval_time=27830.284168, accumulated_logging_time=10.582507, accumulated_submission_time=55238.758024, global_step=169708, preemption_count=0, score=55238.758024, test/accuracy=0.986194, test/loss=0.053560, test/mean_average_precision=0.285910, test/num_examples=43793, total_duration=83084.819425, train/accuracy=0.995583, train/loss=0.013893, train/mean_average_precision=0.777478, validation/accuracy=0.986975, validation/loss=0.050213, validation/mean_average_precision=0.294553, validation/num_examples=43793
I0306 09:16:29.436920 140277079156480 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.13729435205459595, loss=0.018522348254919052
I0306 09:17:01.988193 140252715751168 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.12855330109596252, loss=0.01661445014178753
I0306 09:17:33.744953 140277079156480 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.1414974331855774, loss=0.01896653324365616
I0306 09:18:05.881993 140252715751168 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.13544881343841553, loss=0.016157953068614006
I0306 09:18:37.925777 140277079156480 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.14912308752536774, loss=0.018084470182657242
I0306 09:19:10.008651 140252715751168 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.14637331664562225, loss=0.019403817132115364
I0306 09:19:11.637729 140277079156480 logging_writer.py:48] [170306] global_step=170306, preemption_count=0, score=55431.082716
I0306 09:19:11.691078 140444430841664 checkpoints.py:490] Saving checkpoint at step: 170306
I0306 09:19:11.848392 140444430841664 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1/checkpoint_170306
I0306 09:19:11.853297 140444430841664 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_3/ogbg_jax/trial_1/checkpoint_170306.
I0306 09:19:12.015882 140444430841664 submission_runner.py:676] Final ogbg score: 55431.08271622658
Dataset ogbg_molpcba downloaded and prepared to /root/data/ogbg_molpcba/0.1.3. Subsequent calls will reuse this data.
