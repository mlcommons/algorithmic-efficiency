python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_0 --overwrite=true --save_checkpoints=false --rng_seed=1698470591 --max_global_steps=240000 --tuning_ruleset=self 2>&1 | tee -a /logs/ogbg_jax_03-05-2024-10-01-36.log
I0305 10:01:57.432574 140274064205632 logger_utils.py:61] Removing existing experiment directory /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax because --overwrite was set.
I0305 10:01:57.434300 140274064205632 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax.
I0305 10:01:58.432923 140274064205632 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0305 10:01:58.433713 140274064205632 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0305 10:01:58.433877 140274064205632 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0305 10:01:59.288807 140274064205632 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1.
I0305 10:01:59.489459 140274064205632 submission_runner.py:206] Initializing dataset.
I0305 10:01:59.786752 140274064205632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:01:59.792230 140274064205632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 10:02:00.039163 140274064205632 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 10:02:00.101402 140274064205632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:02:00.174252 140274064205632 submission_runner.py:213] Initializing model.
I0305 10:02:05.454414 140274064205632 submission_runner.py:255] Initializing optimizer.
I0305 10:02:06.198463 140274064205632 submission_runner.py:262] Initializing metrics bundle.
I0305 10:02:06.198711 140274064205632 submission_runner.py:280] Initializing checkpoint and logger.
I0305 10:02:06.199514 140274064205632 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1 with prefix checkpoint_
I0305 10:02:06.199671 140274064205632 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1/meta_data_0.json.
I0305 10:02:06.199899 140274064205632 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0305 10:02:06.199964 140274064205632 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0305 10:02:06.561943 140274064205632 logger_utils.py:220] Unable to record git information. Continuing without it.
I0305 10:02:06.893145 140274064205632 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1/flags_0.json.
I0305 10:02:06.903241 140274064205632 submission_runner.py:314] Starting training loop.
I0305 10:02:27.364370 140108409861888 logging_writer.py:48] [0] global_step=0, grad_norm=1.9044524431228638, loss=0.7641303539276123
I0305 10:02:27.381324 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:02:27.388544 140274064205632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:02:27.394308 140274064205632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:02:27.471654 140274064205632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:04:25.667885 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:04:25.671755 140274064205632 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:04:25.675943 140274064205632 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:04:25.744948 140274064205632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:06:00.281220 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:06:01.108252 140274064205632 dataset_info.py:736] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ogbg_molpcba/0.1.3
I0305 10:06:01.819916 140274064205632 dataset_info.py:578] Load dataset info from /tmp/tmp5s7doe7ntfds
I0305 10:06:01.823367 140274064205632 dataset_info.py:669] Fields info.[description, release_notes, splits, module_name] from disk and from code do not match. Keeping the one from code.
I0305 10:06:01.823819 140274064205632 dataset_builder.py:593] Generating dataset ogbg_molpcba (/root/data/ogbg_molpcba/0.1.3)
Downloading and preparing dataset 37.70 MiB (download: 37.70 MiB, generated: 822.53 MiB, total: 860.23 MiB) to /root/data/ogbg_molpcba/0.1.3...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[AI0305 10:06:02.144976 140274064205632 download_manager.py:400] Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip into /root/data/downloads/snap.stan.edu_ogb_grap_csv_mol_down_pcbapc4I82Cv1THcU-IggPHK8IHZ8qM-BJ3VDk-q_rtqrf4.zip.tmp.51f6a151873d4cc7b9056ffba6332c23...
Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...:   0%|          | 0/37 [00:00<?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:40,  2.78s/ MiB][ADl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:40,  2.78s/ MiB][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:52,  1.50s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:52,  1.50s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:34,  1.02s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:34,  1.02s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.35 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.35 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.72 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.72 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:13,  2.29 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:13,  2.29 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:11,  2.56 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:11,  2.56 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:09,  3.18 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:09,  3.18 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:05<00:08,  3.18 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:05<00:05,  4.94 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:05<00:05,  4.94 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:05<00:04,  5.31 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:05<00:04,  5.31 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:04,  5.65 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:04,  5.65 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:04,  5.65 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:03,  7.47 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:03,  7.47 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:05<00:02,  7.47 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  8.94 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  8.94 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 17/37 [00:05<00:02,  8.94 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:05<00:01, 10.09 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:05<00:01, 10.09 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/37 [00:05<00:01, 10.09 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 11.02 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 11.02 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21/37 [00:06<00:01, 11.02 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22/37 [00:06<00:01, 11.74 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22/37 [00:06<00:01, 11.74 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:06<00:01, 11.74 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/37 [00:06<00:01, 11.74 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.98 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.98 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26/37 [00:06<00:00, 13.98 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27/37 [00:06<00:00, 13.94 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27/37 [00:06<00:00, 13.94 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:06<00:00, 13.94 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/37 [00:06<00:00, 13.94 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 15.62 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 15.62 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/37 [00:06<00:00, 15.62 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [00:06<00:00, 15.62 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 16.84 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 16.84 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [00:06<00:00, 16.84 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/37 [00:06<00:00, 16.84 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:07<00:00, 17.73 MiB/s][ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/2 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/3 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/4 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/5 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/6 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/7 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/8 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/9 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/10 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/11 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   0%|          | 0/12 [00:07<?, ? file/s][A[A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:19,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:19,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  17%|â–ˆâ–‹        | 2/12 [00:07<01:11,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<01:04,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:57,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:50,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:07<00:43,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:35,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:07<00:28,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:07<00:21,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:14,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:07,  7.19s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.06s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 17.73 MiB/s][A

Extraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  7.19s/ file][A[AExtraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  1.67 file/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00,  5.14 MiB/s]
Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.19s/ url]
Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
Generating train examples...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Generating train examples...:   0%|          | 76/350343 [00:00<07:41, 759.79 examples/s][A
Generating train examples...:   0%|          | 163/350343 [00:00<08:17, 704.08 examples/s][A
Generating train examples...:   0%|          | 388/350343 [00:00<04:20, 1344.90 examples/s][A
Generating train examples...:   0%|          | 613/350343 [00:00<03:28, 1680.64 examples/s][A
Generating train examples...:   0%|          | 836/350343 [00:00<03:12, 1818.82 examples/s][A
Generating train examples...:   0%|          | 1063/350343 [00:00<02:57, 1963.74 examples/s][A
Generating train examples...:   0%|          | 1286/350343 [00:00<02:50, 2047.71 examples/s][A
Generating train examples...:   0%|          | 1516/350343 [00:00<02:44, 2124.80 examples/s][A
Generating train examples...:   0%|          | 1748/350343 [00:00<02:39, 2183.20 examples/s][A
Generating train examples...:   1%|          | 1977/350343 [00:01<02:37, 2213.84 examples/s][A
Generating train examples...:   1%|          | 2205/350343 [00:01<02:35, 2232.84 examples/s][A
Generating train examples...:   1%|          | 2430/350343 [00:01<02:35, 2237.89 examples/s][A
Generating train examples...:   1%|          | 2661/350343 [00:01<02:34, 2257.47 examples/s][A
Generating train examples...:   1%|          | 2890/350343 [00:01<02:33, 2265.47 examples/s][A
Generating train examples...:   1%|          | 3123/350343 [00:01<02:32, 2284.02 examples/s][A
Generating train examples...:   1%|          | 3352/350343 [00:01<02:32, 2282.23 examples/s][A
Generating train examples...:   1%|          | 3585/350343 [00:01<02:31, 2294.80 examples/s][A
Generating train examples...:   1%|          | 3816/350343 [00:01<02:30, 2295.15 examples/s][A
Generating train examples...:   1%|          | 4046/350343 [00:01<02:31, 2281.87 examples/s][A
Generating train examples...:   1%|          | 4275/350343 [00:02<02:32, 2270.24 examples/s][A
Generating train examples...:   1%|â–         | 4503/350343 [00:02<02:32, 2267.12 examples/s][A
Generating train examples...:   1%|â–         | 4730/350343 [00:02<02:33, 2258.79 examples/s][A
Generating train examples...:   1%|â–         | 4966/350343 [00:02<02:30, 2288.54 examples/s][A
Generating train examples...:   1%|â–         | 5195/350343 [00:02<02:30, 2288.68 examples/s][A
Generating train examples...:   2%|â–         | 5430/350343 [00:02<02:29, 2306.40 examples/s][A
Generating train examples...:   2%|â–         | 5661/350343 [00:02<02:29, 2304.57 examples/s][A
Generating train examples...:   2%|â–         | 5892/350343 [00:02<02:29, 2298.80 examples/s][A
Generating train examples...:   2%|â–         | 6124/350343 [00:02<02:29, 2304.61 examples/s][A
Generating train examples...:   2%|â–         | 6355/350343 [00:02<02:29, 2293.72 examples/s][A
Generating train examples...:   2%|â–         | 6588/350343 [00:03<02:29, 2301.84 examples/s][A
Generating train examples...:   2%|â–         | 6819/350343 [00:03<02:30, 2287.08 examples/s][A
Generating train examples...:   2%|â–         | 7048/350343 [00:03<02:31, 2270.50 examples/s][A
Generating train examples...:   2%|â–         | 7276/350343 [00:03<02:31, 2270.83 examples/s][A
Generating train examples...:   2%|â–         | 7507/350343 [00:03<02:30, 2282.09 examples/s][A
Generating train examples...:   2%|â–         | 7743/350343 [00:03<02:28, 2303.37 examples/s][A
Generating train examples...:   2%|â–         | 7974/350343 [00:03<02:28, 2301.31 examples/s][A
Generating train examples...:   2%|â–         | 8205/350343 [00:03<02:29, 2285.95 examples/s][A
Generating train examples...:   2%|â–         | 8434/350343 [00:03<02:29, 2286.13 examples/s][A
Generating train examples...:   2%|â–         | 8663/350343 [00:03<02:29, 2282.99 examples/s][A
Generating train examples...:   3%|â–Ž         | 8892/350343 [00:04<02:35, 2189.07 examples/s][A
Generating train examples...:   3%|â–Ž         | 9117/350343 [00:04<02:34, 2205.44 examples/s][A
Generating train examples...:   3%|â–Ž         | 9339/350343 [00:04<02:34, 2206.35 examples/s][A
Generating train examples...:   3%|â–Ž         | 9566/350343 [00:04<02:33, 2224.25 examples/s][A
Generating train examples...:   3%|â–Ž         | 9789/350343 [00:04<02:33, 2215.48 examples/s][A
Generating train examples...:   3%|â–Ž         | 10015/350343 [00:04<02:32, 2225.19 examples/s][A
Generating train examples...:   3%|â–Ž         | 10243/350343 [00:04<02:31, 2239.53 examples/s][A
Generating train examples...:   3%|â–Ž         | 10471/350343 [00:04<02:31, 2250.63 examples/s][A
Generating train examples...:   3%|â–Ž         | 10704/350343 [00:04<02:29, 2272.94 examples/s][A
Generating train examples...:   3%|â–Ž         | 10938/350343 [00:04<02:28, 2292.07 examples/s][A
Generating train examples...:   3%|â–Ž         | 11170/350343 [00:05<02:27, 2299.66 examples/s][A
Generating train examples...:   3%|â–Ž         | 11401/350343 [00:05<02:28, 2284.27 examples/s][A
Generating train examples...:   3%|â–Ž         | 11630/350343 [00:05<02:28, 2277.93 examples/s][A
Generating train examples...:   3%|â–Ž         | 11858/350343 [00:05<02:28, 2274.00 examples/s][A
Generating train examples...:   3%|â–Ž         | 12086/350343 [00:05<02:28, 2273.89 examples/s][A
Generating train examples...:   4%|â–Ž         | 12316/350343 [00:05<02:28, 2278.98 examples/s][A
Generating train examples...:   4%|â–Ž         | 12548/350343 [00:05<02:27, 2289.11 examples/s][A
Generating train examples...:   4%|â–Ž         | 12778/350343 [00:05<02:27, 2290.13 examples/s][A
Generating train examples...:   4%|â–Ž         | 13008/350343 [00:05<02:28, 2275.20 examples/s][A
Generating train examples...:   4%|â–         | 13241/350343 [00:05<02:27, 2289.53 examples/s][A
Generating train examples...:   4%|â–         | 13470/350343 [00:06<02:27, 2278.89 examples/s][A
Generating train examples...:   4%|â–         | 13698/350343 [00:06<02:28, 2267.02 examples/s][A
Generating train examples...:   4%|â–         | 13925/350343 [00:06<02:28, 2265.42 examples/s][A
Generating train examples...:   4%|â–         | 14152/350343 [00:06<02:28, 2259.93 examples/s][A
Generating train examples...:   4%|â–         | 14380/350343 [00:06<02:28, 2265.03 examples/s][A
Generating train examples...:   4%|â–         | 14609/350343 [00:06<02:27, 2271.75 examples/s][A
Generating train examples...:   4%|â–         | 14838/350343 [00:06<02:27, 2276.53 examples/s][A
Generating train examples...:   4%|â–         | 15066/350343 [00:06<02:27, 2277.34 examples/s][A
Generating train examples...:   4%|â–         | 15297/350343 [00:06<02:26, 2286.31 examples/s][A
Generating train examples...:   4%|â–         | 15526/350343 [00:06<02:26, 2285.17 examples/s][A
Generating train examples...:   4%|â–         | 15755/350343 [00:07<02:26, 2281.37 examples/s][A
Generating train examples...:   5%|â–         | 15984/350343 [00:07<02:26, 2278.50 examples/s][A
Generating train examples...:   5%|â–         | 16212/350343 [00:07<02:29, 2239.35 examples/s][A
Generating train examples...:   5%|â–         | 16441/350343 [00:07<02:28, 2252.16 examples/s][A
Generating train examples...:   5%|â–         | 16667/350343 [00:07<02:28, 2250.20 examples/s][A
Generating train examples...:   5%|â–         | 16893/350343 [00:07<02:28, 2251.91 examples/s][A
Generating train examples...:   5%|â–         | 17120/350343 [00:07<02:27, 2256.68 examples/s][A
Generating train examples...:   5%|â–         | 17346/350343 [00:07<02:27, 2255.76 examples/s][A
Generating train examples...:   5%|â–Œ         | 17575/350343 [00:07<02:26, 2265.56 examples/s][A
Generating train examples...:   5%|â–Œ         | 17807/350343 [00:08<02:25, 2280.01 examples/s][A
Generating train examples...:   5%|â–Œ         | 18036/350343 [00:08<02:26, 2274.59 examples/s][A
Generating train examples...:   5%|â–Œ         | 18264/350343 [00:08<02:26, 2274.25 examples/s][A
Generating train examples...:   5%|â–Œ         | 18492/350343 [00:08<02:26, 2267.33 examples/s][A
Generating train examples...:   5%|â–Œ         | 18724/350343 [00:08<02:25, 2280.86 examples/s][A
Generating train examples...:   5%|â–Œ         | 18953/350343 [00:08<02:25, 2281.68 examples/s][A
Generating train examples...:   5%|â–Œ         | 19182/350343 [00:08<02:30, 2195.10 examples/s][A
Generating train examples...:   6%|â–Œ         | 19412/350343 [00:08<02:28, 2223.62 examples/s][A
Generating train examples...:   6%|â–Œ         | 19637/350343 [00:08<02:28, 2230.66 examples/s][A
Generating train examples...:   6%|â–Œ         | 19866/350343 [00:08<02:27, 2247.67 examples/s][A
Generating train examples...:   6%|â–Œ         | 20097/350343 [00:09<02:25, 2265.51 examples/s][A
Generating train examples...:   6%|â–Œ         | 20331/350343 [00:09<02:24, 2286.88 examples/s][A
Generating train examples...:   6%|â–Œ         | 20560/350343 [00:09<02:24, 2285.35 examples/s][A
Generating train examples...:   6%|â–Œ         | 20789/350343 [00:09<02:25, 2271.61 examples/s][A
Generating train examples...:   6%|â–Œ         | 21019/350343 [00:09<02:24, 2277.86 examples/s][A
Generating train examples...:   6%|â–Œ         | 21248/350343 [00:09<02:24, 2280.75 examples/s][A
Generating train examples...:   6%|â–Œ         | 21477/350343 [00:09<02:24, 2280.03 examples/s][A
Generating train examples...:   6%|â–Œ         | 21706/350343 [00:09<02:25, 2265.90 examples/s][A
Generating train examples...:   6%|â–‹         | 21933/350343 [00:09<02:24, 2265.25 examples/s][A
Generating train examples...:   6%|â–‹         | 22160/350343 [00:09<02:25, 2258.22 examples/s][A
Generating train examples...:   6%|â–‹         | 22386/350343 [00:10<02:25, 2258.04 examples/s][A
Generating train examples...:   6%|â–‹         | 22615/350343 [00:10<02:24, 2266.00 examples/s][A
Generating train examples...:   7%|â–‹         | 22842/350343 [00:10<02:25, 2247.58 examples/s][A
Generating train examples...:   7%|â–‹         | 23067/350343 [00:10<02:26, 2231.08 examples/s][A
Generating train examples...:   7%|â–‹         | 23293/350343 [00:10<02:26, 2239.12 examples/s][A
Generating train examples...:   7%|â–‹         | 23521/350343 [00:10<02:25, 2251.01 examples/s][A
Generating train examples...:   7%|â–‹         | 23750/350343 [00:10<02:24, 2261.04 examples/s][A
Generating train examples...:   7%|â–‹         | 23977/350343 [00:10<02:24, 2259.07 examples/s][A
Generating train examples...:   7%|â–‹         | 24206/350343 [00:10<02:23, 2267.60 examples/s][A
Generating train examples...:   7%|â–‹         | 24436/350343 [00:10<02:23, 2276.87 examples/s][A
Generating train examples...:   7%|â–‹         | 24668/350343 [00:11<02:22, 2288.50 examples/s][A
Generating train examples...:   7%|â–‹         | 24899/350343 [00:11<02:21, 2294.20 examples/s][A
Generating train examples...:   7%|â–‹         | 25129/350343 [00:11<02:22, 2287.64 examples/s][A
Generating train examples...:   7%|â–‹         | 25358/350343 [00:11<02:23, 2269.89 examples/s][A
Generating train examples...:   7%|â–‹         | 25589/350343 [00:11<02:22, 2281.65 examples/s][A
Generating train examples...:   7%|â–‹         | 25818/350343 [00:11<02:22, 2282.16 examples/s][A
Generating train examples...:   7%|â–‹         | 26050/350343 [00:11<02:21, 2291.80 examples/s][A
Generating train examples...:   8%|â–Š         | 26283/350343 [00:11<02:20, 2302.51 examples/s][A
Generating train examples...:   8%|â–Š         | 26516/350343 [00:11<02:20, 2308.87 examples/s][A
Generating train examples...:   8%|â–Š         | 26747/350343 [00:11<02:20, 2300.41 examples/s][A
Generating train examples...:   8%|â–Š         | 26979/350343 [00:12<02:20, 2304.62 examples/s][A
Generating train examples...:   8%|â–Š         | 27212/350343 [00:12<02:19, 2310.81 examples/s][A
Generating train examples...:   8%|â–Š         | 27444/350343 [00:12<02:20, 2295.15 examples/s][A
Generating train examples...:   8%|â–Š         | 27674/350343 [00:12<02:20, 2294.39 examples/s][A
Generating train examples...:   8%|â–Š         | 27904/350343 [00:12<02:20, 2292.51 examples/s][A
Generating train examples...:   8%|â–Š         | 28136/350343 [00:12<02:25, 2209.92 examples/s][A
Generating train examples...:   8%|â–Š         | 28367/350343 [00:12<02:23, 2236.86 examples/s][A
Generating train examples...:   8%|â–Š         | 28592/350343 [00:12<02:24, 2230.70 examples/s][A
Generating train examples...:   8%|â–Š         | 28828/350343 [00:12<02:21, 2266.27 examples/s][A
Generating train examples...:   8%|â–Š         | 29062/350343 [00:12<02:20, 2286.65 examples/s][A
Generating train examples...:   8%|â–Š         | 29293/350343 [00:13<02:19, 2293.44 examples/s][A
Generating train examples...:   8%|â–Š         | 29523/350343 [00:13<02:20, 2280.69 examples/s][A
Generating train examples...:   8%|â–Š         | 29752/350343 [00:13<02:20, 2282.96 examples/s][A
Generating train examples...:   9%|â–Š         | 29981/350343 [00:13<02:21, 2269.60 examples/s][A
Generating train examples...:   9%|â–Š         | 30209/350343 [00:13<02:21, 2256.99 examples/s][A
Generating train examples...:   9%|â–Š         | 30441/350343 [00:13<02:20, 2274.18 examples/s][A
Generating train examples...:   9%|â–‰         | 30672/350343 [00:13<02:19, 2284.69 examples/s][A
Generating train examples...:   9%|â–‰         | 30901/350343 [00:13<02:20, 2272.34 examples/s][A
Generating train examples...:   9%|â–‰         | 31130/350343 [00:13<02:20, 2277.36 examples/s][A
Generating train examples...:   9%|â–‰         | 31364/350343 [00:13<02:19, 2294.22 examples/s][A
Generating train examples...:   9%|â–‰         | 31594/350343 [00:14<02:19, 2286.23 examples/s][A
Generating train examples...:   9%|â–‰         | 31823/350343 [00:14<02:19, 2278.79 examples/s][A
Generating train examples...:   9%|â–‰         | 32051/350343 [00:14<02:20, 2265.51 examples/s][A
Generating train examples...:   9%|â–‰         | 32278/350343 [00:14<02:20, 2261.20 examples/s][A
Generating train examples...:   9%|â–‰         | 32506/350343 [00:14<02:20, 2266.30 examples/s][A
Generating train examples...:   9%|â–‰         | 32734/350343 [00:14<02:19, 2269.33 examples/s][A
Generating train examples...:   9%|â–‰         | 32969/350343 [00:14<02:18, 2291.52 examples/s][A
Generating train examples...:   9%|â–‰         | 33199/350343 [00:14<02:18, 2293.83 examples/s][A
Generating train examples...:  10%|â–‰         | 33429/350343 [00:14<02:18, 2287.27 examples/s][A
Generating train examples...:  10%|â–‰         | 33658/350343 [00:14<02:18, 2287.13 examples/s][A
Generating train examples...:  10%|â–‰         | 33889/350343 [00:15<02:18, 2292.56 examples/s][A
Generating train examples...:  10%|â–‰         | 34119/350343 [00:15<02:18, 2282.25 examples/s][A
Generating train examples...:  10%|â–‰         | 34349/350343 [00:15<02:18, 2284.26 examples/s][A
Generating train examples...:  10%|â–‰         | 34578/350343 [00:15<02:19, 2269.43 examples/s][A
Generating train examples...:  10%|â–‰         | 34812/350343 [00:15<02:17, 2289.49 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35041/350343 [00:15<02:18, 2282.88 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35270/350343 [00:15<02:19, 2264.97 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35501/350343 [00:15<02:18, 2277.36 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35736/350343 [00:15<02:16, 2298.55 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35966/350343 [00:16<02:23, 2195.02 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36195/350343 [00:16<02:21, 2220.87 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36418/350343 [00:16<02:21, 2215.71 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36648/350343 [00:16<02:20, 2239.96 examples/s][A
Generating train examples...:  11%|â–ˆ         | 36873/350343 [00:16<02:20, 2234.92 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37103/350343 [00:16<02:19, 2252.59 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37333/350343 [00:16<02:18, 2266.40 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37560/350343 [00:16<02:18, 2263.43 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37790/350343 [00:16<02:17, 2273.67 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38023/350343 [00:16<02:16, 2288.35 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38252/350343 [00:17<02:16, 2281.98 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38481/350343 [00:17<02:16, 2279.52 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38709/350343 [00:17<02:16, 2279.39 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38941/350343 [00:17<02:15, 2290.58 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39171/350343 [00:17<02:16, 2287.63 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39401/350343 [00:17<02:15, 2288.85 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39634/350343 [00:17<02:15, 2299.06 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39864/350343 [00:17<02:15, 2291.76 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 40094/350343 [00:17<02:15, 2291.52 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40324/350343 [00:17<02:15, 2285.50 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40553/350343 [00:18<02:15, 2281.49 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40782/350343 [00:18<02:15, 2280.36 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41011/350343 [00:18<02:16, 2269.06 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41240/350343 [00:18<02:15, 2274.16 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41468/350343 [00:18<02:16, 2267.05 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41697/350343 [00:18<02:15, 2271.68 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41931/350343 [00:18<02:14, 2291.81 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42161/350343 [00:18<02:14, 2288.61 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42391/350343 [00:18<02:14, 2290.28 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42622/350343 [00:18<02:14, 2296.10 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42852/350343 [00:19<02:15, 2275.83 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43080/350343 [00:19<02:15, 2274.86 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43308/350343 [00:19<02:15, 2269.73 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43536/350343 [00:19<02:15, 2270.50 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43764/350343 [00:19<02:15, 2261.10 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 43991/350343 [00:19<02:16, 2242.09 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44218/350343 [00:19<02:16, 2249.69 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44444/350343 [00:19<02:16, 2247.02 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44669/350343 [00:19<02:16, 2236.15 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44893/350343 [00:19<02:16, 2235.95 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45120/350343 [00:20<02:15, 2245.75 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45346/350343 [00:20<02:15, 2249.93 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45579/350343 [00:20<02:19, 2183.10 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45808/350343 [00:20<02:17, 2212.72 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46033/350343 [00:20<02:16, 2222.59 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46259/350343 [00:20<02:16, 2231.91 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46490/350343 [00:20<02:14, 2253.97 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46719/350343 [00:20<02:14, 2262.60 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46946/350343 [00:20<02:14, 2258.87 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 47176/350343 [00:20<02:13, 2268.83 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47403/350343 [00:21<02:13, 2261.84 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47634/350343 [00:21<02:13, 2275.10 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47862/350343 [00:21<02:13, 2268.13 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 48089/350343 [00:21<02:13, 2260.70 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48316/350343 [00:21<02:14, 2243.08 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48553/350343 [00:21<02:12, 2280.30 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48789/350343 [00:21<02:10, 2302.38 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49020/350343 [00:21<02:11, 2295.84 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49250/350343 [00:21<02:11, 2283.77 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49479/350343 [00:21<02:12, 2278.65 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49708/350343 [00:22<02:11, 2280.08 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49937/350343 [00:22<02:12, 2269.75 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50167/350343 [00:22<02:11, 2276.48 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50395/350343 [00:22<02:12, 2263.90 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50622/350343 [00:22<02:13, 2252.38 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 50848/350343 [00:22<02:13, 2251.32 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51077/350343 [00:22<02:12, 2261.58 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51304/350343 [00:22<02:12, 2261.19 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51531/350343 [00:22<02:12, 2261.56 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51765/350343 [00:22<02:10, 2284.51 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51995/350343 [00:23<02:10, 2287.03 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52224/350343 [00:23<02:10, 2286.03 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52455/350343 [00:23<02:10, 2290.52 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52685/350343 [00:23<02:11, 2269.63 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52913/350343 [00:23<02:11, 2265.56 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53140/350343 [00:23<02:12, 2246.83 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53365/350343 [00:23<02:15, 2193.27 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53585/350343 [00:23<02:16, 2175.09 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53810/350343 [00:23<02:15, 2195.33 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 54038/350343 [00:23<02:13, 2218.59 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 54269/350343 [00:24<02:11, 2243.51 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54501/350343 [00:24<02:10, 2264.24 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54730/350343 [00:24<02:10, 2270.03 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54962/350343 [00:24<02:09, 2284.49 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55191/350343 [00:24<02:10, 2265.27 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55422/350343 [00:24<02:09, 2277.46 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55650/350343 [00:24<02:15, 2180.64 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55883/350343 [00:24<02:12, 2223.13 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56114/350343 [00:24<02:10, 2248.08 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56342/350343 [00:25<02:10, 2257.42 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56572/350343 [00:25<02:09, 2267.63 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56803/350343 [00:25<02:08, 2279.47 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57035/350343 [00:25<02:08, 2290.30 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57265/350343 [00:25<02:08, 2284.35 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57494/350343 [00:25<02:09, 2269.70 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57722/350343 [00:25<02:09, 2266.43 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 57950/350343 [00:25<02:08, 2270.37 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58178/350343 [00:25<02:08, 2270.21 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58406/350343 [00:25<02:09, 2257.53 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58632/350343 [00:26<02:09, 2252.87 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58860/350343 [00:26<02:09, 2258.47 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59086/350343 [00:26<02:08, 2258.07 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59314/350343 [00:26<02:08, 2264.37 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59542/350343 [00:26<02:08, 2266.96 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59769/350343 [00:26<02:09, 2245.48 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60000/350343 [00:26<02:08, 2262.57 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60227/350343 [00:26<02:08, 2260.10 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60455/350343 [00:26<02:07, 2265.46 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60682/350343 [00:26<02:07, 2263.11 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60910/350343 [00:27<02:07, 2267.44 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 61144/350343 [00:27<02:06, 2287.11 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61373/350343 [00:27<02:06, 2281.21 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61602/350343 [00:27<02:06, 2280.21 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61834/350343 [00:27<02:05, 2290.02 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62064/350343 [00:27<02:06, 2283.01 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62293/350343 [00:27<02:08, 2244.70 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62518/350343 [00:27<02:08, 2240.06 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62743/350343 [00:27<02:08, 2239.17 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62967/350343 [00:27<02:09, 2224.53 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63198/350343 [00:28<02:07, 2248.43 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63424/350343 [00:28<02:07, 2249.52 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63650/350343 [00:28<02:07, 2252.40 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63880/350343 [00:28<02:06, 2266.30 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64107/350343 [00:28<02:06, 2262.65 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64334/350343 [00:28<02:06, 2259.71 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64560/350343 [00:28<02:06, 2251.42 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64793/350343 [00:28<02:05, 2274.01 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65021/350343 [00:28<02:05, 2272.46 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65253/350343 [00:28<02:04, 2283.64 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65483/350343 [00:29<02:04, 2286.35 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 65712/350343 [00:29<02:04, 2279.88 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 65940/350343 [00:29<02:10, 2179.78 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66167/350343 [00:29<02:08, 2205.57 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66396/350343 [00:29<02:07, 2229.56 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66624/350343 [00:29<02:06, 2244.35 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66849/350343 [00:29<02:06, 2240.89 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67079/350343 [00:29<02:05, 2257.89 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67312/350343 [00:29<02:04, 2278.85 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67542/350343 [00:29<02:03, 2284.68 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67776/350343 [00:30<02:02, 2299.05 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68009/350343 [00:30<02:02, 2306.70 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68240/350343 [00:30<02:03, 2292.74 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68471/350343 [00:30<02:02, 2296.18 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68701/350343 [00:30<02:03, 2284.58 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68930/350343 [00:30<02:04, 2266.44 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69157/350343 [00:30<02:04, 2250.58 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69383/350343 [00:30<02:05, 2240.84 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69608/350343 [00:30<02:05, 2239.69 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69832/350343 [00:30<02:05, 2234.37 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 70061/350343 [00:31<02:04, 2249.57 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70289/350343 [00:31<02:04, 2257.76 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70518/350343 [00:31<02:03, 2267.08 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70746/350343 [00:31<02:03, 2269.64 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70974/350343 [00:31<02:02, 2271.91 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71202/350343 [00:31<02:03, 2266.99 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71429/350343 [00:31<02:04, 2239.05 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71653/350343 [00:31<02:04, 2238.44 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 71884/350343 [00:31<02:03, 2259.07 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72114/350343 [00:31<02:02, 2270.77 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72344/350343 [00:32<02:02, 2277.20 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72572/350343 [00:32<02:02, 2276.32 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72803/350343 [00:32<02:01, 2286.21 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73032/350343 [00:32<02:02, 2265.92 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73261/350343 [00:32<02:02, 2270.77 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73491/350343 [00:32<02:01, 2278.68 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73719/350343 [00:32<02:01, 2269.18 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73946/350343 [00:32<02:01, 2266.69 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74176/350343 [00:32<02:01, 2275.29 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74405/350343 [00:32<02:01, 2278.88 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74633/350343 [00:33<02:01, 2275.82 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74864/350343 [00:33<02:00, 2284.14 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 75095/350343 [00:33<02:00, 2289.46 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75324/350343 [00:33<02:00, 2289.47 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75553/350343 [00:33<02:00, 2288.72 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75784/350343 [00:33<02:04, 2200.88 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76007/350343 [00:33<02:05, 2193.45 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76237/350343 [00:33<02:03, 2223.18 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76460/350343 [00:33<02:03, 2209.11 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76687/350343 [00:34<02:02, 2226.93 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76912/350343 [00:34<02:02, 2233.01 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77141/350343 [00:34<02:01, 2247.72 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77366/350343 [00:34<02:01, 2247.06 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77592/350343 [00:34<02:01, 2249.69 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77820/350343 [00:34<02:00, 2258.39 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78046/350343 [00:34<02:01, 2248.66 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78271/350343 [00:34<02:01, 2241.89 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78496/350343 [00:34<02:01, 2242.20 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78721/350343 [00:34<02:02, 2224.57 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 78947/350343 [00:35<02:01, 2233.08 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79174/350343 [00:35<02:00, 2242.45 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79401/350343 [00:35<02:00, 2248.55 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79626/350343 [00:35<02:00, 2248.69 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79851/350343 [00:35<02:07, 2129.74 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80066/350343 [00:35<02:17, 1971.35 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80267/350343 [00:35<02:19, 1933.92 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80480/350343 [00:35<02:16, 1979.84 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80702/350343 [00:35<02:13, 2023.33 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80926/350343 [00:35<02:09, 2074.59 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81149/350343 [00:36<02:08, 2102.48 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81374/350343 [00:36<02:05, 2142.34 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81596/350343 [00:36<02:04, 2160.70 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81820/350343 [00:36<02:03, 2182.81 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82048/350343 [00:36<02:01, 2209.32 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82277/350343 [00:36<02:00, 2231.33 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82502/350343 [00:36<01:59, 2236.40 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82728/350343 [00:36<01:59, 2240.88 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82953/350343 [00:36<01:59, 2234.81 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 83177/350343 [00:36<01:59, 2234.92 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83401/350343 [00:37<01:59, 2236.01 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83630/350343 [00:37<01:58, 2251.11 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83858/350343 [00:37<01:57, 2258.65 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84086/350343 [00:37<01:57, 2263.89 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84313/350343 [00:37<01:57, 2263.78 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84540/350343 [00:37<01:57, 2264.63 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84767/350343 [00:37<01:57, 2259.86 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84996/350343 [00:37<01:57, 2266.62 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85223/350343 [00:37<01:58, 2231.18 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85447/350343 [00:37<01:58, 2230.37 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85676/350343 [00:38<01:57, 2247.89 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 85901/350343 [00:38<01:57, 2244.54 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86126/350343 [00:38<01:57, 2244.93 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86352/350343 [00:38<01:57, 2248.34 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86581/350343 [00:38<01:56, 2260.58 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86810/350343 [00:38<01:56, 2267.08 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87038/350343 [00:38<01:56, 2268.32 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87265/350343 [00:38<01:56, 2266.04 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87492/350343 [00:38<01:56, 2262.45 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 87720/350343 [00:38<01:55, 2266.59 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 87947/350343 [00:39<01:55, 2263.60 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88174/350343 [00:39<01:56, 2251.52 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88400/350343 [00:39<01:56, 2247.77 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88627/350343 [00:39<01:56, 2252.70 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88853/350343 [00:39<01:56, 2248.03 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 89079/350343 [00:39<01:56, 2251.44 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 89309/350343 [00:39<01:55, 2264.04 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89537/350343 [00:39<01:54, 2268.58 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89764/350343 [00:39<01:55, 2253.11 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89990/350343 [00:40<01:55, 2249.91 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90216/350343 [00:40<01:55, 2244.38 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90441/350343 [00:40<01:55, 2243.63 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90666/350343 [00:40<01:55, 2242.62 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90891/350343 [00:40<01:55, 2240.18 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91117/350343 [00:40<01:55, 2244.28 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91343/350343 [00:40<01:55, 2247.91 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91568/350343 [00:40<01:55, 2238.89 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91792/350343 [00:40<01:55, 2239.12 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92016/350343 [00:40<01:56, 2215.85 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92248/350343 [00:41<01:54, 2245.78 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92473/350343 [00:41<01:54, 2246.24 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92705/350343 [00:41<01:53, 2266.57 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 92932/350343 [00:41<01:53, 2258.18 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93160/350343 [00:41<01:53, 2263.75 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93389/350343 [00:41<01:53, 2269.73 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93616/350343 [00:41<01:53, 2267.97 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93843/350343 [00:41<01:53, 2268.18 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94070/350343 [00:41<01:53, 2261.27 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94297/350343 [00:41<01:54, 2241.98 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94527/350343 [00:42<01:53, 2258.48 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94753/350343 [00:42<01:53, 2257.83 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94985/350343 [00:42<01:52, 2275.89 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95213/350343 [00:42<01:52, 2274.80 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95441/350343 [00:42<01:52, 2265.21 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95668/350343 [00:42<01:52, 2260.16 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95899/350343 [00:42<01:51, 2273.03 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 96128/350343 [00:42<01:51, 2276.63 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96356/350343 [00:42<01:51, 2271.19 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96584/350343 [00:42<01:52, 2257.60 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96810/350343 [00:43<01:56, 2167.32 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97034/350343 [00:43<01:55, 2186.37 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97257/350343 [00:43<01:55, 2198.90 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97483/350343 [00:43<01:54, 2213.55 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97711/350343 [00:43<01:53, 2231.98 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97935/350343 [00:43<01:53, 2229.00 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98164/350343 [00:43<01:52, 2246.32 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98395/350343 [00:43<01:51, 2264.50 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98622/350343 [00:43<01:51, 2261.71 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98849/350343 [00:43<01:56, 2153.76 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99066/350343 [00:44<01:57, 2141.83 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99290/350343 [00:44<01:55, 2168.19 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99514/350343 [00:44<01:54, 2187.01 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99736/350343 [00:44<01:54, 2195.10 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 99962/350343 [00:44<01:53, 2213.45 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100190/350343 [00:44<01:52, 2231.07 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100418/350343 [00:44<01:51, 2243.32 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100649/350343 [00:44<01:50, 2261.64 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 100878/350343 [00:44<01:49, 2269.50 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101111/350343 [00:44<01:48, 2287.57 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101340/350343 [00:45<01:49, 2268.86 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101568/350343 [00:45<01:49, 2271.08 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101797/350343 [00:45<01:49, 2275.93 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102028/350343 [00:45<01:48, 2285.66 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102262/350343 [00:45<01:47, 2301.18 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102494/350343 [00:45<01:47, 2304.27 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102725/350343 [00:45<01:47, 2305.72 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102956/350343 [00:45<01:47, 2303.36 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 103187/350343 [00:45<01:47, 2291.16 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103417/350343 [00:45<01:48, 2280.09 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103646/350343 [00:46<01:48, 2268.44 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103875/350343 [00:46<01:48, 2271.31 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104108/350343 [00:46<01:47, 2287.11 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104337/350343 [00:46<01:47, 2282.09 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104567/350343 [00:46<01:47, 2286.02 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104800/350343 [00:46<01:46, 2299.05 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 105030/350343 [00:46<01:46, 2295.05 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105260/350343 [00:46<01:47, 2288.83 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105489/350343 [00:46<01:47, 2287.92 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105719/350343 [00:46<01:46, 2291.13 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105949/350343 [00:47<01:47, 2265.05 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106180/350343 [00:47<01:47, 2277.31 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106413/350343 [00:47<01:46, 2291.25 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106643/350343 [00:47<01:47, 2274.74 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 106871/350343 [00:47<01:48, 2253.08 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107101/350343 [00:47<01:47, 2265.17 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107328/350343 [00:47<01:47, 2263.75 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107555/350343 [00:47<01:47, 2258.52 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107781/350343 [00:47<01:51, 2167.20 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108004/350343 [00:48<01:51, 2181.54 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108228/350343 [00:48<01:50, 2192.29 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108451/350343 [00:48<01:50, 2185.01 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108678/350343 [00:48<01:49, 2207.97 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108905/350343 [00:48<01:48, 2225.89 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109131/350343 [00:48<01:47, 2233.83 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109356/350343 [00:48<01:47, 2237.82 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109581/350343 [00:48<01:47, 2238.86 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109809/350343 [00:48<01:46, 2248.30 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110034/350343 [00:48<01:47, 2241.41 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110259/350343 [00:49<01:47, 2243.28 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110484/350343 [00:49<01:46, 2242.99 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110709/350343 [00:49<01:47, 2228.34 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110936/350343 [00:49<01:46, 2238.61 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111161/350343 [00:49<01:46, 2241.71 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111391/350343 [00:49<01:45, 2259.03 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111624/350343 [00:49<01:44, 2279.89 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111856/350343 [00:49<01:44, 2290.58 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112086/350343 [00:49<01:45, 2261.74 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112313/350343 [00:49<01:45, 2261.44 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112541/350343 [00:50<01:44, 2266.39 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112768/350343 [00:50<01:45, 2249.11 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112993/350343 [00:50<01:45, 2244.55 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113222/350343 [00:50<01:45, 2257.31 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113453/350343 [00:50<01:44, 2272.47 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113684/350343 [00:50<01:43, 2282.32 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 113915/350343 [00:50<01:43, 2288.37 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114147/350343 [00:50<01:42, 2296.09 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114383/350343 [00:50<01:41, 2314.90 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114615/350343 [00:50<01:41, 2315.24 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114847/350343 [00:51<01:41, 2313.36 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115079/350343 [00:51<01:42, 2305.09 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115310/350343 [00:51<01:43, 2281.65 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115541/350343 [00:51<01:42, 2289.63 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115771/350343 [00:51<01:42, 2290.70 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116001/350343 [00:51<01:42, 2279.91 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116231/350343 [00:51<01:42, 2284.58 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116460/350343 [00:51<01:42, 2278.50 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116692/350343 [00:51<01:41, 2290.84 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116924/350343 [00:51<01:41, 2298.53 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117154/350343 [00:52<01:41, 2291.66 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117384/350343 [00:52<01:42, 2266.62 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117611/350343 [00:52<01:42, 2266.09 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117841/350343 [00:52<01:42, 2274.82 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 118071/350343 [00:52<01:46, 2187.77 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118303/350343 [00:52<01:44, 2223.70 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118529/350343 [00:52<01:43, 2231.66 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118755/350343 [00:52<01:43, 2239.81 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118982/350343 [00:52<01:42, 2247.24 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119207/350343 [00:52<01:42, 2247.32 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119432/350343 [00:53<01:42, 2245.09 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119657/350343 [00:53<01:43, 2236.99 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119881/350343 [00:53<01:43, 2236.17 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120107/350343 [00:53<01:42, 2242.56 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120332/350343 [00:53<01:42, 2242.54 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120557/350343 [00:53<01:43, 2228.44 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120784/350343 [00:53<01:42, 2238.48 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121012/350343 [00:53<01:41, 2250.70 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121243/350343 [00:53<01:41, 2267.00 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121470/350343 [00:53<01:42, 2237.97 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121694/350343 [00:54<01:44, 2191.04 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121914/350343 [00:54<01:47, 2130.13 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122147/350343 [00:54<01:44, 2186.49 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122374/350343 [00:54<01:43, 2210.43 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122603/350343 [00:54<01:42, 2232.09 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122828/350343 [00:54<01:41, 2237.13 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123054/350343 [00:54<01:41, 2243.09 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123279/350343 [00:54<01:41, 2234.19 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123507/350343 [00:54<01:41, 2245.66 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123736/350343 [00:54<01:40, 2258.25 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123964/350343 [00:55<01:39, 2264.21 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 124191/350343 [00:55<01:40, 2256.96 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124419/350343 [00:55<01:39, 2261.93 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124646/350343 [00:55<01:40, 2255.83 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124876/350343 [00:55<01:39, 2268.38 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125103/350343 [00:55<01:39, 2264.32 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125332/350343 [00:55<01:39, 2271.91 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125560/350343 [00:55<01:39, 2267.60 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125787/350343 [00:55<01:39, 2258.66 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126018/350343 [00:55<01:38, 2272.58 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126246/350343 [00:56<01:38, 2269.16 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126473/350343 [00:56<01:38, 2269.25 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126700/350343 [00:56<01:38, 2259.76 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126935/350343 [00:56<01:37, 2284.27 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127169/350343 [00:56<01:37, 2298.80 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127399/350343 [00:56<01:37, 2290.55 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127629/350343 [00:56<01:37, 2284.98 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127858/350343 [00:56<01:37, 2286.00 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128088/350343 [00:56<01:37, 2289.76 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128318/350343 [00:57<01:36, 2292.17 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128548/350343 [00:57<01:37, 2282.73 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128777/350343 [00:57<01:37, 2275.38 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129005/350343 [00:57<01:37, 2264.31 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129234/350343 [00:57<01:37, 2269.33 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129462/350343 [00:57<01:37, 2271.75 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129690/350343 [00:57<01:37, 2260.24 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129918/350343 [00:57<01:37, 2264.37 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130150/350343 [00:57<01:40, 2189.97 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130379/350343 [00:57<01:39, 2216.96 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130605/350343 [00:58<01:38, 2228.23 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130835/350343 [00:58<01:37, 2247.51 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131063/350343 [00:58<01:37, 2255.52 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131289/350343 [00:58<01:37, 2253.91 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131515/350343 [00:58<01:38, 2220.92 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131738/350343 [00:58<01:38, 2218.35 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131967/350343 [00:58<01:37, 2237.54 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132198/350343 [00:58<01:36, 2256.98 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132431/350343 [00:58<01:35, 2276.93 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132659/350343 [00:58<01:35, 2274.52 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132888/350343 [00:59<01:35, 2278.99 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133117/350343 [00:59<01:35, 2280.18 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133347/350343 [00:59<01:34, 2284.50 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133576/350343 [00:59<01:36, 2257.70 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133802/350343 [00:59<01:35, 2258.29 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134033/350343 [00:59<01:35, 2272.20 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134261/350343 [00:59<01:35, 2262.10 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134488/350343 [00:59<01:35, 2263.87 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134715/350343 [00:59<01:35, 2251.00 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 134941/350343 [00:59<01:36, 2238.64 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135168/350343 [01:00<01:35, 2246.09 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135393/350343 [01:00<01:35, 2245.47 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135618/350343 [01:00<01:36, 2226.42 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 135841/350343 [01:00<01:36, 2220.98 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136069/350343 [01:00<01:35, 2237.93 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136298/350343 [01:00<01:35, 2251.69 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136524/350343 [01:00<01:35, 2250.36 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136752/350343 [01:00<01:34, 2258.59 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136978/350343 [01:00<01:34, 2258.42 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137204/350343 [01:00<01:34, 2245.02 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137429/350343 [01:01<01:34, 2244.18 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137654/350343 [01:01<01:34, 2241.35 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137882/350343 [01:01<01:34, 2252.38 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138108/350343 [01:01<01:34, 2252.18 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138334/350343 [01:01<01:34, 2249.18 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138566/350343 [01:01<01:33, 2268.93 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138796/350343 [01:01<01:32, 2275.80 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139024/350343 [01:01<01:35, 2209.77 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139246/350343 [01:01<01:36, 2190.23 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139469/350343 [01:01<01:35, 2200.83 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139698/350343 [01:02<01:34, 2227.11 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139923/350343 [01:02<01:34, 2233.35 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140154/350343 [01:02<01:33, 2255.47 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140380/350343 [01:02<01:33, 2237.93 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140610/350343 [01:02<01:32, 2255.31 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140841/350343 [01:02<01:32, 2270.94 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141073/350343 [01:02<01:31, 2283.90 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141305/350343 [01:02<01:31, 2293.99 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141535/350343 [01:02<01:31, 2290.23 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141765/350343 [01:02<01:31, 2286.03 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141994/350343 [01:03<01:31, 2275.35 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142222/350343 [01:03<01:31, 2275.95 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142450/350343 [01:03<01:31, 2265.98 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142677/350343 [01:03<01:31, 2263.09 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142906/350343 [01:03<01:34, 2186.33 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143136/350343 [01:03<01:33, 2218.06 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143359/350343 [01:03<01:33, 2221.42 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143583/350343 [01:03<01:32, 2226.83 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143806/350343 [01:03<01:32, 2227.21 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144033/350343 [01:03<01:32, 2239.11 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144259/350343 [01:04<01:31, 2243.50 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144484/350343 [01:04<01:31, 2244.56 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144709/350343 [01:04<01:31, 2245.02 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144934/350343 [01:04<01:32, 2230.33 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145161/350343 [01:04<01:31, 2241.17 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145395/350343 [01:04<01:30, 2269.32 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145623/350343 [01:04<01:30, 2272.12 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145852/350343 [01:04<01:29, 2277.15 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146083/350343 [01:04<01:29, 2284.79 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146312/350343 [01:04<01:29, 2282.61 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146546/350343 [01:05<01:28, 2297.43 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146776/350343 [01:05<01:28, 2292.75 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147006/350343 [01:05<01:29, 2281.87 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147235/350343 [01:05<01:28, 2282.58 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147467/350343 [01:05<01:28, 2293.64 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147699/350343 [01:05<01:28, 2300.47 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147930/350343 [01:05<01:28, 2282.99 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148159/350343 [01:05<01:28, 2284.54 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148388/350343 [01:05<01:28, 2284.54 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148617/350343 [01:06<01:28, 2284.65 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148846/350343 [01:06<01:28, 2281.37 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149079/350343 [01:06<01:27, 2295.57 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149309/350343 [01:06<01:28, 2280.45 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149538/350343 [01:06<01:28, 2269.78 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149766/350343 [01:06<01:28, 2271.60 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149994/350343 [01:06<01:28, 2268.89 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150222/350343 [01:06<01:28, 2272.18 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150450/350343 [01:06<01:27, 2272.86 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150678/350343 [01:06<01:27, 2274.73 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150907/350343 [01:07<01:27, 2277.92 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151138/350343 [01:07<01:27, 2285.63 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151367/350343 [01:07<01:27, 2281.13 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151599/350343 [01:07<01:26, 2290.11 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151829/350343 [01:07<01:27, 2275.22 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152057/350343 [01:07<01:27, 2257.00 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152285/350343 [01:07<01:27, 2261.60 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152516/350343 [01:07<01:26, 2275.48 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152744/350343 [01:07<01:27, 2266.54 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152975/350343 [01:07<01:30, 2186.75 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153204/350343 [01:08<01:29, 2215.01 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153435/350343 [01:08<01:27, 2242.78 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153668/350343 [01:08<01:26, 2268.30 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153897/350343 [01:08<01:26, 2273.83 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154127/350343 [01:08<01:26, 2278.86 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154356/350343 [01:08<01:26, 2277.63 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154584/350343 [01:08<01:25, 2276.44 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154812/350343 [01:08<01:25, 2277.39 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155040/350343 [01:08<01:26, 2259.07 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155271/350343 [01:08<01:25, 2273.49 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155499/350343 [01:09<01:25, 2269.56 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155726/350343 [01:09<01:25, 2266.43 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155955/350343 [01:09<01:25, 2271.22 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156183/350343 [01:09<01:25, 2272.52 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156411/350343 [01:09<01:25, 2273.27 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156639/350343 [01:09<01:26, 2250.94 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156869/350343 [01:09<01:25, 2263.09 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157096/350343 [01:09<01:25, 2265.04 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157323/350343 [01:09<01:25, 2244.71 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157548/350343 [01:09<01:26, 2237.74 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157774/350343 [01:10<01:25, 2243.18 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157999/350343 [01:10<01:25, 2243.38 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158224/350343 [01:10<01:25, 2237.04 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158452/350343 [01:10<01:25, 2249.30 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158681/350343 [01:10<01:24, 2260.50 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158908/350343 [01:10<01:24, 2257.07 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159134/350343 [01:10<01:24, 2257.22 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159360/350343 [01:10<01:24, 2254.54 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159589/350343 [01:10<01:24, 2264.28 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159817/350343 [01:10<01:23, 2268.66 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160044/350343 [01:11<01:23, 2266.96 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160272/350343 [01:11<01:23, 2269.55 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160501/350343 [01:11<01:23, 2275.02 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160729/350343 [01:11<01:23, 2266.25 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160956/350343 [01:11<01:23, 2260.40 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161183/350343 [01:11<01:24, 2240.58 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161409/350343 [01:11<01:24, 2245.52 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161634/350343 [01:11<01:24, 2237.83 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161858/350343 [01:11<01:24, 2236.17 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162083/350343 [01:11<01:24, 2238.02 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162310/350343 [01:12<01:23, 2247.33 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162537/350343 [01:12<01:23, 2252.70 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162763/350343 [01:12<01:23, 2253.95 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162997/350343 [01:12<01:22, 2278.51 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163225/350343 [01:12<01:22, 2272.00 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163453/350343 [01:12<01:22, 2258.85 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163679/350343 [01:12<01:22, 2250.09 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163905/350343 [01:12<01:22, 2248.76 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164130/350343 [01:12<01:23, 2241.36 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164356/350343 [01:12<01:22, 2244.77 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164586/350343 [01:13<01:22, 2260.47 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164813/350343 [01:13<01:22, 2261.49 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165040/350343 [01:13<01:22, 2247.80 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165265/350343 [01:13<01:22, 2235.71 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165489/350343 [01:13<01:22, 2232.43 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165713/350343 [01:13<01:23, 2205.32 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165938/350343 [01:13<01:23, 2217.39 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166165/350343 [01:13<01:22, 2231.55 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166390/350343 [01:13<01:22, 2234.73 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166620/350343 [01:13<01:21, 2252.87 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166846/350343 [01:14<01:24, 2170.61 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167074/350343 [01:14<01:23, 2201.66 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167295/350343 [01:14<01:23, 2200.80 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167525/350343 [01:14<01:21, 2229.79 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167754/350343 [01:14<01:21, 2246.79 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167979/350343 [01:14<01:22, 2223.78 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168206/350343 [01:14<01:21, 2236.00 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168436/350343 [01:14<01:20, 2253.63 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168662/350343 [01:14<01:20, 2255.02 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168893/350343 [01:14<01:19, 2268.82 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169120/350343 [01:15<01:20, 2260.85 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169347/350343 [01:15<01:20, 2254.98 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169575/350343 [01:15<01:19, 2260.67 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169802/350343 [01:15<01:19, 2260.81 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170036/350343 [01:15<01:18, 2284.20 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170265/350343 [01:15<01:19, 2264.02 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170492/350343 [01:15<01:19, 2253.94 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170721/350343 [01:15<01:19, 2263.69 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 170948/350343 [01:15<01:19, 2260.22 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171179/350343 [01:16<01:18, 2274.55 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171409/350343 [01:16<01:18, 2281.06 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171638/350343 [01:16<01:18, 2273.78 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171866/350343 [01:16<01:18, 2269.57 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172094/350343 [01:16<01:18, 2270.49 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172322/350343 [01:16<01:18, 2271.88 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172550/350343 [01:16<01:19, 2244.17 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172775/350343 [01:16<01:19, 2223.89 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173004/350343 [01:16<01:19, 2242.58 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173229/350343 [01:16<01:19, 2240.46 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173457/350343 [01:17<01:18, 2250.35 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173686/350343 [01:17<01:18, 2262.02 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173917/350343 [01:17<01:17, 2274.02 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174146/350343 [01:17<01:17, 2276.54 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174374/350343 [01:17<01:17, 2268.82 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174603/350343 [01:17<01:17, 2272.83 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174831/350343 [01:17<01:17, 2268.45 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 175058/350343 [01:17<01:17, 2253.54 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175285/350343 [01:17<01:17, 2257.64 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175511/350343 [01:17<01:17, 2256.05 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175741/350343 [01:18<01:16, 2267.92 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175968/350343 [01:18<01:17, 2245.75 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176193/350343 [01:18<01:17, 2234.21 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176417/350343 [01:18<01:18, 2224.62 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176642/350343 [01:18<01:17, 2230.92 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176869/350343 [01:18<01:17, 2241.97 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177095/350343 [01:18<01:17, 2247.23 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177320/350343 [01:18<01:17, 2241.89 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177545/350343 [01:18<01:17, 2240.59 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177776/350343 [01:18<01:16, 2259.35 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178002/350343 [01:19<01:16, 2256.75 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178234/350343 [01:19<01:15, 2275.36 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178462/350343 [01:19<01:15, 2263.59 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178691/350343 [01:19<01:15, 2268.34 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178918/350343 [01:19<01:15, 2262.52 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179147/350343 [01:19<01:18, 2181.21 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179375/350343 [01:19<01:17, 2208.18 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179597/350343 [01:19<01:17, 2189.43 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179818/350343 [01:19<01:17, 2193.96 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180041/350343 [01:19<01:17, 2202.60 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180263/350343 [01:20<01:17, 2205.85 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180488/350343 [01:20<01:16, 2217.86 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180712/350343 [01:20<01:16, 2222.92 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180935/350343 [01:20<01:16, 2218.72 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181161/350343 [01:20<01:15, 2230.87 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181390/350343 [01:20<01:15, 2248.31 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181622/350343 [01:20<01:14, 2269.28 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181849/350343 [01:20<01:14, 2256.60 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182080/350343 [01:20<01:14, 2270.53 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182311/350343 [01:20<01:13, 2280.38 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182540/350343 [01:21<01:13, 2278.08 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182768/350343 [01:21<01:13, 2276.11 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182996/350343 [01:21<01:13, 2272.59 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183224/350343 [01:21<01:13, 2271.07 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183452/350343 [01:21<01:13, 2267.02 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183679/350343 [01:21<01:13, 2266.48 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183906/350343 [01:21<01:13, 2264.69 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184133/350343 [01:21<01:14, 2244.75 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184363/350343 [01:21<01:13, 2258.88 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184589/350343 [01:21<01:13, 2247.50 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184814/350343 [01:22<01:13, 2247.09 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185044/350343 [01:22<01:13, 2262.73 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185271/350343 [01:22<01:13, 2253.72 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185497/350343 [01:22<01:13, 2251.61 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185723/350343 [01:22<01:13, 2247.00 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185948/350343 [01:22<01:13, 2243.88 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186177/350343 [01:22<01:12, 2256.58 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186403/350343 [01:22<01:12, 2251.85 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186630/350343 [01:22<01:12, 2255.88 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186856/350343 [01:22<01:12, 2242.45 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187081/350343 [01:23<01:12, 2239.97 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187307/350343 [01:23<01:12, 2243.95 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187532/350343 [01:23<01:12, 2237.66 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187756/350343 [01:23<01:12, 2234.72 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187981/350343 [01:23<01:12, 2238.86 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188209/350343 [01:23<01:12, 2248.67 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188435/350343 [01:23<01:11, 2250.88 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188661/350343 [01:23<01:11, 2249.77 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188886/350343 [01:23<01:11, 2247.26 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189117/350343 [01:23<01:11, 2265.40 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189345/350343 [01:24<01:10, 2268.44 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189575/350343 [01:24<01:10, 2277.30 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189803/350343 [01:24<01:10, 2271.94 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190036/350343 [01:24<01:10, 2287.70 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190265/350343 [01:24<01:10, 2275.73 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190493/350343 [01:24<01:12, 2195.80 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190714/350343 [01:24<01:13, 2166.26 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190937/350343 [01:24<01:13, 2182.52 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191160/350343 [01:24<01:12, 2196.20 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191390/350343 [01:25<01:11, 2226.19 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191617/350343 [01:25<01:10, 2238.63 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191845/350343 [01:25<01:10, 2249.35 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192071/350343 [01:25<01:10, 2252.39 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192300/350343 [01:25<01:09, 2262.91 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192527/350343 [01:25<01:09, 2262.87 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192754/350343 [01:25<01:09, 2262.15 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192982/350343 [01:25<01:09, 2266.76 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193211/350343 [01:25<01:09, 2271.23 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193439/350343 [01:25<01:09, 2266.04 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193666/350343 [01:26<01:09, 2260.40 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193893/350343 [01:26<01:09, 2256.99 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194120/350343 [01:26<01:09, 2258.18 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194350/350343 [01:26<01:08, 2269.97 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194578/350343 [01:26<01:08, 2266.79 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194805/350343 [01:26<01:11, 2180.94 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195033/350343 [01:26<01:10, 2208.64 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195262/350343 [01:26<01:09, 2230.29 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195489/350343 [01:26<01:09, 2240.17 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195714/350343 [01:26<01:08, 2241.38 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195942/350343 [01:27<01:08, 2251.25 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196168/350343 [01:27<01:08, 2249.06 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196394/350343 [01:27<01:08, 2247.95 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196619/350343 [01:27<01:08, 2247.70 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196847/350343 [01:27<01:08, 2255.10 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197077/350343 [01:27<01:07, 2267.96 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197304/350343 [01:27<01:07, 2262.41 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197531/350343 [01:27<01:07, 2254.18 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197757/350343 [01:27<01:07, 2246.44 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197983/350343 [01:27<01:07, 2248.74 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198209/350343 [01:28<01:07, 2250.41 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198435/350343 [01:28<01:09, 2190.00 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198655/350343 [01:28<01:10, 2165.58 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198872/350343 [01:28<01:10, 2150.65 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199088/350343 [01:28<01:10, 2140.90 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199309/350343 [01:28<01:09, 2161.15 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199526/350343 [01:28<01:09, 2163.19 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199746/350343 [01:28<01:09, 2173.12 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199968/350343 [01:28<01:08, 2186.87 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200193/350343 [01:28<01:08, 2204.29 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200415/350343 [01:29<01:07, 2208.60 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200641/350343 [01:29<01:07, 2223.05 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200869/350343 [01:29<01:06, 2239.78 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201094/350343 [01:29<01:07, 2221.78 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201318/350343 [01:29<01:06, 2225.48 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201541/350343 [01:29<01:06, 2224.15 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201766/350343 [01:29<01:06, 2230.78 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201993/350343 [01:29<01:06, 2241.82 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202218/350343 [01:29<01:06, 2229.72 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202443/350343 [01:29<01:06, 2234.01 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202668/350343 [01:30<01:05, 2238.33 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202897/350343 [01:30<01:05, 2253.27 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203126/350343 [01:30<01:05, 2262.22 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203353/350343 [01:30<01:04, 2262.76 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203580/350343 [01:30<01:05, 2256.66 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203810/350343 [01:30<01:04, 2267.57 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204037/350343 [01:30<01:04, 2262.35 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204264/350343 [01:30<01:04, 2252.62 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204490/350343 [01:30<01:04, 2252.70 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204716/350343 [01:30<01:04, 2241.60 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204941/350343 [01:31<01:05, 2220.70 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205164/350343 [01:31<01:05, 2218.79 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205389/350343 [01:31<01:05, 2225.55 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205613/350343 [01:31<01:04, 2229.07 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 205836/350343 [01:31<01:05, 2216.70 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206058/350343 [01:31<01:05, 2207.37 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206281/350343 [01:31<01:05, 2212.28 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206505/350343 [01:31<01:04, 2219.08 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206728/350343 [01:31<01:04, 2220.25 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206951/350343 [01:31<01:04, 2220.05 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207183/350343 [01:32<01:03, 2247.81 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207408/350343 [01:32<01:03, 2244.18 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207633/350343 [01:32<01:03, 2245.39 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207860/350343 [01:32<01:03, 2251.19 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208086/350343 [01:32<01:03, 2253.01 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208312/350343 [01:32<01:03, 2251.10 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208538/350343 [01:32<01:02, 2253.45 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208769/350343 [01:32<01:02, 2268.32 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208996/350343 [01:32<01:02, 2258.63 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209222/350343 [01:32<01:03, 2238.77 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209446/350343 [01:33<01:06, 2122.12 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209660/350343 [01:33<01:08, 2042.68 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209866/350343 [01:33<01:09, 2035.74 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210087/350343 [01:33<01:07, 2083.94 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210314/350343 [01:33<01:05, 2135.92 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210539/350343 [01:33<01:04, 2169.20 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210762/350343 [01:33<01:03, 2185.00 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210986/350343 [01:33<01:03, 2198.94 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211215/350343 [01:33<01:02, 2223.28 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211444/350343 [01:34<01:01, 2241.10 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211671/350343 [01:34<01:01, 2248.39 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211899/350343 [01:34<01:01, 2255.35 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212125/350343 [01:34<01:01, 2255.70 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212355/350343 [01:34<01:00, 2266.70 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212583/350343 [01:34<01:00, 2268.05 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212810/350343 [01:34<01:00, 2261.88 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213037/350343 [01:34<01:00, 2254.75 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213263/350343 [01:34<01:01, 2244.44 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213488/350343 [01:34<01:01, 2236.44 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213719/350343 [01:35<01:00, 2256.01 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213945/350343 [01:35<01:00, 2250.65 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214171/350343 [01:35<01:00, 2245.01 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214396/350343 [01:35<01:00, 2232.62 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214620/350343 [01:35<01:00, 2228.34 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214843/350343 [01:35<01:01, 2193.64 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215063/350343 [01:35<01:02, 2158.90 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215280/350343 [01:35<01:12, 1861.46 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215474/350343 [01:35<01:15, 1791.23 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215677/350343 [01:36<01:12, 1853.60 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215895/350343 [01:36<01:09, 1942.64 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216115/350343 [01:36<01:06, 2012.28 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216335/350343 [01:36<01:04, 2064.31 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216563/350343 [01:36<01:02, 2126.03 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216793/350343 [01:36<01:01, 2176.61 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217018/350343 [01:36<01:00, 2195.56 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217249/350343 [01:36<00:59, 2228.39 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217481/350343 [01:36<00:58, 2255.01 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217709/350343 [01:36<00:58, 2260.51 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217936/350343 [01:37<00:58, 2255.24 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218162/350343 [01:37<00:58, 2255.12 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218388/350343 [01:37<00:58, 2251.22 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218614/350343 [01:37<00:58, 2237.90 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218839/350343 [01:37<00:58, 2239.61 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219064/350343 [01:37<00:58, 2235.08 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219292/350343 [01:37<00:58, 2248.00 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219521/350343 [01:37<00:57, 2259.03 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219752/350343 [01:37<00:57, 2271.73 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219982/350343 [01:37<00:57, 2278.52 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220213/350343 [01:38<00:56, 2286.53 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220442/350343 [01:38<00:56, 2280.25 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220671/350343 [01:38<00:56, 2281.54 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220900/350343 [01:38<00:57, 2252.37 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221126/350343 [01:38<01:00, 2136.13 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221341/350343 [01:38<01:01, 2097.49 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221570/350343 [01:38<00:59, 2151.37 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221790/350343 [01:38<00:59, 2163.61 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222020/350343 [01:38<00:58, 2201.40 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222248/350343 [01:38<00:57, 2224.33 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222471/350343 [01:39<00:57, 2223.50 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222697/350343 [01:39<00:57, 2231.61 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222923/350343 [01:39<00:56, 2238.86 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223148/350343 [01:39<00:56, 2240.10 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223374/350343 [01:39<00:56, 2245.90 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223603/350343 [01:39<00:56, 2257.12 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223834/350343 [01:39<00:55, 2271.94 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224062/350343 [01:39<00:55, 2266.64 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224289/350343 [01:39<00:55, 2259.53 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224518/350343 [01:39<00:55, 2268.39 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224745/350343 [01:40<00:55, 2259.68 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224974/350343 [01:40<00:55, 2267.56 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225201/350343 [01:40<00:55, 2265.06 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225430/350343 [01:40<00:54, 2272.07 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225658/350343 [01:40<00:55, 2266.74 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225888/350343 [01:40<00:54, 2276.58 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226116/350343 [01:40<00:54, 2261.12 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226343/350343 [01:40<00:57, 2163.41 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226567/350343 [01:40<00:56, 2184.67 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226796/350343 [01:40<00:55, 2215.31 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227021/350343 [01:41<00:55, 2224.50 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227244/350343 [01:41<00:55, 2224.25 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227474/350343 [01:41<00:54, 2245.66 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227699/350343 [01:41<00:54, 2243.59 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 227924/350343 [01:41<00:54, 2235.84 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228152/350343 [01:41<00:54, 2247.35 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228377/350343 [01:41<00:54, 2247.72 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228608/350343 [01:41<00:53, 2264.32 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228835/350343 [01:41<00:53, 2257.90 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229067/350343 [01:41<00:53, 2275.05 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229295/350343 [01:42<00:53, 2253.07 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229522/350343 [01:42<00:53, 2258.08 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229748/350343 [01:42<00:53, 2249.81 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229978/350343 [01:42<00:53, 2263.24 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230209/350343 [01:42<00:52, 2277.05 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230437/350343 [01:42<00:52, 2270.69 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230666/350343 [01:42<00:52, 2274.44 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230894/350343 [01:42<00:52, 2262.66 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231121/350343 [01:42<00:52, 2257.07 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231347/350343 [01:42<00:52, 2253.75 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231573/350343 [01:43<00:52, 2254.30 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231801/350343 [01:43<00:52, 2259.97 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 232028/350343 [01:43<00:52, 2262.94 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232256/350343 [01:43<00:52, 2267.60 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232483/350343 [01:43<00:52, 2262.94 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232710/350343 [01:43<00:52, 2247.29 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232935/350343 [01:43<00:52, 2247.81 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233161/350343 [01:43<00:52, 2249.71 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233386/350343 [01:43<00:52, 2248.65 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233614/350343 [01:44<00:51, 2256.39 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233841/350343 [01:44<00:51, 2258.98 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234068/350343 [01:44<00:51, 2261.56 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234295/350343 [01:44<00:51, 2250.95 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234527/350343 [01:44<00:51, 2270.63 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234755/350343 [01:44<00:51, 2243.45 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234981/350343 [01:44<00:51, 2247.36 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235206/350343 [01:44<00:51, 2229.12 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235432/350343 [01:44<00:51, 2237.17 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235664/350343 [01:44<00:50, 2260.73 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235898/350343 [01:45<00:50, 2282.70 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236127/350343 [01:45<00:50, 2271.23 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236356/350343 [01:45<00:50, 2275.80 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236584/350343 [01:45<00:50, 2263.68 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236812/350343 [01:45<00:50, 2268.38 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237039/350343 [01:45<00:50, 2261.67 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237266/350343 [01:45<00:49, 2263.82 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237495/350343 [01:45<00:49, 2269.74 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237723/350343 [01:45<00:49, 2272.52 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237951/350343 [01:45<00:49, 2272.58 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238179/350343 [01:46<00:49, 2265.94 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238406/350343 [01:46<00:49, 2264.88 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238635/350343 [01:46<00:49, 2270.26 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238863/350343 [01:46<00:49, 2270.99 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239091/350343 [01:46<00:49, 2264.14 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239318/350343 [01:46<00:51, 2151.78 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239550/350343 [01:46<00:50, 2199.90 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239771/350343 [01:46<00:50, 2197.08 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239997/350343 [01:46<00:49, 2213.24 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240221/350343 [01:46<00:49, 2220.05 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240450/350343 [01:47<00:49, 2238.74 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240675/350343 [01:47<00:48, 2239.16 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 240901/350343 [01:47<00:48, 2243.60 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241126/350343 [01:47<00:48, 2239.62 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241355/350343 [01:47<00:48, 2252.81 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241581/350343 [01:47<00:48, 2251.76 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241807/350343 [01:47<00:48, 2251.26 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242039/350343 [01:47<00:47, 2269.30 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242266/350343 [01:47<00:47, 2258.73 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242492/350343 [01:47<00:47, 2258.38 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242719/350343 [01:48<00:47, 2260.92 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242946/350343 [01:48<00:47, 2251.74 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243173/350343 [01:48<00:47, 2256.77 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243399/350343 [01:48<00:47, 2248.70 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243624/350343 [01:48<00:47, 2248.88 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243849/350343 [01:48<00:47, 2237.47 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244073/350343 [01:48<00:47, 2233.73 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244299/350343 [01:48<00:47, 2239.63 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244525/350343 [01:48<00:47, 2243.54 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244754/350343 [01:48<00:46, 2257.13 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244982/350343 [01:49<00:46, 2263.72 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 245210/350343 [01:49<00:46, 2268.02 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245437/350343 [01:49<00:46, 2244.92 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245668/350343 [01:49<00:46, 2263.78 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245895/350343 [01:49<00:46, 2254.28 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246121/350343 [01:49<00:46, 2255.90 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246351/350343 [01:49<00:45, 2268.02 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246578/350343 [01:49<00:45, 2265.17 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246806/350343 [01:49<00:45, 2269.39 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247035/350343 [01:49<00:45, 2274.23 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247264/350343 [01:50<00:45, 2278.04 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247496/350343 [01:50<00:44, 2289.71 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247725/350343 [01:50<00:44, 2285.20 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247954/350343 [01:50<00:45, 2273.90 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248183/350343 [01:50<00:44, 2276.42 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248411/350343 [01:50<00:44, 2276.35 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248639/350343 [01:50<00:45, 2253.11 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248865/350343 [01:50<00:45, 2248.80 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249090/350343 [01:50<00:45, 2240.61 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249317/350343 [01:50<00:44, 2249.19 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249544/350343 [01:51<00:44, 2252.21 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249770/350343 [01:51<00:44, 2254.36 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249997/350343 [01:51<00:44, 2255.98 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250230/350343 [01:51<00:43, 2276.68 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250460/350343 [01:51<00:43, 2283.27 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250689/350343 [01:51<00:43, 2272.44 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250917/350343 [01:51<00:44, 2237.62 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251145/350343 [01:51<00:44, 2249.61 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251371/350343 [01:51<00:43, 2250.83 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251601/350343 [01:51<00:43, 2264.93 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251828/350343 [01:52<00:43, 2257.42 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252058/350343 [01:52<00:43, 2269.43 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252285/350343 [01:52<00:43, 2267.54 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252512/350343 [01:52<00:45, 2171.76 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252739/350343 [01:52<00:44, 2199.98 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252966/350343 [01:52<00:43, 2220.01 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253192/350343 [01:52<00:43, 2231.66 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253416/350343 [01:52<00:43, 2228.55 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253640/350343 [01:52<00:43, 2227.64 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253870/350343 [01:52<00:42, 2248.91 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254098/350343 [01:53<00:42, 2256.04 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254324/350343 [01:53<00:42, 2250.58 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254550/350343 [01:53<00:42, 2240.62 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254775/350343 [01:53<00:42, 2232.00 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255000/350343 [01:53<00:42, 2237.32 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255229/350343 [01:53<00:42, 2250.79 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255455/350343 [01:53<00:42, 2246.54 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255680/350343 [01:53<00:42, 2246.01 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255905/350343 [01:53<00:42, 2244.11 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256135/350343 [01:54<00:41, 2259.34 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256361/350343 [01:54<00:41, 2252.73 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256589/350343 [01:54<00:41, 2259.66 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256815/350343 [01:54<00:41, 2237.17 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257043/350343 [01:54<00:41, 2249.51 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257268/350343 [01:54<00:41, 2241.74 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257495/350343 [01:54<00:41, 2248.91 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257724/350343 [01:54<00:41, 2258.45 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257950/350343 [01:54<00:41, 2239.70 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 258175/350343 [01:54<00:42, 2173.06 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258393/350343 [01:55<00:43, 2115.20 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258611/350343 [01:55<00:43, 2133.28 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258831/350343 [01:55<00:42, 2152.09 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259059/350343 [01:55<00:41, 2188.75 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259286/350343 [01:55<00:41, 2211.06 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259508/350343 [01:55<00:41, 2210.76 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259737/350343 [01:55<00:40, 2233.07 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259964/350343 [01:55<00:40, 2242.91 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260189/350343 [01:55<00:40, 2225.82 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260412/350343 [01:55<00:40, 2215.73 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260637/350343 [01:56<00:40, 2225.85 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260860/350343 [01:56<00:40, 2226.62 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261083/350343 [01:56<00:40, 2216.74 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261312/350343 [01:56<00:39, 2237.16 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261536/350343 [01:56<00:39, 2235.12 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261763/350343 [01:56<00:39, 2243.38 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261988/350343 [01:56<00:39, 2244.63 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262213/350343 [01:56<00:39, 2241.45 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262438/350343 [01:56<00:39, 2233.78 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262662/350343 [01:56<00:39, 2211.07 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262884/350343 [01:57<00:39, 2202.61 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263105/350343 [01:57<00:39, 2202.41 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263327/350343 [01:57<00:39, 2205.97 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263553/350343 [01:57<00:39, 2221.99 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263778/350343 [01:57<00:38, 2229.22 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264006/350343 [01:57<00:38, 2243.67 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264231/350343 [01:57<00:38, 2232.49 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264455/350343 [01:57<00:38, 2224.87 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264678/350343 [01:57<00:38, 2215.13 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264900/350343 [01:57<00:38, 2212.24 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265122/350343 [01:58<00:38, 2209.82 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265346/350343 [01:58<00:38, 2216.49 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265569/350343 [01:58<00:38, 2219.45 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265794/350343 [01:58<00:37, 2227.77 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266019/350343 [01:58<00:37, 2234.38 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266243/350343 [01:58<00:37, 2225.84 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266473/350343 [01:58<00:37, 2246.94 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266698/350343 [01:58<00:37, 2244.75 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266927/350343 [01:58<00:36, 2257.48 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267153/350343 [01:58<00:37, 2224.56 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267376/350343 [01:59<00:37, 2217.62 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267598/350343 [01:59<00:37, 2210.80 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267821/350343 [01:59<00:37, 2216.02 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268043/350343 [01:59<00:37, 2211.83 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268269/350343 [01:59<00:36, 2224.33 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268497/350343 [01:59<00:36, 2237.93 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268724/350343 [01:59<00:36, 2244.41 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268956/350343 [01:59<00:35, 2265.03 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269183/350343 [01:59<00:36, 2252.32 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269409/350343 [01:59<00:36, 2244.49 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269637/350343 [02:00<00:35, 2254.85 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269863/350343 [02:00<00:35, 2244.13 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270088/350343 [02:00<00:35, 2234.08 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270312/350343 [02:00<00:35, 2229.24 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270536/350343 [02:00<00:35, 2230.63 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270766/350343 [02:00<00:35, 2249.70 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270991/350343 [02:00<00:35, 2248.06 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271217/350343 [02:00<00:35, 2249.64 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271442/350343 [02:00<00:35, 2246.80 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271667/350343 [02:00<00:35, 2222.44 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271892/350343 [02:01<00:35, 2229.02 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272119/350343 [02:01<00:34, 2238.64 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272344/350343 [02:01<00:34, 2240.08 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272569/350343 [02:01<00:34, 2232.92 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272798/350343 [02:01<00:34, 2249.36 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273027/350343 [02:01<00:34, 2259.31 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273253/350343 [02:01<00:34, 2257.90 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273479/350343 [02:01<00:34, 2252.11 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273706/350343 [02:01<00:33, 2256.32 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273932/350343 [02:01<00:34, 2243.44 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274157/350343 [02:02<00:34, 2224.10 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274380/350343 [02:02<00:34, 2215.99 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274603/350343 [02:02<00:34, 2219.31 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274827/350343 [02:02<00:33, 2223.86 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275058/350343 [02:02<00:33, 2248.05 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275286/350343 [02:02<00:33, 2256.97 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275516/350343 [02:02<00:32, 2269.07 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275743/350343 [02:02<00:33, 2258.33 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 275969/350343 [02:02<00:33, 2250.62 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276195/350343 [02:03<00:33, 2237.38 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276419/350343 [02:03<00:33, 2231.22 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276645/350343 [02:03<00:32, 2238.75 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276869/350343 [02:03<00:32, 2233.06 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277093/350343 [02:03<00:32, 2226.87 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277325/350343 [02:03<00:32, 2253.26 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277554/350343 [02:03<00:33, 2175.91 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277777/350343 [02:03<00:33, 2176.54 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278007/350343 [02:03<00:32, 2212.33 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278229/350343 [02:03<00:32, 2212.44 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278453/350343 [02:04<00:32, 2218.03 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278676/350343 [02:04<00:32, 2204.02 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278899/350343 [02:04<00:32, 2211.44 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279121/350343 [02:04<00:32, 2194.71 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279343/350343 [02:04<00:32, 2200.43 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279567/350343 [02:04<00:32, 2210.02 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279794/350343 [02:04<00:31, 2227.42 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280023/350343 [02:04<00:31, 2244.07 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280251/350343 [02:04<00:31, 2254.34 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280477/350343 [02:04<00:32, 2163.48 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280702/350343 [02:05<00:31, 2186.44 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280925/350343 [02:05<00:31, 2198.28 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281147/350343 [02:05<00:31, 2202.99 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281368/350343 [02:05<00:31, 2204.76 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281589/350343 [02:05<00:31, 2198.45 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281813/350343 [02:05<00:31, 2209.35 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282041/350343 [02:05<00:30, 2228.19 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282264/350343 [02:05<00:30, 2225.89 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282488/350343 [02:05<00:30, 2228.27 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282714/350343 [02:05<00:30, 2235.54 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282938/350343 [02:06<00:30, 2234.80 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283162/350343 [02:06<00:30, 2220.68 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283390/350343 [02:06<00:29, 2237.25 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283617/350343 [02:06<00:29, 2244.61 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283844/350343 [02:06<00:29, 2250.48 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284071/350343 [02:06<00:29, 2255.98 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284299/350343 [02:06<00:29, 2263.05 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284526/350343 [02:06<00:29, 2236.32 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284751/350343 [02:06<00:29, 2238.55 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284975/350343 [02:06<00:29, 2236.29 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285202/350343 [02:07<00:29, 2246.20 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285431/350343 [02:07<00:28, 2258.78 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285659/350343 [02:07<00:28, 2264.87 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285889/350343 [02:07<00:28, 2273.92 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286117/350343 [02:07<00:28, 2273.66 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286345/350343 [02:07<00:28, 2265.32 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286574/350343 [02:07<00:28, 2270.33 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286802/350343 [02:07<00:28, 2268.55 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287029/350343 [02:07<00:28, 2241.33 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287254/350343 [02:07<00:28, 2232.96 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287478/350343 [02:08<00:28, 2232.06 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287703/350343 [02:08<00:28, 2235.62 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287928/350343 [02:08<00:27, 2239.69 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288154/350343 [02:08<00:27, 2245.07 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288383/350343 [02:08<00:27, 2257.53 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288611/350343 [02:08<00:27, 2263.73 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288838/350343 [02:08<00:27, 2265.02 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289065/350343 [02:08<00:27, 2257.69 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289291/350343 [02:08<00:27, 2247.27 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289520/350343 [02:08<00:26, 2259.61 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289746/350343 [02:09<00:26, 2245.84 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289971/350343 [02:09<00:27, 2233.35 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290195/350343 [02:09<00:27, 2225.17 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290418/350343 [02:09<00:26, 2223.14 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290644/350343 [02:09<00:26, 2232.03 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290868/350343 [02:09<00:26, 2231.46 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291096/350343 [02:09<00:26, 2244.40 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291321/350343 [02:09<00:26, 2243.11 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291549/350343 [02:09<00:26, 2253.82 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291775/350343 [02:09<00:25, 2254.15 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292005/350343 [02:10<00:25, 2266.70 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292234/350343 [02:10<00:25, 2271.07 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292462/350343 [02:10<00:25, 2270.11 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292690/350343 [02:10<00:25, 2267.97 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292917/350343 [02:10<00:25, 2264.26 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293144/350343 [02:10<00:25, 2258.28 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293370/350343 [02:10<00:25, 2252.98 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293596/350343 [02:10<00:25, 2240.06 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293824/350343 [02:10<00:25, 2251.92 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294050/350343 [02:10<00:25, 2244.28 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294276/350343 [02:11<00:24, 2246.93 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294505/350343 [02:11<00:24, 2258.14 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294731/350343 [02:11<00:24, 2245.61 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294956/350343 [02:11<00:24, 2242.41 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295182/350343 [02:11<00:24, 2245.22 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295410/350343 [02:11<00:24, 2254.36 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295636/350343 [02:11<00:24, 2248.25 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295861/350343 [02:11<00:24, 2230.95 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296085/350343 [02:11<00:24, 2227.46 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296312/350343 [02:11<00:24, 2238.55 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296540/350343 [02:12<00:23, 2249.41 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296766/350343 [02:12<00:23, 2251.72 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296995/350343 [02:12<00:23, 2261.40 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297222/350343 [02:12<00:23, 2254.70 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297452/350343 [02:12<00:24, 2180.60 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297675/350343 [02:12<00:24, 2185.30 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 297897/350343 [02:12<00:24, 2176.99 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298122/350343 [02:12<00:23, 2196.02 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298347/350343 [02:12<00:23, 2211.59 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298569/350343 [02:13<00:23, 2211.61 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298796/350343 [02:13<00:23, 2226.50 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299019/350343 [02:13<00:23, 2216.20 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299241/350343 [02:13<00:23, 2215.79 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299464/350343 [02:13<00:22, 2219.07 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299689/350343 [02:13<00:22, 2227.66 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299916/350343 [02:13<00:22, 2238.31 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300144/350343 [02:13<00:22, 2249.28 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300372/350343 [02:13<00:22, 2257.41 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300601/350343 [02:13<00:21, 2265.60 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300829/350343 [02:14<00:21, 2267.67 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301056/350343 [02:14<00:21, 2266.16 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301284/350343 [02:14<00:21, 2270.12 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301512/350343 [02:14<00:21, 2266.54 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301739/350343 [02:14<00:21, 2267.35 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301968/350343 [02:14<00:21, 2272.04 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302196/350343 [02:14<00:21, 2268.77 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302423/350343 [02:14<00:21, 2264.44 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302654/350343 [02:14<00:20, 2276.34 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302885/350343 [02:14<00:20, 2283.40 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303114/350343 [02:15<00:20, 2274.80 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303345/350343 [02:15<00:20, 2282.21 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303574/350343 [02:15<00:20, 2283.21 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303803/350343 [02:15<00:20, 2265.63 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304030/350343 [02:15<00:20, 2264.33 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304257/350343 [02:15<00:20, 2262.34 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304484/350343 [02:15<00:20, 2255.29 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304710/350343 [02:15<00:20, 2256.52 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304940/350343 [02:15<00:20, 2268.21 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305167/350343 [02:15<00:19, 2267.59 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305394/350343 [02:16<00:19, 2264.68 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305624/350343 [02:16<00:19, 2273.91 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305852/350343 [02:16<00:19, 2274.74 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306080/350343 [02:16<00:19, 2251.61 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306306/350343 [02:16<00:19, 2251.82 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306532/350343 [02:16<00:19, 2244.84 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306757/350343 [02:16<00:19, 2245.52 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306982/350343 [02:16<00:19, 2233.66 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307206/350343 [02:16<00:19, 2231.07 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307430/350343 [02:16<00:19, 2228.49 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307655/350343 [02:17<00:19, 2233.86 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307880/350343 [02:17<00:18, 2238.39 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308106/350343 [02:17<00:18, 2244.39 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308331/350343 [02:17<00:18, 2232.07 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308555/350343 [02:17<00:18, 2224.30 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308786/350343 [02:17<00:18, 2248.34 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309011/350343 [02:17<00:18, 2241.93 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309239/350343 [02:17<00:18, 2253.01 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309465/350343 [02:17<00:18, 2252.23 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309692/350343 [02:17<00:18, 2257.11 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309918/350343 [02:18<00:17, 2256.90 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310148/350343 [02:18<00:17, 2267.52 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310375/350343 [02:18<00:17, 2253.98 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310604/350343 [02:18<00:17, 2263.28 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310831/350343 [02:18<00:17, 2255.37 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311057/350343 [02:18<00:17, 2253.91 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311288/350343 [02:18<00:17, 2269.29 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311515/350343 [02:18<00:17, 2268.66 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311742/350343 [02:18<00:17, 2268.37 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311970/350343 [02:18<00:16, 2270.75 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312198/350343 [02:19<00:16, 2267.01 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312430/350343 [02:19<00:16, 2280.35 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312659/350343 [02:19<00:17, 2179.53 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312882/350343 [02:19<00:17, 2193.98 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313103/350343 [02:19<00:16, 2194.84 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313324/350343 [02:19<00:16, 2198.39 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313551/350343 [02:19<00:16, 2218.13 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313777/350343 [02:19<00:16, 2229.43 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314001/350343 [02:19<00:16, 2232.38 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314229/350343 [02:19<00:16, 2246.20 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314459/350343 [02:20<00:15, 2260.46 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314690/350343 [02:20<00:15, 2272.99 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314918/350343 [02:20<00:15, 2253.35 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 315144/350343 [02:20<00:15, 2243.52 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315370/350343 [02:20<00:15, 2247.06 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315595/350343 [02:20<00:15, 2242.69 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315822/350343 [02:20<00:15, 2249.90 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316048/350343 [02:20<00:15, 2248.73 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316273/350343 [02:20<00:15, 2232.76 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316500/350343 [02:20<00:15, 2242.22 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316727/350343 [02:21<00:14, 2247.94 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316955/350343 [02:21<00:14, 2256.28 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317181/350343 [02:21<00:14, 2246.30 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317412/350343 [02:21<00:14, 2262.76 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317639/350343 [02:21<00:14, 2263.87 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317866/350343 [02:21<00:14, 2254.45 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318095/350343 [02:21<00:14, 2263.54 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318322/350343 [02:21<00:14, 2259.28 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318548/350343 [02:21<00:14, 2258.63 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318779/350343 [02:21<00:13, 2271.51 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319007/350343 [02:22<00:13, 2265.16 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319236/350343 [02:22<00:13, 2272.12 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319464/350343 [02:22<00:13, 2264.79 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319692/350343 [02:22<00:13, 2269.06 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319919/350343 [02:22<00:13, 2263.30 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320149/350343 [02:22<00:13, 2274.18 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320377/350343 [02:22<00:13, 2263.19 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320604/350343 [02:22<00:13, 2249.07 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320830/350343 [02:22<00:13, 2250.62 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321056/350343 [02:22<00:13, 2250.62 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321283/350343 [02:23<00:12, 2255.26 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321509/350343 [02:23<00:12, 2253.67 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321741/350343 [02:23<00:12, 2273.46 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321970/350343 [02:23<00:12, 2276.40 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322198/350343 [02:23<00:12, 2271.07 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322426/350343 [02:23<00:12, 2247.82 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322654/350343 [02:23<00:12, 2255.62 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322880/350343 [02:23<00:12, 2253.88 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323106/350343 [02:23<00:12, 2248.59 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323334/350343 [02:24<00:11, 2256.92 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323561/350343 [02:24<00:11, 2260.73 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323788/350343 [02:24<00:11, 2252.98 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 324014/350343 [02:24<00:11, 2246.30 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324241/350343 [02:24<00:11, 2252.34 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324469/350343 [02:24<00:11, 2259.69 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324697/350343 [02:24<00:11, 2264.09 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324924/350343 [02:24<00:11, 2259.60 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325150/350343 [02:24<00:11, 2250.01 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325376/350343 [02:24<00:11, 2250.94 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325602/350343 [02:25<00:10, 2250.83 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325829/350343 [02:25<00:10, 2256.20 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326058/350343 [02:25<00:10, 2265.03 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326285/350343 [02:25<00:10, 2257.12 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326511/350343 [02:25<00:10, 2237.60 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326735/350343 [02:25<00:10, 2202.17 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326956/350343 [02:25<00:10, 2193.11 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327183/350343 [02:25<00:10, 2214.47 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327409/350343 [02:25<00:10, 2226.60 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327635/350343 [02:25<00:10, 2143.30 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327863/350343 [02:26<00:10, 2181.77 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328093/350343 [02:26<00:10, 2215.21 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328322/350343 [02:26<00:09, 2235.98 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328548/350343 [02:26<00:09, 2241.66 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328776/350343 [02:26<00:09, 2252.23 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329002/350343 [02:26<00:09, 2244.05 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329231/350343 [02:26<00:09, 2257.19 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329457/350343 [02:26<00:09, 2244.74 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329684/350343 [02:26<00:09, 2251.65 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329912/350343 [02:26<00:09, 2259.11 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330139/350343 [02:27<00:08, 2259.94 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330366/350343 [02:27<00:08, 2257.63 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330596/350343 [02:27<00:08, 2268.31 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330824/350343 [02:27<00:08, 2270.03 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331052/350343 [02:27<00:08, 2271.10 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331280/350343 [02:27<00:08, 2258.26 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331506/350343 [02:27<00:08, 2224.33 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331729/350343 [02:27<00:08, 2218.08 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331951/350343 [02:27<00:08, 2218.26 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332173/350343 [02:27<00:08, 2211.37 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332401/350343 [02:28<00:08, 2230.59 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332628/350343 [02:28<00:07, 2240.40 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 332853/350343 [02:28<00:07, 2240.67 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333079/350343 [02:28<00:07, 2244.56 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333304/350343 [02:28<00:07, 2245.78 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333534/350343 [02:28<00:07, 2258.88 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333760/350343 [02:28<00:07, 2255.01 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333988/350343 [02:28<00:07, 2260.23 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334215/350343 [02:28<00:07, 2256.59 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334441/350343 [02:28<00:07, 2253.04 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334667/350343 [02:29<00:06, 2243.97 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334892/350343 [02:29<00:06, 2237.34 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335120/350343 [02:29<00:06, 2248.88 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335348/350343 [02:29<00:06, 2256.34 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335576/350343 [02:29<00:06, 2261.68 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335805/350343 [02:29<00:06, 2269.82 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336032/350343 [02:29<00:06, 2269.40 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336263/350343 [02:29<00:06, 2280.45 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336492/350343 [02:29<00:06, 2269.76 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336721/350343 [02:29<00:05, 2274.01 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336949/350343 [02:30<00:05, 2264.91 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 337177/350343 [02:30<00:05, 2269.05 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337404/350343 [02:30<00:05, 2269.25 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337631/350343 [02:30<00:05, 2263.96 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337862/350343 [02:30<00:05, 2276.08 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338090/350343 [02:30<00:05, 2264.75 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338320/350343 [02:30<00:05, 2274.53 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338548/350343 [02:30<00:05, 2257.92 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338774/350343 [02:30<00:05, 2258.40 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339003/350343 [02:30<00:05, 2265.32 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339231/350343 [02:31<00:04, 2269.59 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339461/350343 [02:31<00:04, 2276.58 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339690/350343 [02:31<00:04, 2279.72 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339918/350343 [02:31<00:04, 2274.38 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340146/350343 [02:31<00:04, 2271.79 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340374/350343 [02:31<00:04, 2266.67 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340602/350343 [02:31<00:04, 2268.38 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340829/350343 [02:31<00:04, 2254.90 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341056/350343 [02:31<00:04, 2171.40 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341280/350343 [02:31<00:04, 2191.17 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341506/350343 [02:32<00:03, 2210.50 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341734/350343 [02:32<00:03, 2228.42 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341962/350343 [02:32<00:03, 2243.18 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342188/350343 [02:32<00:03, 2247.15 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342413/350343 [02:32<00:03, 2247.43 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342642/350343 [02:32<00:03, 2257.75 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342868/350343 [02:32<00:03, 2249.34 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343094/350343 [02:32<00:03, 2251.57 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343320/350343 [02:32<00:03, 2244.22 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343552/350343 [02:32<00:02, 2264.63 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343779/350343 [02:33<00:02, 2261.00 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344010/350343 [02:33<00:02, 2274.35 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344238/350343 [02:33<00:02, 2254.90 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344464/350343 [02:33<00:02, 2228.87 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344687/350343 [02:33<00:02, 2217.08 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344915/350343 [02:33<00:02, 2234.03 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345142/350343 [02:33<00:02, 2243.20 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345371/350343 [02:33<00:02, 2256.77 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345599/350343 [02:33<00:02, 2261.29 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345826/350343 [02:34<00:01, 2263.30 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346058/350343 [02:34<00:01, 2278.18 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346290/350343 [02:34<00:01, 2289.71 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346519/350343 [02:34<00:01, 2279.17 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346748/350343 [02:34<00:01, 2282.31 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346977/350343 [02:34<00:01, 2278.14 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347205/350343 [02:34<00:01, 2274.93 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347433/350343 [02:34<00:01, 2273.19 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347661/350343 [02:34<00:01, 2261.47 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347888/350343 [02:34<00:01, 2253.04 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348114/350343 [02:35<00:01, 1961.47 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348317/350343 [02:35<00:01, 1844.79 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348516/350343 [02:35<00:00, 1882.90 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348729/350343 [02:35<00:00, 1949.97 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348954/350343 [02:35<00:00, 2033.56 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349169/350343 [02:35<00:00, 2066.61 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349378/350343 [02:35<00:00, 2018.31 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349583/350343 [02:35<00:00, 2025.89 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349806/350343 [02:35<00:00, 2083.08 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350020/350343 [02:35<00:00, 2097.94 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350248/350343 [02:36<00:00, 2150.71 examples/s][A
                                                                                              [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:   0%|          | 1/350343 [00:00<42:17:43,  2.30 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:   4%|â–         | 13876/350343 [00:00<00:09, 34325.45 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:   8%|â–Š         | 28505/350343 [00:00<00:05, 63559.64 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  12%|â–ˆâ–        | 43238/350343 [00:00<00:03, 86315.90 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  17%|â–ˆâ–‹        | 58124/350343 [00:00<00:02, 103800.86 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  21%|â–ˆâ–ˆ        | 72975/350343 [00:00<00:02, 116557.30 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  25%|â–ˆâ–ˆâ–Œ       | 87869/350343 [00:01<00:02, 125931.65 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  29%|â–ˆâ–ˆâ–‰       | 102819/350343 [00:01<00:01, 132821.07 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117267/350343 [00:01<00:01, 130937.01 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131168/350343 [00:01<00:01, 131038.24 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146140/350343 [00:01<00:01, 136418.54 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161144/350343 [00:01<00:01, 140388.39 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176170/350343 [00:01<00:01, 143286.55 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191271/350343 [00:01<00:01, 145568.03 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206397/350343 [00:01<00:00, 147256.44 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221468/350343 [00:01<00:00, 148282.43 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236567/350343 [00:02<00:00, 149088.13 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251534/350343 [00:02<00:00, 149149.76 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266709/350343 [00:02<00:00, 149924.60 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281879/350343 [00:02<00:00, 150452.51 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297041/350343 [00:02<00:00, 150799.92 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312172/350343 [00:02<00:00, 150942.80 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327396/350343 [00:02<00:00, 151325.86 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342655/350343 [00:02<00:00, 151702.65 examples/s][A
                                                                                                                                                            [AI0305 10:09:05.264518 140274064205632 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-train.tfrecord*. Number of examples: 350343 (shards: [43793, 43793, 43793, 43793, 43792, 43793, 43793, 43793])
Generating splits...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:39<05:18, 159.06s/ splits]
Generating validation examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating validation examples...:   0%|          | 108/43793 [00:00<00:48, 903.55 examples/s][A
Generating validation examples...:   1%|          | 330/43793 [00:00<00:27, 1580.57 examples/s][A
Generating validation examples...:   1%|â–         | 552/43793 [00:00<00:23, 1840.93 examples/s][A
Generating validation examples...:   2%|â–         | 774/43793 [00:00<00:21, 1965.42 examples/s][A
Generating validation examples...:   2%|â–         | 996/43793 [00:00<00:21, 2035.25 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1218/43793 [00:00<00:20, 2077.48 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1440/43793 [00:00<00:20, 2107.49 examples/s][A
Generating validation examples...:   4%|â–         | 1662/43793 [00:00<00:19, 2114.99 examples/s][A
Generating validation examples...:   4%|â–         | 1885/43793 [00:00<00:19, 2122.42 examples/s][A
Generating validation examples...:   5%|â–         | 2107/43793 [00:01<00:19, 2135.76 examples/s][A
Generating validation examples...:   5%|â–Œ         | 2329/43793 [00:01<00:19, 2119.35 examples/s][A
Generating validation examples...:   6%|â–Œ         | 2552/43793 [00:01<00:19, 2133.86 examples/s][A
Generating validation examples...:   6%|â–‹         | 2773/43793 [00:01<00:19, 2146.02 examples/s][A
Generating validation examples...:   7%|â–‹         | 2997/43793 [00:01<00:18, 2160.25 examples/s][A
Generating validation examples...:   7%|â–‹         | 3219/43793 [00:01<00:18, 2166.62 examples/s][A
Generating validation examples...:   8%|â–Š         | 3441/43793 [00:01<00:18, 2151.22 examples/s][A
Generating validation examples...:   8%|â–Š         | 3663/43793 [00:01<00:18, 2150.73 examples/s][A
Generating validation examples...:   9%|â–‰         | 3886/43793 [00:01<00:18, 2160.36 examples/s][A
Generating validation examples...:   9%|â–‰         | 4108/43793 [00:01<00:18, 2155.77 examples/s][A
Generating validation examples...:  10%|â–‰         | 4330/43793 [00:02<00:18, 2142.15 examples/s][A
Generating validation examples...:  10%|â–ˆ         | 4552/43793 [00:02<00:18, 2145.34 examples/s][A
Generating validation examples...:  11%|â–ˆ         | 4774/43793 [00:02<00:18, 2145.56 examples/s][A
Generating validation examples...:  11%|â–ˆâ–        | 4997/43793 [00:02<00:17, 2155.91 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5220/43793 [00:02<00:17, 2169.93 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5443/43793 [00:02<00:17, 2173.25 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5665/43793 [00:02<00:17, 2178.08 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5888/43793 [00:02<00:17, 2176.46 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6109/43793 [00:02<00:17, 2164.97 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6330/43793 [00:03<00:17, 2158.72 examples/s][A
Generating validation examples...:  15%|â–ˆâ–        | 6553/43793 [00:03<00:17, 2165.60 examples/s][A
Generating validation examples...:  15%|â–ˆâ–Œ        | 6777/43793 [00:03<00:17, 2175.11 examples/s][A
Generating validation examples...:  16%|â–ˆâ–Œ        | 7000/43793 [00:03<00:16, 2172.79 examples/s][A
Generating validation examples...:  16%|â–ˆâ–‹        | 7223/43793 [00:03<00:16, 2178.38 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7447/43793 [00:03<00:16, 2190.18 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7669/43793 [00:03<00:16, 2180.39 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7891/43793 [00:03<00:16, 2175.59 examples/s][A
Generating validation examples...:  19%|â–ˆâ–Š        | 8113/43793 [00:03<00:16, 2165.96 examples/s][A
Generating validation examples...:  19%|â–ˆâ–‰        | 8335/43793 [00:03<00:16, 2161.58 examples/s][A
Generating validation examples...:  20%|â–ˆâ–‰        | 8558/43793 [00:04<00:16, 2166.84 examples/s][A
Generating validation examples...:  20%|â–ˆâ–ˆ        | 8780/43793 [00:04<00:16, 2165.61 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9002/43793 [00:04<00:16, 2157.18 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9224/43793 [00:04<00:16, 2152.43 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9446/43793 [00:04<00:15, 2156.65 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9669/43793 [00:04<00:15, 2160.41 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 9892/43793 [00:04<00:15, 2175.58 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 10114/43793 [00:04<00:15, 2174.95 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–Ž       | 10338/43793 [00:04<00:15, 2175.48 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–       | 10561/43793 [00:04<00:15, 2178.17 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–       | 10783/43793 [00:05<00:15, 2167.99 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–Œ       | 11005/43793 [00:05<00:15, 2165.71 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11227/43793 [00:05<00:15, 2162.23 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11450/43793 [00:05<00:14, 2168.81 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11672/43793 [00:05<00:14, 2174.51 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11894/43793 [00:05<00:14, 2177.77 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12117/43793 [00:05<00:14, 2177.18 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12341/43793 [00:05<00:14, 2189.06 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–Š       | 12561/43793 [00:05<00:14, 2175.77 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–‰       | 12783/43793 [00:05<00:14, 2176.23 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–‰       | 13007/43793 [00:06<00:14, 2181.01 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13229/43793 [00:06<00:13, 2187.88 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13452/43793 [00:06<00:13, 2191.03 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13674/43793 [00:06<00:13, 2181.45 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13897/43793 [00:06<00:13, 2186.34 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14119/43793 [00:06<00:13, 2183.54 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14342/43793 [00:06<00:13, 2188.72 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14563/43793 [00:06<00:13, 2177.09 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14786/43793 [00:06<00:13, 2179.37 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 15009/43793 [00:06<00:13, 2181.32 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15232/43793 [00:07<00:13, 2184.37 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15454/43793 [00:07<00:13, 2165.20 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15676/43793 [00:07<00:12, 2175.72 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 15899/43793 [00:07<00:12, 2176.89 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16121/43793 [00:07<00:12, 2177.59 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16344/43793 [00:07<00:12, 2181.50 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16567/43793 [00:07<00:12, 2164.66 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16789/43793 [00:07<00:12, 2159.40 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17012/43793 [00:07<00:12, 2160.59 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17235/43793 [00:08<00:12, 2150.26 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17457/43793 [00:08<00:12, 2145.11 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17679/43793 [00:08<00:12, 2137.53 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17901/43793 [00:08<00:12, 2134.77 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18124/43793 [00:08<00:12, 2133.60 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18347/43793 [00:08<00:11, 2141.12 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18570/43793 [00:08<00:11, 2150.71 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18792/43793 [00:08<00:11, 2146.74 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19015/43793 [00:08<00:11, 2141.30 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19238/43793 [00:08<00:11, 2142.90 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19459/43793 [00:09<00:11, 2126.60 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19681/43793 [00:09<00:11, 2128.77 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19903/43793 [00:09<00:11, 2119.46 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20126/43793 [00:09<00:11, 2121.22 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20348/43793 [00:09<00:11, 2128.08 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20569/43793 [00:09<00:10, 2132.75 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20792/43793 [00:09<00:10, 2148.63 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21014/43793 [00:09<00:10, 2157.13 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21235/43793 [00:09<00:10, 2167.44 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21457/43793 [00:09<00:10, 2176.85 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21679/43793 [00:10<00:10, 2178.13 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21902/43793 [00:10<00:09, 2190.13 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22124/43793 [00:10<00:09, 2189.94 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22346/43793 [00:10<00:09, 2188.18 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22568/43793 [00:10<00:09, 2186.18 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22789/43793 [00:10<00:09, 2173.76 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23011/43793 [00:10<00:09, 2176.88 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23233/43793 [00:10<00:09, 2183.63 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23454/43793 [00:10<00:09, 2174.43 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23676/43793 [00:11<00:09, 2165.78 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23898/43793 [00:11<00:09, 2178.60 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24121/43793 [00:11<00:09, 2184.84 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24344/43793 [00:11<00:08, 2178.07 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24566/43793 [00:11<00:08, 2168.27 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24788/43793 [00:11<00:08, 2168.85 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25009/43793 [00:11<00:08, 2162.89 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25233/43793 [00:11<00:08, 2180.87 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25455/43793 [00:11<00:08, 2167.37 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25677/43793 [00:11<00:08, 2177.63 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25897/43793 [00:12<00:08, 2167.99 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26118/43793 [00:12<00:08, 2169.03 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26340/43793 [00:12<00:08, 2172.62 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26562/43793 [00:12<00:07, 2184.15 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26784/43793 [00:12<00:07, 2192.96 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27006/43793 [00:12<00:07, 2191.34 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27229/43793 [00:12<00:07, 2201.51 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27453/43793 [00:12<00:07, 2210.54 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27675/43793 [00:12<00:07, 2208.25 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27898/43793 [00:12<00:07, 2213.89 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28120/43793 [00:13<00:07, 2179.00 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28339/43793 [00:13<00:07, 2170.93 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28561/43793 [00:13<00:06, 2178.42 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28784/43793 [00:13<00:06, 2184.91 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 29006/43793 [00:13<00:06, 2185.94 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29228/43793 [00:13<00:06, 2195.38 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29450/43793 [00:13<00:06, 2184.01 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29672/43793 [00:13<00:06, 2187.83 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29895/43793 [00:13<00:06, 2186.21 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30116/43793 [00:13<00:06, 2182.08 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30339/43793 [00:14<00:06, 2185.72 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30559/43793 [00:14<00:06, 2155.96 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30782/43793 [00:14<00:06, 2165.62 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31005/43793 [00:14<00:05, 2176.35 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31227/43793 [00:14<00:05, 2171.26 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31448/43793 [00:14<00:05, 2160.45 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31669/43793 [00:14<00:05, 2158.79 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31893/43793 [00:14<00:05, 2182.10 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32115/43793 [00:14<00:05, 2183.31 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32337/43793 [00:14<00:05, 2186.21 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32560/43793 [00:15<00:05, 2193.68 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32783/43793 [00:15<00:04, 2202.21 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33004/43793 [00:15<00:04, 2184.75 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33225/43793 [00:15<00:04, 2184.77 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33447/43793 [00:15<00:04, 2191.01 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33669/43793 [00:15<00:04, 2160.66 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33891/43793 [00:15<00:04, 2169.72 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34114/43793 [00:15<00:04, 2177.98 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34336/43793 [00:15<00:04, 2175.99 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34558/43793 [00:16<00:04, 2150.30 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34781/43793 [00:16<00:04, 2144.43 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 35003/43793 [00:16<00:04, 2151.74 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35224/43793 [00:16<00:04, 2130.18 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35446/43793 [00:16<00:03, 2125.35 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35667/43793 [00:16<00:03, 2127.97 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35890/43793 [00:16<00:03, 2135.81 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36112/43793 [00:16<00:03, 2077.89 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36334/43793 [00:16<00:03, 2087.78 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36556/43793 [00:16<00:03, 2101.50 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36777/43793 [00:17<00:03, 2115.19 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36999/43793 [00:17<00:03, 2132.85 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37221/43793 [00:17<00:03, 2143.98 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37443/43793 [00:17<00:02, 2156.70 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37665/43793 [00:17<00:02, 2166.20 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37887/43793 [00:17<00:02, 2162.38 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38110/43793 [00:17<00:02, 2166.05 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38332/43793 [00:17<00:02, 2170.90 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38554/43793 [00:17<00:02, 2176.95 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38775/43793 [00:17<00:02, 2169.46 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 38997/43793 [00:18<00:02, 2172.35 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39218/43793 [00:18<00:02, 2173.20 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39441/43793 [00:18<00:01, 2182.33 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39663/43793 [00:18<00:01, 2189.32 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39886/43793 [00:18<00:01, 2193.80 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40108/43793 [00:18<00:01, 2199.17 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40330/43793 [00:18<00:01, 2196.97 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40552/43793 [00:18<00:01, 2196.60 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40774/43793 [00:18<00:01, 2191.03 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40996/43793 [00:18<00:01, 2194.32 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41219/43793 [00:19<00:01, 2194.47 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41440/43793 [00:19<00:01, 2186.07 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41661/43793 [00:19<00:00, 2166.36 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41883/43793 [00:19<00:00, 2175.44 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42105/43793 [00:19<00:00, 2181.31 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42328/43793 [00:19<00:00, 2184.92 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42549/43793 [00:19<00:00, 2173.75 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42771/43793 [00:19<00:00, 2183.18 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42993/43793 [00:19<00:00, 2189.87 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43215/43793 [00:20<00:00, 2190.84 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43440/43793 [00:20<00:00, 2205.38 examples/s][A
Generating validation examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43661/43793 [00:20<00:00, 2191.81 examples/s][A
                                                                                                 [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-validation.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-validation.tfrecord*...:  23%|â–ˆâ–ˆâ–Ž       | 10129/43793 [00:00<00:00, 101280.92 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-validation.tfrecord*...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25164/43793 [00:00<00:00, 130134.42 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-validation.tfrecord*...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40269/43793 [00:00<00:00, 139679.08 examples/s][A
                                                                                                                                                               [AI0305 10:09:25.939879 140274064205632 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-validation.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:59<01:17, 77.61s/ splits] 
Generating test examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating test examples...:   0%|          | 157/43793 [00:00<00:27, 1569.58 examples/s][A
Generating test examples...:   1%|          | 369/43793 [00:00<00:22, 1890.52 examples/s][A
Generating test examples...:   1%|â–         | 586/43793 [00:00<00:21, 2017.82 examples/s][A
Generating test examples...:   2%|â–         | 803/43793 [00:00<00:20, 2074.88 examples/s][A
Generating test examples...:   2%|â–         | 1023/43793 [00:00<00:20, 2119.52 examples/s][A
Generating test examples...:   3%|â–Ž         | 1243/43793 [00:00<00:19, 2145.78 examples/s][A
Generating test examples...:   3%|â–Ž         | 1461/43793 [00:00<00:19, 2156.64 examples/s][A
Generating test examples...:   4%|â–         | 1682/43793 [00:00<00:19, 2172.71 examples/s][A
Generating test examples...:   4%|â–         | 1904/43793 [00:00<00:19, 2185.95 examples/s][A
Generating test examples...:   5%|â–         | 2126/43793 [00:01<00:18, 2193.88 examples/s][A
Generating test examples...:   5%|â–Œ         | 2346/43793 [00:01<00:18, 2195.71 examples/s][A
Generating test examples...:   6%|â–Œ         | 2566/43793 [00:01<00:19, 2157.02 examples/s][A
Generating test examples...:   6%|â–‹         | 2790/43793 [00:01<00:18, 2181.76 examples/s][A
Generating test examples...:   7%|â–‹         | 3012/43793 [00:01<00:18, 2192.15 examples/s][A
Generating test examples...:   7%|â–‹         | 3233/43793 [00:01<00:18, 2194.04 examples/s][A
Generating test examples...:   8%|â–Š         | 3457/43793 [00:01<00:18, 2206.21 examples/s][A
Generating test examples...:   8%|â–Š         | 3678/43793 [00:01<00:18, 2199.64 examples/s][A
Generating test examples...:   9%|â–‰         | 3899/43793 [00:01<00:18, 2195.79 examples/s][A
Generating test examples...:   9%|â–‰         | 4119/43793 [00:01<00:18, 2187.16 examples/s][A
Generating test examples...:  10%|â–‰         | 4338/43793 [00:02<00:18, 2184.86 examples/s][A
Generating test examples...:  10%|â–ˆ         | 4558/43793 [00:02<00:17, 2187.92 examples/s][A
Generating test examples...:  11%|â–ˆ         | 4782/43793 [00:02<00:17, 2200.93 examples/s][A
Generating test examples...:  11%|â–ˆâ–        | 5005/43793 [00:02<00:17, 2209.44 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5230/43793 [00:02<00:17, 2219.72 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5452/43793 [00:02<00:17, 2198.83 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5672/43793 [00:02<00:17, 2188.12 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5894/43793 [00:02<00:17, 2196.01 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6114/43793 [00:02<00:17, 2184.78 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6333/43793 [00:02<00:17, 2183.65 examples/s][A
Generating test examples...:  15%|â–ˆâ–        | 6552/43793 [00:03<00:17, 2183.93 examples/s][A
Generating test examples...:  15%|â–ˆâ–Œ        | 6771/43793 [00:03<00:17, 2171.46 examples/s][A
Generating test examples...:  16%|â–ˆâ–Œ        | 6994/43793 [00:03<00:16, 2188.71 examples/s][A
Generating test examples...:  16%|â–ˆâ–‹        | 7213/43793 [00:03<00:16, 2182.32 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7438/43793 [00:03<00:16, 2199.69 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7658/43793 [00:03<00:16, 2194.10 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7885/43793 [00:03<00:16, 2216.19 examples/s][A
Generating test examples...:  19%|â–ˆâ–Š        | 8107/43793 [00:03<00:16, 2205.33 examples/s][A
Generating test examples...:  19%|â–ˆâ–‰        | 8328/43793 [00:03<00:16, 2194.42 examples/s][A
Generating test examples...:  20%|â–ˆâ–‰        | 8548/43793 [00:03<00:16, 2191.40 examples/s][A
Generating test examples...:  20%|â–ˆâ–ˆ        | 8768/43793 [00:04<00:15, 2191.67 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 8989/43793 [00:04<00:15, 2197.07 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 9212/43793 [00:04<00:15, 2204.88 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9434/43793 [00:04<00:15, 2207.00 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9658/43793 [00:04<00:15, 2216.78 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 9880/43793 [00:04<00:15, 2212.81 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 10102/43793 [00:04<00:15, 2214.19 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–Ž       | 10324/43793 [00:04<00:15, 2215.13 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–       | 10546/43793 [00:04<00:15, 2211.05 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–       | 10768/43793 [00:04<00:14, 2206.81 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–Œ       | 10989/43793 [00:05<00:14, 2203.09 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11210/43793 [00:05<00:15, 2170.12 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11428/43793 [00:05<00:14, 2169.86 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11646/43793 [00:05<00:14, 2172.42 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11869/43793 [00:05<00:14, 2187.54 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12088/43793 [00:05<00:14, 2187.98 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12309/43793 [00:05<00:14, 2193.66 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–Š       | 12529/43793 [00:05<00:14, 2194.44 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–‰       | 12750/43793 [00:05<00:14, 2193.02 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–‰       | 12971/43793 [00:05<00:14, 2191.24 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13192/43793 [00:06<00:14, 2183.45 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13416/43793 [00:06<00:13, 2196.23 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13636/43793 [00:06<00:13, 2193.63 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13859/43793 [00:06<00:13, 2198.79 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14080/43793 [00:06<00:13, 2189.95 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14303/43793 [00:06<00:13, 2196.68 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14526/43793 [00:06<00:13, 2204.94 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 14747/43793 [00:06<00:13, 2189.82 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14968/43793 [00:06<00:13, 2188.55 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15190/43793 [00:06<00:13, 2160.26 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15410/43793 [00:07<00:13, 2148.59 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15632/43793 [00:07<00:13, 2147.39 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15854/43793 [00:07<00:13, 2144.33 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16078/43793 [00:07<00:12, 2171.82 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16298/43793 [00:07<00:12, 2163.02 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16521/43793 [00:07<00:12, 2176.50 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16742/43793 [00:07<00:12, 2181.04 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 16963/43793 [00:07<00:12, 2171.05 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17185/43793 [00:07<00:12, 2171.63 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17406/43793 [00:07<00:12, 2158.93 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17629/43793 [00:08<00:12, 2163.31 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17852/43793 [00:08<00:11, 2175.61 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18074/43793 [00:08<00:11, 2178.40 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18296/43793 [00:08<00:11, 2181.00 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18518/43793 [00:08<00:11, 2181.55 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18740/43793 [00:08<00:11, 2176.58 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18963/43793 [00:08<00:11, 2169.63 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19187/43793 [00:08<00:11, 2189.88 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19408/43793 [00:08<00:11, 2186.13 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19630/43793 [00:09<00:11, 2181.67 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19851/43793 [00:09<00:11, 2170.33 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20074/43793 [00:09<00:10, 2187.55 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20296/43793 [00:09<00:10, 2184.57 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20517/43793 [00:09<00:10, 2181.04 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20739/43793 [00:09<00:10, 2189.23 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20961/43793 [00:09<00:10, 2181.75 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21183/43793 [00:09<00:10, 2187.69 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21406/43793 [00:09<00:10, 2185.98 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21630/43793 [00:09<00:10, 2200.38 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21852/43793 [00:10<00:09, 2205.55 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22077/43793 [00:10<00:09, 2216.13 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22299/43793 [00:10<00:09, 2214.20 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22523/43793 [00:10<00:09, 2219.79 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22745/43793 [00:10<00:09, 2211.01 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22969/43793 [00:10<00:09, 2217.60 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23191/43793 [00:10<00:09, 2195.02 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23411/43793 [00:10<00:09, 2185.86 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23631/43793 [00:10<00:09, 2186.40 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23853/43793 [00:10<00:09, 2192.15 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24076/43793 [00:11<00:09, 2190.78 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24298/43793 [00:11<00:08, 2182.92 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24521/43793 [00:11<00:08, 2188.59 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24744/43793 [00:11<00:08, 2182.13 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24966/43793 [00:11<00:08, 2179.56 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25189/43793 [00:11<00:08, 2188.04 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25411/43793 [00:11<00:08, 2187.17 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25632/43793 [00:11<00:08, 2178.89 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25854/43793 [00:11<00:08, 2178.70 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26077/43793 [00:11<00:08, 2179.19 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26299/43793 [00:12<00:07, 2188.78 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26521/43793 [00:12<00:07, 2163.37 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26743/43793 [00:12<00:07, 2161.49 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26966/43793 [00:12<00:07, 2161.54 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27187/43793 [00:12<00:07, 2165.35 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27410/43793 [00:12<00:07, 2181.12 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27632/43793 [00:12<00:07, 2174.61 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27853/43793 [00:12<00:07, 2177.38 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28075/43793 [00:12<00:07, 2179.15 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28297/43793 [00:12<00:07, 2183.56 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28519/43793 [00:13<00:06, 2182.10 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28740/43793 [00:13<00:06, 2163.58 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28961/43793 [00:13<00:06, 2165.04 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29183/43793 [00:13<00:06, 2166.48 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29404/43793 [00:13<00:06, 2178.72 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29627/43793 [00:13<00:06, 2186.92 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29848/43793 [00:13<00:06, 2176.55 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 30071/43793 [00:13<00:06, 2167.40 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30293/43793 [00:13<00:06, 2160.56 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30515/43793 [00:13<00:06, 2165.60 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30737/43793 [00:14<00:05, 2176.82 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30959/43793 [00:14<00:05, 2181.00 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31181/43793 [00:14<00:05, 2175.13 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31403/43793 [00:14<00:05, 2177.00 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31624/43793 [00:14<00:05, 2179.96 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31846/43793 [00:14<00:05, 2184.21 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32069/43793 [00:14<00:05, 2190.05 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32291/43793 [00:14<00:05, 2191.27 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32513/43793 [00:14<00:05, 2177.57 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32735/43793 [00:15<00:05, 2170.75 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32957/43793 [00:15<00:04, 2171.15 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33179/43793 [00:15<00:04, 2167.71 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33401/43793 [00:15<00:04, 2167.76 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33622/43793 [00:15<00:04, 2159.07 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33844/43793 [00:15<00:04, 2168.19 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34066/43793 [00:15<00:04, 2168.86 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34287/43793 [00:15<00:04, 2161.27 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34510/43793 [00:15<00:04, 2180.16 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34732/43793 [00:15<00:04, 2185.25 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34954/43793 [00:16<00:04, 2183.86 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35176/43793 [00:16<00:03, 2162.76 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35398/43793 [00:16<00:03, 2162.87 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35620/43793 [00:16<00:03, 2158.72 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35843/43793 [00:16<00:03, 2165.56 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36067/43793 [00:16<00:03, 2178.35 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36289/43793 [00:16<00:03, 2184.25 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36511/43793 [00:16<00:03, 2176.40 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36733/43793 [00:16<00:03, 2181.81 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36955/43793 [00:16<00:03, 2176.12 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37177/43793 [00:17<00:03, 2171.59 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37400/43793 [00:17<00:02, 2169.66 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37621/43793 [00:17<00:02, 2164.18 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37844/43793 [00:17<00:02, 2168.58 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38066/43793 [00:17<00:02, 2176.10 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38288/43793 [00:17<00:02, 2174.80 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38510/43793 [00:17<00:02, 2171.73 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38733/43793 [00:17<00:02, 2177.94 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 38955/43793 [00:17<00:02, 2184.35 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39177/43793 [00:17<00:02, 2165.81 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39400/43793 [00:18<00:02, 2166.28 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39623/43793 [00:18<00:01, 2168.21 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39845/43793 [00:18<00:01, 2158.48 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40067/43793 [00:18<00:01, 2155.34 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40289/43793 [00:18<00:01, 2155.22 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40511/43793 [00:18<00:01, 2159.54 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40733/43793 [00:18<00:01, 2161.93 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40956/43793 [00:18<00:01, 2166.75 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41177/43793 [00:18<00:01, 2168.25 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41398/43793 [00:19<00:01, 2158.78 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41618/43793 [00:19<00:01, 2128.84 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41840/43793 [00:19<00:00, 2139.60 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42062/43793 [00:19<00:00, 2140.67 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42284/43793 [00:19<00:00, 2136.32 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42506/43793 [00:19<00:00, 2144.41 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42729/43793 [00:19<00:00, 2152.31 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42950/43793 [00:19<00:00, 2150.57 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43172/43793 [00:19<00:00, 2152.21 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43393/43793 [00:19<00:00, 2134.40 examples/s][A
Generating test examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43615/43793 [00:20<00:00, 2149.10 examples/s][A
                                                                                           [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-test.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-test.tfrecord*...:  19%|â–ˆâ–‰        | 8275/43793 [00:00<00:00, 82740.81 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-test.tfrecord*...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20870/43793 [00:00<00:00, 108152.33 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-test.tfrecord*...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33540/43793 [00:00<00:00, 116614.76 examples/s][A
                                                                                                                                                         [AI0305 10:09:46.461599 140274064205632 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQ3V0S4/ogbg_molpcba-test.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:20<00:00, 51.55s/ splits]                                                                        I0305 10:09:46.550973 140274064205632 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /root/data/ogbg_molpcba/0.1.3
I0305 10:11:24.369708 140274064205632 submission_runner.py:411] Time since start: 557.47s, 	Step: 1, 	{'train/accuracy': 0.4663219749927521, 'train/loss': 0.7589666843414307, 'train/mean_average_precision': 0.024637641742493905, 'validation/accuracy': 0.4651613235473633, 'validation/loss': 0.755351185798645, 'validation/mean_average_precision': 0.028880314867671546, 'validation/num_examples': 43793, 'test/accuracy': 0.4661603569984436, 'test/loss': 0.7539003491401672, 'test/mean_average_precision': 0.02953375112836849, 'test/num_examples': 43793, 'score': 20.478039264678955, 'total_duration': 557.4664070606232, 'accumulated_submission_time': 20.478039264678955, 'accumulated_eval_time': 536.988322019577, 'accumulated_logging_time': 0}
I0305 10:11:24.388189 140105252910848 logging_writer.py:48] [1] accumulated_eval_time=536.988322, accumulated_logging_time=0, accumulated_submission_time=20.478039, global_step=1, preemption_count=0, score=20.478039, test/accuracy=0.466160, test/loss=0.753900, test/mean_average_precision=0.029534, test/num_examples=43793, total_duration=557.466407, train/accuracy=0.466322, train/loss=0.758967, train/mean_average_precision=0.024638, validation/accuracy=0.465161, validation/loss=0.755351, validation/mean_average_precision=0.028880, validation/num_examples=43793
I0305 10:11:56.034251 140106455312128 logging_writer.py:48] [100] global_step=100, grad_norm=0.3174622356891632, loss=0.2850150167942047
I0305 10:12:27.877205 140105252910848 logging_writer.py:48] [200] global_step=200, grad_norm=0.1017189621925354, loss=0.11562753468751907
I0305 10:12:59.658321 140106455312128 logging_writer.py:48] [300] global_step=300, grad_norm=0.03350352495908737, loss=0.07099892199039459
I0305 10:13:31.103046 140105252910848 logging_writer.py:48] [400] global_step=400, grad_norm=0.01703055575489998, loss=0.06206027790904045
I0305 10:14:02.805278 140106455312128 logging_writer.py:48] [500] global_step=500, grad_norm=0.016552269458770752, loss=0.06549001485109329
I0305 10:14:33.956321 140105252910848 logging_writer.py:48] [600] global_step=600, grad_norm=0.011499420739710331, loss=0.046450816094875336
I0305 10:15:05.300368 140106455312128 logging_writer.py:48] [700] global_step=700, grad_norm=0.048181112855672836, loss=0.057810988277196884
I0305 10:15:24.408756 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:17:14.066803 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:17:17.092876 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:17:20.014079 140274064205632 submission_runner.py:411] Time since start: 913.11s, 	Step: 762, 	{'train/accuracy': 0.9867117404937744, 'train/loss': 0.05126923322677612, 'train/mean_average_precision': 0.05497621842508563, 'validation/accuracy': 0.9842259287834167, 'validation/loss': 0.06067381054162979, 'validation/mean_average_precision': 0.05492517184066116, 'validation/num_examples': 43793, 'test/accuracy': 0.9832456707954407, 'test/loss': 0.06395905464887619, 'test/mean_average_precision': 0.05638866048721417, 'test/num_examples': 43793, 'score': 260.46736550331116, 'total_duration': 913.1106786727905, 'accumulated_submission_time': 260.46736550331116, 'accumulated_eval_time': 652.5934982299805, 'accumulated_logging_time': 0.029597997665405273}
I0305 10:17:20.028768 140106545223424 logging_writer.py:48] [762] accumulated_eval_time=652.593498, accumulated_logging_time=0.029598, accumulated_submission_time=260.467366, global_step=762, preemption_count=0, score=260.467366, test/accuracy=0.983246, test/loss=0.063959, test/mean_average_precision=0.056389, test/num_examples=43793, total_duration=913.110679, train/accuracy=0.986712, train/loss=0.051269, train/mean_average_precision=0.054976, validation/accuracy=0.984226, validation/loss=0.060674, validation/mean_average_precision=0.054925, validation/num_examples=43793
I0305 10:17:32.414391 140106748385024 logging_writer.py:48] [800] global_step=800, grad_norm=0.032954443246126175, loss=0.05619414895772934
I0305 10:18:03.929796 140106545223424 logging_writer.py:48] [900] global_step=900, grad_norm=0.012765742838382721, loss=0.053348470479249954
I0305 10:18:35.387388 140106748385024 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.047234661877155304, loss=0.05254364386200905
I0305 10:19:06.755227 140106545223424 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.03228483721613884, loss=0.038926225155591965
I0305 10:19:38.120429 140106748385024 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.022336695343255997, loss=0.045801546424627304
I0305 10:20:09.525728 140106545223424 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.020581571385264397, loss=0.04635021090507507
I0305 10:20:41.363634 140106748385024 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.015615560114383698, loss=0.05166340991854668
I0305 10:21:12.942507 140106545223424 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.02145233564078808, loss=0.049216244369745255
I0305 10:21:20.236969 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:23:15.818992 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:23:18.840699 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:23:21.841057 140274064205632 submission_runner.py:411] Time since start: 1274.94s, 	Step: 1524, 	{'train/accuracy': 0.9874638319015503, 'train/loss': 0.046258796006441116, 'train/mean_average_precision': 0.10521572712168312, 'validation/accuracy': 0.9847134351730347, 'validation/loss': 0.05548234283924103, 'validation/mean_average_precision': 0.10355462908149915, 'validation/num_examples': 43793, 'test/accuracy': 0.983719527721405, 'test/loss': 0.05869704484939575, 'test/mean_average_precision': 0.10370158532952593, 'test/num_examples': 43793, 'score': 500.6432864665985, 'total_duration': 1274.9377574920654, 'accumulated_submission_time': 500.6432864665985, 'accumulated_eval_time': 774.1975507736206, 'accumulated_logging_time': 0.05696678161621094}
I0305 10:23:21.856006 140106455312128 logging_writer.py:48] [1524] accumulated_eval_time=774.197551, accumulated_logging_time=0.056967, accumulated_submission_time=500.643286, global_step=1524, preemption_count=0, score=500.643286, test/accuracy=0.983720, test/loss=0.058697, test/mean_average_precision=0.103702, test/num_examples=43793, total_duration=1274.937757, train/accuracy=0.987464, train/loss=0.046259, train/mean_average_precision=0.105216, validation/accuracy=0.984713, validation/loss=0.055482, validation/mean_average_precision=0.103555, validation/num_examples=43793
I0305 10:23:46.825533 140106463704832 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.023426517844200134, loss=0.053246039897203445
I0305 10:24:18.582384 140106455312128 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.018855251371860504, loss=0.0442679338157177
I0305 10:24:50.253759 140106463704832 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.02463187277317047, loss=0.048922937363386154
I0305 10:25:22.011232 140106455312128 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.019481675699353218, loss=0.04291495680809021
I0305 10:25:53.395231 140106463704832 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.01546517014503479, loss=0.04738203436136246
I0305 10:26:25.541235 140106455312128 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.015742411836981773, loss=0.04148963838815689
I0305 10:26:57.685447 140106463704832 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.01917070709168911, loss=0.04505950212478638
I0305 10:27:21.920465 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:29:16.458570 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:29:19.575739 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:29:22.516950 140274064205632 submission_runner.py:411] Time since start: 1635.61s, 	Step: 2276, 	{'train/accuracy': 0.9878709316253662, 'train/loss': 0.044067274779081345, 'train/mean_average_precision': 0.135036552981234, 'validation/accuracy': 0.985028862953186, 'validation/loss': 0.05289539694786072, 'validation/mean_average_precision': 0.13200092438716454, 'validation/num_examples': 43793, 'test/accuracy': 0.9840750098228455, 'test/loss': 0.055751532316207886, 'test/mean_average_precision': 0.1282670616537205, 'test/num_examples': 43793, 'score': 740.6774151325226, 'total_duration': 1635.6136515140533, 'accumulated_submission_time': 740.6774151325226, 'accumulated_eval_time': 894.7939918041229, 'accumulated_logging_time': 0.08276200294494629}
I0305 10:29:22.532091 140106385827584 logging_writer.py:48] [2276] accumulated_eval_time=894.793992, accumulated_logging_time=0.082762, accumulated_submission_time=740.677415, global_step=2276, preemption_count=0, score=740.677415, test/accuracy=0.984075, test/loss=0.055752, test/mean_average_precision=0.128267, test/num_examples=43793, total_duration=1635.613652, train/accuracy=0.987871, train/loss=0.044067, train/mean_average_precision=0.135037, validation/accuracy=0.985029, validation/loss=0.052895, validation/mean_average_precision=0.132001, validation/num_examples=43793
I0305 10:29:31.552124 140106748385024 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.011966249905526638, loss=0.04166660085320473
I0305 10:30:03.134699 140106385827584 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.010211817920207977, loss=0.03997798264026642
I0305 10:30:34.497851 140106748385024 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0113758509978652, loss=0.04271593689918518
I0305 10:31:05.647526 140106385827584 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.014976673759520054, loss=0.046878598630428314
I0305 10:31:36.728436 140106748385024 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.015607723966240883, loss=0.039282284677028656
I0305 10:32:08.619960 140106385827584 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.010811956599354744, loss=0.041928116232156754
I0305 10:32:39.849544 140106748385024 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.011977562680840492, loss=0.041615672409534454
I0305 10:33:11.076770 140106385827584 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.017376873642206192, loss=0.042702578008174896
I0305 10:33:22.754610 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:35:20.413178 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:35:23.766865 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:35:27.080711 140274064205632 submission_runner.py:411] Time since start: 2000.18s, 	Step: 3038, 	{'train/accuracy': 0.9881895780563354, 'train/loss': 0.041560638695955276, 'train/mean_average_precision': 0.16760269932125438, 'validation/accuracy': 0.9853706955909729, 'validation/loss': 0.050984688103199005, 'validation/mean_average_precision': 0.1571880636251882, 'validation/num_examples': 43793, 'test/accuracy': 0.9844068884849548, 'test/loss': 0.05398542433977127, 'test/mean_average_precision': 0.15404320700353508, 'test/num_examples': 43793, 'score': 980.8679234981537, 'total_duration': 2000.1773257255554, 'accumulated_submission_time': 980.8679234981537, 'accumulated_eval_time': 1019.1199588775635, 'accumulated_logging_time': 0.110504150390625}
I0305 10:35:27.098422 140106455312128 logging_writer.py:48] [3038] accumulated_eval_time=1019.119959, accumulated_logging_time=0.110504, accumulated_submission_time=980.867923, global_step=3038, preemption_count=0, score=980.867923, test/accuracy=0.984407, test/loss=0.053985, test/mean_average_precision=0.154043, test/num_examples=43793, total_duration=2000.177326, train/accuracy=0.988190, train/loss=0.041561, train/mean_average_precision=0.167603, validation/accuracy=0.985371, validation/loss=0.050985, validation/mean_average_precision=0.157188, validation/num_examples=43793
I0305 10:35:47.352287 140106463704832 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01855151169002056, loss=0.04488015174865723
I0305 10:36:19.144268 140106455312128 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.014274404384195805, loss=0.04591027647256851
I0305 10:36:51.228443 140106463704832 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.017604701220989227, loss=0.04185045138001442
I0305 10:37:23.337441 140106455312128 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012720700353384018, loss=0.043645963072776794
I0305 10:37:55.155246 140106463704832 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.016742803156375885, loss=0.046814899891614914
I0305 10:38:26.861747 140106455312128 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.01123096514493227, loss=0.041004519909620285
I0305 10:38:58.223902 140106463704832 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.011545217595994473, loss=0.04163957014679909
I0305 10:39:27.167869 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:41:25.609025 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:41:28.925908 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:41:32.232100 140274064205632 submission_runner.py:411] Time since start: 2365.33s, 	Step: 3793, 	{'train/accuracy': 0.988350510597229, 'train/loss': 0.04006464406847954, 'train/mean_average_precision': 0.20196065405844177, 'validation/accuracy': 0.985541582107544, 'validation/loss': 0.049708738923072815, 'validation/mean_average_precision': 0.1713321946448269, 'validation/num_examples': 43793, 'test/accuracy': 0.9846153855323792, 'test/loss': 0.05248701572418213, 'test/mean_average_precision': 0.1727824752855969, 'test/num_examples': 43793, 'score': 1220.9018499851227, 'total_duration': 2365.3287811279297, 'accumulated_submission_time': 1220.9018499851227, 'accumulated_eval_time': 1144.1841297149658, 'accumulated_logging_time': 0.14036965370178223}
I0305 10:41:32.250514 140106545223424 logging_writer.py:48] [3793] accumulated_eval_time=1144.184130, accumulated_logging_time=0.140370, accumulated_submission_time=1220.901850, global_step=3793, preemption_count=0, score=1220.901850, test/accuracy=0.984615, test/loss=0.052487, test/mean_average_precision=0.172782, test/num_examples=43793, total_duration=2365.328781, train/accuracy=0.988351, train/loss=0.040065, train/mean_average_precision=0.201961, validation/accuracy=0.985542, validation/loss=0.049709, validation/mean_average_precision=0.171332, validation/num_examples=43793
I0305 10:41:35.071511 140106748385024 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.013538933359086514, loss=0.04291144385933876
I0305 10:42:06.991157 140106545223424 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.011670928448438644, loss=0.04013720899820328
I0305 10:42:38.942617 140106748385024 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.01334957592189312, loss=0.04178899526596069
I0305 10:43:11.012039 140106545223424 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01405035238713026, loss=0.04046333208680153
I0305 10:43:43.119484 140106748385024 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.01164054125547409, loss=0.04495340958237648
I0305 10:44:14.806131 140106545223424 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.019826767966151237, loss=0.04216265305876732
I0305 10:44:46.473967 140106748385024 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.010327539406716824, loss=0.039183855056762695
I0305 10:45:18.344329 140106545223424 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.012165811844170094, loss=0.03987182304263115
I0305 10:45:32.384587 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:47:33.429600 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:47:36.479281 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:47:39.542726 140274064205632 submission_runner.py:411] Time since start: 2732.64s, 	Step: 4545, 	{'train/accuracy': 0.988611102104187, 'train/loss': 0.03896178677678108, 'train/mean_average_precision': 0.22024903128454, 'validation/accuracy': 0.9857437014579773, 'validation/loss': 0.048564840108156204, 'validation/mean_average_precision': 0.1875371005404125, 'validation/num_examples': 43793, 'test/accuracy': 0.9848293662071228, 'test/loss': 0.05127822607755661, 'test/mean_average_precision': 0.19180570269707156, 'test/num_examples': 43793, 'score': 1460.9999330043793, 'total_duration': 2732.639394760132, 'accumulated_submission_time': 1460.9999330043793, 'accumulated_eval_time': 1271.3422000408173, 'accumulated_logging_time': 0.17128276824951172}
I0305 10:47:39.559593 140106781955840 logging_writer.py:48] [4545] accumulated_eval_time=1271.342200, accumulated_logging_time=0.171283, accumulated_submission_time=1460.999933, global_step=4545, preemption_count=0, score=1460.999933, test/accuracy=0.984829, test/loss=0.051278, test/mean_average_precision=0.191806, test/num_examples=43793, total_duration=2732.639395, train/accuracy=0.988611, train/loss=0.038962, train/mean_average_precision=0.220249, validation/accuracy=0.985744, validation/loss=0.048565, validation/mean_average_precision=0.187537, validation/num_examples=43793
I0305 10:47:57.233444 140211866552064 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.00998578779399395, loss=0.03422066941857338
I0305 10:48:28.458837 140106781955840 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014310251921415329, loss=0.04134431853890419
I0305 10:49:00.264180 140211866552064 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.016047243028879166, loss=0.03815556317567825
I0305 10:49:32.143628 140106781955840 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.013985062949359417, loss=0.034867219626903534
I0305 10:50:03.420491 140211866552064 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.010955698788166046, loss=0.04044941067695618
I0305 10:50:35.061386 140106781955840 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.009007850661873817, loss=0.035975970327854156
I0305 10:51:06.391650 140211866552064 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.014058479107916355, loss=0.04032913222908974
I0305 10:51:37.806468 140106781955840 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.013816525228321552, loss=0.03926368057727814
I0305 10:51:39.734982 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:53:39.663980 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:53:42.699451 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:53:45.707892 140274064205632 submission_runner.py:411] Time since start: 3098.80s, 	Step: 5307, 	{'train/accuracy': 0.9888646006584167, 'train/loss': 0.037855785340070724, 'train/mean_average_precision': 0.23974225601226462, 'validation/accuracy': 0.9859409928321838, 'validation/loss': 0.04752502217888832, 'validation/mean_average_precision': 0.20579979409806712, 'validation/num_examples': 43793, 'test/accuracy': 0.9850513339042664, 'test/loss': 0.050154536962509155, 'test/mean_average_precision': 0.2047523244605464, 'test/num_examples': 43793, 'score': 1701.144421339035, 'total_duration': 3098.8045933246613, 'accumulated_submission_time': 1701.144421339035, 'accumulated_eval_time': 1397.3150610923767, 'accumulated_logging_time': 0.19907379150390625}
I0305 10:53:45.723812 140113222047488 logging_writer.py:48] [5307] accumulated_eval_time=1397.315061, accumulated_logging_time=0.199074, accumulated_submission_time=1701.144421, global_step=5307, preemption_count=0, score=1701.144421, test/accuracy=0.985051, test/loss=0.050155, test/mean_average_precision=0.204752, test/num_examples=43793, total_duration=3098.804593, train/accuracy=0.988865, train/loss=0.037856, train/mean_average_precision=0.239742, validation/accuracy=0.985941, validation/loss=0.047525, validation/mean_average_precision=0.205800, validation/num_examples=43793
I0305 10:54:15.301281 140113230440192 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013475674204528332, loss=0.04259199649095535
I0305 10:54:47.054038 140113222047488 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.013311909511685371, loss=0.03900415822863579
I0305 10:55:19.371628 140113230440192 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.010410701856017113, loss=0.040053144097328186
I0305 10:55:53.142027 140113222047488 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.012829091399908066, loss=0.03810659050941467
I0305 10:56:27.446759 140113230440192 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.01230205874890089, loss=0.03460428863763809
I0305 10:57:00.959179 140113222047488 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.010137812234461308, loss=0.0352608747780323
I0305 10:57:34.971195 140113230440192 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.022040050476789474, loss=0.040904175490140915
I0305 10:57:45.755094 140274064205632 spec.py:321] Evaluating on the training split.
I0305 10:59:49.923947 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 10:59:53.301073 140274064205632 spec.py:349] Evaluating on the test split.
I0305 10:59:56.607999 140274064205632 submission_runner.py:411] Time since start: 3469.70s, 	Step: 6033, 	{'train/accuracy': 0.9893951416015625, 'train/loss': 0.03582870215177536, 'train/mean_average_precision': 0.28313104891656443, 'validation/accuracy': 0.9862219095230103, 'validation/loss': 0.046505916863679886, 'validation/mean_average_precision': 0.21595523296519545, 'validation/num_examples': 43793, 'test/accuracy': 0.9853643178939819, 'test/loss': 0.04905930161476135, 'test/mean_average_precision': 0.22050657317153444, 'test/num_examples': 43793, 'score': 1941.1350138187408, 'total_duration': 3469.7046773433685, 'accumulated_submission_time': 1941.1350138187408, 'accumulated_eval_time': 1528.1678965091705, 'accumulated_logging_time': 0.22595810890197754}
I0305 10:59:56.627846 140106781955840 logging_writer.py:48] [6033] accumulated_eval_time=1528.167897, accumulated_logging_time=0.225958, accumulated_submission_time=1941.135014, global_step=6033, preemption_count=0, score=1941.135014, test/accuracy=0.985364, test/loss=0.049059, test/mean_average_precision=0.220507, test/num_examples=43793, total_duration=3469.704677, train/accuracy=0.989395, train/loss=0.035829, train/mean_average_precision=0.283131, validation/accuracy=0.986222, validation/loss=0.046506, validation/mean_average_precision=0.215955, validation/num_examples=43793
I0305 11:00:19.146468 140211866552064 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.01588396728038788, loss=0.033691536635160446
I0305 11:00:51.813905 140106781955840 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.010745752602815628, loss=0.03163865953683853
I0305 11:01:24.444182 140211866552064 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.013655818067491055, loss=0.040306687355041504
I0305 11:01:56.892559 140106781955840 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.014112530276179314, loss=0.03660601004958153
I0305 11:02:29.622109 140211866552064 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01771736890077591, loss=0.03659157082438469
I0305 11:03:01.750133 140106781955840 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.014242330566048622, loss=0.03784472122788429
I0305 11:03:33.602500 140211866552064 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.010979046113789082, loss=0.03790554776787758
I0305 11:03:56.845033 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:05:58.162596 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:06:01.233126 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:06:04.346626 140274064205632 submission_runner.py:411] Time since start: 3837.44s, 	Step: 6774, 	{'train/accuracy': 0.9893807172775269, 'train/loss': 0.03569101542234421, 'train/mean_average_precision': 0.28073876956945915, 'validation/accuracy': 0.9862409830093384, 'validation/loss': 0.046377215534448624, 'validation/mean_average_precision': 0.22457076290578903, 'validation/num_examples': 43793, 'test/accuracy': 0.9852455258369446, 'test/loss': 0.049256179481744766, 'test/mean_average_precision': 0.22154515529967, 'test/num_examples': 43793, 'score': 2181.31711101532, 'total_duration': 3837.4433250427246, 'accumulated_submission_time': 2181.31711101532, 'accumulated_eval_time': 1655.6694447994232, 'accumulated_logging_time': 0.2586400508880615}
I0305 11:06:04.363194 140113230440192 logging_writer.py:48] [6774] accumulated_eval_time=1655.669445, accumulated_logging_time=0.258640, accumulated_submission_time=2181.317111, global_step=6774, preemption_count=0, score=2181.317111, test/accuracy=0.985246, test/loss=0.049256, test/mean_average_precision=0.221545, test/num_examples=43793, total_duration=3837.443325, train/accuracy=0.989381, train/loss=0.035691, train/mean_average_precision=0.280739, validation/accuracy=0.986241, validation/loss=0.046377, validation/mean_average_precision=0.224571, validation/num_examples=43793
I0305 11:06:12.950239 140211874944768 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.014384742826223373, loss=0.034863196313381195
I0305 11:06:44.827835 140113230440192 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.014083531685173512, loss=0.03334372863173485
I0305 11:07:16.152334 140211874944768 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.018064044415950775, loss=0.035982243716716766
I0305 11:07:47.489758 140113230440192 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.013307202607393265, loss=0.03390524536371231
I0305 11:08:19.055500 140211874944768 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.01602444052696228, loss=0.0386749766767025
I0305 11:08:50.384733 140113230440192 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.016402829438447952, loss=0.03591501712799072
I0305 11:09:22.107225 140211874944768 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.013426518999040127, loss=0.03778090327978134
I0305 11:09:53.508633 140113230440192 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.014844515360891819, loss=0.037040580064058304
I0305 11:10:04.569189 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:12:02.331992 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:12:05.365638 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:12:08.360018 140274064205632 submission_runner.py:411] Time since start: 4201.46s, 	Step: 7536, 	{'train/accuracy': 0.989676833152771, 'train/loss': 0.03462738171219826, 'train/mean_average_precision': 0.2996095669035396, 'validation/accuracy': 0.9863530397415161, 'validation/loss': 0.045615147799253464, 'validation/mean_average_precision': 0.2320336506118515, 'validation/num_examples': 43793, 'test/accuracy': 0.985526442527771, 'test/loss': 0.04825940728187561, 'test/mean_average_precision': 0.23453552246979048, 'test/num_examples': 43793, 'score': 2421.491445541382, 'total_duration': 4201.456716775894, 'accumulated_submission_time': 2421.491445541382, 'accumulated_eval_time': 1779.4602222442627, 'accumulated_logging_time': 0.2872166633605957}
I0305 11:12:08.378571 140113222047488 logging_writer.py:48] [7536] accumulated_eval_time=1779.460222, accumulated_logging_time=0.287217, accumulated_submission_time=2421.491446, global_step=7536, preemption_count=0, score=2421.491446, test/accuracy=0.985526, test/loss=0.048259, test/mean_average_precision=0.234536, test/num_examples=43793, total_duration=4201.456717, train/accuracy=0.989677, train/loss=0.034627, train/mean_average_precision=0.299610, validation/accuracy=0.986353, validation/loss=0.045615, validation/mean_average_precision=0.232034, validation/num_examples=43793
I0305 11:12:29.308611 140211866552064 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.014727742411196232, loss=0.032329801470041275
I0305 11:13:01.226109 140113222047488 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01852297969162464, loss=0.03451772779226303
I0305 11:13:33.029401 140211866552064 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.017651624977588654, loss=0.04184167459607124
I0305 11:14:04.472776 140113222047488 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.012280693277716637, loss=0.033575911074876785
I0305 11:14:36.023626 140211866552064 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.012734628282487392, loss=0.03004361130297184
I0305 11:15:07.827562 140113222047488 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.013425863347947598, loss=0.03639733046293259
I0305 11:15:39.428915 140211866552064 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.029360882937908173, loss=0.038100410252809525
I0305 11:16:08.671109 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:18:09.507350 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:18:12.594052 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:18:15.631844 140274064205632 submission_runner.py:411] Time since start: 4568.73s, 	Step: 8293, 	{'train/accuracy': 0.9896586537361145, 'train/loss': 0.03453819081187248, 'train/mean_average_precision': 0.3084910291337902, 'validation/accuracy': 0.9863985180854797, 'validation/loss': 0.04534177854657173, 'validation/mean_average_precision': 0.24100909412505864, 'validation/num_examples': 43793, 'test/accuracy': 0.9855167865753174, 'test/loss': 0.04799918457865715, 'test/mean_average_precision': 0.2388739386660848, 'test/num_examples': 43793, 'score': 2661.753399848938, 'total_duration': 4568.728547811508, 'accumulated_submission_time': 2661.753399848938, 'accumulated_eval_time': 1906.420913219452, 'accumulated_logging_time': 0.31652355194091797}
I0305 11:18:15.648948 140113230440192 logging_writer.py:48] [8293] accumulated_eval_time=1906.420913, accumulated_logging_time=0.316524, accumulated_submission_time=2661.753400, global_step=8293, preemption_count=0, score=2661.753400, test/accuracy=0.985517, test/loss=0.047999, test/mean_average_precision=0.238874, test/num_examples=43793, total_duration=4568.728548, train/accuracy=0.989659, train/loss=0.034538, train/mean_average_precision=0.308491, validation/accuracy=0.986399, validation/loss=0.045342, validation/mean_average_precision=0.241009, validation/num_examples=43793
I0305 11:18:18.179457 140211874944768 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.013988818973302841, loss=0.0377621091902256
I0305 11:18:50.231794 140113230440192 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.02495875209569931, loss=0.03144586831331253
I0305 11:19:22.153096 140211874944768 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.01658022217452526, loss=0.034861430525779724
I0305 11:19:54.029493 140113230440192 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.02195625938475132, loss=0.03473297879099846
I0305 11:20:26.638595 140211874944768 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.01675276644527912, loss=0.03430226072669029
I0305 11:20:58.345627 140113230440192 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.021209511905908585, loss=0.03458461910486221
I0305 11:21:30.773994 140211874944768 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01889548823237419, loss=0.0371006578207016
I0305 11:22:02.726510 140113230440192 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.022423915565013885, loss=0.034959517419338226
I0305 11:22:15.760496 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:24:16.529300 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:24:19.576218 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:24:22.585692 140274064205632 submission_runner.py:411] Time since start: 4935.68s, 	Step: 9042, 	{'train/accuracy': 0.9897889494895935, 'train/loss': 0.0342106819152832, 'train/mean_average_precision': 0.31903444271066894, 'validation/accuracy': 0.9864873886108398, 'validation/loss': 0.04519851878285408, 'validation/mean_average_precision': 0.24183148479934255, 'validation/num_examples': 43793, 'test/accuracy': 0.9856544733047485, 'test/loss': 0.047887805849313736, 'test/mean_average_precision': 0.2396084057737746, 'test/num_examples': 43793, 'score': 2901.8338191509247, 'total_duration': 4935.682396650314, 'accumulated_submission_time': 2901.8338191509247, 'accumulated_eval_time': 2033.2460660934448, 'accumulated_logging_time': 0.34440016746520996}
I0305 11:24:22.602159 140106781955840 logging_writer.py:48] [9042] accumulated_eval_time=2033.246066, accumulated_logging_time=0.344400, accumulated_submission_time=2901.833819, global_step=9042, preemption_count=0, score=2901.833819, test/accuracy=0.985654, test/loss=0.047888, test/mean_average_precision=0.239608, test/num_examples=43793, total_duration=4935.682397, train/accuracy=0.989789, train/loss=0.034211, train/mean_average_precision=0.319034, validation/accuracy=0.986487, validation/loss=0.045199, validation/mean_average_precision=0.241831, validation/num_examples=43793
I0305 11:24:41.226443 140211866552064 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.024532204493880272, loss=0.029497181996703148
I0305 11:25:13.190489 140106781955840 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.016424907371401787, loss=0.031736742705106735
I0305 11:25:45.372192 140211866552064 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.02473454736173153, loss=0.03191814944148064
I0305 11:26:17.359733 140106781955840 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.018439529463648796, loss=0.03525207191705704
I0305 11:26:49.177918 140211866552064 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.01867678202688694, loss=0.03761701285839081
I0305 11:27:20.801544 140106781955840 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.02473522536456585, loss=0.03552910313010216
I0305 11:27:52.178468 140211866552064 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.02330585941672325, loss=0.03366510570049286
I0305 11:28:22.640419 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:30:25.876072 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:30:28.942701 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:30:31.951935 140274064205632 submission_runner.py:411] Time since start: 5305.05s, 	Step: 9798, 	{'train/accuracy': 0.9901061654090881, 'train/loss': 0.03315761685371399, 'train/mean_average_precision': 0.3252865702974921, 'validation/accuracy': 0.9866160750389099, 'validation/loss': 0.0447712168097496, 'validation/mean_average_precision': 0.2498949257991057, 'validation/num_examples': 43793, 'test/accuracy': 0.9857429265975952, 'test/loss': 0.04743359610438347, 'test/mean_average_precision': 0.2517587761230952, 'test/num_examples': 43793, 'score': 3141.838902950287, 'total_duration': 5305.048637628555, 'accumulated_submission_time': 3141.838902950287, 'accumulated_eval_time': 2162.5575363636017, 'accumulated_logging_time': 0.37346911430358887}
I0305 11:30:31.968881 140113222047488 logging_writer.py:48] [9798] accumulated_eval_time=2162.557536, accumulated_logging_time=0.373469, accumulated_submission_time=3141.838903, global_step=9798, preemption_count=0, score=3141.838903, test/accuracy=0.985743, test/loss=0.047434, test/mean_average_precision=0.251759, test/num_examples=43793, total_duration=5305.048638, train/accuracy=0.990106, train/loss=0.033158, train/mean_average_precision=0.325287, validation/accuracy=0.986616, validation/loss=0.044771, validation/mean_average_precision=0.249895, validation/num_examples=43793
I0305 11:30:32.986842 140211874944768 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.018685732036828995, loss=0.03356749936938286
I0305 11:31:05.115770 140113222047488 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.031140675768256187, loss=0.038865357637405396
I0305 11:31:36.533289 140211874944768 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.02437455579638481, loss=0.03560186177492142
I0305 11:32:08.259205 140113222047488 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.021254202350974083, loss=0.030186526477336884
I0305 11:32:39.851355 140211874944768 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02491314522922039, loss=0.0339079424738884
I0305 11:33:11.646116 140113222047488 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.03404966741800308, loss=0.03422323614358902
I0305 11:33:43.394003 140211874944768 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.020920980721712112, loss=0.03295205533504486
I0305 11:34:15.886386 140113222047488 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.028918465599417686, loss=0.0369177870452404
I0305 11:34:32.024495 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:36:36.364487 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:36:39.420991 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:36:42.410798 140274064205632 submission_runner.py:411] Time since start: 5675.51s, 	Step: 10550, 	{'train/accuracy': 0.9900806546211243, 'train/loss': 0.03288881853222847, 'train/mean_average_precision': 0.35468996497847927, 'validation/accuracy': 0.9866286516189575, 'validation/loss': 0.04491862654685974, 'validation/mean_average_precision': 0.25294712988808443, 'validation/num_examples': 43793, 'test/accuracy': 0.9857412576675415, 'test/loss': 0.04772142693400383, 'test/mean_average_precision': 0.25062422955787916, 'test/num_examples': 43793, 'score': 3381.862954854965, 'total_duration': 5675.507498025894, 'accumulated_submission_time': 3381.862954854965, 'accumulated_eval_time': 2292.9438071250916, 'accumulated_logging_time': 0.401355504989624}
I0305 11:36:42.428103 140106748385024 logging_writer.py:48] [10550] accumulated_eval_time=2292.943807, accumulated_logging_time=0.401356, accumulated_submission_time=3381.862955, global_step=10550, preemption_count=0, score=3381.862955, test/accuracy=0.985741, test/loss=0.047721, test/mean_average_precision=0.250624, test/num_examples=43793, total_duration=5675.507498, train/accuracy=0.990081, train/loss=0.032889, train/mean_average_precision=0.354690, validation/accuracy=0.986629, validation/loss=0.044919, validation/mean_average_precision=0.252947, validation/num_examples=43793
I0305 11:36:58.717572 140113230440192 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.024122243747115135, loss=0.03922316059470177
I0305 11:37:31.604987 140106748385024 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.02078794687986374, loss=0.03306173160672188
I0305 11:38:03.848468 140113230440192 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.01908300071954727, loss=0.03311914950609207
I0305 11:38:35.740823 140106748385024 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03693171590566635, loss=0.031112033873796463
I0305 11:39:07.556246 140113230440192 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.017637083306908607, loss=0.030944738537073135
I0305 11:39:39.379468 140106748385024 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.035382937639951706, loss=0.03408120572566986
I0305 11:40:11.347368 140113230440192 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.023732751607894897, loss=0.03149527683854103
I0305 11:40:42.455809 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:42:47.431985 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:42:50.494579 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:42:53.498443 140274064205632 submission_runner.py:411] Time since start: 6046.60s, 	Step: 11298, 	{'train/accuracy': 0.9903729557991028, 'train/loss': 0.032004404813051224, 'train/mean_average_precision': 0.3639937596876242, 'validation/accuracy': 0.986629068851471, 'validation/loss': 0.04461873322725296, 'validation/mean_average_precision': 0.2582009507227083, 'validation/num_examples': 43793, 'test/accuracy': 0.9856927990913391, 'test/loss': 0.04749705642461777, 'test/mean_average_precision': 0.25194758651125077, 'test/num_examples': 43793, 'score': 3621.858566761017, 'total_duration': 6046.595143318176, 'accumulated_submission_time': 3621.858566761017, 'accumulated_eval_time': 2423.986411333084, 'accumulated_logging_time': 0.43006181716918945}
I0305 11:42:53.516446 140113222047488 logging_writer.py:48] [11298] accumulated_eval_time=2423.986411, accumulated_logging_time=0.430062, accumulated_submission_time=3621.858567, global_step=11298, preemption_count=0, score=3621.858567, test/accuracy=0.985693, test/loss=0.047497, test/mean_average_precision=0.251948, test/num_examples=43793, total_duration=6046.595143, train/accuracy=0.990373, train/loss=0.032004, train/mean_average_precision=0.363994, validation/accuracy=0.986629, validation/loss=0.044619, validation/mean_average_precision=0.258201, validation/num_examples=43793
I0305 11:42:54.517465 140211874944768 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.03208523988723755, loss=0.038371022790670395
I0305 11:43:26.580879 140113222047488 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.0365198515355587, loss=0.033335551619529724
I0305 11:43:58.184687 140211874944768 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02494446374475956, loss=0.03525877743959427
I0305 11:44:29.945061 140113222047488 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.023861577734351158, loss=0.03311331570148468
I0305 11:45:01.878649 140211874944768 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.02355441264808178, loss=0.03271177038550377
I0305 11:45:34.120366 140113222047488 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.0430915430188179, loss=0.03817315027117729
I0305 11:46:07.620962 140211874944768 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.024653637781739235, loss=0.03173668310046196
I0305 11:46:39.906197 140113222047488 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.03400306776165962, loss=0.03557214513421059
I0305 11:46:53.504328 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:48:57.166314 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:49:00.208651 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:49:03.312513 140274064205632 submission_runner.py:411] Time since start: 6416.41s, 	Step: 12043, 	{'train/accuracy': 0.9904464483261108, 'train/loss': 0.03140455111861229, 'train/mean_average_precision': 0.38602966603838074, 'validation/accuracy': 0.986702561378479, 'validation/loss': 0.0443083681166172, 'validation/mean_average_precision': 0.26026662924080934, 'validation/num_examples': 43793, 'test/accuracy': 0.9858107566833496, 'test/loss': 0.047289974987506866, 'test/mean_average_precision': 0.2561519833967316, 'test/num_examples': 43793, 'score': 3861.815475463867, 'total_duration': 6416.409091234207, 'accumulated_submission_time': 3861.815475463867, 'accumulated_eval_time': 2553.7944264411926, 'accumulated_logging_time': 0.45916271209716797}
I0305 11:49:03.329702 140106748385024 logging_writer.py:48] [12043] accumulated_eval_time=2553.794426, accumulated_logging_time=0.459163, accumulated_submission_time=3861.815475, global_step=12043, preemption_count=0, score=3861.815475, test/accuracy=0.985811, test/loss=0.047290, test/mean_average_precision=0.256152, test/num_examples=43793, total_duration=6416.409091, train/accuracy=0.990446, train/loss=0.031405, train/mean_average_precision=0.386030, validation/accuracy=0.986703, validation/loss=0.044308, validation/mean_average_precision=0.260267, validation/num_examples=43793
I0305 11:49:22.371950 140113230440192 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.02419428899884224, loss=0.030701670795679092
I0305 11:49:55.050333 140106748385024 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.030397873371839523, loss=0.03204124793410301
I0305 11:50:27.106445 140113230440192 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.04566110670566559, loss=0.03600916266441345
I0305 11:50:59.360328 140106748385024 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.027472786605358124, loss=0.03357469663023949
I0305 11:51:31.654874 140113230440192 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03466920182108879, loss=0.034230124205350876
I0305 11:52:04.111509 140106748385024 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.023062387481331825, loss=0.0335124172270298
I0305 11:52:36.326205 140113230440192 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.03012995235621929, loss=0.036040980368852615
I0305 11:53:03.595057 140274064205632 spec.py:321] Evaluating on the training split.
I0305 11:55:02.708515 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 11:55:05.776902 140274064205632 spec.py:349] Evaluating on the test split.
I0305 11:55:08.816521 140274064205632 submission_runner.py:411] Time since start: 6781.91s, 	Step: 12786, 	{'train/accuracy': 0.9906971454620361, 'train/loss': 0.0306289941072464, 'train/mean_average_precision': 0.39852124429162367, 'validation/accuracy': 0.9868206977844238, 'validation/loss': 0.04400556907057762, 'validation/mean_average_precision': 0.2653274057273426, 'validation/num_examples': 43793, 'test/accuracy': 0.9859880805015564, 'test/loss': 0.04692231863737106, 'test/mean_average_precision': 0.2588306255104661, 'test/num_examples': 43793, 'score': 4102.04964017868, 'total_duration': 6781.913225889206, 'accumulated_submission_time': 4102.04964017868, 'accumulated_eval_time': 2679.0158491134644, 'accumulated_logging_time': 0.487302303314209}
I0305 11:55:08.833907 140211866552064 logging_writer.py:48] [12786] accumulated_eval_time=2679.015849, accumulated_logging_time=0.487302, accumulated_submission_time=4102.049640, global_step=12786, preemption_count=0, score=4102.049640, test/accuracy=0.985988, test/loss=0.046922, test/mean_average_precision=0.258831, test/num_examples=43793, total_duration=6781.913226, train/accuracy=0.990697, train/loss=0.030629, train/mean_average_precision=0.398521, validation/accuracy=0.986821, validation/loss=0.044006, validation/mean_average_precision=0.265327, validation/num_examples=43793
I0305 11:55:13.722083 140211874944768 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03465033695101738, loss=0.034470733255147934
I0305 11:55:45.713384 140211866552064 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.03413168713450432, loss=0.031525928527116776
I0305 11:56:17.552977 140211874944768 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.02780122682452202, loss=0.03792667016386986
I0305 11:56:49.729504 140211866552064 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.031597621738910675, loss=0.030972378328442574
I0305 11:57:22.006622 140211874944768 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.029833003878593445, loss=0.032910190522670746
I0305 11:57:53.718120 140211866552064 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.026460057124495506, loss=0.033487219363451004
I0305 11:58:24.998021 140211874944768 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02665264531970024, loss=0.029321974143385887
I0305 11:58:56.558307 140211866552064 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.02977570705115795, loss=0.03497234731912613
I0305 11:59:09.003194 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:01:12.171889 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:01:15.220939 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:01:18.218458 140274064205632 submission_runner.py:411] Time since start: 7151.32s, 	Step: 13540, 	{'train/accuracy': 0.9909554123878479, 'train/loss': 0.030052347108721733, 'train/mean_average_precision': 0.41593367552509586, 'validation/accuracy': 0.9867321848869324, 'validation/loss': 0.04410630837082863, 'validation/mean_average_precision': 0.26450981945747104, 'validation/num_examples': 43793, 'test/accuracy': 0.9859177470207214, 'test/loss': 0.046688053756952286, 'test/mean_average_precision': 0.25660116459688287, 'test/num_examples': 43793, 'score': 4342.18771481514, 'total_duration': 7151.315158605576, 'accumulated_submission_time': 4342.18771481514, 'accumulated_eval_time': 2808.231062889099, 'accumulated_logging_time': 0.5157938003540039}
I0305 12:01:18.236093 140106748385024 logging_writer.py:48] [13540] accumulated_eval_time=2808.231063, accumulated_logging_time=0.515794, accumulated_submission_time=4342.187715, global_step=13540, preemption_count=0, score=4342.187715, test/accuracy=0.985918, test/loss=0.046688, test/mean_average_precision=0.256601, test/num_examples=43793, total_duration=7151.315159, train/accuracy=0.990955, train/loss=0.030052, train/mean_average_precision=0.415934, validation/accuracy=0.986732, validation/loss=0.044106, validation/mean_average_precision=0.264510, validation/num_examples=43793
I0305 12:01:37.579152 140113222047488 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.029692720621824265, loss=0.03103458322584629
I0305 12:02:09.368330 140106748385024 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.03229832276701927, loss=0.03047935850918293
I0305 12:02:40.918132 140113222047488 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.04483838751912117, loss=0.03252885490655899
I0305 12:03:12.694786 140106748385024 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.03248075395822525, loss=0.03427526354789734
I0305 12:03:44.515733 140113222047488 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03668705374002457, loss=0.03471432626247406
I0305 12:04:15.953623 140106748385024 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03010431118309498, loss=0.033416759222745895
I0305 12:04:47.724103 140113222047488 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03703887760639191, loss=0.03245054557919502
I0305 12:05:18.231897 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:07:19.850192 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:07:22.915177 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:07:25.961909 140274064205632 submission_runner.py:411] Time since start: 7519.06s, 	Step: 14297, 	{'train/accuracy': 0.9908493757247925, 'train/loss': 0.03024035133421421, 'train/mean_average_precision': 0.4088204630160475, 'validation/accuracy': 0.9867675304412842, 'validation/loss': 0.04432271048426628, 'validation/mean_average_precision': 0.2648037087964535, 'validation/num_examples': 43793, 'test/accuracy': 0.9858878254890442, 'test/loss': 0.04713056609034538, 'test/mean_average_precision': 0.25515752789511154, 'test/num_examples': 43793, 'score': 4582.151921987534, 'total_duration': 7519.058586359024, 'accumulated_submission_time': 4582.151921987534, 'accumulated_eval_time': 2935.961008310318, 'accumulated_logging_time': 0.5450277328491211}
I0305 12:07:25.979547 140113230440192 logging_writer.py:48] [14297] accumulated_eval_time=2935.961008, accumulated_logging_time=0.545028, accumulated_submission_time=4582.151922, global_step=14297, preemption_count=0, score=4582.151922, test/accuracy=0.985888, test/loss=0.047131, test/mean_average_precision=0.255158, test/num_examples=43793, total_duration=7519.058586, train/accuracy=0.990849, train/loss=0.030240, train/mean_average_precision=0.408820, validation/accuracy=0.986768, validation/loss=0.044323, validation/mean_average_precision=0.264804, validation/num_examples=43793
I0305 12:07:27.263953 140211874944768 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03890123963356018, loss=0.03543277084827423
I0305 12:07:58.795935 140113230440192 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.0466492623090744, loss=0.03215193748474121
I0305 12:08:31.285115 140211874944768 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.028674278408288956, loss=0.028629347681999207
I0305 12:09:03.683072 140113230440192 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.02942552976310253, loss=0.029410071671009064
I0305 12:09:35.588757 140211874944768 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03339099511504173, loss=0.0318770557641983
I0305 12:10:07.757366 140113230440192 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03572014719247818, loss=0.030027471482753754
I0305 12:10:39.752265 140211874944768 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.03754934296011925, loss=0.029196418821811676
I0305 12:11:11.487855 140113230440192 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03249875828623772, loss=0.030861910432577133
I0305 12:11:26.205024 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:13:30.515971 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:13:33.559512 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:13:36.570405 140274064205632 submission_runner.py:411] Time since start: 7889.67s, 	Step: 15047, 	{'train/accuracy': 0.9907960891723633, 'train/loss': 0.030470391735434532, 'train/mean_average_precision': 0.40658089597271635, 'validation/accuracy': 0.9867001175880432, 'validation/loss': 0.044139716774225235, 'validation/mean_average_precision': 0.26861528107565397, 'validation/num_examples': 43793, 'test/accuracy': 0.9858798384666443, 'test/loss': 0.04688284918665886, 'test/mean_average_precision': 0.2637279712481232, 'test/num_examples': 43793, 'score': 4822.344773769379, 'total_duration': 7889.667108535767, 'accumulated_submission_time': 4822.344773769379, 'accumulated_eval_time': 3066.32634806633, 'accumulated_logging_time': 0.5752942562103271}
I0305 12:13:36.588409 140113222047488 logging_writer.py:48] [15047] accumulated_eval_time=3066.326348, accumulated_logging_time=0.575294, accumulated_submission_time=4822.344774, global_step=15047, preemption_count=0, score=4822.344774, test/accuracy=0.985880, test/loss=0.046883, test/mean_average_precision=0.263728, test/num_examples=43793, total_duration=7889.667109, train/accuracy=0.990796, train/loss=0.030470, train/mean_average_precision=0.406581, validation/accuracy=0.986700, validation/loss=0.044140, validation/mean_average_precision=0.268615, validation/num_examples=43793
I0305 12:13:53.796058 140211866552064 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.03301180899143219, loss=0.028696319088339806
I0305 12:14:25.883424 140113222047488 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.04621083661913872, loss=0.034578561782836914
I0305 12:14:57.472758 140211866552064 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.04355301707983017, loss=0.03322960063815117
I0305 12:15:29.176002 140113222047488 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.027639076113700867, loss=0.027740614488720894
I0305 12:16:00.576088 140211866552064 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.042119454592466354, loss=0.0348331555724144
I0305 12:16:32.202612 140113222047488 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.037431128323078156, loss=0.03255481272935867
I0305 12:17:03.834310 140211866552064 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.04018790274858475, loss=0.03694625198841095
I0305 12:17:35.690485 140113222047488 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03980209678411484, loss=0.03039243072271347
I0305 12:17:36.617248 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:19:46.389633 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:19:49.766291 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:19:53.066104 140274064205632 submission_runner.py:411] Time since start: 8266.16s, 	Step: 15804, 	{'train/accuracy': 0.990820586681366, 'train/loss': 0.030322598293423653, 'train/mean_average_precision': 0.4107530372449284, 'validation/accuracy': 0.9867711663246155, 'validation/loss': 0.044030435383319855, 'validation/mean_average_precision': 0.26807086942336233, 'validation/num_examples': 43793, 'test/accuracy': 0.9858170747756958, 'test/loss': 0.04695561155676842, 'test/mean_average_precision': 0.25425818560788466, 'test/num_examples': 43793, 'score': 5062.34095621109, 'total_duration': 8266.162788152695, 'accumulated_submission_time': 5062.34095621109, 'accumulated_eval_time': 3202.7751603126526, 'accumulated_logging_time': 0.6057493686676025}
I0305 12:19:53.086229 140106748385024 logging_writer.py:48] [15804] accumulated_eval_time=3202.775160, accumulated_logging_time=0.605749, accumulated_submission_time=5062.340956, global_step=15804, preemption_count=0, score=5062.340956, test/accuracy=0.985817, test/loss=0.046956, test/mean_average_precision=0.254258, test/num_examples=43793, total_duration=8266.162788, train/accuracy=0.990821, train/loss=0.030323, train/mean_average_precision=0.410753, validation/accuracy=0.986771, validation/loss=0.044030, validation/mean_average_precision=0.268071, validation/num_examples=43793
I0305 12:20:24.998242 140211874944768 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.05032424256205559, loss=0.03328067436814308
I0305 12:20:57.619149 140106748385024 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0374450758099556, loss=0.03141481801867485
I0305 12:21:30.168177 140211874944768 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.04689156264066696, loss=0.03640969097614288
I0305 12:22:02.686224 140106748385024 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.030626915395259857, loss=0.030304191634058952
I0305 12:22:35.467040 140211874944768 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.043916091322898865, loss=0.036155957728624344
I0305 12:23:08.050501 140106748385024 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.04021022468805313, loss=0.031083865091204643
I0305 12:23:40.621586 140211874944768 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03227000683546066, loss=0.03273652866482735
I0305 12:23:53.183817 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:26:00.511584 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:26:03.670711 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:26:06.697669 140274064205632 submission_runner.py:411] Time since start: 8639.79s, 	Step: 16540, 	{'train/accuracy': 0.9907112717628479, 'train/loss': 0.030399184674024582, 'train/mean_average_precision': 0.4025933595309913, 'validation/accuracy': 0.9867861866950989, 'validation/loss': 0.04456831142306328, 'validation/mean_average_precision': 0.26585178161253475, 'validation/num_examples': 43793, 'test/accuracy': 0.9859287142753601, 'test/loss': 0.04742281511425972, 'test/mean_average_precision': 0.2621201168497622, 'test/num_examples': 43793, 'score': 5302.402543067932, 'total_duration': 8639.794367313385, 'accumulated_submission_time': 5302.402543067932, 'accumulated_eval_time': 3336.2889742851257, 'accumulated_logging_time': 0.6370806694030762}
I0305 12:26:06.716817 140113222047488 logging_writer.py:48] [16540] accumulated_eval_time=3336.288974, accumulated_logging_time=0.637081, accumulated_submission_time=5302.402543, global_step=16540, preemption_count=0, score=5302.402543, test/accuracy=0.985929, test/loss=0.047423, test/mean_average_precision=0.262120, test/num_examples=43793, total_duration=8639.794367, train/accuracy=0.990711, train/loss=0.030399, train/mean_average_precision=0.402593, validation/accuracy=0.986786, validation/loss=0.044568, validation/mean_average_precision=0.265852, validation/num_examples=43793
I0305 12:26:26.620343 140113230440192 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03754972666501999, loss=0.030096879228949547
I0305 12:26:58.585866 140113222047488 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.05011497437953949, loss=0.03248284012079239
I0305 12:27:30.942527 140113230440192 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.040381066501140594, loss=0.03217422962188721
I0305 12:28:03.316066 140113222047488 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.03770177811384201, loss=0.03048618882894516
I0305 12:28:35.522736 140113230440192 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03900820016860962, loss=0.029544003307819366
I0305 12:29:07.908375 140113222047488 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.04325399920344353, loss=0.030955921858549118
I0305 12:29:39.497108 140113230440192 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03163408115506172, loss=0.02835405059158802
I0305 12:30:06.719268 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:32:09.390672 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:32:12.432298 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:32:15.447728 140274064205632 submission_runner.py:411] Time since start: 9008.54s, 	Step: 17286, 	{'train/accuracy': 0.9908052682876587, 'train/loss': 0.03007584437727928, 'train/mean_average_precision': 0.4174778803693812, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.04405622556805611, 'validation/mean_average_precision': 0.26798737202400263, 'validation/num_examples': 43793, 'test/accuracy': 0.9860125184059143, 'test/loss': 0.04680762439966202, 'test/mean_average_precision': 0.2634333099377213, 'test/num_examples': 43793, 'score': 5542.3714718818665, 'total_duration': 9008.544433832169, 'accumulated_submission_time': 5542.3714718818665, 'accumulated_eval_time': 3465.017394065857, 'accumulated_logging_time': 0.669407844543457}
I0305 12:32:15.466227 140211866552064 logging_writer.py:48] [17286] accumulated_eval_time=3465.017394, accumulated_logging_time=0.669408, accumulated_submission_time=5542.371472, global_step=17286, preemption_count=0, score=5542.371472, test/accuracy=0.986013, test/loss=0.046808, test/mean_average_precision=0.263433, test/num_examples=43793, total_duration=9008.544434, train/accuracy=0.990805, train/loss=0.030076, train/mean_average_precision=0.417478, validation/accuracy=0.986845, validation/loss=0.044056, validation/mean_average_precision=0.267987, validation/num_examples=43793
I0305 12:32:20.230867 140211874944768 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03326452895998955, loss=0.030843771994113922
I0305 12:32:51.877402 140211866552064 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03734736889600754, loss=0.028919890522956848
I0305 12:33:23.709669 140211874944768 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.042813949286937714, loss=0.03281369060277939
I0305 12:33:55.677285 140211866552064 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.0348171666264534, loss=0.03227907046675682
I0305 12:34:27.647971 140211874944768 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.052637796849012375, loss=0.03676929697394371
I0305 12:34:59.297417 140211866552064 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.055949196219444275, loss=0.0351313054561615
I0305 12:35:31.202475 140211874944768 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.04240047559142113, loss=0.02890891022980213
I0305 12:36:02.537477 140211866552064 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.046882130205631256, loss=0.030353548005223274
I0305 12:36:15.548855 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:38:18.506296 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:38:21.603027 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:38:24.618521 140274064205632 submission_runner.py:411] Time since start: 9377.72s, 	Step: 18042, 	{'train/accuracy': 0.9909988045692444, 'train/loss': 0.029407411813735962, 'train/mean_average_precision': 0.42416870106685767, 'validation/accuracy': 0.9868454337120056, 'validation/loss': 0.044065941125154495, 'validation/mean_average_precision': 0.27076625240549707, 'validation/num_examples': 43793, 'test/accuracy': 0.9859421849250793, 'test/loss': 0.04680788516998291, 'test/mean_average_precision': 0.26670880613231573, 'test/num_examples': 43793, 'score': 5782.42107629776, 'total_duration': 9377.715222358704, 'accumulated_submission_time': 5782.42107629776, 'accumulated_eval_time': 3594.087012052536, 'accumulated_logging_time': 0.7005889415740967}
I0305 12:38:24.637536 140106748385024 logging_writer.py:48] [18042] accumulated_eval_time=3594.087012, accumulated_logging_time=0.700589, accumulated_submission_time=5782.421076, global_step=18042, preemption_count=0, score=5782.421076, test/accuracy=0.985942, test/loss=0.046808, test/mean_average_precision=0.266709, test/num_examples=43793, total_duration=9377.715222, train/accuracy=0.990999, train/loss=0.029407, train/mean_average_precision=0.424169, validation/accuracy=0.986845, validation/loss=0.044066, validation/mean_average_precision=0.270766, validation/num_examples=43793
I0305 12:38:43.438059 140113230440192 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.04855837672948837, loss=0.032949041575193405
I0305 12:39:15.391979 140106748385024 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.04364844039082527, loss=0.03137548640370369
I0305 12:39:47.687484 140113230440192 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04498741030693054, loss=0.029380815103650093
I0305 12:40:19.575352 140106748385024 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.03605438396334648, loss=0.02988763526082039
I0305 12:40:51.217025 140113230440192 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.043274298310279846, loss=0.03304813429713249
I0305 12:41:23.187184 140106748385024 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04416845366358757, loss=0.030069515109062195
I0305 12:41:55.370172 140113230440192 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.046259064227342606, loss=0.031830187886953354
I0305 12:42:24.822010 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:44:29.175864 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:44:32.264051 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:44:35.265562 140274064205632 submission_runner.py:411] Time since start: 9748.36s, 	Step: 18793, 	{'train/accuracy': 0.9910372495651245, 'train/loss': 0.029095936566591263, 'train/mean_average_precision': 0.42595295319393855, 'validation/accuracy': 0.9867752194404602, 'validation/loss': 0.04390951991081238, 'validation/mean_average_precision': 0.27388958936337726, 'validation/num_examples': 43793, 'test/accuracy': 0.9859703779220581, 'test/loss': 0.04678963124752045, 'test/mean_average_precision': 0.2599671178064437, 'test/num_examples': 43793, 'score': 6022.573717832565, 'total_duration': 9748.362263441086, 'accumulated_submission_time': 6022.573717832565, 'accumulated_eval_time': 3724.5305168628693, 'accumulated_logging_time': 0.7313892841339111}
I0305 12:44:35.284300 140113222047488 logging_writer.py:48] [18793] accumulated_eval_time=3724.530517, accumulated_logging_time=0.731389, accumulated_submission_time=6022.573718, global_step=18793, preemption_count=0, score=6022.573718, test/accuracy=0.985970, test/loss=0.046790, test/mean_average_precision=0.259967, test/num_examples=43793, total_duration=9748.362263, train/accuracy=0.991037, train/loss=0.029096, train/mean_average_precision=0.425953, validation/accuracy=0.986775, validation/loss=0.043910, validation/mean_average_precision=0.273890, validation/num_examples=43793
I0305 12:44:37.914883 140211866552064 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.04902530089020729, loss=0.033739980310201645
I0305 12:45:09.632289 140113222047488 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.03543514385819435, loss=0.030424149706959724
I0305 12:45:41.112898 140211866552064 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.05875485762953758, loss=0.031648553907871246
I0305 12:46:13.000938 140113222047488 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03946293517947197, loss=0.02993086911737919
I0305 12:46:44.603221 140211866552064 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.032634440809488297, loss=0.02619275450706482
I0305 12:47:16.732495 140113222047488 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.05345285311341286, loss=0.0335342176258564
I0305 12:47:48.463768 140211866552064 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.0582442432641983, loss=0.029682008549571037
I0305 12:48:20.289377 140113222047488 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.04264387488365173, loss=0.03329594433307648
I0305 12:48:35.279927 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:50:37.371888 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:50:40.438037 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:50:43.430977 140274064205632 submission_runner.py:411] Time since start: 10116.53s, 	Step: 19548, 	{'train/accuracy': 0.9911220073699951, 'train/loss': 0.028678828850388527, 'train/mean_average_precision': 0.45240739248899986, 'validation/accuracy': 0.9870005249977112, 'validation/loss': 0.04398788511753082, 'validation/mean_average_precision': 0.274081041031077, 'validation/num_examples': 43793, 'test/accuracy': 0.9860718846321106, 'test/loss': 0.04697831720113754, 'test/mean_average_precision': 0.26973527007806825, 'test/num_examples': 43793, 'score': 6262.538156986237, 'total_duration': 10116.527661561966, 'accumulated_submission_time': 6262.538156986237, 'accumulated_eval_time': 3852.681501150131, 'accumulated_logging_time': 0.7612929344177246}
I0305 12:50:43.449924 140113230440192 logging_writer.py:48] [19548] accumulated_eval_time=3852.681501, accumulated_logging_time=0.761293, accumulated_submission_time=6262.538157, global_step=19548, preemption_count=0, score=6262.538157, test/accuracy=0.986072, test/loss=0.046978, test/mean_average_precision=0.269735, test/num_examples=43793, total_duration=10116.527662, train/accuracy=0.991122, train/loss=0.028679, train/mean_average_precision=0.452407, validation/accuracy=0.987001, validation/loss=0.043988, validation/mean_average_precision=0.274081, validation/num_examples=43793
I0305 12:51:00.347473 140211874944768 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.03411472588777542, loss=0.03081877902150154
I0305 12:51:32.666128 140113230440192 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.041869375854730606, loss=0.028848301619291306
I0305 12:52:04.798688 140211874944768 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.04351063445210457, loss=0.03045315109193325
I0305 12:52:36.433068 140113230440192 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.059077195823192596, loss=0.03233572095632553
I0305 12:53:08.041358 140211874944768 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.041290923953056335, loss=0.030988141894340515
I0305 12:53:40.177990 140113230440192 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.056972406804561615, loss=0.03250579535961151
I0305 12:54:12.127518 140211874944768 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.06821762025356293, loss=0.03394890949130058
I0305 12:54:43.513739 140274064205632 spec.py:321] Evaluating on the training split.
I0305 12:56:47.934449 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 12:56:50.994116 140274064205632 spec.py:349] Evaluating on the test split.
I0305 12:56:54.018728 140274064205632 submission_runner.py:411] Time since start: 10487.12s, 	Step: 20299, 	{'train/accuracy': 0.9913285970687866, 'train/loss': 0.028059540316462517, 'train/mean_average_precision': 0.4674548046421443, 'validation/accuracy': 0.9869655966758728, 'validation/loss': 0.04380568861961365, 'validation/mean_average_precision': 0.2766592911034783, 'validation/num_examples': 43793, 'test/accuracy': 0.9862125515937805, 'test/loss': 0.04654303193092346, 'test/mean_average_precision': 0.2696994986515265, 'test/num_examples': 43793, 'score': 6502.570947647095, 'total_duration': 10487.115420341492, 'accumulated_submission_time': 6502.570947647095, 'accumulated_eval_time': 3983.1864354610443, 'accumulated_logging_time': 0.7912535667419434}
I0305 12:56:54.038659 140113222047488 logging_writer.py:48] [20299] accumulated_eval_time=3983.186435, accumulated_logging_time=0.791254, accumulated_submission_time=6502.570948, global_step=20299, preemption_count=0, score=6502.570948, test/accuracy=0.986213, test/loss=0.046543, test/mean_average_precision=0.269699, test/num_examples=43793, total_duration=10487.115420, train/accuracy=0.991329, train/loss=0.028060, train/mean_average_precision=0.467455, validation/accuracy=0.986966, validation/loss=0.043806, validation/mean_average_precision=0.276659, validation/num_examples=43793
I0305 12:56:54.730520 140211866552064 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.045908838510513306, loss=0.02936834655702114
I0305 12:57:26.840741 140113222047488 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.054943572729825974, loss=0.030833769589662552
I0305 12:57:58.739248 140211866552064 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.04048466682434082, loss=0.03169586881995201
I0305 12:58:30.967825 140113222047488 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.0406704917550087, loss=0.02851807326078415
I0305 12:59:03.230490 140211866552064 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04172113910317421, loss=0.0336637981235981
I0305 12:59:35.837826 140113222047488 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.0400460883975029, loss=0.02726554684340954
I0305 13:00:08.649822 140211866552064 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06334386765956879, loss=0.032415036112070084
I0305 13:00:40.851369 140113222047488 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.040246374905109406, loss=0.03239757940173149
I0305 13:00:54.122877 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:03:05.137162 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:03:08.243687 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:03:11.262411 140274064205632 submission_runner.py:411] Time since start: 10864.36s, 	Step: 21043, 	{'train/accuracy': 0.9913150072097778, 'train/loss': 0.028120845556259155, 'train/mean_average_precision': 0.45987087891357675, 'validation/accuracy': 0.9869221448898315, 'validation/loss': 0.04410303384065628, 'validation/mean_average_precision': 0.2744794112477171, 'validation/num_examples': 43793, 'test/accuracy': 0.9861034750938416, 'test/loss': 0.04690973833203316, 'test/mean_average_precision': 0.26586312144099267, 'test/num_examples': 43793, 'score': 6742.623478651047, 'total_duration': 10864.359109163284, 'accumulated_submission_time': 6742.623478651047, 'accumulated_eval_time': 4120.325926780701, 'accumulated_logging_time': 0.8225059509277344}
I0305 13:03:11.282429 140106748385024 logging_writer.py:48] [21043] accumulated_eval_time=4120.325927, accumulated_logging_time=0.822506, accumulated_submission_time=6742.623479, global_step=21043, preemption_count=0, score=6742.623479, test/accuracy=0.986103, test/loss=0.046910, test/mean_average_precision=0.265863, test/num_examples=43793, total_duration=10864.359109, train/accuracy=0.991315, train/loss=0.028121, train/mean_average_precision=0.459871, validation/accuracy=0.986922, validation/loss=0.044103, validation/mean_average_precision=0.274479, validation/num_examples=43793
I0305 13:03:30.087178 140113230440192 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04153715446591377, loss=0.03148924931883812
I0305 13:04:02.774347 140106748385024 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.04424574226140976, loss=0.030281847342848778
I0305 13:04:35.117571 140113230440192 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.052072156220674515, loss=0.03076762519776821
I0305 13:05:07.649163 140106748385024 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.044443342834711075, loss=0.031823404133319855
I0305 13:05:40.284116 140113230440192 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.059674594551324844, loss=0.03435331583023071
I0305 13:06:12.616794 140106748385024 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.06627721339464188, loss=0.028390219435095787
I0305 13:06:45.176267 140113230440192 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.04244834929704666, loss=0.02851657196879387
I0305 13:07:11.426384 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:09:18.328922 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:09:21.435099 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:09:24.513489 140274064205632 submission_runner.py:411] Time since start: 11237.61s, 	Step: 21784, 	{'train/accuracy': 0.9913945198059082, 'train/loss': 0.02820894680917263, 'train/mean_average_precision': 0.44559433108747226, 'validation/accuracy': 0.9868649244308472, 'validation/loss': 0.043948836624622345, 'validation/mean_average_precision': 0.27483588400027503, 'validation/num_examples': 43793, 'test/accuracy': 0.9860184192657471, 'test/loss': 0.046649981290102005, 'test/mean_average_precision': 0.2655067753076269, 'test/num_examples': 43793, 'score': 6982.736036777496, 'total_duration': 11237.61018204689, 'accumulated_submission_time': 6982.736036777496, 'accumulated_eval_time': 4253.412977218628, 'accumulated_logging_time': 0.853546142578125}
I0305 13:09:24.533207 140113222047488 logging_writer.py:48] [21784] accumulated_eval_time=4253.412977, accumulated_logging_time=0.853546, accumulated_submission_time=6982.736037, global_step=21784, preemption_count=0, score=6982.736037, test/accuracy=0.986018, test/loss=0.046650, test/mean_average_precision=0.265507, test/num_examples=43793, total_duration=11237.610182, train/accuracy=0.991395, train/loss=0.028209, train/mean_average_precision=0.445594, validation/accuracy=0.986865, validation/loss=0.043949, validation/mean_average_precision=0.274836, validation/num_examples=43793
I0305 13:09:29.914298 140211874944768 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.046649571508169174, loss=0.03243162855505943
I0305 13:10:01.789953 140113222047488 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.057188041508197784, loss=0.0277774166315794
I0305 13:10:33.577886 140211874944768 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.06704939156770706, loss=0.031003711745142937
I0305 13:11:06.075319 140113222047488 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.04021483287215233, loss=0.030793972313404083
I0305 13:11:38.292790 140211874944768 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.04832534119486809, loss=0.028540093451738358
I0305 13:12:10.642804 140113222047488 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05332552641630173, loss=0.0334031842648983
I0305 13:12:42.567092 140211874944768 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.05575130507349968, loss=0.02699146792292595
I0305 13:13:14.807508 140113222047488 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.047386396676301956, loss=0.030761295929551125
I0305 13:13:24.631665 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:15:28.672452 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:15:32.087083 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:15:35.421567 140274064205632 submission_runner.py:411] Time since start: 11608.52s, 	Step: 22532, 	{'train/accuracy': 0.9913437366485596, 'train/loss': 0.028254391625523567, 'train/mean_average_precision': 0.44833149413640555, 'validation/accuracy': 0.9869745373725891, 'validation/loss': 0.04376159980893135, 'validation/mean_average_precision': 0.2805751517421683, 'validation/num_examples': 43793, 'test/accuracy': 0.9860659837722778, 'test/loss': 0.04667449742555618, 'test/mean_average_precision': 0.26205441476190955, 'test/num_examples': 43793, 'score': 7222.8034081459045, 'total_duration': 11608.518146514893, 'accumulated_submission_time': 7222.8034081459045, 'accumulated_eval_time': 4384.202717542648, 'accumulated_logging_time': 0.8843135833740234}
I0305 13:15:35.443813 140106748385024 logging_writer.py:48] [22532] accumulated_eval_time=4384.202718, accumulated_logging_time=0.884314, accumulated_submission_time=7222.803408, global_step=22532, preemption_count=0, score=7222.803408, test/accuracy=0.986066, test/loss=0.046674, test/mean_average_precision=0.262054, test/num_examples=43793, total_duration=11608.518147, train/accuracy=0.991344, train/loss=0.028254, train/mean_average_precision=0.448331, validation/accuracy=0.986975, validation/loss=0.043762, validation/mean_average_precision=0.280575, validation/num_examples=43793
I0305 13:15:58.081762 140113230440192 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.048993274569511414, loss=0.0286969393491745
I0305 13:16:31.095016 140106748385024 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04757943004369736, loss=0.02837199717760086
I0305 13:17:03.777447 140113230440192 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04631073772907257, loss=0.03090626746416092
I0305 13:17:35.562723 140106748385024 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.052640460431575775, loss=0.031757768243551254
I0305 13:18:07.455784 140113230440192 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.05994247645139694, loss=0.03049694187939167
I0305 13:18:39.158443 140106748385024 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.05819219723343849, loss=0.031888995319604874
I0305 13:19:11.071595 140113230440192 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.04463402181863785, loss=0.028152436017990112
I0305 13:19:35.611348 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:21:38.471180 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:21:41.527662 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:21:44.517253 140274064205632 submission_runner.py:411] Time since start: 11977.61s, 	Step: 23278, 	{'train/accuracy': 0.991249144077301, 'train/loss': 0.028415771201252937, 'train/mean_average_precision': 0.4628828997940778, 'validation/accuracy': 0.9869765639305115, 'validation/loss': 0.04399452731013298, 'validation/mean_average_precision': 0.2760817632379464, 'validation/num_examples': 43793, 'test/accuracy': 0.9861236810684204, 'test/loss': 0.04672761261463165, 'test/mean_average_precision': 0.2661157909791088, 'test/num_examples': 43793, 'score': 7462.937109947205, 'total_duration': 11977.613828420639, 'accumulated_submission_time': 7462.937109947205, 'accumulated_eval_time': 4513.108451843262, 'accumulated_logging_time': 0.9187815189361572}
I0305 13:21:44.536477 140113222047488 logging_writer.py:48] [23278] accumulated_eval_time=4513.108452, accumulated_logging_time=0.918782, accumulated_submission_time=7462.937110, global_step=23278, preemption_count=0, score=7462.937110, test/accuracy=0.986124, test/loss=0.046728, test/mean_average_precision=0.266116, test/num_examples=43793, total_duration=11977.613828, train/accuracy=0.991249, train/loss=0.028416, train/mean_average_precision=0.462883, validation/accuracy=0.986977, validation/loss=0.043995, validation/mean_average_precision=0.276082, validation/num_examples=43793
I0305 13:21:51.886671 140211866552064 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.04568861052393913, loss=0.025470243766903877
I0305 13:22:23.636417 140113222047488 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0469697080552578, loss=0.028221286833286285
I0305 13:22:55.629395 140211866552064 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.046737734228372574, loss=0.031230052933096886
I0305 13:23:27.483952 140113222047488 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.05387108772993088, loss=0.028028491884469986
I0305 13:23:59.459244 140211866552064 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.048319119960069656, loss=0.02852250449359417
I0305 13:24:31.646254 140113222047488 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04883867874741554, loss=0.029330240562558174
I0305 13:25:03.435021 140211866552064 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.04911431670188904, loss=0.031116385012865067
I0305 13:25:35.553520 140113222047488 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05433034524321556, loss=0.03046104870736599
I0305 13:25:44.546279 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:27:48.813478 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:27:51.883501 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:27:54.899271 140274064205632 submission_runner.py:411] Time since start: 12348.00s, 	Step: 24029, 	{'train/accuracy': 0.9913162589073181, 'train/loss': 0.02817481756210327, 'train/mean_average_precision': 0.44899889476697225, 'validation/accuracy': 0.9868953824043274, 'validation/loss': 0.04449484497308731, 'validation/mean_average_precision': 0.27705578804006, 'validation/num_examples': 43793, 'test/accuracy': 0.9860647320747375, 'test/loss': 0.04756991192698479, 'test/mean_average_precision': 0.2683217825679791, 'test/num_examples': 43793, 'score': 7702.915818929672, 'total_duration': 12347.995973825455, 'accumulated_submission_time': 7702.915818929672, 'accumulated_eval_time': 4643.4614017009735, 'accumulated_logging_time': 0.9489178657531738}
I0305 13:27:54.919404 140106748385024 logging_writer.py:48] [24029] accumulated_eval_time=4643.461402, accumulated_logging_time=0.948918, accumulated_submission_time=7702.915819, global_step=24029, preemption_count=0, score=7702.915819, test/accuracy=0.986065, test/loss=0.047570, test/mean_average_precision=0.268322, test/num_examples=43793, total_duration=12347.995974, train/accuracy=0.991316, train/loss=0.028175, train/mean_average_precision=0.448999, validation/accuracy=0.986895, validation/loss=0.044495, validation/mean_average_precision=0.277056, validation/num_examples=43793
I0305 13:28:18.145496 140211874944768 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.048031218349933624, loss=0.028530029579997063
I0305 13:28:49.887009 140106748385024 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.04387606680393219, loss=0.03150395676493645
I0305 13:29:22.033914 140211874944768 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.047661323100328445, loss=0.030391544103622437
I0305 13:29:54.178446 140106748385024 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.06014682725071907, loss=0.0328788086771965
I0305 13:30:26.370471 140211874944768 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.05481686070561409, loss=0.03308696299791336
I0305 13:30:58.117945 140106748385024 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.04773027077317238, loss=0.02832663245499134
I0305 13:31:30.053764 140211874944768 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.048663314431905746, loss=0.028415905311703682
I0305 13:31:55.027460 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:33:56.050771 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:33:59.140366 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:34:02.264258 140274064205632 submission_runner.py:411] Time since start: 12715.36s, 	Step: 24779, 	{'train/accuracy': 0.9912404417991638, 'train/loss': 0.0283038429915905, 'train/mean_average_precision': 0.45167276269461654, 'validation/accuracy': 0.9870346188545227, 'validation/loss': 0.04368169605731964, 'validation/mean_average_precision': 0.28327830571581236, 'validation/num_examples': 43793, 'test/accuracy': 0.9860441088676453, 'test/loss': 0.04680561646819115, 'test/mean_average_precision': 0.2693561350510513, 'test/num_examples': 43793, 'score': 7942.99335026741, 'total_duration': 12715.360845327377, 'accumulated_submission_time': 7942.99335026741, 'accumulated_eval_time': 4770.698044300079, 'accumulated_logging_time': 0.9801278114318848}
I0305 13:34:02.285303 140113222047488 logging_writer.py:48] [24779] accumulated_eval_time=4770.698044, accumulated_logging_time=0.980128, accumulated_submission_time=7942.993350, global_step=24779, preemption_count=0, score=7942.993350, test/accuracy=0.986044, test/loss=0.046806, test/mean_average_precision=0.269356, test/num_examples=43793, total_duration=12715.360845, train/accuracy=0.991240, train/loss=0.028304, train/mean_average_precision=0.451673, validation/accuracy=0.987035, validation/loss=0.043682, validation/mean_average_precision=0.283278, validation/num_examples=43793
I0305 13:34:09.273251 140211866552064 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.06066250428557396, loss=0.030956268310546875
I0305 13:34:41.676049 140113222047488 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05846228078007698, loss=0.030945897102355957
I0305 13:35:14.492654 140211866552064 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.05319487303495407, loss=0.0295550636947155
I0305 13:35:46.551106 140113222047488 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.039891671389341354, loss=0.027561454102396965
I0305 13:36:18.745930 140211866552064 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.04363033175468445, loss=0.029737181961536407
I0305 13:36:50.832461 140113222047488 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.043322425335645676, loss=0.029061146080493927
I0305 13:37:22.575499 140211866552064 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.06954921782016754, loss=0.030080242082476616
I0305 13:37:54.377124 140113222047488 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.039872635155916214, loss=0.026465758681297302
I0305 13:38:02.487589 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:40:05.704405 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:40:08.764493 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:40:11.793674 140274064205632 submission_runner.py:411] Time since start: 13084.89s, 	Step: 25526, 	{'train/accuracy': 0.9914011359214783, 'train/loss': 0.02769339457154274, 'train/mean_average_precision': 0.4690910450316292, 'validation/accuracy': 0.986968457698822, 'validation/loss': 0.044117432087659836, 'validation/mean_average_precision': 0.27685863913819864, 'validation/num_examples': 43793, 'test/accuracy': 0.9861544370651245, 'test/loss': 0.04672051966190338, 'test/mean_average_precision': 0.2696622948080067, 'test/num_examples': 43793, 'score': 8183.164006948471, 'total_duration': 13084.890377283096, 'accumulated_submission_time': 8183.164006948471, 'accumulated_eval_time': 4900.004084348679, 'accumulated_logging_time': 1.0125136375427246}
I0305 13:40:11.813876 140106748385024 logging_writer.py:48] [25526] accumulated_eval_time=4900.004084, accumulated_logging_time=1.012514, accumulated_submission_time=8183.164007, global_step=25526, preemption_count=0, score=8183.164007, test/accuracy=0.986154, test/loss=0.046721, test/mean_average_precision=0.269662, test/num_examples=43793, total_duration=13084.890377, train/accuracy=0.991401, train/loss=0.027693, train/mean_average_precision=0.469091, validation/accuracy=0.986968, validation/loss=0.044117, validation/mean_average_precision=0.276859, validation/num_examples=43793
I0305 13:40:36.024445 140113230440192 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.06438778340816498, loss=0.03193974122405052
I0305 13:41:08.033525 140106748385024 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.047033872455358505, loss=0.028261637315154076
I0305 13:41:40.160787 140113230440192 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.0437193363904953, loss=0.02698064036667347
I0305 13:42:12.386890 140106748385024 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.07536832988262177, loss=0.030883869156241417
I0305 13:42:45.066490 140113230440192 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.05202729254961014, loss=0.027947837486863136
I0305 13:43:18.000108 140106748385024 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.05083722248673439, loss=0.02718878909945488
I0305 13:43:50.333322 140113230440192 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.053812820464372635, loss=0.02821185253560543
I0305 13:44:11.803054 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:46:17.283429 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:46:20.746481 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:46:24.122493 140274064205632 submission_runner.py:411] Time since start: 13457.22s, 	Step: 26267, 	{'train/accuracy': 0.9915769696235657, 'train/loss': 0.027113502845168114, 'train/mean_average_precision': 0.47997638070675597, 'validation/accuracy': 0.9870049953460693, 'validation/loss': 0.0444050058722496, 'validation/mean_average_precision': 0.2801886624062228, 'validation/num_examples': 43793, 'test/accuracy': 0.9860975742340088, 'test/loss': 0.047425154596567154, 'test/mean_average_precision': 0.26476293395251477, 'test/num_examples': 43793, 'score': 8423.121535778046, 'total_duration': 13457.219170570374, 'accumulated_submission_time': 8423.121535778046, 'accumulated_eval_time': 5032.323452711105, 'accumulated_logging_time': 1.0439343452453613}
I0305 13:46:24.145694 140211866552064 logging_writer.py:48] [26267] accumulated_eval_time=5032.323453, accumulated_logging_time=1.043934, accumulated_submission_time=8423.121536, global_step=26267, preemption_count=0, score=8423.121536, test/accuracy=0.986098, test/loss=0.047425, test/mean_average_precision=0.264763, test/num_examples=43793, total_duration=13457.219171, train/accuracy=0.991577, train/loss=0.027114, train/mean_average_precision=0.479976, validation/accuracy=0.987005, validation/loss=0.044405, validation/mean_average_precision=0.280189, validation/num_examples=43793
I0305 13:46:35.373999 140211874944768 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.062200579792261124, loss=0.029383519664406776
I0305 13:47:08.538431 140211866552064 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05257880315184593, loss=0.02741861715912819
I0305 13:47:41.289404 140211874944768 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.06151999905705452, loss=0.028405146673321724
I0305 13:48:14.067445 140211866552064 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.05442807823419571, loss=0.030780194327235222
I0305 13:48:46.825616 140211874944768 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.0569952167570591, loss=0.03187625855207443
I0305 13:49:19.770303 140211866552064 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.04442061111330986, loss=0.025133388116955757
I0305 13:49:52.574928 140211874944768 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05553685501217842, loss=0.02951706387102604
I0305 13:50:24.366036 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:52:34.153256 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:52:37.282662 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:52:40.354280 140274064205632 submission_runner.py:411] Time since start: 13833.45s, 	Step: 26998, 	{'train/accuracy': 0.9920740127563477, 'train/loss': 0.025889698415994644, 'train/mean_average_precision': 0.5144971201680135, 'validation/accuracy': 0.9868710041046143, 'validation/loss': 0.04411295801401138, 'validation/mean_average_precision': 0.2735882717769362, 'validation/num_examples': 43793, 'test/accuracy': 0.9860420227050781, 'test/loss': 0.0467803031206131, 'test/mean_average_precision': 0.2685553927331099, 'test/num_examples': 43793, 'score': 8663.304875612259, 'total_duration': 13833.450981378555, 'accumulated_submission_time': 8663.304875612259, 'accumulated_eval_time': 5168.311667442322, 'accumulated_logging_time': 1.0791988372802734}
I0305 13:52:40.375642 140106748385024 logging_writer.py:48] [26998] accumulated_eval_time=5168.311667, accumulated_logging_time=1.079199, accumulated_submission_time=8663.304876, global_step=26998, preemption_count=0, score=8663.304876, test/accuracy=0.986042, test/loss=0.046780, test/mean_average_precision=0.268555, test/num_examples=43793, total_duration=13833.450981, train/accuracy=0.992074, train/loss=0.025890, train/mean_average_precision=0.514497, validation/accuracy=0.986871, validation/loss=0.044113, validation/mean_average_precision=0.273588, validation/num_examples=43793
I0305 13:52:41.356695 140113222047488 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05666426569223404, loss=0.028965163975954056
I0305 13:53:13.408139 140106748385024 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.054368846118450165, loss=0.029153741896152496
I0305 13:53:45.644695 140113222047488 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.0440182164311409, loss=0.0276943426579237
I0305 13:54:17.940619 140106748385024 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.060076091438531876, loss=0.029282929375767708
I0305 13:54:49.995192 140113222047488 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.0532488077878952, loss=0.02750554494559765
I0305 13:55:22.371824 140106748385024 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.05076804384589195, loss=0.027718203142285347
I0305 13:55:54.818210 140113222047488 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.06066614016890526, loss=0.032340094447135925
I0305 13:56:26.939918 140106748385024 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.05828769505023956, loss=0.03182677924633026
I0305 13:56:40.578068 140274064205632 spec.py:321] Evaluating on the training split.
I0305 13:58:45.063560 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 13:58:48.131407 140274064205632 spec.py:349] Evaluating on the test split.
I0305 13:58:51.144286 140274064205632 submission_runner.py:411] Time since start: 14204.24s, 	Step: 27744, 	{'train/accuracy': 0.9919695854187012, 'train/loss': 0.02612740732729435, 'train/mean_average_precision': 0.5067298145105661, 'validation/accuracy': 0.9869599342346191, 'validation/loss': 0.044281501322984695, 'validation/mean_average_precision': 0.277661715150162, 'validation/num_examples': 43793, 'test/accuracy': 0.9861022233963013, 'test/loss': 0.046983763575553894, 'test/mean_average_precision': 0.2671069133928673, 'test/num_examples': 43793, 'score': 8903.47397851944, 'total_duration': 14204.240990161896, 'accumulated_submission_time': 8903.47397851944, 'accumulated_eval_time': 5298.877843379974, 'accumulated_logging_time': 1.1134259700775146}
I0305 13:58:51.164360 140105330788096 logging_writer.py:48] [27744] accumulated_eval_time=5298.877843, accumulated_logging_time=1.113426, accumulated_submission_time=8903.473979, global_step=27744, preemption_count=0, score=8903.473979, test/accuracy=0.986102, test/loss=0.046984, test/mean_average_precision=0.267107, test/num_examples=43793, total_duration=14204.240990, train/accuracy=0.991970, train/loss=0.026127, train/mean_average_precision=0.506730, validation/accuracy=0.986960, validation/loss=0.044282, validation/mean_average_precision=0.277662, validation/num_examples=43793
I0305 13:59:09.555211 140113230440192 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.0505247563123703, loss=0.028768271207809448
I0305 13:59:41.393669 140105330788096 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05576615035533905, loss=0.025026174262166023
I0305 14:00:13.419964 140113230440192 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.052168283611536026, loss=0.029003029689192772
I0305 14:00:45.439101 140105330788096 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.06122925132513046, loss=0.028985727578401566
I0305 14:01:17.589502 140113230440192 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.06188398599624634, loss=0.029909951612353325
I0305 14:01:49.911848 140105330788096 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.06253299117088318, loss=0.030460940673947334
I0305 14:02:22.147142 140113230440192 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.0662742331624031, loss=0.027522921562194824
I0305 14:02:51.183552 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:04:58.441739 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:05:01.537038 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:05:04.699631 140274064205632 submission_runner.py:411] Time since start: 14577.80s, 	Step: 28492, 	{'train/accuracy': 0.9916380047798157, 'train/loss': 0.026933841407299042, 'train/mean_average_precision': 0.49188835108561335, 'validation/accuracy': 0.9869672060012817, 'validation/loss': 0.04486069455742836, 'validation/mean_average_precision': 0.2792009679877025, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.047752972692251205, 'test/mean_average_precision': 0.26359064662229303, 'test/num_examples': 43793, 'score': 9143.46143746376, 'total_duration': 14577.796331167221, 'accumulated_submission_time': 9143.46143746376, 'accumulated_eval_time': 5432.393877744675, 'accumulated_logging_time': 1.1442391872406006}
I0305 14:05:04.720402 140113222047488 logging_writer.py:48] [28492] accumulated_eval_time=5432.393878, accumulated_logging_time=1.144239, accumulated_submission_time=9143.461437, global_step=28492, preemption_count=0, score=9143.461437, test/accuracy=0.986113, test/loss=0.047753, test/mean_average_precision=0.263591, test/num_examples=43793, total_duration=14577.796331, train/accuracy=0.991638, train/loss=0.026934, train/mean_average_precision=0.491888, validation/accuracy=0.986967, validation/loss=0.044861, validation/mean_average_precision=0.279201, validation/num_examples=43793
I0305 14:05:07.566809 140211874944768 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.06249316409230232, loss=0.028214016929268837
I0305 14:05:39.302301 140113222047488 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.055207978934049606, loss=0.028206776827573776
I0305 14:06:11.936533 140211874944768 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.05252457782626152, loss=0.030869295820593834
I0305 14:06:43.899181 140113222047488 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.07308527082204819, loss=0.03254375606775284
I0305 14:07:16.043379 140211874944768 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.06820877641439438, loss=0.03299032896757126
I0305 14:07:48.251342 140113222047488 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0638560950756073, loss=0.0315726101398468
I0305 14:08:20.054861 140211874944768 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.052521202713251114, loss=0.027212897315621376
I0305 14:08:51.847035 140113222047488 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.0629519447684288, loss=0.02553391642868519
I0305 14:09:04.908584 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:11:05.876143 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:11:09.015700 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:11:12.068731 140274064205632 submission_runner.py:411] Time since start: 14945.17s, 	Step: 29242, 	{'train/accuracy': 0.991564154624939, 'train/loss': 0.02720498852431774, 'train/mean_average_precision': 0.4691972139556656, 'validation/accuracy': 0.9871259331703186, 'validation/loss': 0.04426884651184082, 'validation/mean_average_precision': 0.2877655372285986, 'validation/num_examples': 43793, 'test/accuracy': 0.9862269163131714, 'test/loss': 0.047035496681928635, 'test/mean_average_precision': 0.2678343298693436, 'test/num_examples': 43793, 'score': 9383.617679357529, 'total_duration': 14945.165321350098, 'accumulated_submission_time': 9383.617679357529, 'accumulated_eval_time': 5559.553866147995, 'accumulated_logging_time': 1.1761319637298584}
I0305 14:11:12.089909 140105330788096 logging_writer.py:48] [29242] accumulated_eval_time=5559.553866, accumulated_logging_time=1.176132, accumulated_submission_time=9383.617679, global_step=29242, preemption_count=0, score=9383.617679, test/accuracy=0.986227, test/loss=0.047035, test/mean_average_precision=0.267834, test/num_examples=43793, total_duration=14945.165321, train/accuracy=0.991564, train/loss=0.027205, train/mean_average_precision=0.469197, validation/accuracy=0.987126, validation/loss=0.044269, validation/mean_average_precision=0.287766, validation/num_examples=43793
I0305 14:11:31.311599 140106748385024 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05098268762230873, loss=0.026501327753067017
I0305 14:12:03.333996 140105330788096 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05930540710687637, loss=0.028529006987810135
I0305 14:12:35.145669 140106748385024 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0554160512983799, loss=0.028369000181555748
I0305 14:13:07.230252 140105330788096 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.06522864103317261, loss=0.02894444763660431
I0305 14:13:39.144263 140106748385024 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.0639047771692276, loss=0.027539392933249474
I0305 14:14:11.782714 140105330788096 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.059828389436006546, loss=0.030611364170908928
I0305 14:14:43.454487 140106748385024 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.057534124702215195, loss=0.02691507153213024
I0305 14:15:12.224939 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:17:18.158265 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:17:21.283169 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:17:24.353482 140274064205632 submission_runner.py:411] Time since start: 15317.45s, 	Step: 29991, 	{'train/accuracy': 0.991787314414978, 'train/loss': 0.026690585538744926, 'train/mean_average_precision': 0.4814734467220524, 'validation/accuracy': 0.9869627952575684, 'validation/loss': 0.04385329410433769, 'validation/mean_average_precision': 0.2948247113332413, 'validation/num_examples': 43793, 'test/accuracy': 0.986084520816803, 'test/loss': 0.04676615446805954, 'test/mean_average_precision': 0.2690176516848281, 'test/num_examples': 43793, 'score': 9623.721709728241, 'total_duration': 15317.450065851212, 'accumulated_submission_time': 9623.721709728241, 'accumulated_eval_time': 5691.682248830795, 'accumulated_logging_time': 1.2084007263183594}
I0305 14:17:24.375137 140098771031808 logging_writer.py:48] [29991] accumulated_eval_time=5691.682249, accumulated_logging_time=1.208401, accumulated_submission_time=9623.721710, global_step=29991, preemption_count=0, score=9623.721710, test/accuracy=0.986085, test/loss=0.046766, test/mean_average_precision=0.269018, test/num_examples=43793, total_duration=15317.450066, train/accuracy=0.991787, train/loss=0.026691, train/mean_average_precision=0.481473, validation/accuracy=0.986963, validation/loss=0.043853, validation/mean_average_precision=0.294825, validation/num_examples=43793
I0305 14:17:27.716924 140113230440192 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0671769306063652, loss=0.028208095580339432
I0305 14:17:59.572337 140098771031808 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05634830519556999, loss=0.028218695893883705
I0305 14:18:32.492508 140113230440192 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.05516352504491806, loss=0.025975093245506287
I0305 14:19:06.121611 140098771031808 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.0654328241944313, loss=0.02663007192313671
I0305 14:19:38.426740 140113230440192 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.05540071427822113, loss=0.025989407673478127
I0305 14:20:10.466673 140098771031808 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.05381394177675247, loss=0.026609009131789207
I0305 14:20:42.469218 140113230440192 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05321190878748894, loss=0.02576514333486557
I0305 14:21:14.260096 140098771031808 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.06092880293726921, loss=0.027063461020588875
I0305 14:21:24.374265 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:23:27.416924 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:23:30.859601 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:23:34.216054 140274064205632 submission_runner.py:411] Time since start: 15687.31s, 	Step: 30733, 	{'train/accuracy': 0.9916547536849976, 'train/loss': 0.0268725398927927, 'train/mean_average_precision': 0.505886375231136, 'validation/accuracy': 0.9870597720146179, 'validation/loss': 0.04401272162795067, 'validation/mean_average_precision': 0.2781033442700349, 'validation/num_examples': 43793, 'test/accuracy': 0.9861578345298767, 'test/loss': 0.04684344679117203, 'test/mean_average_precision': 0.27064823045383146, 'test/num_examples': 43793, 'score': 9863.68747830391, 'total_duration': 15687.312734127045, 'accumulated_submission_time': 9863.68747830391, 'accumulated_eval_time': 5821.52396774292, 'accumulated_logging_time': 1.2435061931610107}
I0305 14:23:34.239875 140106748385024 logging_writer.py:48] [30733] accumulated_eval_time=5821.523968, accumulated_logging_time=1.243506, accumulated_submission_time=9863.687478, global_step=30733, preemption_count=0, score=9863.687478, test/accuracy=0.986158, test/loss=0.046843, test/mean_average_precision=0.270648, test/num_examples=43793, total_duration=15687.312734, train/accuracy=0.991655, train/loss=0.026873, train/mean_average_precision=0.505886, validation/accuracy=0.987060, validation/loss=0.044013, validation/mean_average_precision=0.278103, validation/num_examples=43793
I0305 14:23:56.798620 140113222047488 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.084751196205616, loss=0.0309541467577219
I0305 14:24:29.560965 140106748385024 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.05588662251830101, loss=0.028047116473317146
I0305 14:25:02.373189 140113222047488 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.055468279868364334, loss=0.022457309067249298
I0305 14:25:35.039813 140106748385024 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.04759584739804268, loss=0.02770881913602352
I0305 14:26:07.664458 140113222047488 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.08477417379617691, loss=0.03153484687209129
I0305 14:26:40.559373 140106748385024 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.055306073278188705, loss=0.030644992366433144
I0305 14:27:13.167792 140113222047488 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.0655830129981041, loss=0.028113115578889847
I0305 14:27:34.512435 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:29:40.763167 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:29:44.158097 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:29:47.441762 140274064205632 submission_runner.py:411] Time since start: 16060.54s, 	Step: 31466, 	{'train/accuracy': 0.991858720779419, 'train/loss': 0.026429558172822, 'train/mean_average_precision': 0.4930497997143744, 'validation/accuracy': 0.9869838953018188, 'validation/loss': 0.04407519847154617, 'validation/mean_average_precision': 0.28272529183134515, 'validation/num_examples': 43793, 'test/accuracy': 0.9861629009246826, 'test/loss': 0.04662102833390236, 'test/mean_average_precision': 0.27407602827921174, 'test/num_examples': 43793, 'score': 10103.922180891037, 'total_duration': 16060.538444519043, 'accumulated_submission_time': 10103.922180891037, 'accumulated_eval_time': 5954.453230142593, 'accumulated_logging_time': 1.2810778617858887}
I0305 14:29:47.465855 140105330788096 logging_writer.py:48] [31466] accumulated_eval_time=5954.453230, accumulated_logging_time=1.281078, accumulated_submission_time=10103.922181, global_step=31466, preemption_count=0, score=10103.922181, test/accuracy=0.986163, test/loss=0.046621, test/mean_average_precision=0.274076, test/num_examples=43793, total_duration=16060.538445, train/accuracy=0.991859, train/loss=0.026430, train/mean_average_precision=0.493050, validation/accuracy=0.986984, validation/loss=0.044075, validation/mean_average_precision=0.282725, validation/num_examples=43793
I0305 14:29:59.029443 140113230440192 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0470394603908062, loss=0.025951243937015533
I0305 14:30:31.712216 140105330788096 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06898275762796402, loss=0.028230872005224228
I0305 14:31:04.271678 140113230440192 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.05534609034657478, loss=0.027385002002120018
I0305 14:31:36.582025 140105330788096 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.07029685378074646, loss=0.026724962517619133
I0305 14:32:09.376555 140113230440192 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0642036572098732, loss=0.025944126769900322
I0305 14:32:42.065901 140105330788096 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.05953076109290123, loss=0.028249498456716537
I0305 14:33:14.652485 140113230440192 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.05617563799023628, loss=0.03057730570435524
I0305 14:33:47.394419 140105330788096 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.05815673992037773, loss=0.02723357081413269
I0305 14:33:47.718229 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:35:53.915365 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:35:56.987161 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:36:00.052098 140274064205632 submission_runner.py:411] Time since start: 16433.15s, 	Step: 32202, 	{'train/accuracy': 0.9919987320899963, 'train/loss': 0.02572508528828621, 'train/mean_average_precision': 0.5162503143904493, 'validation/accuracy': 0.9871016144752502, 'validation/loss': 0.0439109280705452, 'validation/mean_average_precision': 0.28905598740252014, 'validation/num_examples': 43793, 'test/accuracy': 0.9862138628959656, 'test/loss': 0.04681071266531944, 'test/mean_average_precision': 0.2729542577372867, 'test/num_examples': 43793, 'score': 10344.138065576553, 'total_duration': 16433.148668050766, 'accumulated_submission_time': 10344.138065576553, 'accumulated_eval_time': 6086.7869300842285, 'accumulated_logging_time': 1.3168962001800537}
I0305 14:36:00.074254 140098771031808 logging_writer.py:48] [32202] accumulated_eval_time=6086.786930, accumulated_logging_time=1.316896, accumulated_submission_time=10344.138066, global_step=32202, preemption_count=0, score=10344.138066, test/accuracy=0.986214, test/loss=0.046811, test/mean_average_precision=0.272954, test/num_examples=43793, total_duration=16433.148668, train/accuracy=0.991999, train/loss=0.025725, train/mean_average_precision=0.516250, validation/accuracy=0.987102, validation/loss=0.043911, validation/mean_average_precision=0.289056, validation/num_examples=43793
I0305 14:36:31.726445 140113222047488 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.06708363443613052, loss=0.02714165672659874
I0305 14:37:04.014419 140098771031808 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.06403008848428726, loss=0.03135270997881889
I0305 14:37:36.234344 140113222047488 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.07318319380283356, loss=0.02882559970021248
I0305 14:38:08.083811 140098771031808 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.06612806022167206, loss=0.028148261830210686
I0305 14:38:40.108069 140113222047488 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.054481882601976395, loss=0.024294182658195496
I0305 14:39:11.981100 140098771031808 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.07134085893630981, loss=0.02941262349486351
I0305 14:39:43.954309 140113222047488 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05429088696837425, loss=0.024875996634364128
I0305 14:40:00.056430 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:42:04.613840 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:42:07.753628 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:42:10.848269 140274064205632 submission_runner.py:411] Time since start: 16803.94s, 	Step: 32951, 	{'train/accuracy': 0.9919003248214722, 'train/loss': 0.025876253843307495, 'train/mean_average_precision': 0.5181524506162096, 'validation/accuracy': 0.9869920015335083, 'validation/loss': 0.04430317506194115, 'validation/mean_average_precision': 0.27930558229968044, 'validation/num_examples': 43793, 'test/accuracy': 0.9862251877784729, 'test/loss': 0.047044627368450165, 'test/mean_average_precision': 0.27503391949680533, 'test/num_examples': 43793, 'score': 10584.087520360947, 'total_duration': 16803.94484782219, 'accumulated_submission_time': 10584.087520360947, 'accumulated_eval_time': 6217.578600645065, 'accumulated_logging_time': 1.3507368564605713}
I0305 14:42:10.872259 140105330788096 logging_writer.py:48] [32951] accumulated_eval_time=6217.578601, accumulated_logging_time=1.350737, accumulated_submission_time=10584.087520, global_step=32951, preemption_count=0, score=10584.087520, test/accuracy=0.986225, test/loss=0.047045, test/mean_average_precision=0.275034, test/num_examples=43793, total_duration=16803.944848, train/accuracy=0.991900, train/loss=0.025876, train/mean_average_precision=0.518152, validation/accuracy=0.986992, validation/loss=0.044303, validation/mean_average_precision=0.279306, validation/num_examples=43793
I0305 14:42:27.064023 140113230440192 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.06079798936843872, loss=0.029961328953504562
I0305 14:42:59.062984 140105330788096 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.062215566635131836, loss=0.0270132664591074
I0305 14:43:31.245146 140113230440192 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.0680067390203476, loss=0.03231538087129593
I0305 14:44:03.493664 140105330788096 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.058257319033145905, loss=0.02900863252580166
I0305 14:44:35.152893 140113230440192 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.05476429685950279, loss=0.02745305746793747
I0305 14:45:07.259895 140105330788096 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.059993818402290344, loss=0.029925011098384857
I0305 14:45:39.238066 140113230440192 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.06215033680200577, loss=0.031126011162996292
I0305 14:46:11.058387 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:48:12.399479 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:48:15.810739 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:48:19.162921 140274064205632 submission_runner.py:411] Time since start: 17172.26s, 	Step: 33700, 	{'train/accuracy': 0.9923912286758423, 'train/loss': 0.024578940123319626, 'train/mean_average_precision': 0.5401013702391663, 'validation/accuracy': 0.9869599342346191, 'validation/loss': 0.04400760680437088, 'validation/mean_average_precision': 0.28493921637405084, 'validation/num_examples': 43793, 'test/accuracy': 0.9861350655555725, 'test/loss': 0.04677513986825943, 'test/mean_average_precision': 0.27262129464696255, 'test/num_examples': 43793, 'score': 10824.240885734558, 'total_duration': 17172.259604930878, 'accumulated_submission_time': 10824.240885734558, 'accumulated_eval_time': 6345.683080196381, 'accumulated_logging_time': 1.3866889476776123}
I0305 14:48:19.187651 140106748385024 logging_writer.py:48] [33700] accumulated_eval_time=6345.683080, accumulated_logging_time=1.386689, accumulated_submission_time=10824.240886, global_step=33700, preemption_count=0, score=10824.240886, test/accuracy=0.986135, test/loss=0.046775, test/mean_average_precision=0.272621, test/num_examples=43793, total_duration=17172.259605, train/accuracy=0.992391, train/loss=0.024579, train/mean_average_precision=0.540101, validation/accuracy=0.986960, validation/loss=0.044008, validation/mean_average_precision=0.284939, validation/num_examples=43793
I0305 14:48:19.550016 140113222047488 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.06368380039930344, loss=0.031194986775517464
I0305 14:48:51.861902 140106748385024 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06191183999180794, loss=0.0306059792637825
I0305 14:49:24.600484 140113222047488 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.05787738412618637, loss=0.028330756351351738
I0305 14:49:57.231549 140106748385024 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0695478543639183, loss=0.02637936733663082
I0305 14:50:29.318967 140113222047488 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.07487092167139053, loss=0.027526216581463814
I0305 14:51:01.171007 140106748385024 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06175534427165985, loss=0.025470923632383347
I0305 14:51:33.163685 140113222047488 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06048356369137764, loss=0.026885053142905235
I0305 14:52:05.522678 140106748385024 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.07330671697854996, loss=0.030127111822366714
I0305 14:52:19.336662 140274064205632 spec.py:321] Evaluating on the training split.
I0305 14:54:18.322015 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 14:54:21.352625 140274064205632 spec.py:349] Evaluating on the test split.
I0305 14:54:24.419057 140274064205632 submission_runner.py:411] Time since start: 17537.52s, 	Step: 34444, 	{'train/accuracy': 0.9924638271331787, 'train/loss': 0.024425353854894638, 'train/mean_average_precision': 0.5408102506296366, 'validation/accuracy': 0.9870119094848633, 'validation/loss': 0.044088494032621384, 'validation/mean_average_precision': 0.2841427396357607, 'validation/num_examples': 43793, 'test/accuracy': 0.9861670732498169, 'test/loss': 0.04698425903916359, 'test/mean_average_precision': 0.27295448408140094, 'test/num_examples': 43793, 'score': 11064.355593442917, 'total_duration': 17537.51575899124, 'accumulated_submission_time': 11064.355593442917, 'accumulated_eval_time': 6470.765429973602, 'accumulated_logging_time': 1.423579216003418}
I0305 14:54:24.441198 140098771031808 logging_writer.py:48] [34444] accumulated_eval_time=6470.765430, accumulated_logging_time=1.423579, accumulated_submission_time=11064.355593, global_step=34444, preemption_count=0, score=11064.355593, test/accuracy=0.986167, test/loss=0.046984, test/mean_average_precision=0.272954, test/num_examples=43793, total_duration=17537.515759, train/accuracy=0.992464, train/loss=0.024425, train/mean_average_precision=0.540810, validation/accuracy=0.987012, validation/loss=0.044088, validation/mean_average_precision=0.284143, validation/num_examples=43793
I0305 14:54:43.090918 140113230440192 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.06451951712369919, loss=0.026161953806877136
I0305 14:55:15.352145 140098771031808 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.05910486727952957, loss=0.02526463381946087
I0305 14:55:46.916352 140113230440192 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06167391687631607, loss=0.02542675845324993
I0305 14:56:19.197669 140098771031808 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.05757400020956993, loss=0.02725924178957939
I0305 14:56:51.031576 140113230440192 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.06556741893291473, loss=0.028714505955576897
I0305 14:57:23.224320 140098771031808 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.07405246794223785, loss=0.032062701880931854
I0305 14:57:55.414645 140113230440192 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06674905866384506, loss=0.02726607583463192
I0305 14:58:24.671538 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:00:26.580383 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:00:29.641119 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:00:32.688955 140274064205632 submission_runner.py:411] Time since start: 17905.79s, 	Step: 35193, 	{'train/accuracy': 0.992255449295044, 'train/loss': 0.024895818904042244, 'train/mean_average_precision': 0.5338702720010491, 'validation/accuracy': 0.9870837330818176, 'validation/loss': 0.0445379801094532, 'validation/mean_average_precision': 0.27716090540725485, 'validation/num_examples': 43793, 'test/accuracy': 0.986255943775177, 'test/loss': 0.04755125939846039, 'test/mean_average_precision': 0.27122366597014047, 'test/num_examples': 43793, 'score': 11304.5543050766, 'total_duration': 17905.78564977646, 'accumulated_submission_time': 11304.5543050766, 'accumulated_eval_time': 6598.782794237137, 'accumulated_logging_time': 1.4568994045257568}
I0305 15:00:32.711225 140105330788096 logging_writer.py:48] [35193] accumulated_eval_time=6598.782794, accumulated_logging_time=1.456899, accumulated_submission_time=11304.554305, global_step=35193, preemption_count=0, score=11304.554305, test/accuracy=0.986256, test/loss=0.047551, test/mean_average_precision=0.271224, test/num_examples=43793, total_duration=17905.785650, train/accuracy=0.992255, train/loss=0.024896, train/mean_average_precision=0.533870, validation/accuracy=0.987084, validation/loss=0.044538, validation/mean_average_precision=0.277161, validation/num_examples=43793
I0305 15:00:35.289161 140106748385024 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.0723661258816719, loss=0.02808310277760029
I0305 15:01:07.226760 140105330788096 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.0639147236943245, loss=0.025822574272751808
I0305 15:01:39.540134 140106748385024 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.0634303092956543, loss=0.028322506695985794
I0305 15:02:11.753816 140105330788096 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0643070638179779, loss=0.02894732914865017
I0305 15:02:44.075685 140106748385024 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05279507488012314, loss=0.02428857795894146
I0305 15:03:16.477857 140105330788096 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.062144242227077484, loss=0.02790125459432602
I0305 15:03:48.737196 140106748385024 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06589861214160919, loss=0.02433978021144867
I0305 15:04:21.152441 140105330788096 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.0591551698744297, loss=0.024557771161198616
I0305 15:04:32.909536 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:06:36.001036 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:06:39.126127 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:06:42.166311 140274064205632 submission_runner.py:411] Time since start: 18275.26s, 	Step: 35938, 	{'train/accuracy': 0.9919145107269287, 'train/loss': 0.02584361843764782, 'train/mean_average_precision': 0.5134692618480501, 'validation/accuracy': 0.9870756268501282, 'validation/loss': 0.04469547048211098, 'validation/mean_average_precision': 0.2806026018791224, 'validation/num_examples': 43793, 'test/accuracy': 0.9862921833992004, 'test/loss': 0.047468844801187515, 'test/mean_average_precision': 0.27375625335653714, 'test/num_examples': 43793, 'score': 11544.72012758255, 'total_duration': 18275.26300549507, 'accumulated_submission_time': 11544.72012758255, 'accumulated_eval_time': 6728.039513587952, 'accumulated_logging_time': 1.4914841651916504}
I0305 15:06:42.188613 140113222047488 logging_writer.py:48] [35938] accumulated_eval_time=6728.039514, accumulated_logging_time=1.491484, accumulated_submission_time=11544.720128, global_step=35938, preemption_count=0, score=11544.720128, test/accuracy=0.986292, test/loss=0.047469, test/mean_average_precision=0.273756, test/num_examples=43793, total_duration=18275.263005, train/accuracy=0.991915, train/loss=0.025844, train/mean_average_precision=0.513469, validation/accuracy=0.987076, validation/loss=0.044695, validation/mean_average_precision=0.280603, validation/num_examples=43793
I0305 15:07:02.881408 140113230440192 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.05717569589614868, loss=0.027513284236192703
I0305 15:07:34.711379 140113222047488 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.07626314461231232, loss=0.028985798358917236
I0305 15:08:06.917538 140113230440192 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07062715291976929, loss=0.02959512360394001
I0305 15:08:39.387437 140113222047488 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06054083630442619, loss=0.027270877733826637
I0305 15:09:11.238210 140113230440192 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.08177951723337173, loss=0.024684300646185875
I0305 15:09:43.309994 140113222047488 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06758779287338257, loss=0.029750633984804153
I0305 15:10:15.477852 140113230440192 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06892580538988113, loss=0.032168809324502945
I0305 15:10:42.270206 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:12:43.790704 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:12:46.870286 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:12:49.926642 140274064205632 submission_runner.py:411] Time since start: 18643.02s, 	Step: 36684, 	{'train/accuracy': 0.9919720888137817, 'train/loss': 0.025846732780337334, 'train/mean_average_precision': 0.49908316290526733, 'validation/accuracy': 0.9870249032974243, 'validation/loss': 0.044183675199747086, 'validation/mean_average_precision': 0.2844107591122186, 'validation/num_examples': 43793, 'test/accuracy': 0.9862239360809326, 'test/loss': 0.046950921416282654, 'test/mean_average_precision': 0.27379143182807, 'test/num_examples': 43793, 'score': 11784.769032478333, 'total_duration': 18643.02334499359, 'accumulated_submission_time': 11784.769032478333, 'accumulated_eval_time': 6855.695904970169, 'accumulated_logging_time': 1.526174783706665}
I0305 15:12:49.949309 140098771031808 logging_writer.py:48] [36684] accumulated_eval_time=6855.695905, accumulated_logging_time=1.526175, accumulated_submission_time=11784.769032, global_step=36684, preemption_count=0, score=11784.769032, test/accuracy=0.986224, test/loss=0.046951, test/mean_average_precision=0.273791, test/num_examples=43793, total_duration=18643.023345, train/accuracy=0.991972, train/loss=0.025847, train/mean_average_precision=0.499083, validation/accuracy=0.987025, validation/loss=0.044184, validation/mean_average_precision=0.284411, validation/num_examples=43793
I0305 15:12:55.403135 140106748385024 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.08912021666765213, loss=0.028896676376461983
I0305 15:13:27.767107 140098771031808 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.08751649409532547, loss=0.029168643057346344
I0305 15:13:59.806226 140106748385024 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06536507606506348, loss=0.027047140523791313
I0305 15:14:31.723000 140098771031808 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06665978580713272, loss=0.026851441711187363
I0305 15:15:03.634243 140106748385024 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.08612240105867386, loss=0.030189938843250275
I0305 15:15:35.690959 140098771031808 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.060724273324012756, loss=0.02467479184269905
I0305 15:16:07.980315 140106748385024 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06207996979355812, loss=0.027818314731121063
I0305 15:16:40.596563 140098771031808 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06621415913105011, loss=0.026285916566848755
I0305 15:16:50.100431 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:18:53.093655 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:18:56.286418 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:18:59.337196 140274064205632 submission_runner.py:411] Time since start: 19012.43s, 	Step: 37430, 	{'train/accuracy': 0.992161750793457, 'train/loss': 0.025162875652313232, 'train/mean_average_precision': 0.5274201870761099, 'validation/accuracy': 0.9870399236679077, 'validation/loss': 0.04433576762676239, 'validation/mean_average_precision': 0.2835930685939118, 'validation/num_examples': 43793, 'test/accuracy': 0.9861733913421631, 'test/loss': 0.047271646559238434, 'test/mean_average_precision': 0.2732995899724466, 'test/num_examples': 43793, 'score': 12024.889050722122, 'total_duration': 19012.433899879456, 'accumulated_submission_time': 12024.889050722122, 'accumulated_eval_time': 6984.932626485825, 'accumulated_logging_time': 1.5598630905151367}
I0305 15:18:59.359748 140113222047488 logging_writer.py:48] [37430] accumulated_eval_time=6984.932626, accumulated_logging_time=1.559863, accumulated_submission_time=12024.889051, global_step=37430, preemption_count=0, score=12024.889051, test/accuracy=0.986173, test/loss=0.047272, test/mean_average_precision=0.273300, test/num_examples=43793, total_duration=19012.433900, train/accuracy=0.992162, train/loss=0.025163, train/mean_average_precision=0.527420, validation/accuracy=0.987040, validation/loss=0.044336, validation/mean_average_precision=0.283593, validation/num_examples=43793
I0305 15:19:22.128079 140113230440192 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.06855127960443497, loss=0.030428845435380936
I0305 15:19:54.219330 140113222047488 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06606228649616241, loss=0.02573997527360916
I0305 15:20:26.254744 140113230440192 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06412876397371292, loss=0.0279054194688797
I0305 15:20:58.453373 140113222047488 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06361223757266998, loss=0.02905871346592903
I0305 15:21:30.914537 140113230440192 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.09068625420331955, loss=0.030198954045772552
I0305 15:22:03.266489 140113222047488 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0710812583565712, loss=0.02898448519408703
I0305 15:22:35.266221 140113230440192 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.07139139622449875, loss=0.02719373255968094
I0305 15:22:59.384363 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:25:02.473283 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:25:05.546376 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:25:08.537939 140274064205632 submission_runner.py:411] Time since start: 19381.63s, 	Step: 38176, 	{'train/accuracy': 0.9922487139701843, 'train/loss': 0.024734342470765114, 'train/mean_average_precision': 0.5345395525532157, 'validation/accuracy': 0.9869737029075623, 'validation/loss': 0.044567886739969254, 'validation/mean_average_precision': 0.2862172278706824, 'validation/num_examples': 43793, 'test/accuracy': 0.9861935973167419, 'test/loss': 0.047460202127695084, 'test/mean_average_precision': 0.27831728075744855, 'test/num_examples': 43793, 'score': 12264.882412672043, 'total_duration': 19381.634644031525, 'accumulated_submission_time': 12264.882412672043, 'accumulated_eval_time': 7114.0861604213715, 'accumulated_logging_time': 1.593503713607788}
I0305 15:25:08.560516 140105330788096 logging_writer.py:48] [38176] accumulated_eval_time=7114.086160, accumulated_logging_time=1.593504, accumulated_submission_time=12264.882413, global_step=38176, preemption_count=0, score=12264.882413, test/accuracy=0.986194, test/loss=0.047460, test/mean_average_precision=0.278317, test/num_examples=43793, total_duration=19381.634644, train/accuracy=0.992249, train/loss=0.024734, train/mean_average_precision=0.534540, validation/accuracy=0.986974, validation/loss=0.044568, validation/mean_average_precision=0.286217, validation/num_examples=43793
I0305 15:25:16.707248 140106748385024 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.10686492174863815, loss=0.02775450609624386
I0305 15:25:48.813628 140105330788096 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.06083232909440994, loss=0.02590973675251007
I0305 15:26:20.891031 140106748385024 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06269553303718567, loss=0.02330302633345127
I0305 15:26:52.549719 140105330788096 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.05933474376797676, loss=0.025534477084875107
I0305 15:27:24.660473 140106748385024 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.07605911791324615, loss=0.02761630341410637
I0305 15:27:56.779471 140105330788096 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.07337237149477005, loss=0.024950359016656876
I0305 15:28:29.063777 140106748385024 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.07424730062484741, loss=0.027041707187891006
I0305 15:29:01.741072 140105330788096 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.07028292864561081, loss=0.0288822203874588
I0305 15:29:08.759454 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:31:09.104421 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:31:12.208777 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:31:15.232874 140274064205632 submission_runner.py:411] Time since start: 19748.33s, 	Step: 38923, 	{'train/accuracy': 0.9923862218856812, 'train/loss': 0.024311818182468414, 'train/mean_average_precision': 0.5500534552972087, 'validation/accuracy': 0.9870415329933167, 'validation/loss': 0.04468541219830513, 'validation/mean_average_precision': 0.2939358720634637, 'validation/num_examples': 43793, 'test/accuracy': 0.9862353205680847, 'test/loss': 0.04769499599933624, 'test/mean_average_precision': 0.27204012315947035, 'test/num_examples': 43793, 'score': 12505.049992084503, 'total_duration': 19748.329575061798, 'accumulated_submission_time': 12505.049992084503, 'accumulated_eval_time': 7240.5595326423645, 'accumulated_logging_time': 1.6269724369049072}
I0305 15:31:15.256400 140113222047488 logging_writer.py:48] [38923] accumulated_eval_time=7240.559533, accumulated_logging_time=1.626972, accumulated_submission_time=12505.049992, global_step=38923, preemption_count=0, score=12505.049992, test/accuracy=0.986235, test/loss=0.047695, test/mean_average_precision=0.272040, test/num_examples=43793, total_duration=19748.329575, train/accuracy=0.992386, train/loss=0.024312, train/mean_average_precision=0.550053, validation/accuracy=0.987042, validation/loss=0.044685, validation/mean_average_precision=0.293936, validation/num_examples=43793
I0305 15:31:40.697836 140113230440192 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07223329693078995, loss=0.027404487133026123
I0305 15:32:13.317050 140113222047488 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.07101328670978546, loss=0.026676872745156288
I0305 15:32:45.424797 140113230440192 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07183073461055756, loss=0.029468508437275887
I0305 15:33:17.399734 140113222047488 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.06793860346078873, loss=0.027210881933569908
I0305 15:33:49.310863 140113230440192 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06257390975952148, loss=0.026399003341794014
I0305 15:34:21.212602 140113222047488 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.06594662368297577, loss=0.02373637445271015
I0305 15:34:53.029259 140113230440192 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07583567500114441, loss=0.02878129482269287
I0305 15:35:15.249391 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:37:17.828199 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:37:20.914225 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:37:23.884486 140274064205632 submission_runner.py:411] Time since start: 20116.98s, 	Step: 39670, 	{'train/accuracy': 0.9924398064613342, 'train/loss': 0.02402450516819954, 'train/mean_average_precision': 0.5614326523796441, 'validation/accuracy': 0.9870817065238953, 'validation/loss': 0.04455561563372612, 'validation/mean_average_precision': 0.2897896197496002, 'validation/num_examples': 43793, 'test/accuracy': 0.9862989187240601, 'test/loss': 0.04731585830450058, 'test/mean_average_precision': 0.2731693622950285, 'test/num_examples': 43793, 'score': 12745.011418104172, 'total_duration': 20116.981176376343, 'accumulated_submission_time': 12745.011418104172, 'accumulated_eval_time': 7369.194571733475, 'accumulated_logging_time': 1.6616127490997314}
I0305 15:37:23.908177 140098771031808 logging_writer.py:48] [39670] accumulated_eval_time=7369.194572, accumulated_logging_time=1.661613, accumulated_submission_time=12745.011418, global_step=39670, preemption_count=0, score=12745.011418, test/accuracy=0.986299, test/loss=0.047316, test/mean_average_precision=0.273169, test/num_examples=43793, total_duration=20116.981176, train/accuracy=0.992440, train/loss=0.024025, train/mean_average_precision=0.561433, validation/accuracy=0.987082, validation/loss=0.044556, validation/mean_average_precision=0.289790, validation/num_examples=43793
I0305 15:37:34.136087 140106748385024 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.06842414289712906, loss=0.026836834847927094
I0305 15:38:07.326836 140098771031808 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.06950438767671585, loss=0.024420391768217087
I0305 15:38:39.389144 140106748385024 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.0780538022518158, loss=0.027216244488954544
I0305 15:39:11.675132 140098771031808 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.064536452293396, loss=0.0274952482432127
I0305 15:39:44.184215 140106748385024 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.07780155539512634, loss=0.02714928612112999
I0305 15:40:16.401878 140098771031808 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07186388224363327, loss=0.025707583874464035
I0305 15:40:49.057991 140106748385024 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.0799039900302887, loss=0.029083091765642166
I0305 15:41:21.365144 140098771031808 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.06824754178524017, loss=0.025224875658750534
I0305 15:41:23.893683 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:43:25.659888 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:43:28.722992 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:43:31.736452 140274064205632 submission_runner.py:411] Time since start: 20484.83s, 	Step: 40409, 	{'train/accuracy': 0.9927424192428589, 'train/loss': 0.023308221250772476, 'train/mean_average_precision': 0.574511917572232, 'validation/accuracy': 0.9868040680885315, 'validation/loss': 0.044756412506103516, 'validation/mean_average_precision': 0.28826395224240636, 'validation/num_examples': 43793, 'test/accuracy': 0.9859733581542969, 'test/loss': 0.04751187935471535, 'test/mean_average_precision': 0.2730102876132786, 'test/num_examples': 43793, 'score': 12984.965381383896, 'total_duration': 20484.833154678345, 'accumulated_submission_time': 12984.965381383896, 'accumulated_eval_time': 7497.037292003632, 'accumulated_logging_time': 1.6968517303466797}
I0305 15:43:31.759699 140113222047488 logging_writer.py:48] [40409] accumulated_eval_time=7497.037292, accumulated_logging_time=1.696852, accumulated_submission_time=12984.965381, global_step=40409, preemption_count=0, score=12984.965381, test/accuracy=0.985973, test/loss=0.047512, test/mean_average_precision=0.273010, test/num_examples=43793, total_duration=20484.833155, train/accuracy=0.992742, train/loss=0.023308, train/mean_average_precision=0.574512, validation/accuracy=0.986804, validation/loss=0.044756, validation/mean_average_precision=0.288264, validation/num_examples=43793
I0305 15:44:01.557806 140113230440192 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.08263690024614334, loss=0.027728619053959846
I0305 15:44:33.901306 140113222047488 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.07795652747154236, loss=0.03159582242369652
I0305 15:45:05.990390 140113230440192 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.0705883800983429, loss=0.02779988758265972
I0305 15:45:37.897619 140113222047488 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.069552481174469, loss=0.025547971948981285
I0305 15:46:09.903324 140113230440192 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.06557195633649826, loss=0.02788904495537281
I0305 15:46:41.692232 140113222047488 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.07412177324295044, loss=0.024962536990642548
I0305 15:47:13.777332 140113230440192 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.08914771676063538, loss=0.02984212338924408
I0305 15:47:31.790116 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:49:35.321197 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:49:38.506631 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:49:41.569686 140274064205632 submission_runner.py:411] Time since start: 20854.67s, 	Step: 41157, 	{'train/accuracy': 0.9929094910621643, 'train/loss': 0.02282385528087616, 'train/mean_average_precision': 0.5796415843969315, 'validation/accuracy': 0.9870049953460693, 'validation/loss': 0.044508446007966995, 'validation/mean_average_precision': 0.29028518713773244, 'validation/num_examples': 43793, 'test/accuracy': 0.986185610294342, 'test/loss': 0.047266677021980286, 'test/mean_average_precision': 0.2766887350322903, 'test/num_examples': 43793, 'score': 13224.677162885666, 'total_duration': 20854.66638636589, 'accumulated_submission_time': 13224.677162885666, 'accumulated_eval_time': 7626.816812753677, 'accumulated_logging_time': 2.0183424949645996}
I0305 15:49:41.595345 140098771031808 logging_writer.py:48] [41157] accumulated_eval_time=7626.816813, accumulated_logging_time=2.018342, accumulated_submission_time=13224.677163, global_step=41157, preemption_count=0, score=13224.677163, test/accuracy=0.986186, test/loss=0.047267, test/mean_average_precision=0.276689, test/num_examples=43793, total_duration=20854.666386, train/accuracy=0.992909, train/loss=0.022824, train/mean_average_precision=0.579642, validation/accuracy=0.987005, validation/loss=0.044508, validation/mean_average_precision=0.290285, validation/num_examples=43793
I0305 15:49:55.826560 140106748385024 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.0700046643614769, loss=0.02496970258653164
I0305 15:50:27.906466 140098771031808 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.06588610261678696, loss=0.02360648661851883
I0305 15:50:59.777091 140106748385024 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.0787707194685936, loss=0.02380451001226902
I0305 15:51:32.034602 140098771031808 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.09013500809669495, loss=0.02736133709549904
I0305 15:52:04.388124 140106748385024 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.08111333847045898, loss=0.026167629286646843
I0305 15:52:36.400651 140098771031808 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07723142951726913, loss=0.02476341277360916
I0305 15:53:08.127019 140106748385024 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.08408597856760025, loss=0.029280776157975197
I0305 15:53:40.265498 140098771031808 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.06923091411590576, loss=0.025622645393013954
I0305 15:53:41.878566 140274064205632 spec.py:321] Evaluating on the training split.
I0305 15:55:42.035703 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 15:55:45.130545 140274064205632 spec.py:349] Evaluating on the test split.
I0305 15:55:48.177372 140274064205632 submission_runner.py:411] Time since start: 21221.27s, 	Step: 41906, 	{'train/accuracy': 0.9926960468292236, 'train/loss': 0.023340485990047455, 'train/mean_average_precision': 0.5810469593506353, 'validation/accuracy': 0.9869651794433594, 'validation/loss': 0.044880032539367676, 'validation/mean_average_precision': 0.29165487653247485, 'validation/num_examples': 43793, 'test/accuracy': 0.9861674904823303, 'test/loss': 0.047705285251140594, 'test/mean_average_precision': 0.26936929027808, 'test/num_examples': 43793, 'score': 13464.927323102951, 'total_duration': 21221.274071455002, 'accumulated_submission_time': 13464.927323102951, 'accumulated_eval_time': 7753.115566253662, 'accumulated_logging_time': 2.056342363357544}
I0305 15:55:48.200640 140113222047488 logging_writer.py:48] [41906] accumulated_eval_time=7753.115566, accumulated_logging_time=2.056342, accumulated_submission_time=13464.927323, global_step=41906, preemption_count=0, score=13464.927323, test/accuracy=0.986167, test/loss=0.047705, test/mean_average_precision=0.269369, test/num_examples=43793, total_duration=21221.274071, train/accuracy=0.992696, train/loss=0.023340, train/mean_average_precision=0.581047, validation/accuracy=0.986965, validation/loss=0.044880, validation/mean_average_precision=0.291655, validation/num_examples=43793
I0305 15:56:19.251544 140113230440192 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07273372262716293, loss=0.02555459924042225
I0305 15:56:51.573087 140113222047488 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.06955353170633316, loss=0.027680853381752968
I0305 15:57:23.760966 140113230440192 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07693371176719666, loss=0.025651149451732635
I0305 15:57:55.898035 140113222047488 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.066995769739151, loss=0.029283082112669945
I0305 15:58:28.551350 140113230440192 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08234736323356628, loss=0.023530159145593643
I0305 15:59:00.913258 140113222047488 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.08663682639598846, loss=0.028822744265198708
I0305 15:59:33.067844 140113230440192 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.08454481512308121, loss=0.027438266202807426
I0305 15:59:48.469495 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:01:51.725357 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:01:54.868957 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:01:57.934636 140274064205632 submission_runner.py:411] Time since start: 21591.03s, 	Step: 42649, 	{'train/accuracy': 0.9926057457923889, 'train/loss': 0.023624831810593605, 'train/mean_average_precision': 0.5628984806159489, 'validation/accuracy': 0.9871000051498413, 'validation/loss': 0.04496300593018532, 'validation/mean_average_precision': 0.29105890621246194, 'validation/num_examples': 43793, 'test/accuracy': 0.9862475395202637, 'test/loss': 0.047857485711574554, 'test/mean_average_precision': 0.2744560090343512, 'test/num_examples': 43793, 'score': 13705.163032531738, 'total_duration': 21591.031335115433, 'accumulated_submission_time': 13705.163032531738, 'accumulated_eval_time': 7882.5806567668915, 'accumulated_logging_time': 2.0924253463745117}
I0305 16:01:57.958748 140105330788096 logging_writer.py:48] [42649] accumulated_eval_time=7882.580657, accumulated_logging_time=2.092425, accumulated_submission_time=13705.163033, global_step=42649, preemption_count=0, score=13705.163033, test/accuracy=0.986248, test/loss=0.047857, test/mean_average_precision=0.274456, test/num_examples=43793, total_duration=21591.031335, train/accuracy=0.992606, train/loss=0.023625, train/mean_average_precision=0.562898, validation/accuracy=0.987100, validation/loss=0.044963, validation/mean_average_precision=0.291059, validation/num_examples=43793
I0305 16:02:14.990695 140106748385024 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.08529943972826004, loss=0.02839849889278412
I0305 16:02:47.862570 140105330788096 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.07652390003204346, loss=0.025315431877970695
I0305 16:03:20.520066 140106748385024 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08251921832561493, loss=0.025036051869392395
I0305 16:03:52.865863 140105330788096 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.08212905377149582, loss=0.023320330306887627
I0305 16:04:24.906850 140106748385024 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.07513396441936493, loss=0.02609918639063835
I0305 16:04:57.585249 140105330788096 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.0766289085149765, loss=0.02639591507613659
I0305 16:05:30.073535 140106748385024 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07461154460906982, loss=0.026393013074994087
I0305 16:05:58.016476 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:07:57.731278 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:08:00.800210 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:08:03.953278 140274064205632 submission_runner.py:411] Time since start: 21957.05s, 	Step: 43386, 	{'train/accuracy': 0.9926634430885315, 'train/loss': 0.023337163031101227, 'train/mean_average_precision': 0.5653618151959215, 'validation/accuracy': 0.9871446490287781, 'validation/loss': 0.04441265016794205, 'validation/mean_average_precision': 0.2941698091762321, 'validation/num_examples': 43793, 'test/accuracy': 0.9862290024757385, 'test/loss': 0.047452911734580994, 'test/mean_average_precision': 0.27458368503663694, 'test/num_examples': 43793, 'score': 13945.18832564354, 'total_duration': 21957.04997587204, 'accumulated_submission_time': 13945.18832564354, 'accumulated_eval_time': 8008.517409801483, 'accumulated_logging_time': 2.128856897354126}
I0305 16:08:03.978569 140098771031808 logging_writer.py:48] [43386] accumulated_eval_time=8008.517410, accumulated_logging_time=2.128857, accumulated_submission_time=13945.188326, global_step=43386, preemption_count=0, score=13945.188326, test/accuracy=0.986229, test/loss=0.047453, test/mean_average_precision=0.274584, test/num_examples=43793, total_duration=21957.049976, train/accuracy=0.992663, train/loss=0.023337, train/mean_average_precision=0.565362, validation/accuracy=0.987145, validation/loss=0.044413, validation/mean_average_precision=0.294170, validation/num_examples=43793
I0305 16:08:09.090046 140113222047488 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.09623073041439056, loss=0.028057347983121872
I0305 16:08:41.854832 140098771031808 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.07393766939640045, loss=0.028238924220204353
I0305 16:09:14.502482 140113222047488 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07964920252561569, loss=0.02780752070248127
I0305 16:09:46.700639 140098771031808 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07158716768026352, loss=0.024690134450793266
I0305 16:10:19.171666 140113222047488 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07717298716306686, loss=0.025728195905685425
I0305 16:10:51.814320 140098771031808 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.09537393599748611, loss=0.026859348639845848
I0305 16:11:24.231772 140113222047488 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.08688899129629135, loss=0.02584143541753292
I0305 16:11:56.622685 140098771031808 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07901894301176071, loss=0.027087252587080002
I0305 16:12:04.133338 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:14:08.601170 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:14:11.724726 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:14:14.770285 140274064205632 submission_runner.py:411] Time since start: 22327.87s, 	Step: 44124, 	{'train/accuracy': 0.9925113320350647, 'train/loss': 0.023640137165784836, 'train/mean_average_precision': 0.5700971758111578, 'validation/accuracy': 0.987098753452301, 'validation/loss': 0.04545262083411217, 'validation/mean_average_precision': 0.2921075738306386, 'validation/num_examples': 43793, 'test/accuracy': 0.9862479567527771, 'test/loss': 0.048608772456645966, 'test/mean_average_precision': 0.2739571897071809, 'test/num_examples': 43793, 'score': 14185.310455322266, 'total_duration': 22327.866988182068, 'accumulated_submission_time': 14185.310455322266, 'accumulated_eval_time': 8139.154308795929, 'accumulated_logging_time': 2.1666409969329834}
I0305 16:14:14.794646 140105330788096 logging_writer.py:48] [44124] accumulated_eval_time=8139.154309, accumulated_logging_time=2.166641, accumulated_submission_time=14185.310455, global_step=44124, preemption_count=0, score=14185.310455, test/accuracy=0.986248, test/loss=0.048609, test/mean_average_precision=0.273957, test/num_examples=43793, total_duration=22327.866988, train/accuracy=0.992511, train/loss=0.023640, train/mean_average_precision=0.570097, validation/accuracy=0.987099, validation/loss=0.045453, validation/mean_average_precision=0.292108, validation/num_examples=43793
I0305 16:14:39.527344 140113230440192 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.08870737254619598, loss=0.026621051132678986
I0305 16:15:11.910940 140105330788096 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.09319819509983063, loss=0.025952240452170372
I0305 16:15:44.069184 140113230440192 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08358946442604065, loss=0.028245847672224045
I0305 16:16:16.251487 140105330788096 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.07266891002655029, loss=0.02545548416674137
I0305 16:16:48.484199 140113230440192 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.08055518567562103, loss=0.026139313355088234
I0305 16:17:20.684566 140105330788096 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.08159547299146652, loss=0.026038235053420067
I0305 16:17:52.681041 140113230440192 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.07667136937379837, loss=0.02684873156249523
I0305 16:18:14.959704 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:20:20.130337 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:20:23.241879 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:20:26.320106 140274064205632 submission_runner.py:411] Time since start: 22699.42s, 	Step: 44870, 	{'train/accuracy': 0.9926388263702393, 'train/loss': 0.023288769647479057, 'train/mean_average_precision': 0.5691611133646597, 'validation/accuracy': 0.9871628880500793, 'validation/loss': 0.044840723276138306, 'validation/mean_average_precision': 0.295265385933204, 'validation/num_examples': 43793, 'test/accuracy': 0.9861940741539001, 'test/loss': 0.04812253639101982, 'test/mean_average_precision': 0.272632916276983, 'test/num_examples': 43793, 'score': 14425.444154977798, 'total_duration': 22699.416808366776, 'accumulated_submission_time': 14425.444154977798, 'accumulated_eval_time': 8270.51467037201, 'accumulated_logging_time': 2.20216703414917}
I0305 16:20:26.344090 140098771031808 logging_writer.py:48] [44870] accumulated_eval_time=8270.514670, accumulated_logging_time=2.202167, accumulated_submission_time=14425.444155, global_step=44870, preemption_count=0, score=14425.444155, test/accuracy=0.986194, test/loss=0.048123, test/mean_average_precision=0.272633, test/num_examples=43793, total_duration=22699.416808, train/accuracy=0.992639, train/loss=0.023289, train/mean_average_precision=0.569161, validation/accuracy=0.987163, validation/loss=0.044841, validation/mean_average_precision=0.295265, validation/num_examples=43793
I0305 16:20:36.958918 140106748385024 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07459690421819687, loss=0.026569295674562454
I0305 16:21:09.127827 140098771031808 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.07465802133083344, loss=0.024963194504380226
I0305 16:21:41.155014 140106748385024 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.09228309988975525, loss=0.026831824332475662
I0305 16:22:13.209071 140098771031808 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08282595127820969, loss=0.024269510060548782
I0305 16:22:45.368152 140106748385024 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.08439303189516068, loss=0.025533251464366913
I0305 16:23:17.537093 140098771031808 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.08115991950035095, loss=0.026685763150453568
I0305 16:23:49.832225 140106748385024 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.08096279203891754, loss=0.022946495562791824
I0305 16:24:21.978754 140098771031808 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.06792184710502625, loss=0.024859553202986717
I0305 16:24:26.524588 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:26:26.665235 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:26:29.781831 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:26:32.849192 140274064205632 submission_runner.py:411] Time since start: 23065.95s, 	Step: 45615, 	{'train/accuracy': 0.9928351044654846, 'train/loss': 0.02258169837296009, 'train/mean_average_precision': 0.5853443199422982, 'validation/accuracy': 0.9871637225151062, 'validation/loss': 0.04551349952816963, 'validation/mean_average_precision': 0.2881334524893654, 'validation/num_examples': 43793, 'test/accuracy': 0.9862799644470215, 'test/loss': 0.048672668635845184, 'test/mean_average_precision': 0.2769721109562906, 'test/num_examples': 43793, 'score': 14665.592813968658, 'total_duration': 23065.945892572403, 'accumulated_submission_time': 14665.592813968658, 'accumulated_eval_time': 8396.839224815369, 'accumulated_logging_time': 2.237090587615967}
I0305 16:26:32.874235 140105330788096 logging_writer.py:48] [45615] accumulated_eval_time=8396.839225, accumulated_logging_time=2.237091, accumulated_submission_time=14665.592814, global_step=45615, preemption_count=0, score=14665.592814, test/accuracy=0.986280, test/loss=0.048673, test/mean_average_precision=0.276972, test/num_examples=43793, total_duration=23065.945893, train/accuracy=0.992835, train/loss=0.022582, train/mean_average_precision=0.585344, validation/accuracy=0.987164, validation/loss=0.045513, validation/mean_average_precision=0.288133, validation/num_examples=43793
I0305 16:27:02.326612 140113230440192 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08658399432897568, loss=0.024787096306681633
I0305 16:27:34.257167 140105330788096 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.0816909670829773, loss=0.027176041156053543
I0305 16:28:06.498413 140113230440192 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08314798027276993, loss=0.025596871972084045
I0305 16:28:38.601813 140105330788096 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.06955795735120773, loss=0.02376713789999485
I0305 16:29:10.885390 140113230440192 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.07177085429430008, loss=0.02517031505703926
I0305 16:29:43.295420 140105330788096 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.0889359787106514, loss=0.028488045558333397
I0305 16:30:16.000136 140113230440192 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08689109236001968, loss=0.026694955304265022
I0305 16:30:33.137194 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:32:37.652578 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:32:40.982341 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:32:44.286382 140274064205632 submission_runner.py:411] Time since start: 23437.38s, 	Step: 46353, 	{'train/accuracy': 0.993068516254425, 'train/loss': 0.02195209264755249, 'train/mean_average_precision': 0.6135675762189512, 'validation/accuracy': 0.9870699644088745, 'validation/loss': 0.04543901979923248, 'validation/mean_average_precision': 0.2857170722059383, 'validation/num_examples': 43793, 'test/accuracy': 0.9861839413642883, 'test/loss': 0.04849877208471298, 'test/mean_average_precision': 0.27349098718488857, 'test/num_examples': 43793, 'score': 14905.821466445923, 'total_duration': 23437.383046627045, 'accumulated_submission_time': 14905.821466445923, 'accumulated_eval_time': 8527.988328695297, 'accumulated_logging_time': 2.2753024101257324}
I0305 16:32:44.314065 140098771031808 logging_writer.py:48] [46353] accumulated_eval_time=8527.988329, accumulated_logging_time=2.275302, accumulated_submission_time=14905.821466, global_step=46353, preemption_count=0, score=14905.821466, test/accuracy=0.986184, test/loss=0.048499, test/mean_average_precision=0.273491, test/num_examples=43793, total_duration=23437.383047, train/accuracy=0.993069, train/loss=0.021952, train/mean_average_precision=0.613568, validation/accuracy=0.987070, validation/loss=0.045439, validation/mean_average_precision=0.285717, validation/num_examples=43793
I0305 16:33:00.330250 140113222047488 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.08010289818048477, loss=0.020646248012781143
I0305 16:33:32.641705 140098771031808 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0886061042547226, loss=0.026249239221215248
I0305 16:34:04.776804 140113222047488 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.07611486315727234, loss=0.025149893015623093
I0305 16:34:37.572651 140098771031808 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08210799098014832, loss=0.025114143267273903
I0305 16:35:09.956921 140113222047488 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.07360022515058517, loss=0.022218385711312294
I0305 16:35:42.235621 140098771031808 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.0933188647031784, loss=0.02527993731200695
I0305 16:36:14.911202 140113222047488 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.08712967485189438, loss=0.024920817464590073
I0305 16:36:44.312123 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:38:45.403231 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:38:48.510769 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:38:51.551978 140274064205632 submission_runner.py:411] Time since start: 23804.65s, 	Step: 47091, 	{'train/accuracy': 0.9933658838272095, 'train/loss': 0.021166056394577026, 'train/mean_average_precision': 0.6093570684686115, 'validation/accuracy': 0.9869733452796936, 'validation/loss': 0.04543027654290199, 'validation/mean_average_precision': 0.2875142031287636, 'validation/num_examples': 43793, 'test/accuracy': 0.98616623878479, 'test/loss': 0.04822048172354698, 'test/mean_average_precision': 0.27682154639626366, 'test/num_examples': 43793, 'score': 15145.786823511124, 'total_duration': 23804.648680448532, 'accumulated_submission_time': 15145.786823511124, 'accumulated_eval_time': 8655.228150367737, 'accumulated_logging_time': 2.3150885105133057}
I0305 16:38:51.577040 140106748385024 logging_writer.py:48] [47091] accumulated_eval_time=8655.228150, accumulated_logging_time=2.315089, accumulated_submission_time=15145.786824, global_step=47091, preemption_count=0, score=15145.786824, test/accuracy=0.986166, test/loss=0.048220, test/mean_average_precision=0.276822, test/num_examples=43793, total_duration=23804.648680, train/accuracy=0.993366, train/loss=0.021166, train/mean_average_precision=0.609357, validation/accuracy=0.986973, validation/loss=0.045430, validation/mean_average_precision=0.287514, validation/num_examples=43793
I0305 16:38:54.797598 140113230440192 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.09435834735631943, loss=0.025538885965943336
I0305 16:39:26.826300 140106748385024 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.07366636395454407, loss=0.024169158190488815
I0305 16:39:59.024146 140113230440192 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.09012208878993988, loss=0.026390589773654938
I0305 16:40:31.028928 140106748385024 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.07801119983196259, loss=0.022401204332709312
I0305 16:41:03.003832 140113230440192 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.08321383595466614, loss=0.023738818243145943
I0305 16:41:35.011241 140106748385024 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.09266555309295654, loss=0.021733317524194717
I0305 16:42:07.106909 140113230440192 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.09851155430078506, loss=0.0245396438986063
I0305 16:42:39.711922 140106748385024 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.07643048465251923, loss=0.02215973101556301
I0305 16:42:51.614403 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:44:56.377669 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:44:59.842601 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:45:03.039609 140274064205632 submission_runner.py:411] Time since start: 24176.14s, 	Step: 47838, 	{'train/accuracy': 0.9935070872306824, 'train/loss': 0.020580321550369263, 'train/mean_average_precision': 0.6371483651392227, 'validation/accuracy': 0.987105667591095, 'validation/loss': 0.045628923922777176, 'validation/mean_average_precision': 0.29109546882853293, 'validation/num_examples': 43793, 'test/accuracy': 0.9862921833992004, 'test/loss': 0.04865970090031624, 'test/mean_average_precision': 0.2697960378710056, 'test/num_examples': 43793, 'score': 15385.791709899902, 'total_duration': 24176.136302232742, 'accumulated_submission_time': 15385.791709899902, 'accumulated_eval_time': 8786.653314113617, 'accumulated_logging_time': 2.3512327671051025}
I0305 16:45:03.064612 140098771031808 logging_writer.py:48] [47838] accumulated_eval_time=8786.653314, accumulated_logging_time=2.351233, accumulated_submission_time=15385.791710, global_step=47838, preemption_count=0, score=15385.791710, test/accuracy=0.986292, test/loss=0.048660, test/mean_average_precision=0.269796, test/num_examples=43793, total_duration=24176.136302, train/accuracy=0.993507, train/loss=0.020580, train/mean_average_precision=0.637148, validation/accuracy=0.987106, validation/loss=0.045629, validation/mean_average_precision=0.291095, validation/num_examples=43793
I0305 16:45:23.392651 140113222047488 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.09872332960367203, loss=0.026685891672968864
I0305 16:45:55.857995 140098771031808 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.08091416954994202, loss=0.02472163364291191
I0305 16:46:28.514074 140113222047488 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.09561445564031601, loss=0.026290567591786385
I0305 16:47:01.074673 140098771031808 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08177324384450912, loss=0.023458458483219147
I0305 16:47:33.087213 140113222047488 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.07748015224933624, loss=0.021460672840476036
I0305 16:48:05.449199 140098771031808 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.07900109142065048, loss=0.02477659471333027
I0305 16:48:37.229683 140113222047488 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.09893757104873657, loss=0.028130006045103073
I0305 16:49:03.286039 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:51:02.030997 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:51:05.096958 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:51:08.115110 140274064205632 submission_runner.py:411] Time since start: 24541.21s, 	Step: 48582, 	{'train/accuracy': 0.993273913860321, 'train/loss': 0.021422401070594788, 'train/mean_average_precision': 0.6035744046576249, 'validation/accuracy': 0.9871032238006592, 'validation/loss': 0.04572542756795883, 'validation/mean_average_precision': 0.29124228495340837, 'validation/num_examples': 43793, 'test/accuracy': 0.986214280128479, 'test/loss': 0.04867551848292351, 'test/mean_average_precision': 0.27779217324874844, 'test/num_examples': 43793, 'score': 15625.98142337799, 'total_duration': 24541.211814403534, 'accumulated_submission_time': 15625.98142337799, 'accumulated_eval_time': 8911.482343435287, 'accumulated_logging_time': 2.387014865875244}
I0305 16:51:08.139909 140106748385024 logging_writer.py:48] [48582] accumulated_eval_time=8911.482343, accumulated_logging_time=2.387015, accumulated_submission_time=15625.981423, global_step=48582, preemption_count=0, score=15625.981423, test/accuracy=0.986214, test/loss=0.048676, test/mean_average_precision=0.277792, test/num_examples=43793, total_duration=24541.211814, train/accuracy=0.993274, train/loss=0.021422, train/mean_average_precision=0.603574, validation/accuracy=0.987103, validation/loss=0.045725, validation/mean_average_precision=0.291242, validation/num_examples=43793
I0305 16:51:14.411401 140113230440192 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.09294231981039047, loss=0.025350822135806084
I0305 16:51:46.369738 140106748385024 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.08765226602554321, loss=0.02462264522910118
I0305 16:52:18.410002 140113230440192 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.08890986442565918, loss=0.027002930641174316
I0305 16:52:50.463066 140106748385024 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.0974087119102478, loss=0.027436789125204086
I0305 16:53:22.708631 140113230440192 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.09742208570241928, loss=0.02643194980919361
I0305 16:53:54.814052 140106748385024 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.0894792303442955, loss=0.024755455553531647
I0305 16:54:26.840770 140113230440192 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.11227871477603912, loss=0.024275245144963264
I0305 16:54:59.084254 140106748385024 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08439114689826965, loss=0.023849869146943092
I0305 16:55:08.232144 140274064205632 spec.py:321] Evaluating on the training split.
I0305 16:57:03.800263 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 16:57:06.896894 140274064205632 spec.py:349] Evaluating on the test split.
I0305 16:57:09.953014 140274064205632 submission_runner.py:411] Time since start: 24903.05s, 	Step: 49329, 	{'train/accuracy': 0.9930155873298645, 'train/loss': 0.021947942674160004, 'train/mean_average_precision': 0.6046991869074054, 'validation/accuracy': 0.9870277047157288, 'validation/loss': 0.04608699306845665, 'validation/mean_average_precision': 0.2934004400441242, 'validation/num_examples': 43793, 'test/accuracy': 0.9862509369850159, 'test/loss': 0.048951100558042526, 'test/mean_average_precision': 0.27428389698218814, 'test/num_examples': 43793, 'score': 15866.042525291443, 'total_duration': 24903.049714803696, 'accumulated_submission_time': 15866.042525291443, 'accumulated_eval_time': 9033.203171491623, 'accumulated_logging_time': 2.4227333068847656}
I0305 16:57:09.978697 140098771031808 logging_writer.py:48] [49329] accumulated_eval_time=9033.203171, accumulated_logging_time=2.422733, accumulated_submission_time=15866.042525, global_step=49329, preemption_count=0, score=15866.042525, test/accuracy=0.986251, test/loss=0.048951, test/mean_average_precision=0.274284, test/num_examples=43793, total_duration=24903.049715, train/accuracy=0.993016, train/loss=0.021948, train/mean_average_precision=0.604699, validation/accuracy=0.987028, validation/loss=0.046087, validation/mean_average_precision=0.293400, validation/num_examples=43793
I0305 16:57:33.452322 140113222047488 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.08410180360078812, loss=0.023965511471033096
I0305 16:58:06.228197 140098771031808 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.09802626818418503, loss=0.025407878682017326
I0305 16:58:38.977743 140113222047488 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.09250523149967194, loss=0.022563181817531586
I0305 16:59:11.509519 140098771031808 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.0769326314330101, loss=0.02262362278997898
I0305 16:59:44.201166 140113222047488 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.10076631605625153, loss=0.026276638731360435
I0305 17:00:17.588533 140098771031808 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.09344972670078278, loss=0.023890839889645576
I0305 17:00:50.401619 140113222047488 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.09811938554048538, loss=0.02575363777577877
I0305 17:01:10.145827 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:03:09.583271 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:03:12.675718 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:03:15.739377 140274064205632 submission_runner.py:411] Time since start: 25268.84s, 	Step: 50061, 	{'train/accuracy': 0.9932344555854797, 'train/loss': 0.021381719037890434, 'train/mean_average_precision': 0.6176617595712219, 'validation/accuracy': 0.9871576428413391, 'validation/loss': 0.046049851924180984, 'validation/mean_average_precision': 0.293558521584603, 'validation/num_examples': 43793, 'test/accuracy': 0.9863073229789734, 'test/loss': 0.04901185259222984, 'test/mean_average_precision': 0.2727283348585548, 'test/num_examples': 43793, 'score': 16106.172878026962, 'total_duration': 25268.836081027985, 'accumulated_submission_time': 16106.172878026962, 'accumulated_eval_time': 9158.79671406746, 'accumulated_logging_time': 2.4613735675811768}
I0305 17:03:15.764860 140106748385024 logging_writer.py:48] [50061] accumulated_eval_time=9158.796714, accumulated_logging_time=2.461374, accumulated_submission_time=16106.172878, global_step=50061, preemption_count=0, score=16106.172878, test/accuracy=0.986307, test/loss=0.049012, test/mean_average_precision=0.272728, test/num_examples=43793, total_duration=25268.836081, train/accuracy=0.993234, train/loss=0.021382, train/mean_average_precision=0.617662, validation/accuracy=0.987158, validation/loss=0.046050, validation/mean_average_precision=0.293559, validation/num_examples=43793
I0305 17:03:28.846779 140113230440192 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.08674874156713486, loss=0.022328047081828117
I0305 17:04:01.458801 140106748385024 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.09024372696876526, loss=0.025069694966077805
I0305 17:04:33.978177 140113230440192 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.10045026987791061, loss=0.0230982955545187
I0305 17:05:06.012912 140106748385024 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08561497926712036, loss=0.02192671224474907
I0305 17:05:38.014367 140113230440192 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08412396162748337, loss=0.021846234798431396
I0305 17:06:10.265565 140106748385024 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09755579382181168, loss=0.024049313738942146
I0305 17:06:43.066598 140113230440192 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.10251905769109726, loss=0.023067010566592216
I0305 17:07:15.992592 140106748385024 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.0888083428144455, loss=0.024971090257167816
I0305 17:07:15.999253 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:09:18.673103 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:09:21.783898 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:09:24.886444 140274064205632 submission_runner.py:411] Time since start: 25637.98s, 	Step: 50801, 	{'train/accuracy': 0.9931604862213135, 'train/loss': 0.021644195541739464, 'train/mean_average_precision': 0.6141723415553106, 'validation/accuracy': 0.987022876739502, 'validation/loss': 0.04590703919529915, 'validation/mean_average_precision': 0.2900934698570349, 'validation/num_examples': 43793, 'test/accuracy': 0.9861944913864136, 'test/loss': 0.04880130663514137, 'test/mean_average_precision': 0.2790345826699184, 'test/num_examples': 43793, 'score': 16346.374148607254, 'total_duration': 25637.983144044876, 'accumulated_submission_time': 16346.374148607254, 'accumulated_eval_time': 9287.683853626251, 'accumulated_logging_time': 2.49810791015625}
I0305 17:09:24.911670 140098771031808 logging_writer.py:48] [50801] accumulated_eval_time=9287.683854, accumulated_logging_time=2.498108, accumulated_submission_time=16346.374149, global_step=50801, preemption_count=0, score=16346.374149, test/accuracy=0.986194, test/loss=0.048801, test/mean_average_precision=0.279035, test/num_examples=43793, total_duration=25637.983144, train/accuracy=0.993160, train/loss=0.021644, train/mean_average_precision=0.614172, validation/accuracy=0.987023, validation/loss=0.045907, validation/mean_average_precision=0.290093, validation/num_examples=43793
I0305 17:09:58.096350 140105330788096 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.0903613269329071, loss=0.024553772062063217
I0305 17:10:30.253701 140098771031808 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.08944088965654373, loss=0.023180527612566948
I0305 17:11:02.576709 140105330788096 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.09468546509742737, loss=0.02424546331167221
I0305 17:11:34.737278 140098771031808 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09485135972499847, loss=0.02391575463116169
I0305 17:12:06.983128 140105330788096 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.0857677087187767, loss=0.023777831345796585
I0305 17:12:39.514512 140098771031808 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.09123152494430542, loss=0.02213965728878975
I0305 17:13:11.628372 140105330788096 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.09163518249988556, loss=0.02375856600701809
I0305 17:13:25.075510 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:15:20.576988 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:15:23.647641 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:15:26.734043 140274064205632 submission_runner.py:411] Time since start: 25999.83s, 	Step: 51543, 	{'train/accuracy': 0.9934256076812744, 'train/loss': 0.020602652803063393, 'train/mean_average_precision': 0.6302478098406785, 'validation/accuracy': 0.9871271848678589, 'validation/loss': 0.04611130803823471, 'validation/mean_average_precision': 0.2967554986096878, 'validation/num_examples': 43793, 'test/accuracy': 0.9862997531890869, 'test/loss': 0.0489075742661953, 'test/mean_average_precision': 0.2789417867146801, 'test/num_examples': 43793, 'score': 16586.50678873062, 'total_duration': 25999.830745220184, 'accumulated_submission_time': 16586.50678873062, 'accumulated_eval_time': 9409.34234046936, 'accumulated_logging_time': 2.5340957641601562}
I0305 17:15:26.762070 140106748385024 logging_writer.py:48] [51543] accumulated_eval_time=9409.342340, accumulated_logging_time=2.534096, accumulated_submission_time=16586.506789, global_step=51543, preemption_count=0, score=16586.506789, test/accuracy=0.986300, test/loss=0.048908, test/mean_average_precision=0.278942, test/num_examples=43793, total_duration=25999.830745, train/accuracy=0.993426, train/loss=0.020603, train/mean_average_precision=0.630248, validation/accuracy=0.987127, validation/loss=0.046111, validation/mean_average_precision=0.296755, validation/num_examples=43793
I0305 17:15:45.378290 140113222047488 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.09296591579914093, loss=0.0214129239320755
I0305 17:16:17.522364 140106748385024 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09873272478580475, loss=0.02406376600265503
I0305 17:16:49.674172 140113222047488 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.09074805676937103, loss=0.022602014243602753
I0305 17:17:21.735537 140106748385024 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.10064943134784698, loss=0.024614861235022545
I0305 17:17:54.131760 140113222047488 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.10614262521266937, loss=0.025217585265636444
I0305 17:18:26.525723 140106748385024 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08628549426794052, loss=0.02151813916862011
I0305 17:18:58.964874 140113222047488 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.10162624716758728, loss=0.025083567947149277
I0305 17:19:26.994666 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:21:23.993838 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:21:27.101458 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:21:30.149089 140274064205632 submission_runner.py:411] Time since start: 26363.25s, 	Step: 52287, 	{'train/accuracy': 0.9934813976287842, 'train/loss': 0.02034526690840721, 'train/mean_average_precision': 0.6378640103243176, 'validation/accuracy': 0.9871352910995483, 'validation/loss': 0.04662230238318443, 'validation/mean_average_precision': 0.29483392644605455, 'validation/num_examples': 43793, 'test/accuracy': 0.9863343238830566, 'test/loss': 0.04956304281949997, 'test/mean_average_precision': 0.2742401976552608, 'test/num_examples': 43793, 'score': 16826.707840442657, 'total_duration': 26363.245794296265, 'accumulated_submission_time': 16826.707840442657, 'accumulated_eval_time': 9532.496723175049, 'accumulated_logging_time': 2.5732104778289795}
I0305 17:21:30.174500 140098771031808 logging_writer.py:48] [52287] accumulated_eval_time=9532.496723, accumulated_logging_time=2.573210, accumulated_submission_time=16826.707840, global_step=52287, preemption_count=0, score=16826.707840, test/accuracy=0.986334, test/loss=0.049563, test/mean_average_precision=0.274240, test/num_examples=43793, total_duration=26363.245794, train/accuracy=0.993481, train/loss=0.020345, train/mean_average_precision=0.637864, validation/accuracy=0.987135, validation/loss=0.046622, validation/mean_average_precision=0.294834, validation/num_examples=43793
I0305 17:21:34.742917 140105330788096 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.10205487906932831, loss=0.0262035820633173
I0305 17:22:07.665856 140098771031808 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09330864250659943, loss=0.02220660075545311
I0305 17:22:40.177901 140105330788096 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.10758397728204727, loss=0.023471714928746223
I0305 17:23:12.627728 140098771031808 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.09546933323144913, loss=0.024095837026834488
I0305 17:23:44.469876 140105330788096 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.0970311313867569, loss=0.023019831627607346
I0305 17:24:16.348062 140098771031808 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.10064023733139038, loss=0.023564381524920464
I0305 17:24:48.565836 140105330788096 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09517563879489899, loss=0.02127399668097496
I0305 17:25:20.213918 140098771031808 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09066066890954971, loss=0.022091561928391457
I0305 17:25:30.257649 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:27:28.179755 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:27:31.326955 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:27:34.451233 140274064205632 submission_runner.py:411] Time since start: 26727.55s, 	Step: 53032, 	{'train/accuracy': 0.9936859011650085, 'train/loss': 0.0199180506169796, 'train/mean_average_precision': 0.6607255578238254, 'validation/accuracy': 0.9869903922080994, 'validation/loss': 0.04592856019735336, 'validation/mean_average_precision': 0.2948226204101228, 'validation/num_examples': 43793, 'test/accuracy': 0.9861392974853516, 'test/loss': 0.04891257733106613, 'test/mean_average_precision': 0.2728305192982156, 'test/num_examples': 43793, 'score': 17066.75954055786, 'total_duration': 26727.54792380333, 'accumulated_submission_time': 17066.75954055786, 'accumulated_eval_time': 9656.69024682045, 'accumulated_logging_time': 2.6094748973846436}
I0305 17:27:34.477568 140106748385024 logging_writer.py:48] [53032] accumulated_eval_time=9656.690247, accumulated_logging_time=2.609475, accumulated_submission_time=17066.759541, global_step=53032, preemption_count=0, score=17066.759541, test/accuracy=0.986139, test/loss=0.048913, test/mean_average_precision=0.272831, test/num_examples=43793, total_duration=26727.547924, train/accuracy=0.993686, train/loss=0.019918, train/mean_average_precision=0.660726, validation/accuracy=0.986990, validation/loss=0.045929, validation/mean_average_precision=0.294823, validation/num_examples=43793
I0305 17:27:57.091186 140113230440192 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.10093647986650467, loss=0.01937524601817131
I0305 17:28:29.286391 140106748385024 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.10035132616758347, loss=0.02257114090025425
I0305 17:29:01.664777 140113230440192 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.0941670686006546, loss=0.023450301960110664
I0305 17:29:34.057097 140106748385024 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.11427943408489227, loss=0.024399271234869957
I0305 17:30:06.958643 140113230440192 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.10118229687213898, loss=0.02282518334686756
I0305 17:30:38.987406 140106748385024 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.09532711654901505, loss=0.023645326495170593
I0305 17:31:11.260605 140113230440192 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.1015983372926712, loss=0.023446809500455856
I0305 17:31:34.571323 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:33:34.637106 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:33:38.116259 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:33:41.441713 140274064205632 submission_runner.py:411] Time since start: 27094.54s, 	Step: 53774, 	{'train/accuracy': 0.9940152168273926, 'train/loss': 0.018954306840896606, 'train/mean_average_precision': 0.6742533910555928, 'validation/accuracy': 0.9870938658714294, 'validation/loss': 0.04626242071390152, 'validation/mean_average_precision': 0.3016247706823539, 'validation/num_examples': 43793, 'test/accuracy': 0.9861776232719421, 'test/loss': 0.04940260946750641, 'test/mean_average_precision': 0.2757804694010088, 'test/num_examples': 43793, 'score': 17306.817973852158, 'total_duration': 27094.53839492798, 'accumulated_submission_time': 17306.817973852158, 'accumulated_eval_time': 9783.560585260391, 'accumulated_logging_time': 2.648601531982422}
I0305 17:33:41.471696 140105330788096 logging_writer.py:48] [53774] accumulated_eval_time=9783.560585, accumulated_logging_time=2.648602, accumulated_submission_time=17306.817974, global_step=53774, preemption_count=0, score=17306.817974, test/accuracy=0.986178, test/loss=0.049403, test/mean_average_precision=0.275780, test/num_examples=43793, total_duration=27094.538395, train/accuracy=0.994015, train/loss=0.018954, train/mean_average_precision=0.674253, validation/accuracy=0.987094, validation/loss=0.046262, validation/mean_average_precision=0.301625, validation/num_examples=43793
I0305 17:33:50.236482 140113222047488 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1130606010556221, loss=0.0251897145062685
I0305 17:34:22.682450 140105330788096 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10940995067358017, loss=0.023456919938325882
I0305 17:34:55.169307 140113222047488 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10430935025215149, loss=0.021124038845300674
I0305 17:35:27.607539 140105330788096 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.09584195166826248, loss=0.02082221582531929
I0305 17:35:59.578019 140113222047488 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.1002451628446579, loss=0.021356632933020592
I0305 17:36:31.922601 140105330788096 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09707682579755783, loss=0.021494600921869278
I0305 17:37:04.108484 140113222047488 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.10554900765419006, loss=0.025526270270347595
I0305 17:37:36.272758 140105330788096 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.10447648912668228, loss=0.024471787735819817
I0305 17:37:41.474987 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:39:42.612867 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:39:45.760296 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:39:48.833078 140274064205632 submission_runner.py:411] Time since start: 27461.93s, 	Step: 54517, 	{'train/accuracy': 0.9941158294677734, 'train/loss': 0.018751882016658783, 'train/mean_average_precision': 0.6797247754253466, 'validation/accuracy': 0.98710036277771, 'validation/loss': 0.04648426175117493, 'validation/mean_average_precision': 0.2955374417131669, 'validation/num_examples': 43793, 'test/accuracy': 0.9862989187240601, 'test/loss': 0.04952726885676384, 'test/mean_average_precision': 0.2791162088408168, 'test/num_examples': 43793, 'score': 17546.784031391144, 'total_duration': 27461.929767370224, 'accumulated_submission_time': 17546.784031391144, 'accumulated_eval_time': 9910.918627500534, 'accumulated_logging_time': 2.690823554992676}
I0305 17:39:48.859474 140098771031808 logging_writer.py:48] [54517] accumulated_eval_time=9910.918628, accumulated_logging_time=2.690824, accumulated_submission_time=17546.784031, global_step=54517, preemption_count=0, score=17546.784031, test/accuracy=0.986299, test/loss=0.049527, test/mean_average_precision=0.279116, test/num_examples=43793, total_duration=27461.929767, train/accuracy=0.994116, train/loss=0.018752, train/mean_average_precision=0.679725, validation/accuracy=0.987100, validation/loss=0.046484, validation/mean_average_precision=0.295537, validation/num_examples=43793
I0305 17:40:15.996033 140106748385024 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.09224113821983337, loss=0.021367739886045456
I0305 17:40:48.430229 140098771031808 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.09983749687671661, loss=0.019420454278588295
I0305 17:41:20.832389 140106748385024 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.1170966848731041, loss=0.024582238867878914
I0305 17:41:52.787648 140098771031808 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.11251938343048096, loss=0.021872999146580696
I0305 17:42:24.970141 140106748385024 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.11470411717891693, loss=0.022298427298665047
I0305 17:42:56.952260 140098771031808 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.08777336776256561, loss=0.021238943561911583
I0305 17:43:29.899647 140106748385024 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10239405184984207, loss=0.021521875634789467
I0305 17:43:48.989744 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:45:45.682469 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:45:48.750693 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:45:51.793647 140274064205632 submission_runner.py:411] Time since start: 27824.89s, 	Step: 55260, 	{'train/accuracy': 0.9938734769821167, 'train/loss': 0.019236132502555847, 'train/mean_average_precision': 0.6635183115064012, 'validation/accuracy': 0.9870488047599792, 'validation/loss': 0.046553198248147964, 'validation/mean_average_precision': 0.29428047727998863, 'validation/num_examples': 43793, 'test/accuracy': 0.9862226843833923, 'test/loss': 0.04973120987415314, 'test/mean_average_precision': 0.27561426826715607, 'test/num_examples': 43793, 'score': 17786.882937908173, 'total_duration': 27824.890347719193, 'accumulated_submission_time': 17786.882937908173, 'accumulated_eval_time': 10033.722482919693, 'accumulated_logging_time': 2.7282874584198}
I0305 17:45:51.821034 140105330788096 logging_writer.py:48] [55260] accumulated_eval_time=10033.722483, accumulated_logging_time=2.728287, accumulated_submission_time=17786.882938, global_step=55260, preemption_count=0, score=17786.882938, test/accuracy=0.986223, test/loss=0.049731, test/mean_average_precision=0.275614, test/num_examples=43793, total_duration=27824.890348, train/accuracy=0.993873, train/loss=0.019236, train/mean_average_precision=0.663518, validation/accuracy=0.987049, validation/loss=0.046553, validation/mean_average_precision=0.294280, validation/num_examples=43793
I0305 17:46:05.057628 140113230440192 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.10880108922719955, loss=0.023499080911278725
I0305 17:46:37.322548 140105330788096 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.10063959658145905, loss=0.023960797116160393
I0305 17:47:09.095194 140113230440192 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.09998176991939545, loss=0.021908259019255638
I0305 17:47:40.856332 140105330788096 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.11459192633628845, loss=0.02336416020989418
I0305 17:48:12.837366 140113230440192 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10103354603052139, loss=0.0209746602922678
I0305 17:48:45.373675 140105330788096 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.13068409264087677, loss=0.023832963779568672
I0305 17:49:17.731387 140113230440192 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.11825276166200638, loss=0.02385338395833969
I0305 17:49:49.807568 140105330788096 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09654856473207474, loss=0.021700503304600716
I0305 17:49:52.067137 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:51:51.140676 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:51:54.295974 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:51:57.355889 140274064205632 submission_runner.py:411] Time since start: 28190.45s, 	Step: 56008, 	{'train/accuracy': 0.9936686158180237, 'train/loss': 0.01988225430250168, 'train/mean_average_precision': 0.6484135803660225, 'validation/accuracy': 0.9869859218597412, 'validation/loss': 0.04695124179124832, 'validation/mean_average_precision': 0.28819649146759385, 'validation/num_examples': 43793, 'test/accuracy': 0.9861831068992615, 'test/loss': 0.05009276792407036, 'test/mean_average_precision': 0.2752872680004323, 'test/num_examples': 43793, 'score': 18027.09535241127, 'total_duration': 28190.45258665085, 'accumulated_submission_time': 18027.09535241127, 'accumulated_eval_time': 10159.011183738708, 'accumulated_logging_time': 2.768742561340332}
I0305 17:51:57.382801 140106748385024 logging_writer.py:48] [56008] accumulated_eval_time=10159.011184, accumulated_logging_time=2.768743, accumulated_submission_time=18027.095352, global_step=56008, preemption_count=0, score=18027.095352, test/accuracy=0.986183, test/loss=0.050093, test/mean_average_precision=0.275287, test/num_examples=43793, total_duration=28190.452587, train/accuracy=0.993669, train/loss=0.019882, train/mean_average_precision=0.648414, validation/accuracy=0.986986, validation/loss=0.046951, validation/mean_average_precision=0.288196, validation/num_examples=43793
I0305 17:52:27.308138 140113222047488 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.11524559557437897, loss=0.023324029520154
I0305 17:52:59.303768 140106748385024 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.10264919698238373, loss=0.021388618275523186
I0305 17:53:31.512314 140113222047488 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.13143174350261688, loss=0.025504333898425102
I0305 17:54:03.434914 140106748385024 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.11248812824487686, loss=0.022438930347561836
I0305 17:54:35.501332 140113222047488 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.11084453016519547, loss=0.024866381660103798
I0305 17:55:07.659805 140106748385024 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.11093717813491821, loss=0.022300852462649345
I0305 17:55:39.969202 140113222047488 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.10757628828287125, loss=0.023588785901665688
I0305 17:55:57.606588 140274064205632 spec.py:321] Evaluating on the training split.
I0305 17:57:58.005149 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 17:58:01.097430 140274064205632 spec.py:349] Evaluating on the test split.
I0305 17:58:04.189550 140274064205632 submission_runner.py:411] Time since start: 28557.29s, 	Step: 56756, 	{'train/accuracy': 0.9936248660087585, 'train/loss': 0.01993406191468239, 'train/mean_average_precision': 0.6411082926218209, 'validation/accuracy': 0.9869481325149536, 'validation/loss': 0.046904418617486954, 'validation/mean_average_precision': 0.2937136247289022, 'validation/num_examples': 43793, 'test/accuracy': 0.9860533475875854, 'test/loss': 0.05013333261013031, 'test/mean_average_precision': 0.274866405576504, 'test/num_examples': 43793, 'score': 18267.287185430527, 'total_duration': 28557.286256313324, 'accumulated_submission_time': 18267.287185430527, 'accumulated_eval_time': 10285.594108343124, 'accumulated_logging_time': 2.806877851486206}
I0305 17:58:04.215518 140098771031808 logging_writer.py:48] [56756] accumulated_eval_time=10285.594108, accumulated_logging_time=2.806878, accumulated_submission_time=18267.287185, global_step=56756, preemption_count=0, score=18267.287185, test/accuracy=0.986053, test/loss=0.050133, test/mean_average_precision=0.274866, test/num_examples=43793, total_duration=28557.286256, train/accuracy=0.993625, train/loss=0.019934, train/mean_average_precision=0.641108, validation/accuracy=0.986948, validation/loss=0.046904, validation/mean_average_precision=0.293714, validation/num_examples=43793
I0305 17:58:19.121468 140113230440192 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.10288869589567184, loss=0.02093007229268551
I0305 17:58:51.323561 140098771031808 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.09576112031936646, loss=0.01960439793765545
I0305 17:59:23.463830 140113230440192 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.09526491165161133, loss=0.019607113674283028
I0305 17:59:55.963712 140098771031808 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.12062451988458633, loss=0.023006338626146317
I0305 18:00:28.594436 140113230440192 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1112307533621788, loss=0.021840058267116547
I0305 18:01:00.796705 140098771031808 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10070440918207169, loss=0.021653827279806137
I0305 18:01:33.462965 140113230440192 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.09581129997968674, loss=0.023329544812440872
I0305 18:02:04.416622 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:04:06.343708 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:04:09.422963 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:04:12.475727 140274064205632 submission_runner.py:411] Time since start: 28925.57s, 	Step: 57497, 	{'train/accuracy': 0.9938535094261169, 'train/loss': 0.019285598769783974, 'train/mean_average_precision': 0.6639852026299888, 'validation/accuracy': 0.9869769811630249, 'validation/loss': 0.04774486646056175, 'validation/mean_average_precision': 0.29131519443612547, 'validation/num_examples': 43793, 'test/accuracy': 0.9861485362052917, 'test/loss': 0.050796300172805786, 'test/mean_average_precision': 0.2768490884306481, 'test/num_examples': 43793, 'score': 18507.456956386566, 'total_duration': 28925.572428703308, 'accumulated_submission_time': 18507.456956386566, 'accumulated_eval_time': 10413.653168201447, 'accumulated_logging_time': 2.8439853191375732}
I0305 18:04:12.501900 140106748385024 logging_writer.py:48] [57497] accumulated_eval_time=10413.653168, accumulated_logging_time=2.843985, accumulated_submission_time=18507.456956, global_step=57497, preemption_count=0, score=18507.456956, test/accuracy=0.986149, test/loss=0.050796, test/mean_average_precision=0.276849, test/num_examples=43793, total_duration=28925.572429, train/accuracy=0.993854, train/loss=0.019286, train/mean_average_precision=0.663985, validation/accuracy=0.986977, validation/loss=0.047745, validation/mean_average_precision=0.291315, validation/num_examples=43793
I0305 18:04:13.840376 140113222047488 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.11663095653057098, loss=0.022510075941681862
I0305 18:04:46.228921 140106748385024 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.0964496061205864, loss=0.02001073583960533
I0305 18:05:18.449864 140113222047488 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.12463198602199554, loss=0.02308192290365696
I0305 18:05:50.627159 140106748385024 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.14523287117481232, loss=0.025066858157515526
I0305 18:06:23.438597 140113222047488 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10411646217107773, loss=0.019940707832574844
I0305 18:06:55.553718 140106748385024 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.13353723287582397, loss=0.022744396701455116
I0305 18:07:28.439815 140113222047488 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.12232394516468048, loss=0.02363344468176365
I0305 18:08:00.633477 140106748385024 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.10269016027450562, loss=0.020348401740193367
I0305 18:08:12.663750 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:10:09.042339 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:10:12.111820 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:10:15.147303 140274064205632 submission_runner.py:411] Time since start: 29288.24s, 	Step: 58239, 	{'train/accuracy': 0.9939899444580078, 'train/loss': 0.018729353323578835, 'train/mean_average_precision': 0.6713579688097611, 'validation/accuracy': 0.9869345426559448, 'validation/loss': 0.04745245352387428, 'validation/mean_average_precision': 0.293276872576415, 'validation/num_examples': 43793, 'test/accuracy': 0.9861140251159668, 'test/loss': 0.050452522933483124, 'test/mean_average_precision': 0.2746775260475992, 'test/num_examples': 43793, 'score': 18747.58773970604, 'total_duration': 29288.24400830269, 'accumulated_submission_time': 18747.58773970604, 'accumulated_eval_time': 10536.1366751194, 'accumulated_logging_time': 2.8811538219451904}
I0305 18:10:15.173773 140098771031808 logging_writer.py:48] [58239] accumulated_eval_time=10536.136675, accumulated_logging_time=2.881154, accumulated_submission_time=18747.587740, global_step=58239, preemption_count=0, score=18747.587740, test/accuracy=0.986114, test/loss=0.050453, test/mean_average_precision=0.274678, test/num_examples=43793, total_duration=29288.244008, train/accuracy=0.993990, train/loss=0.018729, train/mean_average_precision=0.671358, validation/accuracy=0.986935, validation/loss=0.047452, validation/mean_average_precision=0.293277, validation/num_examples=43793
I0305 18:10:34.932392 140105330788096 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.09992268681526184, loss=0.01982731558382511
I0305 18:11:06.947352 140098771031808 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.1196853369474411, loss=0.022745799273252487
I0305 18:11:38.769598 140105330788096 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.09326881915330887, loss=0.020288720726966858
I0305 18:12:10.820491 140098771031808 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.11944619566202164, loss=0.022240495309233665
I0305 18:12:42.991322 140105330788096 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.11072143167257309, loss=0.022492991760373116
I0305 18:13:15.432844 140098771031808 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.10671364516019821, loss=0.02075769565999508
I0305 18:13:47.570188 140105330788096 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.1026199534535408, loss=0.021038930863142014
I0305 18:14:15.360275 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:16:14.322404 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:16:17.384040 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:16:20.419387 140274064205632 submission_runner.py:411] Time since start: 29653.52s, 	Step: 58988, 	{'train/accuracy': 0.9940062165260315, 'train/loss': 0.018574265763163567, 'train/mean_average_precision': 0.6838410034958001, 'validation/accuracy': 0.9871000051498413, 'validation/loss': 0.04771089181303978, 'validation/mean_average_precision': 0.29635709226317963, 'validation/num_examples': 43793, 'test/accuracy': 0.9863199591636658, 'test/loss': 0.05079415440559387, 'test/mean_average_precision': 0.27806617618960766, 'test/num_examples': 43793, 'score': 18987.742092847824, 'total_duration': 29653.516090154648, 'accumulated_submission_time': 18987.742092847824, 'accumulated_eval_time': 10661.195743560791, 'accumulated_logging_time': 2.9196202754974365}
I0305 18:16:20.445755 140113222047488 logging_writer.py:48] [58988] accumulated_eval_time=10661.195744, accumulated_logging_time=2.919620, accumulated_submission_time=18987.742093, global_step=58988, preemption_count=0, score=18987.742093, test/accuracy=0.986320, test/loss=0.050794, test/mean_average_precision=0.278066, test/num_examples=43793, total_duration=29653.516090, train/accuracy=0.994006, train/loss=0.018574, train/mean_average_precision=0.683841, validation/accuracy=0.987100, validation/loss=0.047711, validation/mean_average_precision=0.296357, validation/num_examples=43793
I0305 18:16:24.624941 140113230440192 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.148769810795784, loss=0.02311672270298004
I0305 18:16:56.741291 140113222047488 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.10868233442306519, loss=0.021305657923221588
I0305 18:17:28.542647 140113230440192 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.13280463218688965, loss=0.022299837321043015
I0305 18:18:00.275576 140113222047488 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.11924726516008377, loss=0.021386267617344856
I0305 18:18:32.506429 140113230440192 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.12017110735177994, loss=0.022141197696328163
I0305 18:19:04.369127 140113222047488 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.13071058690547943, loss=0.0234235692769289
I0305 18:19:36.378293 140113230440192 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.11479116976261139, loss=0.021131101995706558
I0305 18:20:08.849368 140113222047488 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.10795258730649948, loss=0.020146280527114868
I0305 18:20:20.498969 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:22:19.104789 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:22:22.276830 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:22:25.373029 140274064205632 submission_runner.py:411] Time since start: 30018.47s, 	Step: 59736, 	{'train/accuracy': 0.9945381283760071, 'train/loss': 0.017136039212346077, 'train/mean_average_precision': 0.7119890023519173, 'validation/accuracy': 0.9870768189430237, 'validation/loss': 0.04772680252790451, 'validation/mean_average_precision': 0.2921333089367812, 'validation/num_examples': 43793, 'test/accuracy': 0.9861990809440613, 'test/loss': 0.05083529278635979, 'test/mean_average_precision': 0.27545050391401155, 'test/num_examples': 43793, 'score': 19227.76380634308, 'total_duration': 30018.469710588455, 'accumulated_submission_time': 19227.76380634308, 'accumulated_eval_time': 10786.069747924805, 'accumulated_logging_time': 2.9572134017944336}
I0305 18:22:25.400365 140098771031808 logging_writer.py:48] [59736] accumulated_eval_time=10786.069748, accumulated_logging_time=2.957213, accumulated_submission_time=19227.763806, global_step=59736, preemption_count=0, score=19227.763806, test/accuracy=0.986199, test/loss=0.050835, test/mean_average_precision=0.275451, test/num_examples=43793, total_duration=30018.469711, train/accuracy=0.994538, train/loss=0.017136, train/mean_average_precision=0.711989, validation/accuracy=0.987077, validation/loss=0.047727, validation/mean_average_precision=0.292133, validation/num_examples=43793
I0305 18:22:46.436290 140106748385024 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.11743754148483276, loss=0.022418532520532608
I0305 18:23:18.757171 140098771031808 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.11602488905191422, loss=0.02234748937189579
I0305 18:23:50.571358 140106748385024 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.13228413462638855, loss=0.02314775623381138
I0305 18:24:23.069913 140098771031808 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.1129678413271904, loss=0.020294010639190674
I0305 18:24:56.390324 140106748385024 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.1266472339630127, loss=0.02187102660536766
I0305 18:25:28.902743 140098771031808 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.1186332181096077, loss=0.023709144443273544
I0305 18:26:00.846410 140106748385024 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.11819213628768921, loss=0.020435359328985214
I0305 18:26:25.553472 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:28:26.709970 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:28:29.771568 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:28:32.821645 140274064205632 submission_runner.py:411] Time since start: 30385.92s, 	Step: 60478, 	{'train/accuracy': 0.9946551322937012, 'train/loss': 0.01688678190112114, 'train/mean_average_precision': 0.7121866182580209, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.047978416085243225, 'validation/mean_average_precision': 0.29578886823271044, 'validation/num_examples': 43793, 'test/accuracy': 0.9862260818481445, 'test/loss': 0.050933606922626495, 'test/mean_average_precision': 0.27696953945176744, 'test/num_examples': 43793, 'score': 19467.88423514366, 'total_duration': 30385.918347597122, 'accumulated_submission_time': 19467.88423514366, 'accumulated_eval_time': 10913.337877035141, 'accumulated_logging_time': 2.9957222938537598}
I0305 18:28:32.848312 140105330788096 logging_writer.py:48] [60478] accumulated_eval_time=10913.337877, accumulated_logging_time=2.995722, accumulated_submission_time=19467.884235, global_step=60478, preemption_count=0, score=19467.884235, test/accuracy=0.986226, test/loss=0.050934, test/mean_average_precision=0.276970, test/num_examples=43793, total_duration=30385.918348, train/accuracy=0.994655, train/loss=0.016887, train/mean_average_precision=0.712187, validation/accuracy=0.986986, validation/loss=0.047978, validation/mean_average_precision=0.295789, validation/num_examples=43793
I0305 18:28:40.258112 140113230440192 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.10686996579170227, loss=0.019470300525426865
I0305 18:29:12.207816 140105330788096 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.12107210606336594, loss=0.019845711067318916
I0305 18:29:44.381553 140113230440192 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.10761094093322754, loss=0.020295385271310806
I0305 18:30:16.553419 140105330788096 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.12603121995925903, loss=0.019456446170806885
I0305 18:30:48.838773 140113230440192 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.11049427837133408, loss=0.021869974210858345
I0305 18:31:20.887494 140105330788096 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.12154819071292877, loss=0.018718045204877853
I0305 18:31:52.799163 140113230440192 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.11085939407348633, loss=0.019223714247345924
I0305 18:32:24.891901 140105330788096 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.12626881897449493, loss=0.021126603707671165
I0305 18:32:32.838600 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:34:27.100691 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:34:30.175116 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:34:33.224913 140274064205632 submission_runner.py:411] Time since start: 30746.32s, 	Step: 61226, 	{'train/accuracy': 0.994626522064209, 'train/loss': 0.0167669877409935, 'train/mean_average_precision': 0.7171335605289063, 'validation/accuracy': 0.9870147109031677, 'validation/loss': 0.04804197698831558, 'validation/mean_average_precision': 0.2928131691734593, 'validation/num_examples': 43793, 'test/accuracy': 0.9861772060394287, 'test/loss': 0.051029447466135025, 'test/mean_average_precision': 0.27596752123310386, 'test/num_examples': 43793, 'score': 19707.84122633934, 'total_duration': 30746.321582317352, 'accumulated_submission_time': 19707.84122633934, 'accumulated_eval_time': 11033.724110364914, 'accumulated_logging_time': 3.034928560256958}
I0305 18:34:33.253350 140098771031808 logging_writer.py:48] [61226] accumulated_eval_time=11033.724110, accumulated_logging_time=3.034929, accumulated_submission_time=19707.841226, global_step=61226, preemption_count=0, score=19707.841226, test/accuracy=0.986177, test/loss=0.051029, test/mean_average_precision=0.275968, test/num_examples=43793, total_duration=30746.321582, train/accuracy=0.994627, train/loss=0.016767, train/mean_average_precision=0.717134, validation/accuracy=0.987015, validation/loss=0.048042, validation/mean_average_precision=0.292813, validation/num_examples=43793
I0305 18:34:57.552361 140113222047488 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.1263974905014038, loss=0.020893342792987823
I0305 18:35:30.229550 140098771031808 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.12741556763648987, loss=0.019975539296865463
I0305 18:36:02.967141 140113222047488 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.11791551858186722, loss=0.01881689392030239
I0305 18:36:35.577138 140098771031808 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.12386077642440796, loss=0.019901731982827187
I0305 18:37:08.612176 140113222047488 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.1431235373020172, loss=0.021702628582715988
I0305 18:37:41.184678 140098771031808 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.12039027363061905, loss=0.01983552612364292
I0305 18:38:13.709475 140113222047488 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.10956621170043945, loss=0.0196723323315382
I0305 18:38:33.532297 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:40:31.905235 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:40:34.960272 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:40:38.077875 140274064205632 submission_runner.py:411] Time since start: 31111.17s, 	Step: 61962, 	{'train/accuracy': 0.9944984316825867, 'train/loss': 0.017098691314458847, 'train/mean_average_precision': 0.7176627280464143, 'validation/accuracy': 0.9870285391807556, 'validation/loss': 0.04894905537366867, 'validation/mean_average_precision': 0.29461229739871203, 'validation/num_examples': 43793, 'test/accuracy': 0.9862551093101501, 'test/loss': 0.05199601501226425, 'test/mean_average_precision': 0.274682914531157, 'test/num_examples': 43793, 'score': 19948.088528633118, 'total_duration': 31111.174551725388, 'accumulated_submission_time': 19948.088528633118, 'accumulated_eval_time': 11158.269618988037, 'accumulated_logging_time': 3.074516534805298}
I0305 18:40:38.106203 140106748385024 logging_writer.py:48] [61962] accumulated_eval_time=11158.269619, accumulated_logging_time=3.074517, accumulated_submission_time=19948.088529, global_step=61962, preemption_count=0, score=19948.088529, test/accuracy=0.986255, test/loss=0.051996, test/mean_average_precision=0.274683, test/num_examples=43793, total_duration=31111.174552, train/accuracy=0.994498, train/loss=0.017099, train/mean_average_precision=0.717663, validation/accuracy=0.987029, validation/loss=0.048949, validation/mean_average_precision=0.294612, validation/num_examples=43793
I0305 18:40:51.048780 140113230440192 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.12028194963932037, loss=0.021487848833203316
I0305 18:41:23.248972 140106748385024 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.1468927562236786, loss=0.02241438813507557
I0305 18:41:55.391716 140113230440192 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.13390026986598969, loss=0.019429940730333328
I0305 18:42:27.746159 140106748385024 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.11029549688100815, loss=0.016417667269706726
I0305 18:42:59.818120 140113230440192 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.11535753309726715, loss=0.018934309482574463
I0305 18:43:31.644856 140106748385024 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.11200384050607681, loss=0.020696552470326424
I0305 18:44:04.182793 140113230440192 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.13513845205307007, loss=0.022324740886688232
I0305 18:44:36.736227 140106748385024 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.1212400272488594, loss=0.020708361640572548
I0305 18:44:38.370074 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:46:37.267889 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:46:40.471716 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:46:43.534255 140274064205632 submission_runner.py:411] Time since start: 31476.63s, 	Step: 62706, 	{'train/accuracy': 0.9945216178894043, 'train/loss': 0.01711542345583439, 'train/mean_average_precision': 0.7133070344489623, 'validation/accuracy': 0.9869871139526367, 'validation/loss': 0.04834806174039841, 'validation/mean_average_precision': 0.2968437849571173, 'validation/num_examples': 43793, 'test/accuracy': 0.9861464500427246, 'test/loss': 0.05165763199329376, 'test/mean_average_precision': 0.2757454189249902, 'test/num_examples': 43793, 'score': 20188.32045006752, 'total_duration': 31476.630955457687, 'accumulated_submission_time': 20188.32045006752, 'accumulated_eval_time': 11283.433761835098, 'accumulated_logging_time': 3.114187479019165}
I0305 18:46:43.561952 140098771031808 logging_writer.py:48] [62706] accumulated_eval_time=11283.433762, accumulated_logging_time=3.114187, accumulated_submission_time=20188.320450, global_step=62706, preemption_count=0, score=20188.320450, test/accuracy=0.986146, test/loss=0.051658, test/mean_average_precision=0.275745, test/num_examples=43793, total_duration=31476.630955, train/accuracy=0.994522, train/loss=0.017115, train/mean_average_precision=0.713307, validation/accuracy=0.986987, validation/loss=0.048348, validation/mean_average_precision=0.296844, validation/num_examples=43793
I0305 18:47:14.600746 140105330788096 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.11877267062664032, loss=0.01992129161953926
I0305 18:47:47.284939 140098771031808 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.1167576014995575, loss=0.01870802789926529
I0305 18:48:19.929485 140105330788096 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.11674931645393372, loss=0.018047984689474106
I0305 18:48:52.591084 140098771031808 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.15906178951263428, loss=0.02082435041666031
I0305 18:49:24.785134 140105330788096 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.1259484887123108, loss=0.019534777849912643
I0305 18:49:56.739911 140098771031808 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.11652066558599472, loss=0.019649509340524673
I0305 18:50:29.214538 140105330788096 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.12461661547422409, loss=0.020892981439828873
I0305 18:50:43.754347 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:52:40.461145 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:52:43.573630 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:52:46.654048 140274064205632 submission_runner.py:411] Time since start: 31839.75s, 	Step: 63446, 	{'train/accuracy': 0.9945242404937744, 'train/loss': 0.01698518730700016, 'train/mean_average_precision': 0.7093867418780908, 'validation/accuracy': 0.9869940280914307, 'validation/loss': 0.04864724725484848, 'validation/mean_average_precision': 0.29489668688704584, 'validation/num_examples': 43793, 'test/accuracy': 0.9861767888069153, 'test/loss': 0.05186263471841812, 'test/mean_average_precision': 0.2775576912809816, 'test/num_examples': 43793, 'score': 20428.48164987564, 'total_duration': 31839.75074863434, 'accumulated_submission_time': 20428.48164987564, 'accumulated_eval_time': 11406.333416223526, 'accumulated_logging_time': 3.152745485305786}
I0305 18:52:46.681765 140106748385024 logging_writer.py:48] [63446] accumulated_eval_time=11406.333416, accumulated_logging_time=3.152745, accumulated_submission_time=20428.481650, global_step=63446, preemption_count=0, score=20428.481650, test/accuracy=0.986177, test/loss=0.051863, test/mean_average_precision=0.277558, test/num_examples=43793, total_duration=31839.750749, train/accuracy=0.994524, train/loss=0.016985, train/mean_average_precision=0.709387, validation/accuracy=0.986994, validation/loss=0.048647, validation/mean_average_precision=0.294897, validation/num_examples=43793
I0305 18:53:04.600860 140113222047488 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.12810131907463074, loss=0.019645562395453453
I0305 18:53:37.017039 140106748385024 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.11836297810077667, loss=0.02106943540275097
I0305 18:54:09.402611 140113222047488 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.1317014992237091, loss=0.020582253113389015
I0305 18:54:41.800043 140106748385024 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.12982186675071716, loss=0.019119547680020332
I0305 18:55:14.271941 140113222047488 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.11802194267511368, loss=0.019877590239048004
I0305 18:55:46.204568 140106748385024 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.13945133984088898, loss=0.02202531136572361
I0305 18:56:18.466219 140113222047488 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.1247401088476181, loss=0.020549556240439415
I0305 18:56:46.870126 140274064205632 spec.py:321] Evaluating on the training split.
I0305 18:58:45.681247 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 18:58:48.811836 140274064205632 spec.py:349] Evaluating on the test split.
I0305 18:58:51.910352 140274064205632 submission_runner.py:411] Time since start: 32205.01s, 	Step: 64189, 	{'train/accuracy': 0.9944551587104797, 'train/loss': 0.017058368772268295, 'train/mean_average_precision': 0.7121639624300242, 'validation/accuracy': 0.9870054125785828, 'validation/loss': 0.048753298819065094, 'validation/mean_average_precision': 0.29760508364872107, 'validation/num_examples': 43793, 'test/accuracy': 0.9862075448036194, 'test/loss': 0.05215508118271828, 'test/mean_average_precision': 0.27386526672275935, 'test/num_examples': 43793, 'score': 20668.63866043091, 'total_duration': 32205.007052898407, 'accumulated_submission_time': 20668.63866043091, 'accumulated_eval_time': 11531.373598575592, 'accumulated_logging_time': 3.1913905143737793}
I0305 18:58:51.938482 140098771031808 logging_writer.py:48] [64189] accumulated_eval_time=11531.373599, accumulated_logging_time=3.191391, accumulated_submission_time=20668.638660, global_step=64189, preemption_count=0, score=20668.638660, test/accuracy=0.986208, test/loss=0.052155, test/mean_average_precision=0.273865, test/num_examples=43793, total_duration=32205.007053, train/accuracy=0.994455, train/loss=0.017058, train/mean_average_precision=0.712164, validation/accuracy=0.987005, validation/loss=0.048753, validation/mean_average_precision=0.297605, validation/num_examples=43793
I0305 18:58:55.861035 140113230440192 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.14715375006198883, loss=0.021418984979391098
I0305 18:59:28.219496 140098771031808 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.13362851738929749, loss=0.020447149872779846
I0305 19:00:00.572794 140113230440192 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.1133577823638916, loss=0.017463956028223038
I0305 19:00:32.730072 140098771031808 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.1346445530653, loss=0.019247615709900856
I0305 19:01:05.314206 140113230440192 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.13156180083751678, loss=0.021450825035572052
I0305 19:01:37.432638 140098771031808 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.13548649847507477, loss=0.01818862371146679
I0305 19:02:09.832732 140113230440192 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.12912535667419434, loss=0.01882365718483925
I0305 19:02:41.864458 140098771031808 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.1566341668367386, loss=0.022282330319285393
I0305 19:02:52.223449 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:04:49.505665 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:04:52.579615 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:04:55.602710 140274064205632 submission_runner.py:411] Time since start: 32568.70s, 	Step: 64933, 	{'train/accuracy': 0.9945725202560425, 'train/loss': 0.016744229942560196, 'train/mean_average_precision': 0.7202841801572984, 'validation/accuracy': 0.9870171546936035, 'validation/loss': 0.049075957387685776, 'validation/mean_average_precision': 0.2939358425075371, 'validation/num_examples': 43793, 'test/accuracy': 0.9862125515937805, 'test/loss': 0.052299417555332184, 'test/mean_average_precision': 0.2767880296537022, 'test/num_examples': 43793, 'score': 20908.890762090683, 'total_duration': 32568.699409723282, 'accumulated_submission_time': 20908.890762090683, 'accumulated_eval_time': 11654.752810955048, 'accumulated_logging_time': 3.231707811355591}
I0305 19:04:55.631008 140106748385024 logging_writer.py:48] [64933] accumulated_eval_time=11654.752811, accumulated_logging_time=3.231708, accumulated_submission_time=20908.890762, global_step=64933, preemption_count=0, score=20908.890762, test/accuracy=0.986213, test/loss=0.052299, test/mean_average_precision=0.276788, test/num_examples=43793, total_duration=32568.699410, train/accuracy=0.994573, train/loss=0.016744, train/mean_average_precision=0.720284, validation/accuracy=0.987017, validation/loss=0.049076, validation/mean_average_precision=0.293936, validation/num_examples=43793
I0305 19:05:17.583067 140113222047488 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.11619514226913452, loss=0.01866113767027855
I0305 19:05:49.741140 140106748385024 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.1347455531358719, loss=0.018209224566817284
I0305 19:06:21.677522 140113222047488 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.12163008749485016, loss=0.01721901074051857
I0305 19:06:53.273690 140106748385024 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.12880714237689972, loss=0.020169531926512718
I0305 19:07:24.848987 140113222047488 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.13478536903858185, loss=0.02019168809056282
I0305 19:07:56.612469 140106748385024 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.12542927265167236, loss=0.01937011443078518
I0305 19:08:28.459366 140113222047488 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.12637780606746674, loss=0.018704522401094437
I0305 19:08:55.642342 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:10:50.804979 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:10:53.864291 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:10:56.834357 140274064205632 submission_runner.py:411] Time since start: 32929.93s, 	Step: 65686, 	{'train/accuracy': 0.9948503971099854, 'train/loss': 0.015980828553438187, 'train/mean_average_precision': 0.7366013362986004, 'validation/accuracy': 0.9870244860649109, 'validation/loss': 0.049368225038051605, 'validation/mean_average_precision': 0.29443311643309766, 'validation/num_examples': 43793, 'test/accuracy': 0.9862125515937805, 'test/loss': 0.05255308374762535, 'test/mean_average_precision': 0.276474986311574, 'test/num_examples': 43793, 'score': 21148.8704726696, 'total_duration': 32929.93105196953, 'accumulated_submission_time': 21148.8704726696, 'accumulated_eval_time': 11775.944782495499, 'accumulated_logging_time': 3.270869255065918}
I0305 19:10:56.862315 140098771031808 logging_writer.py:48] [65686] accumulated_eval_time=11775.944782, accumulated_logging_time=3.270869, accumulated_submission_time=21148.870473, global_step=65686, preemption_count=0, score=21148.870473, test/accuracy=0.986213, test/loss=0.052553, test/mean_average_precision=0.276475, test/num_examples=43793, total_duration=32929.931052, train/accuracy=0.994850, train/loss=0.015981, train/mean_average_precision=0.736601, validation/accuracy=0.987024, validation/loss=0.049368, validation/mean_average_precision=0.294433, validation/num_examples=43793
I0305 19:11:01.908020 140113230440192 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.14527879655361176, loss=0.01891922950744629
I0305 19:11:33.992064 140098771031808 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.12394846975803375, loss=0.01812427118420601
I0305 19:12:06.992027 140113230440192 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.13341401517391205, loss=0.019518865272402763
I0305 19:12:39.467049 140098771031808 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.13489897549152374, loss=0.02089395932853222
I0305 19:13:12.016175 140113230440192 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.12010876089334488, loss=0.016242532059550285
I0305 19:13:44.195183 140098771031808 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.1450820118188858, loss=0.01957235485315323
I0305 19:14:16.526890 140113230440192 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.1275787055492401, loss=0.019230565056204796
I0305 19:14:48.537053 140098771031808 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.147246852517128, loss=0.020198648795485497
I0305 19:14:57.067366 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:16:52.445390 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:16:55.749964 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:16:58.958000 140274064205632 submission_runner.py:411] Time since start: 33292.05s, 	Step: 66427, 	{'train/accuracy': 0.9951132535934448, 'train/loss': 0.015388496220111847, 'train/mean_average_precision': 0.7601626246504376, 'validation/accuracy': 0.9869201183319092, 'validation/loss': 0.0494905523955822, 'validation/mean_average_precision': 0.29309556415490695, 'validation/num_examples': 43793, 'test/accuracy': 0.986159086227417, 'test/loss': 0.05259747430682182, 'test/mean_average_precision': 0.2758683298245158, 'test/num_examples': 43793, 'score': 21389.03907442093, 'total_duration': 33292.05467629433, 'accumulated_submission_time': 21389.03907442093, 'accumulated_eval_time': 11897.835361719131, 'accumulated_logging_time': 3.3115804195404053}
I0305 19:16:58.992630 140105330788096 logging_writer.py:48] [66427] accumulated_eval_time=11897.835362, accumulated_logging_time=3.311580, accumulated_submission_time=21389.039074, global_step=66427, preemption_count=0, score=21389.039074, test/accuracy=0.986159, test/loss=0.052597, test/mean_average_precision=0.275868, test/num_examples=43793, total_duration=33292.054676, train/accuracy=0.995113, train/loss=0.015388, train/mean_average_precision=0.760163, validation/accuracy=0.986920, validation/loss=0.049491, validation/mean_average_precision=0.293096, validation/num_examples=43793
I0305 19:17:23.354321 140113222047488 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.12243790924549103, loss=0.018358584493398666
I0305 19:17:56.156176 140105330788096 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.14563460648059845, loss=0.01952001266181469
I0305 19:18:28.199450 140113222047488 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.1481621414422989, loss=0.019748492166399956
I0305 19:18:59.892266 140105330788096 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.13585512340068817, loss=0.019053898751735687
I0305 19:19:32.417256 140113222047488 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.14222000539302826, loss=0.01978539302945137
I0305 19:20:05.038001 140105330788096 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.13063502311706543, loss=0.019657719880342484
I0305 19:20:37.638388 140113222047488 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.1469334214925766, loss=0.019094541668891907
I0305 19:20:59.078063 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:22:53.117371 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:22:56.267333 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:22:59.318598 140274064205632 submission_runner.py:411] Time since start: 33652.42s, 	Step: 67167, 	{'train/accuracy': 0.9952598214149475, 'train/loss': 0.014869295060634613, 'train/mean_average_precision': 0.7578312354119906, 'validation/accuracy': 0.9869258403778076, 'validation/loss': 0.04947284609079361, 'validation/mean_average_precision': 0.29364985734866383, 'validation/num_examples': 43793, 'test/accuracy': 0.9861615896224976, 'test/loss': 0.052687082439661026, 'test/mean_average_precision': 0.27654702715402923, 'test/num_examples': 43793, 'score': 21629.091300964355, 'total_duration': 33652.41530036926, 'accumulated_submission_time': 21629.091300964355, 'accumulated_eval_time': 12018.075865268707, 'accumulated_logging_time': 3.3581244945526123}
I0305 19:22:59.346468 140098771031808 logging_writer.py:48] [67167] accumulated_eval_time=12018.075865, accumulated_logging_time=3.358124, accumulated_submission_time=21629.091301, global_step=67167, preemption_count=0, score=21629.091301, test/accuracy=0.986162, test/loss=0.052687, test/mean_average_precision=0.276547, test/num_examples=43793, total_duration=33652.415300, train/accuracy=0.995260, train/loss=0.014869, train/mean_average_precision=0.757831, validation/accuracy=0.986926, validation/loss=0.049473, validation/mean_average_precision=0.293650, validation/num_examples=43793
I0305 19:23:10.343445 140113230440192 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.11824218183755875, loss=0.017119044438004494
I0305 19:23:42.277601 140098771031808 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.15858367085456848, loss=0.02058923989534378
I0305 19:24:14.634003 140113230440192 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.12195339053869247, loss=0.019094062969088554
I0305 19:24:47.250275 140098771031808 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.13716669380664825, loss=0.021305762231349945
I0305 19:25:18.894154 140113230440192 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.13660472631454468, loss=0.01816765032708645
I0305 19:25:50.953630 140098771031808 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.14767183363437653, loss=0.019107436761260033
I0305 19:26:23.424515 140113230440192 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.13559891283512115, loss=0.0197895634919405
I0305 19:26:55.824238 140098771031808 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.13186874985694885, loss=0.017075056210160255
I0305 19:26:59.345004 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:28:54.469977 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:28:57.518108 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:29:00.522050 140274064205632 submission_runner.py:411] Time since start: 34013.62s, 	Step: 67912, 	{'train/accuracy': 0.9953340888023376, 'train/loss': 0.014705266803503036, 'train/mean_average_precision': 0.7636951762040912, 'validation/accuracy': 0.9869895577430725, 'validation/loss': 0.04969734326004982, 'validation/mean_average_precision': 0.29479195971224503, 'validation/num_examples': 43793, 'test/accuracy': 0.9861228466033936, 'test/loss': 0.05296842008829117, 'test/mean_average_precision': 0.27365270504160966, 'test/num_examples': 43793, 'score': 21869.056920528412, 'total_duration': 34013.61874079704, 'accumulated_submission_time': 21869.056920528412, 'accumulated_eval_time': 12139.252855539322, 'accumulated_logging_time': 3.3980062007904053}
I0305 19:29:00.551074 140106748385024 logging_writer.py:48] [67912] accumulated_eval_time=12139.252856, accumulated_logging_time=3.398006, accumulated_submission_time=21869.056921, global_step=67912, preemption_count=0, score=21869.056921, test/accuracy=0.986123, test/loss=0.052968, test/mean_average_precision=0.273653, test/num_examples=43793, total_duration=34013.618741, train/accuracy=0.995334, train/loss=0.014705, train/mean_average_precision=0.763695, validation/accuracy=0.986990, validation/loss=0.049697, validation/mean_average_precision=0.294792, validation/num_examples=43793
I0305 19:29:28.903597 140113222047488 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.13433991372585297, loss=0.01809614524245262
I0305 19:30:00.701271 140106748385024 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.12732048332691193, loss=0.018818046897649765
I0305 19:30:32.807387 140113222047488 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.1427561491727829, loss=0.019917398691177368
I0305 19:31:05.158360 140106748385024 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.13529492914676666, loss=0.019513141363859177
I0305 19:31:37.197366 140113222047488 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.15162800252437592, loss=0.021393099799752235
I0305 19:32:09.067777 140106748385024 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.1493128389120102, loss=0.02031988464295864
I0305 19:32:40.532792 140113222047488 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.1457107961177826, loss=0.018667589873075485
I0305 19:33:00.548491 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:34:54.842457 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:34:57.957399 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:35:00.984481 140274064205632 submission_runner.py:411] Time since start: 34374.08s, 	Step: 68664, 	{'train/accuracy': 0.9952453970909119, 'train/loss': 0.01498997863382101, 'train/mean_average_precision': 0.754445749752948, 'validation/accuracy': 0.9869197607040405, 'validation/loss': 0.04988449811935425, 'validation/mean_average_precision': 0.2920263811403023, 'validation/num_examples': 43793, 'test/accuracy': 0.9861595034599304, 'test/loss': 0.05308207869529724, 'test/mean_average_precision': 0.2710669890759926, 'test/num_examples': 43793, 'score': 22109.022665262222, 'total_duration': 34374.08118200302, 'accumulated_submission_time': 22109.022665262222, 'accumulated_eval_time': 12259.688798666, 'accumulated_logging_time': 3.4379780292510986}
I0305 19:35:01.012796 140098771031808 logging_writer.py:48] [68664] accumulated_eval_time=12259.688799, accumulated_logging_time=3.437978, accumulated_submission_time=22109.022665, global_step=68664, preemption_count=0, score=22109.022665, test/accuracy=0.986160, test/loss=0.053082, test/mean_average_precision=0.271067, test/num_examples=43793, total_duration=34374.081182, train/accuracy=0.995245, train/loss=0.014990, train/mean_average_precision=0.754446, validation/accuracy=0.986920, validation/loss=0.049884, validation/mean_average_precision=0.292026, validation/num_examples=43793
I0305 19:35:12.979427 140113230440192 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.1464466005563736, loss=0.017460253089666367
I0305 19:35:45.251667 140098771031808 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.15412375330924988, loss=0.020481904968619347
I0305 19:36:17.316900 140113230440192 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.14838358759880066, loss=0.018894502893090248
I0305 19:36:49.410623 140098771031808 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.12315922975540161, loss=0.016686731949448586
I0305 19:37:21.312196 140113230440192 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.1483387053012848, loss=0.019722606986761093
I0305 19:37:53.557068 140098771031808 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.12297273427248001, loss=0.017488626763224602
I0305 19:38:25.824422 140113230440192 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.15947407484054565, loss=0.017378080636262894
I0305 19:38:57.732020 140098771031808 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.1380506157875061, loss=0.018784228712320328
I0305 19:39:01.263724 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:40:57.558037 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:41:00.660744 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:41:03.805203 140274064205632 submission_runner.py:411] Time since start: 34736.90s, 	Step: 69412, 	{'train/accuracy': 0.9950612187385559, 'train/loss': 0.015234273858368397, 'train/mean_average_precision': 0.7505739178886142, 'validation/accuracy': 0.9869449138641357, 'validation/loss': 0.05006104335188866, 'validation/mean_average_precision': 0.2925591697654719, 'validation/num_examples': 43793, 'test/accuracy': 0.9861894249916077, 'test/loss': 0.0533779002726078, 'test/mean_average_precision': 0.2722586746507549, 'test/num_examples': 43793, 'score': 22349.240227222443, 'total_duration': 34736.90176987648, 'accumulated_submission_time': 22349.240227222443, 'accumulated_eval_time': 12382.230098962784, 'accumulated_logging_time': 3.4789209365844727}
I0305 19:41:03.834155 140105330788096 logging_writer.py:48] [69412] accumulated_eval_time=12382.230099, accumulated_logging_time=3.478921, accumulated_submission_time=22349.240227, global_step=69412, preemption_count=0, score=22349.240227, test/accuracy=0.986189, test/loss=0.053378, test/mean_average_precision=0.272259, test/num_examples=43793, total_duration=34736.901770, train/accuracy=0.995061, train/loss=0.015234, train/mean_average_precision=0.750574, validation/accuracy=0.986945, validation/loss=0.050061, validation/mean_average_precision=0.292559, validation/num_examples=43793
I0305 19:41:32.167850 140106748385024 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.13402369618415833, loss=0.0159500390291214
I0305 19:42:04.274844 140105330788096 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.14054472744464874, loss=0.016815293580293655
I0305 19:42:36.134253 140106748385024 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.14459630846977234, loss=0.021370872855186462
I0305 19:43:07.809871 140105330788096 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.1431722342967987, loss=0.01881612464785576
I0305 19:43:39.958394 140106748385024 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.13126006722450256, loss=0.018678730353713036
I0305 19:44:11.712461 140105330788096 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.14357836544513702, loss=0.019724184647202492
I0305 19:44:43.420768 140106748385024 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.13392005860805511, loss=0.01706666685640812
I0305 19:45:03.941591 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:46:54.980646 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:46:58.088784 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:47:01.154311 140274064205632 submission_runner.py:411] Time since start: 35094.25s, 	Step: 70165, 	{'train/accuracy': 0.9951221942901611, 'train/loss': 0.01517321914434433, 'train/mean_average_precision': 0.7626114380669714, 'validation/accuracy': 0.9870163798332214, 'validation/loss': 0.050270672887563705, 'validation/mean_average_precision': 0.29029621079634543, 'validation/num_examples': 43793, 'test/accuracy': 0.9861894249916077, 'test/loss': 0.05356570705771446, 'test/mean_average_precision': 0.27556860789357773, 'test/num_examples': 43793, 'score': 22589.316182613373, 'total_duration': 35094.25100970268, 'accumulated_submission_time': 22589.316182613373, 'accumulated_eval_time': 12499.442770004272, 'accumulated_logging_time': 3.5188333988189697}
I0305 19:47:01.184163 140098771031808 logging_writer.py:48] [70165] accumulated_eval_time=12499.442770, accumulated_logging_time=3.518833, accumulated_submission_time=22589.316183, global_step=70165, preemption_count=0, score=22589.316183, test/accuracy=0.986189, test/loss=0.053566, test/mean_average_precision=0.275569, test/num_examples=43793, total_duration=35094.251010, train/accuracy=0.995122, train/loss=0.015173, train/mean_average_precision=0.762611, validation/accuracy=0.987016, validation/loss=0.050271, validation/mean_average_precision=0.290296, validation/num_examples=43793
I0305 19:47:13.022968 140113230440192 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.14542177319526672, loss=0.020737571641802788
I0305 19:47:45.324661 140098771031808 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.140763059258461, loss=0.018674785271286964
I0305 19:48:17.283751 140113230440192 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1406368911266327, loss=0.017218196764588356
I0305 19:48:49.262819 140098771031808 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.12111330777406693, loss=0.016456719487905502
I0305 19:49:21.570578 140113230440192 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.1693446934223175, loss=0.01932458207011223
I0305 19:49:53.924987 140098771031808 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.14507752656936646, loss=0.01819145865738392
I0305 19:50:26.137808 140113230440192 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.14011691510677338, loss=0.01791650988161564
I0305 19:50:57.686068 140098771031808 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.1494758129119873, loss=0.018326645717024803
I0305 19:51:01.182796 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:52:51.991672 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:52:55.037603 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:52:58.042362 140274064205632 submission_runner.py:411] Time since start: 35451.14s, 	Step: 70912, 	{'train/accuracy': 0.9949877262115479, 'train/loss': 0.015439958311617374, 'train/mean_average_precision': 0.7428052908604961, 'validation/accuracy': 0.9870102405548096, 'validation/loss': 0.050479333847761154, 'validation/mean_average_precision': 0.2939184261839505, 'validation/num_examples': 43793, 'test/accuracy': 0.9861582517623901, 'test/loss': 0.05383036285638809, 'test/mean_average_precision': 0.27474869020042414, 'test/num_examples': 43793, 'score': 22829.283252239227, 'total_duration': 35451.13889479637, 'accumulated_submission_time': 22829.283252239227, 'accumulated_eval_time': 12616.302125930786, 'accumulated_logging_time': 3.5597028732299805}
I0305 19:52:58.070958 140105330788096 logging_writer.py:48] [70912] accumulated_eval_time=12616.302126, accumulated_logging_time=3.559703, accumulated_submission_time=22829.283252, global_step=70912, preemption_count=0, score=22829.283252, test/accuracy=0.986158, test/loss=0.053830, test/mean_average_precision=0.274749, test/num_examples=43793, total_duration=35451.138895, train/accuracy=0.994988, train/loss=0.015440, train/mean_average_precision=0.742805, validation/accuracy=0.987010, validation/loss=0.050479, validation/mean_average_precision=0.293918, validation/num_examples=43793
I0305 19:53:26.613176 140106748385024 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.1372535079717636, loss=0.019083939492702484
I0305 19:53:58.349200 140105330788096 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.1411907970905304, loss=0.017061062157154083
I0305 19:54:30.354318 140106748385024 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.14237910509109497, loss=0.01838028058409691
I0305 19:55:02.634024 140105330788096 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.15686248242855072, loss=0.019224390387535095
I0305 19:55:34.350329 140106748385024 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.15060506761074066, loss=0.016838438808918
I0305 19:56:06.674836 140105330788096 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.14746253192424774, loss=0.018456265330314636
I0305 19:56:38.500162 140106748385024 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.13780206441879272, loss=0.01745842956006527
I0305 19:56:58.239650 140274064205632 spec.py:321] Evaluating on the training split.
I0305 19:58:46.526427 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 19:58:49.615095 140274064205632 spec.py:349] Evaluating on the test split.
I0305 19:58:52.776946 140274064205632 submission_runner.py:411] Time since start: 35805.87s, 	Step: 71663, 	{'train/accuracy': 0.9952868819236755, 'train/loss': 0.014525205828249454, 'train/mean_average_precision': 0.7632115012875428, 'validation/accuracy': 0.9869834780693054, 'validation/loss': 0.050310663878917694, 'validation/mean_average_precision': 0.29127564583374943, 'validation/num_examples': 43793, 'test/accuracy': 0.9861262440681458, 'test/loss': 0.053686510771512985, 'test/mean_average_precision': 0.2744871260934867, 'test/num_examples': 43793, 'score': 23069.419085025787, 'total_duration': 35805.873645067215, 'accumulated_submission_time': 23069.419085025787, 'accumulated_eval_time': 12730.839373111725, 'accumulated_logging_time': 3.6005778312683105}
I0305 19:58:52.808393 140098771031808 logging_writer.py:48] [71663] accumulated_eval_time=12730.839373, accumulated_logging_time=3.600578, accumulated_submission_time=23069.419085, global_step=71663, preemption_count=0, score=23069.419085, test/accuracy=0.986126, test/loss=0.053687, test/mean_average_precision=0.274487, test/num_examples=43793, total_duration=35805.873645, train/accuracy=0.995287, train/loss=0.014525, train/mean_average_precision=0.763212, validation/accuracy=0.986983, validation/loss=0.050311, validation/mean_average_precision=0.291276, validation/num_examples=43793
I0305 19:59:05.076650 140113230440192 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.14038479328155518, loss=0.017615655437111855
I0305 19:59:37.344046 140098771031808 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.14543186128139496, loss=0.018788108602166176
I0305 20:00:09.379648 140113230440192 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.1543482095003128, loss=0.0191365797072649
I0305 20:00:41.817198 140098771031808 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.12648701667785645, loss=0.014634302817285061
I0305 20:01:14.177930 140113230440192 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.13729992508888245, loss=0.01806587725877762
I0305 20:01:46.320129 140098771031808 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.13754160702228546, loss=0.018166562542319298
I0305 20:02:18.339200 140113230440192 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.13287343084812164, loss=0.018831215798854828
I0305 20:02:50.406425 140098771031808 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.17250338196754456, loss=0.019355420023202896
I0305 20:02:53.023925 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:04:49.074918 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:04:52.112420 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:04:55.114458 140274064205632 submission_runner.py:411] Time since start: 36168.21s, 	Step: 72409, 	{'train/accuracy': 0.995356559753418, 'train/loss': 0.014484100043773651, 'train/mean_average_precision': 0.7616841179855216, 'validation/accuracy': 0.9870256781578064, 'validation/loss': 0.05053041875362396, 'validation/mean_average_precision': 0.2914941165453763, 'validation/num_examples': 43793, 'test/accuracy': 0.9861717224121094, 'test/loss': 0.05390779674053192, 'test/mean_average_precision': 0.2749625676471574, 'test/num_examples': 43793, 'score': 23309.602932929993, 'total_duration': 36168.21115899086, 'accumulated_submission_time': 23309.602932929993, 'accumulated_eval_time': 12852.929856061935, 'accumulated_logging_time': 3.643061876296997}
I0305 20:04:55.144310 140106748385024 logging_writer.py:48] [72409] accumulated_eval_time=12852.929856, accumulated_logging_time=3.643062, accumulated_submission_time=23309.602933, global_step=72409, preemption_count=0, score=23309.602933, test/accuracy=0.986172, test/loss=0.053908, test/mean_average_precision=0.274963, test/num_examples=43793, total_duration=36168.211159, train/accuracy=0.995357, train/loss=0.014484, train/mean_average_precision=0.761684, validation/accuracy=0.987026, validation/loss=0.050530, validation/mean_average_precision=0.291494, validation/num_examples=43793
I0305 20:05:24.763164 140113222047488 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.14079013466835022, loss=0.020621012896299362
I0305 20:05:56.921955 140106748385024 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.15647158026695251, loss=0.01828068122267723
I0305 20:06:29.750562 140113222047488 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.17056994140148163, loss=0.01935652084648609
I0305 20:07:02.221628 140106748385024 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.14912213385105133, loss=0.017665039747953415
I0305 20:07:34.207470 140113222047488 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.15490414202213287, loss=0.019240884110331535
I0305 20:08:06.728703 140106748385024 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.1398617923259735, loss=0.01682264544069767
I0305 20:08:39.202323 140113222047488 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.15841124951839447, loss=0.019226446747779846
I0305 20:08:55.144750 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:10:42.978442 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:10:46.037180 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:10:49.075303 140274064205632 submission_runner.py:411] Time since start: 36522.17s, 	Step: 73150, 	{'train/accuracy': 0.9953998923301697, 'train/loss': 0.014279352501034737, 'train/mean_average_precision': 0.778936084156196, 'validation/accuracy': 0.9870293140411377, 'validation/loss': 0.05052022635936737, 'validation/mean_average_precision': 0.29404639743379535, 'validation/num_examples': 43793, 'test/accuracy': 0.9861780405044556, 'test/loss': 0.05387342721223831, 'test/mean_average_precision': 0.2740218947411233, 'test/num_examples': 43793, 'score': 23549.571672201157, 'total_duration': 36522.171864271164, 'accumulated_submission_time': 23549.571672201157, 'accumulated_eval_time': 12966.860224246979, 'accumulated_logging_time': 3.684103012084961}
I0305 20:10:49.104415 140098771031808 logging_writer.py:48] [73150] accumulated_eval_time=12966.860224, accumulated_logging_time=3.684103, accumulated_submission_time=23549.571672, global_step=73150, preemption_count=0, score=23549.571672, test/accuracy=0.986178, test/loss=0.053873, test/mean_average_precision=0.274022, test/num_examples=43793, total_duration=36522.171864, train/accuracy=0.995400, train/loss=0.014279, train/mean_average_precision=0.778936, validation/accuracy=0.987029, validation/loss=0.050520, validation/mean_average_precision=0.294046, validation/num_examples=43793
I0305 20:11:05.525316 140105330788096 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.12748003005981445, loss=0.018731702119112015
I0305 20:11:37.574655 140098771031808 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.1537555456161499, loss=0.0188662838190794
I0305 20:12:09.769020 140105330788096 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.13958218693733215, loss=0.017056873068213463
I0305 20:12:41.728669 140098771031808 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.15955038368701935, loss=0.020952792838215828
I0305 20:13:13.790209 140105330788096 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.14258046448230743, loss=0.018509726971387863
I0305 20:13:45.569866 140098771031808 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.1349315047264099, loss=0.0171853918582201
I0305 20:14:17.259875 140105330788096 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.14869818091392517, loss=0.018934346735477448
I0305 20:14:49.214729 140098771031808 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.15059389173984528, loss=0.017527196556329727
I0305 20:14:49.219768 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:16:41.579119 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:16:44.705056 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:16:47.749943 140274064205632 submission_runner.py:411] Time since start: 36880.85s, 	Step: 73901, 	{'train/accuracy': 0.9956340789794922, 'train/loss': 0.01380576565861702, 'train/mean_average_precision': 0.7875806631898318, 'validation/accuracy': 0.9869623780250549, 'validation/loss': 0.05046612024307251, 'validation/mean_average_precision': 0.29371532187222427, 'validation/num_examples': 43793, 'test/accuracy': 0.9861649870872498, 'test/loss': 0.053868748247623444, 'test/mean_average_precision': 0.2766866151141615, 'test/num_examples': 43793, 'score': 23789.65530896187, 'total_duration': 36880.84664463997, 'accumulated_submission_time': 23789.65530896187, 'accumulated_eval_time': 13085.390329837799, 'accumulated_logging_time': 3.7242658138275146}
I0305 20:16:47.779742 140113222047488 logging_writer.py:48] [73901] accumulated_eval_time=13085.390330, accumulated_logging_time=3.724266, accumulated_submission_time=23789.655309, global_step=73901, preemption_count=0, score=23789.655309, test/accuracy=0.986165, test/loss=0.053869, test/mean_average_precision=0.276687, test/num_examples=43793, total_duration=36880.846645, train/accuracy=0.995634, train/loss=0.013806, train/mean_average_precision=0.787581, validation/accuracy=0.986962, validation/loss=0.050466, validation/mean_average_precision=0.293715, validation/num_examples=43793
I0305 20:17:20.024242 140113230440192 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.14381229877471924, loss=0.017291821539402008
I0305 20:17:52.050870 140113222047488 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.138249009847641, loss=0.01810491271317005
I0305 20:18:24.459746 140113230440192 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.16188417375087738, loss=0.019523730501532555
I0305 20:18:56.930611 140113222047488 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.1310482621192932, loss=0.017614910379052162
I0305 20:19:29.382787 140113230440192 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.14215564727783203, loss=0.018069207668304443
I0305 20:20:01.501805 140113222047488 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.13218967616558075, loss=0.017250794917345047
I0305 20:20:33.528438 140113230440192 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.1444520503282547, loss=0.016820821911096573
I0305 20:20:47.824206 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:22:41.242661 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:22:44.331487 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:22:47.376068 140274064205632 submission_runner.py:411] Time since start: 37240.47s, 	Step: 74646, 	{'train/accuracy': 0.995647132396698, 'train/loss': 0.013728402554988861, 'train/mean_average_precision': 0.7831579439134501, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.05059363320469856, 'validation/mean_average_precision': 0.2927131763169233, 'validation/num_examples': 43793, 'test/accuracy': 0.986185610294342, 'test/loss': 0.053911563009023666, 'test/mean_average_precision': 0.274894312465339, 'test/num_examples': 43793, 'score': 24029.66795539856, 'total_duration': 37240.47275662422, 'accumulated_submission_time': 24029.66795539856, 'accumulated_eval_time': 13204.942134141922, 'accumulated_logging_time': 3.765378952026367}
I0305 20:22:47.406455 140105330788096 logging_writer.py:48] [74646] accumulated_eval_time=13204.942134, accumulated_logging_time=3.765379, accumulated_submission_time=24029.667955, global_step=74646, preemption_count=0, score=24029.667955, test/accuracy=0.986186, test/loss=0.053912, test/mean_average_precision=0.274894, test/num_examples=43793, total_duration=37240.472757, train/accuracy=0.995647, train/loss=0.013728, train/mean_average_precision=0.783158, validation/accuracy=0.986956, validation/loss=0.050594, validation/mean_average_precision=0.292713, validation/num_examples=43793
I0305 20:23:04.990532 140106748385024 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.14350084960460663, loss=0.017781350761651993
I0305 20:23:37.089751 140105330788096 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.14698591828346252, loss=0.017665015533566475
I0305 20:24:09.339395 140106748385024 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.15157866477966309, loss=0.019760338589549065
I0305 20:24:41.362904 140105330788096 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.14595980942249298, loss=0.018457677215337753
I0305 20:25:13.331741 140106748385024 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.1601731926202774, loss=0.018846776336431503
I0305 20:25:45.321605 140105330788096 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.1463116705417633, loss=0.017379725351929665
I0305 20:26:17.584650 140106748385024 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.13415877521038055, loss=0.01490724552422762
I0305 20:26:47.405414 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:28:41.091926 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:28:44.214529 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:28:47.241290 140274064205632 submission_runner.py:411] Time since start: 37600.34s, 	Step: 75394, 	{'train/accuracy': 0.9955528378486633, 'train/loss': 0.013815943151712418, 'train/mean_average_precision': 0.782723698481396, 'validation/accuracy': 0.9869920015335083, 'validation/loss': 0.050742074847221375, 'validation/mean_average_precision': 0.2923844320138635, 'validation/num_examples': 43793, 'test/accuracy': 0.9862277507781982, 'test/loss': 0.05407988652586937, 'test/mean_average_precision': 0.27598886107908965, 'test/num_examples': 43793, 'score': 24269.63495206833, 'total_duration': 37600.337985515594, 'accumulated_submission_time': 24269.63495206833, 'accumulated_eval_time': 13324.777961015701, 'accumulated_logging_time': 3.806945562362671}
I0305 20:28:47.271798 140098771031808 logging_writer.py:48] [75394] accumulated_eval_time=13324.777961, accumulated_logging_time=3.806946, accumulated_submission_time=24269.634952, global_step=75394, preemption_count=0, score=24269.634952, test/accuracy=0.986228, test/loss=0.054080, test/mean_average_precision=0.275989, test/num_examples=43793, total_duration=37600.337986, train/accuracy=0.995553, train/loss=0.013816, train/mean_average_precision=0.782724, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.292384, validation/num_examples=43793
I0305 20:28:49.636506 140113222047488 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.14438238739967346, loss=0.018019001930952072
I0305 20:29:21.800881 140098771031808 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.15121795237064362, loss=0.017230188474059105
I0305 20:29:53.645200 140113222047488 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.14225588738918304, loss=0.01630573533475399
I0305 20:30:25.541952 140098771031808 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.15030182898044586, loss=0.020034223794937134
I0305 20:30:57.599731 140113222047488 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.14075423777103424, loss=0.01629614271223545
I0305 20:31:29.390858 140098771031808 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.14993315935134888, loss=0.01831384189426899
I0305 20:32:01.072577 140113222047488 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.21327993273735046, loss=0.01703065074980259
I0305 20:32:32.868349 140098771031808 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.13152487576007843, loss=0.01603773422539234
I0305 20:32:47.462858 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:34:42.615797 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:34:45.656142 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:34:48.696938 140274064205632 submission_runner.py:411] Time since start: 37961.79s, 	Step: 76147, 	{'train/accuracy': 0.995425283908844, 'train/loss': 0.01416662335395813, 'train/mean_average_precision': 0.7730561717837915, 'validation/accuracy': 0.9869599342346191, 'validation/loss': 0.05076320096850395, 'validation/mean_average_precision': 0.2921613507528562, 'validation/num_examples': 43793, 'test/accuracy': 0.9862134456634521, 'test/loss': 0.05407453700900078, 'test/mean_average_precision': 0.2772311098034813, 'test/num_examples': 43793, 'score': 24509.794549703598, 'total_duration': 37961.79363822937, 'accumulated_submission_time': 24509.794549703598, 'accumulated_eval_time': 13446.011998414993, 'accumulated_logging_time': 3.848209857940674}
I0305 20:34:48.727752 140106748385024 logging_writer.py:48] [76147] accumulated_eval_time=13446.011998, accumulated_logging_time=3.848210, accumulated_submission_time=24509.794550, global_step=76147, preemption_count=0, score=24509.794550, test/accuracy=0.986213, test/loss=0.054075, test/mean_average_precision=0.277231, test/num_examples=43793, total_duration=37961.793638, train/accuracy=0.995425, train/loss=0.014167, train/mean_average_precision=0.773056, validation/accuracy=0.986960, validation/loss=0.050763, validation/mean_average_precision=0.292161, validation/num_examples=43793
I0305 20:35:05.861232 140113230440192 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.14215858280658722, loss=0.017909104004502296
I0305 20:35:37.624738 140106748385024 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.1453184187412262, loss=0.018190326169133186
I0305 20:36:09.632690 140113230440192 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.14488635957241058, loss=0.01758086495101452
I0305 20:36:42.020569 140106748385024 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.14394350349903107, loss=0.019870351999998093
I0305 20:37:14.487157 140113230440192 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.13490915298461914, loss=0.016960885375738144
I0305 20:37:46.487568 140106748385024 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.14812812209129333, loss=0.018527913838624954
I0305 20:38:18.284973 140113230440192 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.1447809785604477, loss=0.01668466627597809
I0305 20:38:48.764048 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:40:39.289286 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:40:42.356283 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:40:45.391857 140274064205632 submission_runner.py:411] Time since start: 38318.49s, 	Step: 76897, 	{'train/accuracy': 0.9954675436019897, 'train/loss': 0.014139185659587383, 'train/mean_average_precision': 0.7729857674740117, 'validation/accuracy': 0.9869920015335083, 'validation/loss': 0.05079095438122749, 'validation/mean_average_precision': 0.2932262344296919, 'validation/num_examples': 43793, 'test/accuracy': 0.986204981803894, 'test/loss': 0.05406547710299492, 'test/mean_average_precision': 0.2752977861531578, 'test/num_examples': 43793, 'score': 24749.799326896667, 'total_duration': 38318.488555669785, 'accumulated_submission_time': 24749.799326896667, 'accumulated_eval_time': 13562.63976407051, 'accumulated_logging_time': 3.8900856971740723}
I0305 20:40:45.422547 140098771031808 logging_writer.py:48] [76897] accumulated_eval_time=13562.639764, accumulated_logging_time=3.890086, accumulated_submission_time=24749.799327, global_step=76897, preemption_count=0, score=24749.799327, test/accuracy=0.986205, test/loss=0.054065, test/mean_average_precision=0.275298, test/num_examples=43793, total_duration=38318.488556, train/accuracy=0.995468, train/loss=0.014139, train/mean_average_precision=0.772986, validation/accuracy=0.986992, validation/loss=0.050791, validation/mean_average_precision=0.293226, validation/num_examples=43793
I0305 20:40:46.751172 140113222047488 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.1573733389377594, loss=0.01753680408000946
I0305 20:41:18.801641 140098771031808 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.12565845251083374, loss=0.015454287640750408
I0305 20:41:50.726752 140113222047488 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.12712855637073517, loss=0.016644125804305077
I0305 20:42:23.074851 140098771031808 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.1353767067193985, loss=0.01477538887411356
I0305 20:42:55.075785 140113222047488 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.1650596559047699, loss=0.019801516085863113
I0305 20:43:26.972306 140098771031808 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.1410834789276123, loss=0.015856675803661346
I0305 20:43:58.771053 140113222047488 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.147927924990654, loss=0.01742567867040634
I0305 20:44:31.344618 140098771031808 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.13717277348041534, loss=0.01712605357170105
I0305 20:44:45.493900 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:46:32.091525 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:46:35.199300 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:46:39.652369 140274064205632 submission_runner.py:411] Time since start: 38672.75s, 	Step: 77645, 	{'train/accuracy': 0.9955635666847229, 'train/loss': 0.01384841836988926, 'train/mean_average_precision': 0.789354634106922, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.05069427564740181, 'validation/mean_average_precision': 0.29516263041223934, 'validation/num_examples': 43793, 'test/accuracy': 0.9862138628959656, 'test/loss': 0.053966738283634186, 'test/mean_average_precision': 0.27515140909165386, 'test/num_examples': 43793, 'score': 24989.839111804962, 'total_duration': 38672.74894404411, 'accumulated_submission_time': 24989.839111804962, 'accumulated_eval_time': 13676.798060655594, 'accumulated_logging_time': 3.931835889816284}
I0305 20:46:39.683362 140105330788096 logging_writer.py:48] [77645] accumulated_eval_time=13676.798061, accumulated_logging_time=3.931836, accumulated_submission_time=24989.839112, global_step=77645, preemption_count=0, score=24989.839112, test/accuracy=0.986214, test/loss=0.053967, test/mean_average_precision=0.275151, test/num_examples=43793, total_duration=38672.748944, train/accuracy=0.995564, train/loss=0.013848, train/mean_average_precision=0.789355, validation/accuracy=0.986986, validation/loss=0.050694, validation/mean_average_precision=0.295163, validation/num_examples=43793
I0305 20:46:57.909346 140113230440192 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.1456722766160965, loss=0.018440570682287216
I0305 20:47:30.193895 140105330788096 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.14984340965747833, loss=0.01871017925441265
I0305 20:48:02.114049 140113230440192 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.14789855480194092, loss=0.01971687190234661
I0305 20:48:34.176389 140105330788096 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.16990546882152557, loss=0.01900334097445011
I0305 20:49:06.223212 140113230440192 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.14414317905902863, loss=0.016084708273410797
I0305 20:49:38.898503 140105330788096 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.14455720782279968, loss=0.019030481576919556
I0305 20:50:11.231980 140113230440192 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.15030469000339508, loss=0.018978767096996307
I0305 20:50:39.667124 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:52:28.461665 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:52:31.595752 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:52:34.668171 140274064205632 submission_runner.py:411] Time since start: 39027.76s, 	Step: 78390, 	{'train/accuracy': 0.9955982565879822, 'train/loss': 0.01383910421282053, 'train/mean_average_precision': 0.7907666497457357, 'validation/accuracy': 0.9869810342788696, 'validation/loss': 0.050711046904325485, 'validation/mean_average_precision': 0.2936789197248196, 'validation/num_examples': 43793, 'test/accuracy': 0.9862096309661865, 'test/loss': 0.05399709939956665, 'test/mean_average_precision': 0.2748707703275417, 'test/num_examples': 43793, 'score': 25229.791610002518, 'total_duration': 39027.764872312546, 'accumulated_submission_time': 25229.791610002518, 'accumulated_eval_time': 13791.799069404602, 'accumulated_logging_time': 3.9740042686462402}
I0305 20:52:34.699395 140106748385024 logging_writer.py:48] [78390] accumulated_eval_time=13791.799069, accumulated_logging_time=3.974004, accumulated_submission_time=25229.791610, global_step=78390, preemption_count=0, score=25229.791610, test/accuracy=0.986210, test/loss=0.053997, test/mean_average_precision=0.274871, test/num_examples=43793, total_duration=39027.764872, train/accuracy=0.995598, train/loss=0.013839, train/mean_average_precision=0.790767, validation/accuracy=0.986981, validation/loss=0.050711, validation/mean_average_precision=0.293679, validation/num_examples=43793
I0305 20:52:38.323894 140113222047488 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.15395218133926392, loss=0.01868409849703312
I0305 20:53:10.816138 140106748385024 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.1529962718486786, loss=0.018056588247418404
I0305 20:53:43.045905 140113222047488 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.1396413892507553, loss=0.016460103914141655
I0305 20:54:14.862991 140106748385024 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.1372908651828766, loss=0.017660731449723244
I0305 20:54:46.729439 140113222047488 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.14469055831432343, loss=0.01839805766940117
I0305 20:55:19.204696 140106748385024 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.1450098156929016, loss=0.01765337772667408
I0305 20:55:51.463484 140113222047488 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.14051999151706696, loss=0.01612374745309353
I0305 20:56:24.014944 140106748385024 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.14969605207443237, loss=0.015507028438150883
I0305 20:56:34.961110 140274064205632 spec.py:321] Evaluating on the training split.
I0305 20:58:33.839722 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 20:58:37.287882 140274064205632 spec.py:349] Evaluating on the test split.
I0305 20:58:40.704322 140274064205632 submission_runner.py:411] Time since start: 39393.80s, 	Step: 79135, 	{'train/accuracy': 0.9955662488937378, 'train/loss': 0.013880492188036442, 'train/mean_average_precision': 0.7777995135357072, 'validation/accuracy': 0.9869794249534607, 'validation/loss': 0.05071641132235527, 'validation/mean_average_precision': 0.29378737854657166, 'validation/num_examples': 43793, 'test/accuracy': 0.9862079620361328, 'test/loss': 0.05400531366467476, 'test/mean_average_precision': 0.2754306980280074, 'test/num_examples': 43793, 'score': 25470.019649267197, 'total_duration': 39393.80100250244, 'accumulated_submission_time': 25470.019649267197, 'accumulated_eval_time': 13917.542226552963, 'accumulated_logging_time': 4.018395185470581}
I0305 20:58:40.738551 140105330788096 logging_writer.py:48] [79135] accumulated_eval_time=13917.542227, accumulated_logging_time=4.018395, accumulated_submission_time=25470.019649, global_step=79135, preemption_count=0, score=25470.019649, test/accuracy=0.986208, test/loss=0.054005, test/mean_average_precision=0.275431, test/num_examples=43793, total_duration=39393.801003, train/accuracy=0.995566, train/loss=0.013880, train/mean_average_precision=0.777800, validation/accuracy=0.986979, validation/loss=0.050716, validation/mean_average_precision=0.293787, validation/num_examples=43793
I0305 20:59:03.811847 140113230440192 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.14460815489292145, loss=0.019207755103707314
I0305 20:59:36.568337 140105330788096 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.13312193751335144, loss=0.017859293147921562
I0305 21:00:09.553945 140113230440192 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.153748020529747, loss=0.0190420001745224
I0305 21:00:42.073545 140105330788096 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.14198508858680725, loss=0.016955094411969185
I0305 21:01:14.727490 140113230440192 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.131205216050148, loss=0.017276620492339134
I0305 21:01:47.116985 140105330788096 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.1283615082502365, loss=0.01579594425857067
I0305 21:02:19.532775 140113230440192 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.14625786244869232, loss=0.018692748621106148
I0305 21:02:40.930669 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:04:34.956856 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:04:38.025677 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:04:41.007937 140274064205632 submission_runner.py:411] Time since start: 39754.10s, 	Step: 79868, 	{'train/accuracy': 0.9955735206604004, 'train/loss': 0.013783560134470463, 'train/mean_average_precision': 0.7806222712920365, 'validation/accuracy': 0.9869920015335083, 'validation/loss': 0.05074244737625122, 'validation/mean_average_precision': 0.29363132704301526, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.054036695510149, 'test/mean_average_precision': 0.2753682278616062, 'test/num_examples': 43793, 'score': 25710.17612195015, 'total_duration': 39754.10463857651, 'accumulated_submission_time': 25710.17612195015, 'accumulated_eval_time': 14037.619457006454, 'accumulated_logging_time': 4.064361095428467}
I0305 21:04:41.038362 140098771031808 logging_writer.py:48] [79868] accumulated_eval_time=14037.619457, accumulated_logging_time=4.064361, accumulated_submission_time=25710.176122, global_step=79868, preemption_count=0, score=25710.176122, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275368, test/num_examples=43793, total_duration=39754.104639, train/accuracy=0.995574, train/loss=0.013784, train/mean_average_precision=0.780622, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293631, validation/num_examples=43793
I0305 21:04:51.573194 140113222047488 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.13697588443756104, loss=0.017269816249608994
I0305 21:05:23.820544 140098771031808 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.14614452421665192, loss=0.018035192042589188
I0305 21:05:55.622587 140113222047488 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.12382300198078156, loss=0.01618087664246559
I0305 21:06:27.887596 140098771031808 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.13712388277053833, loss=0.018028538674116135
I0305 21:07:00.405914 140113222047488 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.14269399642944336, loss=0.01690344698727131
I0305 21:07:33.363625 140098771031808 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.14943888783454895, loss=0.017918499186635017
I0305 21:08:05.721023 140113222047488 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.14577807486057281, loss=0.01709677465260029
I0305 21:08:37.893243 140098771031808 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.15821406245231628, loss=0.017605936154723167
I0305 21:08:41.123736 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:10:33.587767 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:10:36.726827 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:10:39.783086 140274064205632 submission_runner.py:411] Time since start: 40112.88s, 	Step: 80611, 	{'train/accuracy': 0.9955922365188599, 'train/loss': 0.013786165975034237, 'train/mean_average_precision': 0.7772153678768996, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936277846170593, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754226497068658, 'test/num_examples': 43793, 'score': 25950.23030114174, 'total_duration': 40112.879777908325, 'accumulated_submission_time': 25950.23030114174, 'accumulated_eval_time': 14156.27874803543, 'accumulated_logging_time': 4.105771064758301}
I0305 21:10:39.814242 140105330788096 logging_writer.py:48] [80611] accumulated_eval_time=14156.278748, accumulated_logging_time=4.105771, accumulated_submission_time=25950.230301, global_step=80611, preemption_count=0, score=25950.230301, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275423, test/num_examples=43793, total_duration=40112.879778, train/accuracy=0.995592, train/loss=0.013786, train/mean_average_precision=0.777215, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293628, validation/num_examples=43793
I0305 21:11:09.380159 140113230440192 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.138449028134346, loss=0.01777557097375393
I0305 21:11:41.732207 140105330788096 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.14736825227737427, loss=0.017260951921343803
I0305 21:12:14.267942 140113230440192 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.1402277797460556, loss=0.01585584506392479
I0305 21:12:46.719437 140105330788096 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.14483384788036346, loss=0.01655389368534088
I0305 21:13:19.360259 140113230440192 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.13907526433467865, loss=0.016238337382674217
I0305 21:13:51.671224 140105330788096 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.14899642765522003, loss=0.020658260211348534
I0305 21:14:24.055726 140113230440192 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.17055022716522217, loss=0.020270930603146553
I0305 21:14:39.883188 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:16:34.859005 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:16:37.977805 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:16:40.992972 140274064205632 submission_runner.py:411] Time since start: 40474.09s, 	Step: 81350, 	{'train/accuracy': 0.9955688118934631, 'train/loss': 0.01386514212936163, 'train/mean_average_precision': 0.7831689070136559, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936635114911271, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275526511079584, 'test/num_examples': 43793, 'score': 26190.267526865005, 'total_duration': 40474.08954358101, 'accumulated_submission_time': 26190.267526865005, 'accumulated_eval_time': 14277.388365507126, 'accumulated_logging_time': 4.147861957550049}
I0305 21:16:41.025307 140106748385024 logging_writer.py:48] [81350] accumulated_eval_time=14277.388366, accumulated_logging_time=4.147862, accumulated_submission_time=26190.267527, global_step=81350, preemption_count=0, score=26190.267527, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275527, test/num_examples=43793, total_duration=40474.089544, train/accuracy=0.995569, train/loss=0.013865, train/mean_average_precision=0.783169, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293664, validation/num_examples=43793
I0305 21:16:57.445641 140113222047488 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.1337600201368332, loss=0.016882259398698807
I0305 21:17:29.188872 140106748385024 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.15536165237426758, loss=0.017473706975579262
I0305 21:18:01.094689 140113222047488 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.15410083532333374, loss=0.017687546089291573
I0305 21:18:33.532143 140106748385024 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.1390722095966339, loss=0.018535150215029716
I0305 21:19:05.735840 140113222047488 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.14130064845085144, loss=0.017089856788516045
I0305 21:19:37.442185 140106748385024 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.1550799161195755, loss=0.019586369395256042
I0305 21:20:09.093186 140113222047488 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.15513622760772705, loss=0.019655071198940277
I0305 21:20:40.989185 140106748385024 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.14859122037887573, loss=0.017055215314030647
I0305 21:20:40.994482 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:22:30.311237 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:22:33.318798 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:22:36.318627 140274064205632 submission_runner.py:411] Time since start: 40829.42s, 	Step: 82101, 	{'train/accuracy': 0.995573878288269, 'train/loss': 0.013868995010852814, 'train/mean_average_precision': 0.786614385369315, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936718267714238, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754228664505747, 'test/num_examples': 43793, 'score': 26430.20501279831, 'total_duration': 40829.415325164795, 'accumulated_submission_time': 26430.20501279831, 'accumulated_eval_time': 14392.712438821793, 'accumulated_logging_time': 4.19120192527771}
I0305 21:22:36.349812 140098771031808 logging_writer.py:48] [82101] accumulated_eval_time=14392.712439, accumulated_logging_time=4.191202, accumulated_submission_time=26430.205013, global_step=82101, preemption_count=0, score=26430.205013, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275423, test/num_examples=43793, total_duration=40829.415325, train/accuracy=0.995574, train/loss=0.013869, train/mean_average_precision=0.786614, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293672, validation/num_examples=43793
I0305 21:23:08.318395 140113230440192 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.1420706808567047, loss=0.017816288396716118
I0305 21:23:40.454314 140098771031808 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.1483777016401291, loss=0.017481336370110512
I0305 21:24:12.830714 140113230440192 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.14785051345825195, loss=0.0189342238008976
I0305 21:24:44.771566 140098771031808 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.1395808309316635, loss=0.017334675416350365
I0305 21:25:16.936746 140113230440192 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.13883353769779205, loss=0.017657611519098282
I0305 21:25:48.853060 140098771031808 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.1352309137582779, loss=0.016439294442534447
I0305 21:26:21.395040 140113230440192 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.13655053079128265, loss=0.015203258953988552
I0305 21:26:36.354830 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:28:31.156311 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:28:34.236086 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:28:37.214871 140274064205632 submission_runner.py:411] Time since start: 41190.31s, 	Step: 82848, 	{'train/accuracy': 0.9956105351448059, 'train/loss': 0.01377903576940298, 'train/mean_average_precision': 0.7796165680656176, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937034401072733, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27551699297420684, 'test/num_examples': 43793, 'score': 26670.177414417267, 'total_duration': 41190.311574697495, 'accumulated_submission_time': 26670.177414417267, 'accumulated_eval_time': 14513.572434902191, 'accumulated_logging_time': 4.234838008880615}
I0305 21:28:37.246118 140105330788096 logging_writer.py:48] [82848] accumulated_eval_time=14513.572435, accumulated_logging_time=4.234838, accumulated_submission_time=26670.177414, global_step=82848, preemption_count=0, score=26670.177414, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275517, test/num_examples=43793, total_duration=41190.311575, train/accuracy=0.995611, train/loss=0.013779, train/mean_average_precision=0.779617, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293703, validation/num_examples=43793
I0305 21:28:54.462393 140106748385024 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.1518280953168869, loss=0.017983343452215195
I0305 21:29:26.780613 140105330788096 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.13906221091747284, loss=0.018107814714312553
I0305 21:29:58.985829 140106748385024 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.14618931710720062, loss=0.016452161595225334
I0305 21:30:30.660806 140105330788096 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.13290776312351227, loss=0.016020193696022034
I0305 21:31:02.549041 140106748385024 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.14879393577575684, loss=0.016402114182710648
I0305 21:31:34.210769 140105330788096 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.13640421628952026, loss=0.017436791211366653
I0305 21:32:05.714074 140106748385024 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.15064877271652222, loss=0.015936655923724174
I0305 21:32:37.329969 140105330788096 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.13670705258846283, loss=0.016038931906223297
I0305 21:32:37.335222 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:34:28.388302 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:34:31.421102 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:34:34.433241 140274064205632 submission_runner.py:411] Time since start: 41547.53s, 	Step: 83601, 	{'train/accuracy': 0.9955638647079468, 'train/loss': 0.013818764127790928, 'train/mean_average_precision': 0.7790167630847286, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936536998079178, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2753272473537633, 'test/num_examples': 43793, 'score': 26910.234219789505, 'total_duration': 41547.52993321419, 'accumulated_submission_time': 26910.234219789505, 'accumulated_eval_time': 14630.670375823975, 'accumulated_logging_time': 4.278192520141602}
I0305 21:34:34.464572 140098771031808 logging_writer.py:48] [83601] accumulated_eval_time=14630.670376, accumulated_logging_time=4.278193, accumulated_submission_time=26910.234220, global_step=83601, preemption_count=0, score=26910.234220, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275327, test/num_examples=43793, total_duration=41547.529933, train/accuracy=0.995564, train/loss=0.013819, train/mean_average_precision=0.779017, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293654, validation/num_examples=43793
I0305 21:35:06.304075 140113222047488 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.14053767919540405, loss=0.018442204222083092
I0305 21:35:38.135163 140098771031808 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.15586404502391815, loss=0.016334719955921173
I0305 21:36:09.974892 140113222047488 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.13710656762123108, loss=0.018103675916790962
I0305 21:36:42.350315 140098771031808 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.1443871259689331, loss=0.017042331397533417
I0305 21:37:14.745218 140113222047488 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.15855643153190613, loss=0.017095467075705528
I0305 21:37:46.826415 140098771031808 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.14430639147758484, loss=0.01822303794324398
I0305 21:38:19.278159 140113222047488 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.15716493129730225, loss=0.017822831869125366
I0305 21:38:34.562084 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:40:27.719264 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:40:30.818025 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:40:33.839946 140274064205632 submission_runner.py:411] Time since start: 41906.94s, 	Step: 84348, 	{'train/accuracy': 0.9955844879150391, 'train/loss': 0.013803062960505486, 'train/mean_average_precision': 0.774204131037632, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938256946907282, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754699155733398, 'test/num_examples': 43793, 'score': 27150.300426721573, 'total_duration': 41906.93664479256, 'accumulated_submission_time': 27150.300426721573, 'accumulated_eval_time': 14749.948195457458, 'accumulated_logging_time': 4.320634126663208}
I0305 21:40:33.883981 140106748385024 logging_writer.py:48] [84348] accumulated_eval_time=14749.948195, accumulated_logging_time=4.320634, accumulated_submission_time=27150.300427, global_step=84348, preemption_count=0, score=27150.300427, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275470, test/num_examples=43793, total_duration=41906.936645, train/accuracy=0.995584, train/loss=0.013803, train/mean_average_precision=0.774204, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293826, validation/num_examples=43793
I0305 21:40:51.178595 140113230440192 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.15051797032356262, loss=0.018563898280262947
I0305 21:41:23.593262 140106748385024 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.1503952443599701, loss=0.02063378505408764
I0305 21:41:55.536160 140113230440192 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.13890866935253143, loss=0.017022570595145226
I0305 21:42:27.955731 140106748385024 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.14806847274303436, loss=0.017675936222076416
I0305 21:43:00.244246 140113230440192 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.14201688766479492, loss=0.015689972788095474
I0305 21:43:32.523504 140106748385024 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.16769854724407196, loss=0.016792522743344307
I0305 21:44:04.749114 140113230440192 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.14060121774673462, loss=0.018148064613342285
I0305 21:44:34.064021 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:46:29.785392 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:46:32.830814 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:46:35.893672 140274064205632 submission_runner.py:411] Time since start: 42268.99s, 	Step: 85093, 	{'train/accuracy': 0.9954951405525208, 'train/loss': 0.013952811248600483, 'train/mean_average_precision': 0.7879491584598639, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936760948804247, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27540043059728164, 'test/num_examples': 43793, 'score': 27390.44856762886, 'total_duration': 42268.99037504196, 'accumulated_submission_time': 27390.44856762886, 'accumulated_eval_time': 14871.777806282043, 'accumulated_logging_time': 4.376424074172974}
I0305 21:46:35.924725 140098771031808 logging_writer.py:48] [85093] accumulated_eval_time=14871.777806, accumulated_logging_time=4.376424, accumulated_submission_time=27390.448568, global_step=85093, preemption_count=0, score=27390.448568, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275400, test/num_examples=43793, total_duration=42268.990375, train/accuracy=0.995495, train/loss=0.013953, train/mean_average_precision=0.787949, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293676, validation/num_examples=43793
I0305 21:46:38.532237 140113222047488 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.14657045900821686, loss=0.017831729725003242
I0305 21:47:10.659117 140098771031808 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.13648749887943268, loss=0.01554365549236536
I0305 21:47:42.362672 140113222047488 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.14599372446537018, loss=0.017982831224799156
I0305 21:48:14.399049 140098771031808 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.1422480344772339, loss=0.017597926780581474
I0305 21:48:46.365262 140113222047488 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.12013928592205048, loss=0.015548599883913994
I0305 21:49:18.279750 140098771031808 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.13448534905910492, loss=0.017361147329211235
I0305 21:49:50.072417 140113222047488 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.14375074207782745, loss=0.01760878413915634
I0305 21:50:22.333080 140098771031808 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.13442344963550568, loss=0.01745118945837021
I0305 21:50:35.996424 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:52:29.927016 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:52:33.326720 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:52:36.648905 140274064205632 submission_runner.py:411] Time since start: 42629.75s, 	Step: 85844, 	{'train/accuracy': 0.9956097602844238, 'train/loss': 0.013825464062392712, 'train/mean_average_precision': 0.7908804322850795, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357942725369524, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27536376058024736, 'test/num_examples': 43793, 'score': 27630.487211942673, 'total_duration': 42629.74559020996, 'accumulated_submission_time': 27630.487211942673, 'accumulated_eval_time': 14992.430229187012, 'accumulated_logging_time': 4.419990539550781}
I0305 21:52:36.684665 140106748385024 logging_writer.py:48] [85844] accumulated_eval_time=14992.430229, accumulated_logging_time=4.419991, accumulated_submission_time=27630.487212, global_step=85844, preemption_count=0, score=27630.487212, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275364, test/num_examples=43793, total_duration=42629.745590, train/accuracy=0.995610, train/loss=0.013825, train/mean_average_precision=0.790880, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293579, validation/num_examples=43793
I0305 21:52:55.584914 140113230440192 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.14847947657108307, loss=0.019384928047657013
I0305 21:53:28.204049 140106748385024 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.13660524785518646, loss=0.016995251178741455
I0305 21:54:00.751091 140113230440192 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.14129379391670227, loss=0.01873522810637951
I0305 21:54:33.444971 140106748385024 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.1464686542749405, loss=0.015446441248059273
I0305 21:55:05.911181 140113230440192 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.13222059607505798, loss=0.017349708825349808
I0305 21:55:37.958167 140106748385024 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.1306433528661728, loss=0.01699664443731308
I0305 21:56:10.266864 140113230440192 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.14772389829158783, loss=0.0165263619273901
I0305 21:56:36.956862 140274064205632 spec.py:321] Evaluating on the training split.
I0305 21:58:34.987207 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 21:58:38.067275 140274064205632 spec.py:349] Evaluating on the test split.
I0305 21:58:41.059204 140274064205632 submission_runner.py:411] Time since start: 42994.16s, 	Step: 86583, 	{'train/accuracy': 0.9956318140029907, 'train/loss': 0.013718640431761742, 'train/mean_average_precision': 0.78391687670689, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29365131454690174, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754111736117262, 'test/num_examples': 43793, 'score': 27870.723264932632, 'total_duration': 42994.15589976311, 'accumulated_submission_time': 27870.723264932632, 'accumulated_eval_time': 15116.532540082932, 'accumulated_logging_time': 4.467123508453369}
I0305 21:58:41.092284 140105330788096 logging_writer.py:48] [86583] accumulated_eval_time=15116.532540, accumulated_logging_time=4.467124, accumulated_submission_time=27870.723265, global_step=86583, preemption_count=0, score=27870.723265, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275411, test/num_examples=43793, total_duration=42994.155900, train/accuracy=0.995632, train/loss=0.013719, train/mean_average_precision=0.783917, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293651, validation/num_examples=43793
I0305 21:58:47.041131 140113222047488 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.1309508979320526, loss=0.017545461654663086
I0305 21:59:19.331566 140105330788096 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.15594564378261566, loss=0.01816234551370144
I0305 21:59:51.717738 140113222047488 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.14241939783096313, loss=0.016270583495497704
I0305 22:00:23.963960 140105330788096 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.138633131980896, loss=0.01812242902815342
I0305 22:00:56.039246 140113222047488 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.15536485612392426, loss=0.017894865944981575
I0305 22:01:28.544453 140105330788096 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.14204087853431702, loss=0.018897829577326775
I0305 22:02:01.291889 140113222047488 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.15795771777629852, loss=0.019146542996168137
I0305 22:02:33.636308 140105330788096 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.15201254189014435, loss=0.018525520339608192
I0305 22:02:41.081294 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:04:30.510307 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:04:33.586507 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:04:36.640332 140274064205632 submission_runner.py:411] Time since start: 43349.74s, 	Step: 87324, 	{'train/accuracy': 0.99556964635849, 'train/loss': 0.01387191191315651, 'train/mean_average_precision': 0.7821974859985805, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357225674018245, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754694072339723, 'test/num_examples': 43793, 'score': 28110.6790099144, 'total_duration': 43349.73703336716, 'accumulated_submission_time': 28110.6790099144, 'accumulated_eval_time': 15232.091541528702, 'accumulated_logging_time': 4.512559175491333}
I0305 22:04:36.671818 140098771031808 logging_writer.py:48] [87324] accumulated_eval_time=15232.091542, accumulated_logging_time=4.512559, accumulated_submission_time=28110.679010, global_step=87324, preemption_count=0, score=28110.679010, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275469, test/num_examples=43793, total_duration=43349.737033, train/accuracy=0.995570, train/loss=0.013872, train/mean_average_precision=0.782197, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293572, validation/num_examples=43793
I0305 22:05:01.174678 140113230440192 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.15522730350494385, loss=0.01834050565958023
I0305 22:05:33.173203 140098771031808 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.13009123504161835, loss=0.017849665135145187
I0305 22:06:05.339156 140113230440192 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.1539410799741745, loss=0.01949278637766838
I0305 22:06:37.476440 140098771031808 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.12719415128231049, loss=0.014895006082952023
I0305 22:07:09.823368 140113230440192 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.1621907353401184, loss=0.0194676723331213
I0305 22:07:41.786607 140098771031808 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.12339732050895691, loss=0.01750548556447029
I0305 22:08:14.031828 140113230440192 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.14341819286346436, loss=0.018855394795536995
I0305 22:08:36.969040 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:10:33.372488 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:10:36.773963 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:10:40.133619 140274064205632 submission_runner.py:411] Time since start: 43713.23s, 	Step: 88072, 	{'train/accuracy': 0.9955666065216064, 'train/loss': 0.013740360736846924, 'train/mean_average_precision': 0.7812936362956535, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29366139038447764, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753737133918487, 'test/num_examples': 43793, 'score': 28350.945209503174, 'total_duration': 43713.2302980423, 'accumulated_submission_time': 28350.945209503174, 'accumulated_eval_time': 15355.256055355072, 'accumulated_logging_time': 4.554625511169434}
I0305 22:10:40.168870 140106748385024 logging_writer.py:48] [88072] accumulated_eval_time=15355.256055, accumulated_logging_time=4.554626, accumulated_submission_time=28350.945210, global_step=88072, preemption_count=0, score=28350.945210, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275374, test/num_examples=43793, total_duration=43713.230298, train/accuracy=0.995567, train/loss=0.013740, train/mean_average_precision=0.781294, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293661, validation/num_examples=43793
I0305 22:10:49.651486 140113222047488 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.15734094381332397, loss=0.018736746162176132
I0305 22:11:22.630964 140106748385024 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.1568203866481781, loss=0.01779194362461567
I0305 22:11:55.546172 140113222047488 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.14678460359573364, loss=0.017691945657134056
I0305 22:12:28.582318 140106748385024 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.13973800837993622, loss=0.018450114876031876
I0305 22:13:01.123156 140113222047488 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.13863308727741241, loss=0.014982062391936779
I0305 22:13:34.013655 140106748385024 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.13469769060611725, loss=0.014565795660018921
I0305 22:14:06.516454 140113222047488 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.12961435317993164, loss=0.01647205464541912
I0305 22:14:38.841868 140106748385024 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.1670927107334137, loss=0.018476083874702454
I0305 22:14:40.143856 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:16:32.854972 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:16:35.956093 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:16:38.980061 140274064205632 submission_runner.py:411] Time since start: 44072.08s, 	Step: 88805, 	{'train/accuracy': 0.9955418109893799, 'train/loss': 0.013992033898830414, 'train/mean_average_precision': 0.7816320112847811, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29358061323380835, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754791782357467, 'test/num_examples': 43793, 'score': 28590.884345531464, 'total_duration': 44072.07675909996, 'accumulated_submission_time': 28590.884345531464, 'accumulated_eval_time': 15474.092229604721, 'accumulated_logging_time': 4.601079940795898}
I0305 22:16:39.012543 140105330788096 logging_writer.py:48] [88805] accumulated_eval_time=15474.092230, accumulated_logging_time=4.601080, accumulated_submission_time=28590.884346, global_step=88805, preemption_count=0, score=28590.884346, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275479, test/num_examples=43793, total_duration=44072.076759, train/accuracy=0.995542, train/loss=0.013992, train/mean_average_precision=0.781632, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293581, validation/num_examples=43793
I0305 22:17:10.690843 140113230440192 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.16108745336532593, loss=0.019654428586363792
I0305 22:17:43.577935 140105330788096 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.15981076657772064, loss=0.020355047658085823
I0305 22:18:15.941559 140113230440192 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.14652077853679657, loss=0.01758648455142975
I0305 22:18:48.411966 140105330788096 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.15808801352977753, loss=0.01926540397107601
I0305 22:19:20.959593 140113230440192 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.13999700546264648, loss=0.017604639753699303
I0305 22:19:53.294958 140105330788096 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.16042974591255188, loss=0.019057711586356163
I0305 22:20:26.051809 140113230440192 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.13962028920650482, loss=0.01667734980583191
I0305 22:20:39.196883 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:22:27.581985 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:22:30.727468 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:22:33.806147 140274064205632 submission_runner.py:411] Time since start: 44426.90s, 	Step: 89541, 	{'train/accuracy': 0.9955811500549316, 'train/loss': 0.013810724020004272, 'train/mean_average_precision': 0.7874249815139477, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937410135718478, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2755014562042579, 'test/num_examples': 43793, 'score': 28831.03605914116, 'total_duration': 44426.90284395218, 'accumulated_submission_time': 28831.03605914116, 'accumulated_eval_time': 15588.70144701004, 'accumulated_logging_time': 4.645176410675049}
I0305 22:22:33.838823 140098771031808 logging_writer.py:48] [89541] accumulated_eval_time=15588.701447, accumulated_logging_time=4.645176, accumulated_submission_time=28831.036059, global_step=89541, preemption_count=0, score=28831.036059, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275501, test/num_examples=43793, total_duration=44426.902844, train/accuracy=0.995581, train/loss=0.013811, train/mean_average_precision=0.787425, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293741, validation/num_examples=43793
I0305 22:22:53.459496 140113222047488 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.1461287885904312, loss=0.018013162538409233
I0305 22:23:25.481088 140098771031808 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.13898061215877533, loss=0.01760113425552845
I0305 22:23:57.631041 140113222047488 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.13488399982452393, loss=0.015860294923186302
I0305 22:24:30.149180 140098771031808 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.1315179020166397, loss=0.016965968534350395
I0305 22:25:04.301018 140113222047488 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.1589469313621521, loss=0.017582057043910027
I0305 22:25:36.425092 140098771031808 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.13832257688045502, loss=0.01728225126862526
I0305 22:26:08.488936 140113222047488 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.1433369517326355, loss=0.018326900899410248
I0305 22:26:33.980499 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:28:26.879978 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:28:29.938196 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:28:33.007927 140274064205632 submission_runner.py:411] Time since start: 44786.10s, 	Step: 90280, 	{'train/accuracy': 0.9956351518630981, 'train/loss': 0.013681108132004738, 'train/mean_average_precision': 0.7850135717763382, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29363475231942365, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.27534948118737407, 'test/num_examples': 43793, 'score': 29071.14478611946, 'total_duration': 44786.104506492615, 'accumulated_submission_time': 29071.14478611946, 'accumulated_eval_time': 15707.728714704514, 'accumulated_logging_time': 4.690348386764526}
I0305 22:28:33.041094 140105330788096 logging_writer.py:48] [90280] accumulated_eval_time=15707.728715, accumulated_logging_time=4.690348, accumulated_submission_time=29071.144786, global_step=90280, preemption_count=0, score=29071.144786, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275349, test/num_examples=43793, total_duration=44786.104506, train/accuracy=0.995635, train/loss=0.013681, train/mean_average_precision=0.785014, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293635, validation/num_examples=43793
I0305 22:28:39.902449 140113230440192 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.14424128830432892, loss=0.01849617436528206
I0305 22:29:12.206753 140105330788096 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.17057111859321594, loss=0.018949853256344795
I0305 22:29:44.342480 140113230440192 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.14611104130744934, loss=0.018523860722780228
I0305 22:30:16.643563 140105330788096 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.1371631920337677, loss=0.01928303949534893
I0305 22:30:48.745508 140113230440192 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.16568858921527863, loss=0.018868256360292435
I0305 22:31:20.968068 140105330788096 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.12806148827075958, loss=0.016364686191082
I0305 22:31:52.934560 140113230440192 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.13476528227329254, loss=0.016624407842755318
I0305 22:32:25.172996 140105330788096 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.14185431599617004, loss=0.01993488520383835
I0305 22:32:33.017564 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:34:25.674837 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:34:28.706299 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:34:31.724422 140274064205632 submission_runner.py:411] Time since start: 45144.82s, 	Step: 91025, 	{'train/accuracy': 0.9955487251281738, 'train/loss': 0.01392898429185152, 'train/mean_average_precision': 0.7848721563012246, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937334688183136, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753792621221552, 'test/num_examples': 43793, 'score': 29311.08949136734, 'total_duration': 45144.8211247921, 'accumulated_submission_time': 29311.08949136734, 'accumulated_eval_time': 15826.435528993607, 'accumulated_logging_time': 4.734649658203125}
I0305 22:34:31.757430 140098771031808 logging_writer.py:48] [91025] accumulated_eval_time=15826.435529, accumulated_logging_time=4.734650, accumulated_submission_time=29311.089491, global_step=91025, preemption_count=0, score=29311.089491, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275379, test/num_examples=43793, total_duration=45144.821125, train/accuracy=0.995549, train/loss=0.013929, train/mean_average_precision=0.784872, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293733, validation/num_examples=43793
I0305 22:34:56.602343 140106748385024 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.13836981356143951, loss=0.017174120992422104
I0305 22:35:28.671173 140098771031808 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.14028233289718628, loss=0.01752268522977829
I0305 22:36:00.584186 140106748385024 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.16385577619075775, loss=0.020213836804032326
I0305 22:36:32.817875 140098771031808 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.1375204473733902, loss=0.017953822389245033
I0305 22:37:04.728415 140106748385024 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.1382143199443817, loss=0.017986752092838287
I0305 22:37:36.778798 140098771031808 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.15155836939811707, loss=0.01668630540370941
I0305 22:38:09.471583 140106748385024 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.15101811289787292, loss=0.018673481419682503
I0305 22:38:31.783529 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:40:26.056238 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:40:29.121243 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:40:32.078939 140274064205632 submission_runner.py:411] Time since start: 45505.18s, 	Step: 91770, 	{'train/accuracy': 0.9956004619598389, 'train/loss': 0.013716403394937515, 'train/mean_average_precision': 0.7883310032943757, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29373762591392355, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27542035198071385, 'test/num_examples': 43793, 'score': 29551.084138154984, 'total_duration': 45505.17563891411, 'accumulated_submission_time': 29551.084138154984, 'accumulated_eval_time': 15946.73088979721, 'accumulated_logging_time': 4.778702735900879}
I0305 22:40:32.111789 140113222047488 logging_writer.py:48] [91770] accumulated_eval_time=15946.730890, accumulated_logging_time=4.778703, accumulated_submission_time=29551.084138, global_step=91770, preemption_count=0, score=29551.084138, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275420, test/num_examples=43793, total_duration=45505.175639, train/accuracy=0.995600, train/loss=0.013716, train/mean_average_precision=0.788331, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293738, validation/num_examples=43793
I0305 22:40:42.363766 140113230440192 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.15159066021442413, loss=0.018003692850470543
I0305 22:41:14.534984 140113222047488 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.15182408690452576, loss=0.016196860000491142
I0305 22:41:46.560260 140113230440192 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.14874812960624695, loss=0.01932576484978199
I0305 22:42:18.409276 140113222047488 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.14595699310302734, loss=0.017014065757393837
I0305 22:42:50.667795 140113230440192 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.13229826092720032, loss=0.01596244052052498
I0305 22:43:23.177684 140113222047488 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.12506146728992462, loss=0.013819691725075245
I0305 22:43:55.534403 140113230440192 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.14518684148788452, loss=0.018453648313879967
I0305 22:44:27.641202 140113222047488 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.1447264701128006, loss=0.01749221421778202
I0305 22:44:32.080774 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:46:23.427921 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:46:26.471694 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:46:29.518964 140274064205632 submission_runner.py:411] Time since start: 45862.62s, 	Step: 92515, 	{'train/accuracy': 0.9955558180809021, 'train/loss': 0.013904852792620659, 'train/mean_average_precision': 0.7673218996392518, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936585100039495, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754073981223309, 'test/num_examples': 43793, 'score': 29791.020255804062, 'total_duration': 45862.61566567421, 'accumulated_submission_time': 29791.020255804062, 'accumulated_eval_time': 16064.169033050537, 'accumulated_logging_time': 4.824016094207764}
I0305 22:46:29.552016 140098771031808 logging_writer.py:48] [92515] accumulated_eval_time=16064.169033, accumulated_logging_time=4.824016, accumulated_submission_time=29791.020256, global_step=92515, preemption_count=0, score=29791.020256, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275407, test/num_examples=43793, total_duration=45862.615666, train/accuracy=0.995556, train/loss=0.013905, train/mean_average_precision=0.767322, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293659, validation/num_examples=43793
I0305 22:46:57.549376 140106748385024 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.149902805685997, loss=0.020091228187084198
I0305 22:47:29.998571 140098771031808 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.1408974826335907, loss=0.017100363969802856
I0305 22:48:02.186634 140106748385024 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.14839889109134674, loss=0.01619674079120159
I0305 22:48:34.574028 140098771031808 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.15516570210456848, loss=0.018175195902585983
I0305 22:49:07.086769 140106748385024 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.202444389462471, loss=0.019357098266482353
I0305 22:49:39.308907 140098771031808 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.1258154809474945, loss=0.01610749587416649
I0305 22:50:11.905312 140106748385024 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.13983061909675598, loss=0.0173279270529747
I0305 22:50:29.769376 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:52:20.656320 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:52:23.724768 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:52:26.805117 140274064205632 submission_runner.py:411] Time since start: 46219.90s, 	Step: 93256, 	{'train/accuracy': 0.9955539107322693, 'train/loss': 0.013856557197868824, 'train/mean_average_precision': 0.7895852409581146, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937831941628872, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754559623634967, 'test/num_examples': 43793, 'score': 30031.206159114838, 'total_duration': 46219.90180850029, 'accumulated_submission_time': 30031.206159114838, 'accumulated_eval_time': 16181.204716920853, 'accumulated_logging_time': 4.867949724197388}
I0305 22:52:26.837960 140105330788096 logging_writer.py:48] [93256] accumulated_eval_time=16181.204717, accumulated_logging_time=4.867950, accumulated_submission_time=30031.206159, global_step=93256, preemption_count=0, score=30031.206159, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275456, test/num_examples=43793, total_duration=46219.901809, train/accuracy=0.995554, train/loss=0.013857, train/mean_average_precision=0.789585, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293783, validation/num_examples=43793
I0305 22:52:41.820971 140113222047488 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.14494982361793518, loss=0.018352163955569267
I0305 22:53:15.053453 140105330788096 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.16484497487545013, loss=0.018815122544765472
I0305 22:53:47.107394 140113222047488 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.16100867092609406, loss=0.018955621868371964
I0305 22:54:18.984114 140105330788096 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.1506199687719345, loss=0.017664168030023575
I0305 22:54:51.261641 140113222047488 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.12718169391155243, loss=0.017192330211400986
I0305 22:55:23.393625 140105330788096 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.17063648998737335, loss=0.019110199064016342
I0305 22:55:55.361289 140113222047488 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.15050053596496582, loss=0.01843993365764618
I0305 22:56:27.042936 140274064205632 spec.py:321] Evaluating on the training split.
I0305 22:58:17.896811 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 22:58:20.950471 140274064205632 spec.py:349] Evaluating on the test split.
I0305 22:58:23.940434 140274064205632 submission_runner.py:411] Time since start: 46577.04s, 	Step: 93999, 	{'train/accuracy': 0.9955896139144897, 'train/loss': 0.013892409391701221, 'train/mean_average_precision': 0.7888154545262414, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936715268549584, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.27540901031007226, 'test/num_examples': 43793, 'score': 30271.378150701523, 'total_duration': 46577.03712940216, 'accumulated_submission_time': 30271.378150701523, 'accumulated_eval_time': 16298.102165699005, 'accumulated_logging_time': 4.91324520111084}
I0305 22:58:23.974441 140106748385024 logging_writer.py:48] [93999] accumulated_eval_time=16298.102166, accumulated_logging_time=4.913245, accumulated_submission_time=30271.378151, global_step=93999, preemption_count=0, score=30271.378151, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275409, test/num_examples=43793, total_duration=46577.037129, train/accuracy=0.995590, train/loss=0.013892, train/mean_average_precision=0.788815, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293672, validation/num_examples=43793
I0305 22:58:24.663458 140113230440192 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.1463112235069275, loss=0.017490949481725693
I0305 22:58:57.123085 140106748385024 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.1422085016965866, loss=0.017575589939951897
I0305 22:59:29.667978 140113230440192 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.12406246364116669, loss=0.014966079033911228
I0305 23:00:01.793251 140106748385024 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.15009574592113495, loss=0.01816975697875023
I0305 23:00:33.945645 140113230440192 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.12461864948272705, loss=0.014659041538834572
I0305 23:01:05.974623 140106748385024 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.1401379108428955, loss=0.01760115660727024
I0305 23:01:38.379285 140113230440192 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.14207932353019714, loss=0.017823312431573868
I0305 23:02:10.599848 140106748385024 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.15120647847652435, loss=0.019958309829235077
I0305 23:02:24.178874 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:04:10.450283 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:04:13.497639 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:04:16.496962 140274064205632 submission_runner.py:411] Time since start: 46929.59s, 	Step: 94744, 	{'train/accuracy': 0.9956102967262268, 'train/loss': 0.013744886964559555, 'train/mean_average_precision': 0.7833450442984059, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29377498876704156, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27534080366770836, 'test/num_examples': 43793, 'score': 30511.550040006638, 'total_duration': 46929.59366226196, 'accumulated_submission_time': 30511.550040006638, 'accumulated_eval_time': 16410.420204401016, 'accumulated_logging_time': 4.958989381790161}
I0305 23:04:16.531023 140098771031808 logging_writer.py:48] [94744] accumulated_eval_time=16410.420204, accumulated_logging_time=4.958989, accumulated_submission_time=30511.550040, global_step=94744, preemption_count=0, score=30511.550040, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275341, test/num_examples=43793, total_duration=46929.593662, train/accuracy=0.995610, train/loss=0.013745, train/mean_average_precision=0.783345, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293775, validation/num_examples=43793
I0305 23:04:35.072743 140113222047488 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.12581053376197815, loss=0.014909512363374233
I0305 23:05:07.192997 140098771031808 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.1311311572790146, loss=0.01726045459508896
I0305 23:05:39.227918 140113222047488 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.1404573917388916, loss=0.018263408914208412
I0305 23:06:11.838786 140098771031808 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.1427517980337143, loss=0.016251331195235252
I0305 23:06:43.794662 140113222047488 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.13479967415332794, loss=0.018104039132595062
I0305 23:07:15.728156 140098771031808 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.1537792980670929, loss=0.018118923529982567
I0305 23:07:47.698090 140113222047488 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.1509256213903427, loss=0.018719684332609177
I0305 23:08:16.726083 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:10:08.403068 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:10:11.458980 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:10:14.425907 140274064205632 submission_runner.py:411] Time since start: 47287.52s, 	Step: 95490, 	{'train/accuracy': 0.99561607837677, 'train/loss': 0.013665905222296715, 'train/mean_average_precision': 0.7821546171064298, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29374253391838445, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754687286827509, 'test/num_examples': 43793, 'score': 30751.713462352753, 'total_duration': 47287.522605895996, 'accumulated_submission_time': 30751.713462352753, 'accumulated_eval_time': 16528.11999154091, 'accumulated_logging_time': 5.0040812492370605}
I0305 23:10:14.459411 140105330788096 logging_writer.py:48] [95490] accumulated_eval_time=16528.119992, accumulated_logging_time=5.004081, accumulated_submission_time=30751.713462, global_step=95490, preemption_count=0, score=30751.713462, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275469, test/num_examples=43793, total_duration=47287.522606, train/accuracy=0.995616, train/loss=0.013666, train/mean_average_precision=0.782155, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293743, validation/num_examples=43793
I0305 23:10:17.972579 140106748385024 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.12409012764692307, loss=0.016634350642561913
I0305 23:10:49.884890 140105330788096 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.14134283363819122, loss=0.016881948336958885
I0305 23:11:21.758661 140106748385024 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.12709984183311462, loss=0.01566316932439804
I0305 23:11:53.385339 140105330788096 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.13888569176197052, loss=0.017627768218517303
I0305 23:12:25.360620 140106748385024 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.15336239337921143, loss=0.01846233941614628
I0305 23:12:56.839630 140105330788096 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.14868323504924774, loss=0.017747696489095688
I0305 23:13:28.887000 140106748385024 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.15435922145843506, loss=0.02010481245815754
I0305 23:14:00.684243 140105330788096 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.14370614290237427, loss=0.016753599047660828
I0305 23:14:14.457015 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:16:09.074978 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:16:12.214424 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:16:15.223529 140274064205632 submission_runner.py:411] Time since start: 47648.32s, 	Step: 96244, 	{'train/accuracy': 0.9955379366874695, 'train/loss': 0.013964240439236164, 'train/mean_average_precision': 0.777552866971797, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29362858975076517, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754213828360592, 'test/num_examples': 43793, 'score': 30991.67889022827, 'total_duration': 47648.32022738457, 'accumulated_submission_time': 30991.67889022827, 'accumulated_eval_time': 16648.886462688446, 'accumulated_logging_time': 5.048603057861328}
I0305 23:16:15.257875 140098771031808 logging_writer.py:48] [96244] accumulated_eval_time=16648.886463, accumulated_logging_time=5.048603, accumulated_submission_time=30991.678890, global_step=96244, preemption_count=0, score=30991.678890, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275421, test/num_examples=43793, total_duration=47648.320227, train/accuracy=0.995538, train/loss=0.013964, train/mean_average_precision=0.777553, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293629, validation/num_examples=43793
I0305 23:16:33.573523 140113222047488 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.1352466642856598, loss=0.016401048749685287
I0305 23:17:05.770066 140098771031808 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.1387949436903, loss=0.016983672976493835
I0305 23:17:37.710625 140113222047488 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.13505452871322632, loss=0.016739655286073685
I0305 23:18:09.790565 140098771031808 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.13841553032398224, loss=0.01643381640315056
I0305 23:18:41.796013 140113222047488 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.12967300415039062, loss=0.015134746208786964
I0305 23:19:13.942971 140098771031808 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.18389886617660522, loss=0.016108667477965355
I0305 23:19:46.640283 140113222047488 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.15047676861286163, loss=0.017412599176168442
I0305 23:20:15.428816 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:22:06.313457 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:22:09.393079 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:22:12.432491 140274064205632 submission_runner.py:411] Time since start: 48005.53s, 	Step: 96990, 	{'train/accuracy': 0.9955147504806519, 'train/loss': 0.01393084041774273, 'train/mean_average_precision': 0.7831007753855972, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29367230640574665, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754537971353992, 'test/num_examples': 43793, 'score': 31231.81813645363, 'total_duration': 48005.529192209244, 'accumulated_submission_time': 31231.81813645363, 'accumulated_eval_time': 16765.890092611313, 'accumulated_logging_time': 5.093991041183472}
I0305 23:22:12.466583 140105330788096 logging_writer.py:48] [96990] accumulated_eval_time=16765.890093, accumulated_logging_time=5.093991, accumulated_submission_time=31231.818136, global_step=96990, preemption_count=0, score=31231.818136, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275454, test/num_examples=43793, total_duration=48005.529192, train/accuracy=0.995515, train/loss=0.013931, train/mean_average_precision=0.783101, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293672, validation/num_examples=43793
I0305 23:22:16.080021 140106748385024 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.14303532242774963, loss=0.017690740525722504
I0305 23:22:48.252896 140105330788096 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.1486281156539917, loss=0.017451191321015358
I0305 23:23:20.367428 140106748385024 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.13628940284252167, loss=0.015959344804286957
I0305 23:23:52.601414 140105330788096 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.15338151156902313, loss=0.017532184720039368
I0305 23:24:24.745870 140106748385024 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.15960648655891418, loss=0.01759730651974678
I0305 23:24:56.987007 140105330788096 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.1491188108921051, loss=0.018712766468524933
I0305 23:25:28.946186 140106748385024 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.13666704297065735, loss=0.015210989862680435
I0305 23:26:00.660684 140105330788096 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.13175472617149353, loss=0.01735553704202175
I0305 23:26:12.664184 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:28:03.302024 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:28:06.345307 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:28:09.341535 140274064205632 submission_runner.py:411] Time since start: 48362.44s, 	Step: 97738, 	{'train/accuracy': 0.995638906955719, 'train/loss': 0.013695177622139454, 'train/mean_average_precision': 0.7867676729768284, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937441843168288, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754499157951282, 'test/num_examples': 43793, 'score': 31471.98353600502, 'total_duration': 48362.438228845596, 'accumulated_submission_time': 31471.98353600502, 'accumulated_eval_time': 16882.567397117615, 'accumulated_logging_time': 5.139058351516724}
I0305 23:28:09.375248 140098771031808 logging_writer.py:48] [97738] accumulated_eval_time=16882.567397, accumulated_logging_time=5.139058, accumulated_submission_time=31471.983536, global_step=97738, preemption_count=0, score=31471.983536, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275450, test/num_examples=43793, total_duration=48362.438229, train/accuracy=0.995639, train/loss=0.013695, train/mean_average_precision=0.786768, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293744, validation/num_examples=43793
I0305 23:28:29.360454 140113222047488 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.14011238515377045, loss=0.016887087374925613
I0305 23:29:01.733478 140098771031808 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.13205912709236145, loss=0.016575565561652184
I0305 23:29:34.097921 140113222047488 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.1394895315170288, loss=0.014908012002706528
I0305 23:30:06.278824 140098771031808 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.14155955612659454, loss=0.015767816454172134
I0305 23:30:38.724838 140113222047488 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.12075278908014297, loss=0.01631252095103264
I0305 23:31:10.669936 140098771031808 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.1508243978023529, loss=0.019189920276403427
I0305 23:31:42.415761 140113222047488 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.15259334444999695, loss=0.01822873018682003
I0305 23:32:09.493795 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:33:57.399068 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:34:00.504193 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:34:03.578441 140274064205632 submission_runner.py:411] Time since start: 48716.68s, 	Step: 98487, 	{'train/accuracy': 0.995592474937439, 'train/loss': 0.013828258030116558, 'train/mean_average_precision': 0.7842024551917022, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29370383154244084, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754063960084381, 'test/num_examples': 43793, 'score': 31712.067273139954, 'total_duration': 48716.67514181137, 'accumulated_submission_time': 31712.067273139954, 'accumulated_eval_time': 16996.652015209198, 'accumulated_logging_time': 5.186892509460449}
I0305 23:34:03.614920 140105330788096 logging_writer.py:48] [98487] accumulated_eval_time=16996.652015, accumulated_logging_time=5.186893, accumulated_submission_time=31712.067273, global_step=98487, preemption_count=0, score=31712.067273, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275406, test/num_examples=43793, total_duration=48716.675142, train/accuracy=0.995592, train/loss=0.013828, train/mean_average_precision=0.784202, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293704, validation/num_examples=43793
I0305 23:34:08.156292 140106748385024 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.1439339965581894, loss=0.01779821328818798
I0305 23:34:40.295092 140105330788096 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.1453588455915451, loss=0.01774010993540287
I0305 23:35:12.573399 140106748385024 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.16117370128631592, loss=0.017417555674910545
I0305 23:35:44.753643 140105330788096 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.1385548710823059, loss=0.018465513363480568
I0305 23:36:16.916524 140106748385024 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.14413870871067047, loss=0.01877826265990734
I0305 23:36:49.135893 140105330788096 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.12955908477306366, loss=0.016336433589458466
I0305 23:37:21.867675 140106748385024 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.1436794251203537, loss=0.017690924927592278
I0305 23:37:54.539670 140105330788096 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.15429309010505676, loss=0.01723775826394558
I0305 23:38:03.786782 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:39:53.838891 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:39:56.912856 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:39:59.970562 140274064205632 submission_runner.py:411] Time since start: 49073.07s, 	Step: 99229, 	{'train/accuracy': 0.9955712556838989, 'train/loss': 0.013850538991391659, 'train/mean_average_precision': 0.7811136130391136, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29370462393896773, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754442037892525, 'test/num_examples': 43793, 'score': 31952.207515239716, 'total_duration': 49073.06725502014, 'accumulated_submission_time': 31952.207515239716, 'accumulated_eval_time': 17112.835739850998, 'accumulated_logging_time': 5.234352111816406}
I0305 23:40:00.004805 140113222047488 logging_writer.py:48] [99229] accumulated_eval_time=17112.835740, accumulated_logging_time=5.234352, accumulated_submission_time=31952.207515, global_step=99229, preemption_count=0, score=31952.207515, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275444, test/num_examples=43793, total_duration=49073.067255, train/accuracy=0.995571, train/loss=0.013851, train/mean_average_precision=0.781114, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293705, validation/num_examples=43793
I0305 23:40:23.570654 140113230440192 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.13510632514953613, loss=0.01747499220073223
I0305 23:40:55.958820 140113222047488 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.15385767817497253, loss=0.018797006458044052
I0305 23:41:27.948041 140113230440192 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.1479092538356781, loss=0.019601397216320038
I0305 23:42:00.484917 140113222047488 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.14114190638065338, loss=0.018647318705916405
I0305 23:42:32.804630 140113230440192 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.14000321924686432, loss=0.017674999311566353
I0305 23:43:04.544217 140113222047488 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.13387852907180786, loss=0.016296934336423874
I0305 23:43:36.379949 140113230440192 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.16140255331993103, loss=0.018472211435437202
I0305 23:44:00.049844 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:45:44.453577 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:45:47.483384 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:45:50.509602 140274064205632 submission_runner.py:411] Time since start: 49423.61s, 	Step: 99975, 	{'train/accuracy': 0.9955838918685913, 'train/loss': 0.01376936212182045, 'train/mean_average_precision': 0.7773563293839678, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29372618848638266, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753772766361173, 'test/num_examples': 43793, 'score': 32192.220431804657, 'total_duration': 49423.6063015461, 'accumulated_submission_time': 32192.220431804657, 'accumulated_eval_time': 17223.295473337173, 'accumulated_logging_time': 5.279926300048828}
I0305 23:45:50.543771 140105330788096 logging_writer.py:48] [99975] accumulated_eval_time=17223.295473, accumulated_logging_time=5.279926, accumulated_submission_time=32192.220432, global_step=99975, preemption_count=0, score=32192.220432, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275377, test/num_examples=43793, total_duration=49423.606302, train/accuracy=0.995584, train/loss=0.013769, train/mean_average_precision=0.777356, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293726, validation/num_examples=43793
I0305 23:45:58.872920 140106748385024 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.15001720190048218, loss=0.018639137968420982
I0305 23:46:30.869811 140105330788096 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.14192624390125275, loss=0.01922813057899475
I0305 23:47:02.750383 140106748385024 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.1332665979862213, loss=0.017709124833345413
I0305 23:47:34.722358 140105330788096 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.13313019275665283, loss=0.01811845228075981
I0305 23:48:06.963376 140106748385024 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.14628317952156067, loss=0.016266847029328346
I0305 23:48:38.892652 140105330788096 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.14532674849033356, loss=0.018309460952878
I0305 23:49:10.848607 140106748385024 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.13836608827114105, loss=0.017068013548851013
I0305 23:49:43.038599 140105330788096 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.1364337056875229, loss=0.016416244208812714
I0305 23:49:50.804795 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:51:42.435411 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:51:45.765396 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:51:49.010997 140274064205632 submission_runner.py:411] Time since start: 49782.11s, 	Step: 100725, 	{'train/accuracy': 0.9955572485923767, 'train/loss': 0.013895086012780666, 'train/mean_average_precision': 0.7741282285367919, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29367739693633144, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27543378626581216, 'test/num_examples': 43793, 'score': 32432.44850897789, 'total_duration': 49782.10767865181, 'accumulated_submission_time': 32432.44850897789, 'accumulated_eval_time': 17341.50161242485, 'accumulated_logging_time': 5.326424598693848}
I0305 23:51:49.046864 140113222047488 logging_writer.py:48] [100725] accumulated_eval_time=17341.501612, accumulated_logging_time=5.326425, accumulated_submission_time=32432.448509, global_step=100725, preemption_count=0, score=32432.448509, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275434, test/num_examples=43793, total_duration=49782.107679, train/accuracy=0.995557, train/loss=0.013895, train/mean_average_precision=0.774128, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293677, validation/num_examples=43793
I0305 23:52:13.951762 140113230440192 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.14451050758361816, loss=0.017792075872421265
I0305 23:52:46.168241 140113222047488 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.13303644955158234, loss=0.016815727576613426
I0305 23:53:18.360211 140113230440192 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.15763738751411438, loss=0.01632632687687874
I0305 23:53:50.534683 140113222047488 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.13163238763809204, loss=0.016170449554920197
I0305 23:54:22.977988 140113230440192 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.136187344789505, loss=0.016992826014757156
I0305 23:54:55.863329 140113222047488 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.1401914805173874, loss=0.018044430762529373
I0305 23:55:28.678654 140113230440192 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.14211784303188324, loss=0.016275720670819283
I0305 23:55:49.171385 140274064205632 spec.py:321] Evaluating on the training split.
I0305 23:57:37.419161 140274064205632 spec.py:333] Evaluating on the validation split.
I0305 23:57:40.515410 140274064205632 spec.py:349] Evaluating on the test split.
I0305 23:57:43.553319 140274064205632 submission_runner.py:411] Time since start: 50136.65s, 	Step: 101463, 	{'train/accuracy': 0.9955657720565796, 'train/loss': 0.013872967101633549, 'train/mean_average_precision': 0.7906566108249474, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936484156831344, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27549247259698734, 'test/num_examples': 43793, 'score': 32672.53692626953, 'total_duration': 50136.650017261505, 'accumulated_submission_time': 32672.53692626953, 'accumulated_eval_time': 17455.88349723816, 'accumulated_logging_time': 5.3753931522369385}
I0305 23:57:43.588645 140098771031808 logging_writer.py:48] [101463] accumulated_eval_time=17455.883497, accumulated_logging_time=5.375393, accumulated_submission_time=32672.536926, global_step=101463, preemption_count=0, score=32672.536926, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275492, test/num_examples=43793, total_duration=50136.650017, train/accuracy=0.995566, train/loss=0.013873, train/mean_average_precision=0.790657, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293648, validation/num_examples=43793
I0305 23:57:56.090405 140105330788096 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.1584760844707489, loss=0.017884420230984688
I0305 23:58:28.269912 140098771031808 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.13128502666950226, loss=0.01638263277709484
I0305 23:59:00.230508 140105330788096 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.1453091949224472, loss=0.017710493877530098
I0305 23:59:32.309326 140098771031808 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.13980287313461304, loss=0.01487156841903925
I0306 00:00:04.353237 140105330788096 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.13875220715999603, loss=0.01681128889322281
I0306 00:00:36.776262 140098771031808 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.15060803294181824, loss=0.017390916123986244
I0306 00:01:09.142089 140105330788096 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.1338004767894745, loss=0.016740284860134125
I0306 00:01:41.362586 140098771031808 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.16628561913967133, loss=0.018625596538186073
I0306 00:01:43.851688 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:03:33.572880 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:03:36.681585 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:03:39.742432 140274064205632 submission_runner.py:411] Time since start: 50492.84s, 	Step: 102209, 	{'train/accuracy': 0.9956038594245911, 'train/loss': 0.013837792910635471, 'train/mean_average_precision': 0.7844097048287434, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357769158271446, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754421511920781, 'test/num_examples': 43793, 'score': 32912.768273591995, 'total_duration': 50492.8391327858, 'accumulated_submission_time': 32912.768273591995, 'accumulated_eval_time': 17571.774189710617, 'accumulated_logging_time': 5.421801805496216}
I0306 00:03:39.777464 140113222047488 logging_writer.py:48] [102209] accumulated_eval_time=17571.774190, accumulated_logging_time=5.421802, accumulated_submission_time=32912.768274, global_step=102209, preemption_count=0, score=32912.768274, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275442, test/num_examples=43793, total_duration=50492.839133, train/accuracy=0.995604, train/loss=0.013838, train/mean_average_precision=0.784410, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293578, validation/num_examples=43793
I0306 00:04:09.536289 140113230440192 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.15368375182151794, loss=0.021166924387216568
I0306 00:04:41.797287 140113222047488 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.13748463988304138, loss=0.016504861414432526
I0306 00:05:14.512207 140113230440192 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.14414097368717194, loss=0.016468578949570656
I0306 00:05:47.129750 140113222047488 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.15834841132164001, loss=0.01822090335190296
I0306 00:06:19.771700 140113230440192 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.13488638401031494, loss=0.018000401556491852
I0306 00:06:52.255906 140113222047488 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.14404669404029846, loss=0.01691293716430664
I0306 00:07:24.137712 140113230440192 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.14763179421424866, loss=0.01783720962703228
I0306 00:07:39.772330 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:09:26.021379 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:09:29.087565 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:09:32.103127 140274064205632 submission_runner.py:411] Time since start: 50845.20s, 	Step: 102950, 	{'train/accuracy': 0.9955734014511108, 'train/loss': 0.013786436058580875, 'train/mean_average_precision': 0.7839354967984052, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29378658427127535, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753811304726336, 'test/num_examples': 43793, 'score': 33152.73183774948, 'total_duration': 50845.19981837273, 'accumulated_submission_time': 33152.73183774948, 'accumulated_eval_time': 17684.104935884476, 'accumulated_logging_time': 5.467526435852051}
I0306 00:09:32.138901 140105330788096 logging_writer.py:48] [102950] accumulated_eval_time=17684.104936, accumulated_logging_time=5.467526, accumulated_submission_time=33152.731838, global_step=102950, preemption_count=0, score=33152.731838, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275381, test/num_examples=43793, total_duration=50845.199818, train/accuracy=0.995573, train/loss=0.013786, train/mean_average_precision=0.783935, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293787, validation/num_examples=43793
I0306 00:09:48.798191 140106748385024 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.1384926736354828, loss=0.017694909125566483
I0306 00:10:21.455145 140105330788096 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.1437985599040985, loss=0.017752476036548615
I0306 00:10:54.078884 140106748385024 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.13777273893356323, loss=0.01773536019027233
I0306 00:11:26.547536 140105330788096 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.1307898461818695, loss=0.015515335835516453
I0306 00:11:58.714581 140106748385024 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.15798835456371307, loss=0.019663138315081596
I0306 00:12:30.945380 140105330788096 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.15092337131500244, loss=0.01782044768333435
I0306 00:13:03.327122 140106748385024 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.14626877009868622, loss=0.019465124234557152
I0306 00:13:32.165622 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:15:20.971557 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:15:23.987493 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:15:26.977234 140274064205632 submission_runner.py:411] Time since start: 51200.07s, 	Step: 103691, 	{'train/accuracy': 0.9955546855926514, 'train/loss': 0.01383872888982296, 'train/mean_average_precision': 0.7864173109327909, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938074174387261, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754707876834113, 'test/num_examples': 43793, 'score': 33392.72674036026, 'total_duration': 51200.07393550873, 'accumulated_submission_time': 33392.72674036026, 'accumulated_eval_time': 17798.916505098343, 'accumulated_logging_time': 5.51433539390564}
I0306 00:15:27.011779 140098771031808 logging_writer.py:48] [103691] accumulated_eval_time=17798.916505, accumulated_logging_time=5.514335, accumulated_submission_time=33392.726740, global_step=103691, preemption_count=0, score=33392.726740, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275471, test/num_examples=43793, total_duration=51200.073936, train/accuracy=0.995555, train/loss=0.013839, train/mean_average_precision=0.786417, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293807, validation/num_examples=43793
I0306 00:15:30.150504 140113230440192 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.16457697749137878, loss=0.01765965111553669
I0306 00:16:01.802733 140098771031808 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.15122786164283752, loss=0.018021270632743835
I0306 00:16:34.348404 140113230440192 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.12987732887268066, loss=0.016413409262895584
I0306 00:17:06.290207 140098771031808 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.15250754356384277, loss=0.01853923127055168
I0306 00:17:38.143766 140113230440192 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.14395299553871155, loss=0.017395712435245514
I0306 00:18:09.981039 140098771031808 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.1541556864976883, loss=0.01945141702890396
I0306 00:18:41.963849 140113230440192 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.13338224589824677, loss=0.017039403319358826
I0306 00:19:13.852534 140098771031808 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.14766451716423035, loss=0.01720474474132061
I0306 00:19:27.149689 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:21:16.227044 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:21:19.344932 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:21:22.405297 140274064205632 submission_runner.py:411] Time since start: 51555.50s, 	Step: 104443, 	{'train/accuracy': 0.995599091053009, 'train/loss': 0.013780313543975353, 'train/mean_average_precision': 0.7693775071494818, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29373004879119935, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545756887098904, 'test/num_examples': 43793, 'score': 33632.83125758171, 'total_duration': 51555.50200009346, 'accumulated_submission_time': 33632.83125758171, 'accumulated_eval_time': 17914.172067642212, 'accumulated_logging_time': 5.561546802520752}
I0306 00:21:22.440295 140105330788096 logging_writer.py:48] [104443] accumulated_eval_time=17914.172068, accumulated_logging_time=5.561547, accumulated_submission_time=33632.831258, global_step=104443, preemption_count=0, score=33632.831258, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275458, test/num_examples=43793, total_duration=51555.502000, train/accuracy=0.995599, train/loss=0.013780, train/mean_average_precision=0.769378, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293730, validation/num_examples=43793
I0306 00:21:41.296282 140113222047488 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.15079143643379211, loss=0.019883356988430023
I0306 00:22:13.415956 140105330788096 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.14864298701286316, loss=0.018031157553195953
I0306 00:22:45.219063 140113222047488 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.15791848301887512, loss=0.016996312886476517
I0306 00:23:17.309821 140105330788096 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.1353270411491394, loss=0.018002880737185478
I0306 00:23:48.916351 140113222047488 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.1767181158065796, loss=0.02080874890089035
I0306 00:24:20.665124 140105330788096 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.1339036375284195, loss=0.015244551934301853
I0306 00:24:52.503128 140113222047488 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.1443353295326233, loss=0.016858570277690887
I0306 00:25:22.540044 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:27:08.938814 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:27:11.981967 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:27:15.010174 140274064205632 submission_runner.py:411] Time since start: 51908.11s, 	Step: 105195, 	{'train/accuracy': 0.9955223798751831, 'train/loss': 0.013949671760201454, 'train/mean_average_precision': 0.7825516269931497, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29375381299879255, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27542268791377966, 'test/num_examples': 43793, 'score': 33872.89906716347, 'total_duration': 51908.106872558594, 'accumulated_submission_time': 33872.89906716347, 'accumulated_eval_time': 18026.642154455185, 'accumulated_logging_time': 5.607269763946533}
I0306 00:27:15.046685 140098771031808 logging_writer.py:48] [105195] accumulated_eval_time=18026.642154, accumulated_logging_time=5.607270, accumulated_submission_time=33872.899067, global_step=105195, preemption_count=0, score=33872.899067, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275423, test/num_examples=43793, total_duration=51908.106873, train/accuracy=0.995522, train/loss=0.013950, train/mean_average_precision=0.782552, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293754, validation/num_examples=43793
I0306 00:27:17.011882 140113230440192 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.16317366063594818, loss=0.017721522599458694
I0306 00:27:49.319114 140098771031808 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.1370735764503479, loss=0.01751917414367199
I0306 00:28:22.238900 140113230440192 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.14881955087184906, loss=0.0178104005753994
I0306 00:28:54.020566 140098771031808 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.15328307449817657, loss=0.017657330259680748
I0306 00:29:25.917193 140113230440192 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.15398873388767242, loss=0.021617969498038292
I0306 00:29:57.842959 140098771031808 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.1317419558763504, loss=0.01658092439174652
I0306 00:30:29.745111 140113230440192 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.13897036015987396, loss=0.018020685762166977
I0306 00:31:01.906307 140098771031808 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.1421038806438446, loss=0.017345990985631943
I0306 00:31:15.129166 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:33:04.656456 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:33:07.736001 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:33:10.749190 140274064205632 submission_runner.py:411] Time since start: 52263.85s, 	Step: 105942, 	{'train/accuracy': 0.9956201314926147, 'train/loss': 0.013774879276752472, 'train/mean_average_precision': 0.7936656836107864, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937028913704122, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753237852488251, 'test/num_examples': 43793, 'score': 34112.948595285416, 'total_duration': 52263.84589409828, 'accumulated_submission_time': 34112.948595285416, 'accumulated_eval_time': 18142.262134552002, 'accumulated_logging_time': 5.656062602996826}
I0306 00:33:10.785032 140105330788096 logging_writer.py:48] [105942] accumulated_eval_time=18142.262135, accumulated_logging_time=5.656063, accumulated_submission_time=34112.948595, global_step=105942, preemption_count=0, score=34112.948595, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275324, test/num_examples=43793, total_duration=52263.845894, train/accuracy=0.995620, train/loss=0.013775, train/mean_average_precision=0.793666, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293703, validation/num_examples=43793
I0306 00:33:29.585324 140113222047488 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.15363825857639313, loss=0.017829786986112595
I0306 00:34:01.429154 140105330788096 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.15058192610740662, loss=0.017379049211740494
I0306 00:34:33.405454 140113222047488 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.15275461971759796, loss=0.018834268674254417
I0306 00:35:05.496219 140105330788096 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.14092223346233368, loss=0.016676733270287514
I0306 00:35:39.147983 140113222047488 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.14353923499584198, loss=0.01760845258831978
I0306 00:36:11.370061 140105330788096 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.13539546728134155, loss=0.016427606344223022
I0306 00:36:43.465297 140113222047488 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.15013903379440308, loss=0.017380816861987114
I0306 00:37:10.934144 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:38:54.977441 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:38:58.052984 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:39:01.077115 140274064205632 submission_runner.py:411] Time since start: 52614.17s, 	Step: 106687, 	{'train/accuracy': 0.9955931305885315, 'train/loss': 0.013800120912492275, 'train/mean_average_precision': 0.7827427341975708, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29387415711335896, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753481876534774, 'test/num_examples': 43793, 'score': 34353.06476831436, 'total_duration': 52614.17381596565, 'accumulated_submission_time': 34353.06476831436, 'accumulated_eval_time': 18252.405061244965, 'accumulated_logging_time': 5.704350709915161}
I0306 00:39:01.113242 140098771031808 logging_writer.py:48] [106687] accumulated_eval_time=18252.405061, accumulated_logging_time=5.704351, accumulated_submission_time=34353.064768, global_step=106687, preemption_count=0, score=34353.064768, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275348, test/num_examples=43793, total_duration=52614.173816, train/accuracy=0.995593, train/loss=0.013800, train/mean_average_precision=0.782743, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293874, validation/num_examples=43793
I0306 00:39:05.780186 140113230440192 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.1388540267944336, loss=0.015741843730211258
I0306 00:39:37.972893 140098771031808 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.14525772631168365, loss=0.018630901351571083
I0306 00:40:09.948231 140113230440192 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.1298803836107254, loss=0.016714992001652718
I0306 00:40:42.139541 140098771031808 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.1405898630619049, loss=0.017641151323914528
I0306 00:41:14.144123 140113230440192 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.14504292607307434, loss=0.017048830166459084
I0306 00:41:46.360198 140098771031808 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.13152091205120087, loss=0.016107633709907532
I0306 00:42:18.463579 140113230440192 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.15747064352035522, loss=0.019474022090435028
I0306 00:42:50.335062 140098771031808 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.13734053075313568, loss=0.01757121831178665
I0306 00:43:01.226839 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:44:52.858913 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:44:56.160254 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:44:59.370174 140274064205632 submission_runner.py:411] Time since start: 52972.47s, 	Step: 107435, 	{'train/accuracy': 0.9955994486808777, 'train/loss': 0.013732410967350006, 'train/mean_average_precision': 0.7809417040681077, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938712298130228, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27541388066160044, 'test/num_examples': 43793, 'score': 34593.14384675026, 'total_duration': 52972.46685504913, 'accumulated_submission_time': 34593.14384675026, 'accumulated_eval_time': 18370.54836511612, 'accumulated_logging_time': 5.751443147659302}
I0306 00:44:59.410910 140105330788096 logging_writer.py:48] [107435] accumulated_eval_time=18370.548365, accumulated_logging_time=5.751443, accumulated_submission_time=34593.143847, global_step=107435, preemption_count=0, score=34593.143847, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275414, test/num_examples=43793, total_duration=52972.466855, train/accuracy=0.995599, train/loss=0.013732, train/mean_average_precision=0.780942, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293871, validation/num_examples=43793
I0306 00:45:20.702593 140113222047488 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.15103848278522491, loss=0.01798400469124317
I0306 00:45:52.752896 140105330788096 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.12801331281661987, loss=0.01741194538772106
I0306 00:46:25.395054 140113222047488 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.16039086878299713, loss=0.01750602200627327
I0306 00:46:58.215571 140105330788096 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.16000507771968842, loss=0.017345715314149857
I0306 00:47:30.795124 140113222047488 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.1614556908607483, loss=0.017419468611478806
I0306 00:48:03.258195 140105330788096 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.15297676622867584, loss=0.01880069263279438
I0306 00:48:36.268812 140113222047488 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.15603937208652496, loss=0.01783120259642601
I0306 00:48:59.632029 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:50:43.693980 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:50:46.769380 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:50:49.737414 140274064205632 submission_runner.py:411] Time since start: 53322.83s, 	Step: 108172, 	{'train/accuracy': 0.9955727458000183, 'train/loss': 0.013813760131597519, 'train/mean_average_precision': 0.7745374913235604, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936441576869056, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27539671372433594, 'test/num_examples': 43793, 'score': 34833.32964348793, 'total_duration': 53322.83411717415, 'accumulated_submission_time': 34833.32964348793, 'accumulated_eval_time': 18480.653708934784, 'accumulated_logging_time': 5.805322170257568}
I0306 00:50:49.774116 140098771031808 logging_writer.py:48] [108172] accumulated_eval_time=18480.653709, accumulated_logging_time=5.805322, accumulated_submission_time=34833.329643, global_step=108172, preemption_count=0, score=34833.329643, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275397, test/num_examples=43793, total_duration=53322.834117, train/accuracy=0.995573, train/loss=0.013814, train/mean_average_precision=0.774537, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293644, validation/num_examples=43793
I0306 00:50:59.257623 140106748385024 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.14144361019134521, loss=0.016275038942694664
I0306 00:51:31.610648 140098771031808 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.1455792486667633, loss=0.01760552078485489
I0306 00:52:03.866482 140106748385024 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.14984194934368134, loss=0.01791893132030964
I0306 00:52:36.173462 140098771031808 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.13957911729812622, loss=0.015952052548527718
I0306 00:53:08.074136 140106748385024 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.1674586683511734, loss=0.02062862552702427
I0306 00:53:40.124391 140098771031808 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.14934499561786652, loss=0.018577443435788155
I0306 00:54:12.301461 140106748385024 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.16091802716255188, loss=0.01818908005952835
I0306 00:54:44.902135 140098771031808 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.13952332735061646, loss=0.017471812665462494
I0306 00:54:49.826230 140274064205632 spec.py:321] Evaluating on the training split.
I0306 00:56:33.555059 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 00:56:36.589767 140274064205632 spec.py:349] Evaluating on the test split.
I0306 00:56:39.585553 140274064205632 submission_runner.py:411] Time since start: 53672.68s, 	Step: 108916, 	{'train/accuracy': 0.9955095052719116, 'train/loss': 0.014006881974637508, 'train/mean_average_precision': 0.7848383461143658, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936524279192357, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754546663138822, 'test/num_examples': 43793, 'score': 35073.34981870651, 'total_duration': 53672.68225312233, 'accumulated_submission_time': 35073.34981870651, 'accumulated_eval_time': 18590.412984609604, 'accumulated_logging_time': 5.8534626960754395}
I0306 00:56:39.626785 140105330788096 logging_writer.py:48] [108916] accumulated_eval_time=18590.412985, accumulated_logging_time=5.853463, accumulated_submission_time=35073.349819, global_step=108916, preemption_count=0, score=35073.349819, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275455, test/num_examples=43793, total_duration=53672.682253, train/accuracy=0.995510, train/loss=0.014007, train/mean_average_precision=0.784838, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293652, validation/num_examples=43793
I0306 00:57:06.599394 140113222047488 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.12320665270090103, loss=0.015585502609610558
I0306 00:57:38.268390 140105330788096 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.16873043775558472, loss=0.019554322585463524
I0306 00:58:10.256896 140113222047488 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.14452354609966278, loss=0.017378784716129303
I0306 00:58:42.318835 140105330788096 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.13970518112182617, loss=0.016240039840340614
I0306 00:59:14.018517 140113222047488 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.12761537730693817, loss=0.015102764591574669
I0306 00:59:45.789617 140105330788096 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.1442769467830658, loss=0.01810169778764248
I0306 01:00:17.771776 140113222047488 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.1399356871843338, loss=0.017286129295825958
I0306 01:00:39.749481 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:02:23.371521 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:02:26.510865 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:02:29.564836 140274064205632 submission_runner.py:411] Time since start: 54022.66s, 	Step: 109670, 	{'train/accuracy': 0.9956090450286865, 'train/loss': 0.01380245666950941, 'train/mean_average_precision': 0.7855790926404516, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937035017352725, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275395700887565, 'test/num_examples': 43793, 'score': 35313.440516233444, 'total_duration': 54022.661536455154, 'accumulated_submission_time': 35313.440516233444, 'accumulated_eval_time': 18700.228295087814, 'accumulated_logging_time': 5.906094074249268}
I0306 01:02:29.601792 140098771031808 logging_writer.py:48] [109670] accumulated_eval_time=18700.228295, accumulated_logging_time=5.906094, accumulated_submission_time=35313.440516, global_step=109670, preemption_count=0, score=35313.440516, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275396, test/num_examples=43793, total_duration=54022.661536, train/accuracy=0.995609, train/loss=0.013802, train/mean_average_precision=0.785579, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293704, validation/num_examples=43793
I0306 01:02:39.476358 140106748385024 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.14507675170898438, loss=0.016047939658164978
I0306 01:03:11.830067 140098771031808 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.16359570622444153, loss=0.018699374049901962
I0306 01:03:44.210859 140106748385024 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.13713131844997406, loss=0.016158031299710274
I0306 01:04:16.345834 140098771031808 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.14999304711818695, loss=0.019917214289307594
I0306 01:04:48.378600 140106748385024 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.1574164479970932, loss=0.016798553988337517
I0306 01:05:20.403733 140098771031808 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.1389126181602478, loss=0.018523167818784714
I0306 01:05:52.428649 140106748385024 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.14839941263198853, loss=0.020124467089772224
I0306 01:06:24.853490 140098771031808 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.16502591967582703, loss=0.019936129450798035
I0306 01:06:29.666044 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:08:16.368987 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:08:19.527443 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:08:22.561799 140274064205632 submission_runner.py:411] Time since start: 54375.66s, 	Step: 110416, 	{'train/accuracy': 0.9955815076828003, 'train/loss': 0.013784144073724747, 'train/mean_average_precision': 0.7876969888879968, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.293807377697929, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27553018474758667, 'test/num_examples': 43793, 'score': 35553.47294831276, 'total_duration': 54375.65848469734, 'accumulated_submission_time': 35553.47294831276, 'accumulated_eval_time': 18813.123986005783, 'accumulated_logging_time': 5.954303979873657}
I0306 01:08:22.598200 140105330788096 logging_writer.py:48] [110416] accumulated_eval_time=18813.123986, accumulated_logging_time=5.954304, accumulated_submission_time=35553.472948, global_step=110416, preemption_count=0, score=35553.472948, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275530, test/num_examples=43793, total_duration=54375.658485, train/accuracy=0.995582, train/loss=0.013784, train/mean_average_precision=0.787697, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293807, validation/num_examples=43793
I0306 01:08:50.015045 140113230440192 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.12638990581035614, loss=0.016385331749916077
I0306 01:09:22.321928 140105330788096 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.13532178103923798, loss=0.017013903707265854
I0306 01:09:54.648049 140113230440192 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.15355461835861206, loss=0.01779641956090927
I0306 01:10:26.564898 140105330788096 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.13729727268218994, loss=0.017469510436058044
I0306 01:10:57.859245 140113230440192 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.15221424400806427, loss=0.01790597103536129
I0306 01:11:29.805669 140105330788096 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.13468198478221893, loss=0.017577582970261574
I0306 01:12:01.658409 140113230440192 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.15174072980880737, loss=0.017960064113140106
I0306 01:12:22.728257 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:14:15.643112 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:14:18.739835 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:14:21.881468 140274064205632 submission_runner.py:411] Time since start: 54734.98s, 	Step: 111167, 	{'train/accuracy': 0.9955974817276001, 'train/loss': 0.013816900551319122, 'train/mean_average_precision': 0.7778713694892079, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29360510516030347, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2755673129599433, 'test/num_examples': 43793, 'score': 35793.5690510273, 'total_duration': 54734.97816634178, 'accumulated_submission_time': 35793.5690510273, 'accumulated_eval_time': 18932.277148246765, 'accumulated_logging_time': 6.003270387649536}
I0306 01:14:21.919060 140098771031808 logging_writer.py:48] [111167] accumulated_eval_time=18932.277148, accumulated_logging_time=6.003270, accumulated_submission_time=35793.569051, global_step=111167, preemption_count=0, score=35793.569051, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275567, test/num_examples=43793, total_duration=54734.978166, train/accuracy=0.995597, train/loss=0.013817, train/mean_average_precision=0.777871, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293605, validation/num_examples=43793
I0306 01:14:32.890549 140106748385024 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.14418789744377136, loss=0.01855255290865898
I0306 01:15:04.935623 140098771031808 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.142747163772583, loss=0.019156988710165024
I0306 01:15:36.973153 140106748385024 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.13760501146316528, loss=0.01676064170897007
I0306 01:16:09.065196 140098771031808 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.13422895967960358, loss=0.016064368188381195
I0306 01:16:41.239719 140106748385024 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.1351536363363266, loss=0.016520310193300247
I0306 01:17:13.686642 140098771031808 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.1390979140996933, loss=0.017837049439549446
I0306 01:17:45.745970 140106748385024 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.15676002204418182, loss=0.021150067448616028
I0306 01:18:18.315361 140098771031808 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.14515353739261627, loss=0.019599147140979767
I0306 01:18:21.928602 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:20:06.058599 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:20:09.103939 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:20:12.186328 140274064205632 submission_runner.py:411] Time since start: 55085.28s, 	Step: 111912, 	{'train/accuracy': 0.9955654144287109, 'train/loss': 0.01377047412097454, 'train/mean_average_precision': 0.7812751085022197, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29361910371920447, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753573878831423, 'test/num_examples': 43793, 'score': 36033.546392440796, 'total_duration': 55085.2830324173, 'accumulated_submission_time': 36033.546392440796, 'accumulated_eval_time': 19042.534830093384, 'accumulated_logging_time': 6.051767826080322}
I0306 01:20:12.223226 140113222047488 logging_writer.py:48] [111912] accumulated_eval_time=19042.534830, accumulated_logging_time=6.051768, accumulated_submission_time=36033.546392, global_step=111912, preemption_count=0, score=36033.546392, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275357, test/num_examples=43793, total_duration=55085.283032, train/accuracy=0.995565, train/loss=0.013770, train/mean_average_precision=0.781275, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293619, validation/num_examples=43793
I0306 01:20:41.308715 140113230440192 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.14267560839653015, loss=0.016155671328306198
I0306 01:21:13.813317 140113222047488 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.1406780630350113, loss=0.017318271100521088
I0306 01:21:45.684503 140113230440192 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.14214450120925903, loss=0.015294946730136871
I0306 01:22:18.134932 140113222047488 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.14460012316703796, loss=0.01738768257200718
I0306 01:22:51.046873 140113230440192 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.14704567193984985, loss=0.01727323792874813
I0306 01:23:23.502503 140113222047488 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.14166182279586792, loss=0.017627965658903122
I0306 01:23:56.063996 140113230440192 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.13037386536598206, loss=0.01672578789293766
I0306 01:24:12.208552 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:25:54.681360 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:25:57.879523 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:26:00.942648 140274064205632 submission_runner.py:411] Time since start: 55434.04s, 	Step: 112651, 	{'train/accuracy': 0.9955233335494995, 'train/loss': 0.014060000889003277, 'train/mean_average_precision': 0.7775877571357277, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29375970072908975, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27543147278156305, 'test/num_examples': 43793, 'score': 36273.49717617035, 'total_duration': 55434.03934621811, 'accumulated_submission_time': 36273.49717617035, 'accumulated_eval_time': 19151.26888847351, 'accumulated_logging_time': 6.1010048389434814}
I0306 01:26:00.979833 140105330788096 logging_writer.py:48] [112651] accumulated_eval_time=19151.268888, accumulated_logging_time=6.101005, accumulated_submission_time=36273.497176, global_step=112651, preemption_count=0, score=36273.497176, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275431, test/num_examples=43793, total_duration=55434.039346, train/accuracy=0.995523, train/loss=0.014060, train/mean_average_precision=0.777588, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293760, validation/num_examples=43793
I0306 01:26:17.624349 140106748385024 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.13718707859516144, loss=0.016005493700504303
I0306 01:26:50.108524 140105330788096 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.1320025473833084, loss=0.01668710634112358
I0306 01:27:22.564987 140106748385024 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.12599362432956696, loss=0.01632876880466938
I0306 01:27:55.244005 140105330788096 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.1595417708158493, loss=0.018802350386977196
I0306 01:28:27.581383 140106748385024 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.12330709397792816, loss=0.017258690670132637
I0306 01:28:59.840961 140105330788096 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.16154012084007263, loss=0.02100137621164322
I0306 01:29:31.671393 140106748385024 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.14254608750343323, loss=0.01761370152235031
I0306 01:30:01.059611 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:31:48.474333 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:31:51.595959 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:31:54.604364 140274064205632 submission_runner.py:411] Time since start: 55787.70s, 	Step: 113393, 	{'train/accuracy': 0.9956125020980835, 'train/loss': 0.013702832162380219, 'train/mean_average_precision': 0.7856105334997641, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936817305110332, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753357528733862, 'test/num_examples': 43793, 'score': 36513.54458999634, 'total_duration': 55787.701063632965, 'accumulated_submission_time': 36513.54458999634, 'accumulated_eval_time': 19264.81359577179, 'accumulated_logging_time': 6.1495959758758545}
I0306 01:31:54.641364 140098771031808 logging_writer.py:48] [113393] accumulated_eval_time=19264.813596, accumulated_logging_time=6.149596, accumulated_submission_time=36513.544590, global_step=113393, preemption_count=0, score=36513.544590, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275336, test/num_examples=43793, total_duration=55787.701064, train/accuracy=0.995613, train/loss=0.013703, train/mean_average_precision=0.785611, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293682, validation/num_examples=43793
I0306 01:31:57.250588 140113222047488 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.1269163191318512, loss=0.01618795096874237
I0306 01:32:29.804652 140098771031808 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.14423027634620667, loss=0.016658414155244827
I0306 01:33:02.760923 140113222047488 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.14829353988170624, loss=0.018739957362413406
I0306 01:33:35.067864 140098771031808 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.13783323764801025, loss=0.016703903675079346
I0306 01:34:07.577033 140113222047488 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.16433899104595184, loss=0.019746430218219757
I0306 01:34:40.423760 140098771031808 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.13481459021568298, loss=0.017784683033823967
I0306 01:35:12.993736 140113222047488 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.14991547167301178, loss=0.01676013506948948
I0306 01:35:45.230458 140098771031808 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.1514228731393814, loss=0.018060065805912018
I0306 01:35:54.689544 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:37:41.379811 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:37:44.429123 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:37:47.470448 140274064205632 submission_runner.py:411] Time since start: 56140.57s, 	Step: 114130, 	{'train/accuracy': 0.9956178069114685, 'train/loss': 0.013772581703960896, 'train/mean_average_precision': 0.7901904740236783, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935913388519824, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27541571969707035, 'test/num_examples': 43793, 'score': 36753.55989098549, 'total_duration': 56140.56715130806, 'accumulated_submission_time': 36753.55989098549, 'accumulated_eval_time': 19377.594454288483, 'accumulated_logging_time': 6.197968244552612}
I0306 01:37:47.506989 140105330788096 logging_writer.py:48] [114130] accumulated_eval_time=19377.594454, accumulated_logging_time=6.197968, accumulated_submission_time=36753.559891, global_step=114130, preemption_count=0, score=36753.559891, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275416, test/num_examples=43793, total_duration=56140.567151, train/accuracy=0.995618, train/loss=0.013773, train/mean_average_precision=0.790190, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293591, validation/num_examples=43793
I0306 01:38:10.572304 140113230440192 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.15139874815940857, loss=0.018989039584994316
I0306 01:38:43.345877 140105330788096 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.145381897687912, loss=0.01836973987519741
I0306 01:39:15.951578 140113230440192 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.13023194670677185, loss=0.0155830979347229
I0306 01:39:48.390107 140105330788096 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.13937979936599731, loss=0.017913639545440674
I0306 01:40:20.873437 140113230440192 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.14154984056949615, loss=0.015769433230161667
I0306 01:40:53.636275 140105330788096 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.15208278596401215, loss=0.01623178832232952
I0306 01:41:26.083833 140113230440192 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.14047275483608246, loss=0.016789259389042854
I0306 01:41:47.506127 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:43:36.920943 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:43:40.044133 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:43:43.084755 140274064205632 submission_runner.py:411] Time since start: 56496.18s, 	Step: 114866, 	{'train/accuracy': 0.9955707788467407, 'train/loss': 0.013844111934304237, 'train/mean_average_precision': 0.7832189831767615, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357872895233295, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754269244754509, 'test/num_examples': 43793, 'score': 36993.52682876587, 'total_duration': 56496.18143749237, 'accumulated_submission_time': 36993.52682876587, 'accumulated_eval_time': 19493.173021554947, 'accumulated_logging_time': 6.24558687210083}
I0306 01:43:43.125643 140098771031808 logging_writer.py:48] [114866] accumulated_eval_time=19493.173022, accumulated_logging_time=6.245587, accumulated_submission_time=36993.526829, global_step=114866, preemption_count=0, score=36993.526829, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275427, test/num_examples=43793, total_duration=56496.181437, train/accuracy=0.995571, train/loss=0.013844, train/mean_average_precision=0.783219, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293579, validation/num_examples=43793
I0306 01:43:54.647119 140113222047488 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.14058062434196472, loss=0.01722455769777298
I0306 01:44:26.924975 140098771031808 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.14807654917240143, loss=0.019383514299988747
I0306 01:44:58.651154 140113222047488 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.13431289792060852, loss=0.017129404470324516
I0306 01:45:30.757591 140098771031808 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.1461072862148285, loss=0.016950303688645363
I0306 01:46:03.217654 140113222047488 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.1657208502292633, loss=0.017726361751556396
I0306 01:46:35.729838 140098771031808 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.14112186431884766, loss=0.018889090046286583
I0306 01:47:07.802442 140113222047488 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.15775051712989807, loss=0.019076954573392868
I0306 01:47:39.727679 140098771031808 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.13312144577503204, loss=0.017179984599351883
I0306 01:47:43.246773 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:49:30.163877 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:49:33.294445 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:49:36.339694 140274064205632 submission_runner.py:411] Time since start: 56849.44s, 	Step: 115612, 	{'train/accuracy': 0.9955816268920898, 'train/loss': 0.01376497745513916, 'train/mean_average_precision': 0.7833042739703718, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.293672361685288, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545210021383826, 'test/num_examples': 43793, 'score': 37233.61578011513, 'total_duration': 56849.436390161514, 'accumulated_submission_time': 37233.61578011513, 'accumulated_eval_time': 19606.265890836716, 'accumulated_logging_time': 6.298041105270386}
I0306 01:49:36.378351 140105330788096 logging_writer.py:48] [115612] accumulated_eval_time=19606.265891, accumulated_logging_time=6.298041, accumulated_submission_time=37233.615780, global_step=115612, preemption_count=0, score=37233.615780, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275452, test/num_examples=43793, total_duration=56849.436390, train/accuracy=0.995582, train/loss=0.013765, train/mean_average_precision=0.783304, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293672, validation/num_examples=43793
I0306 01:50:05.074030 140113230440192 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.14190585911273956, loss=0.017034199088811874
I0306 01:50:37.038671 140105330788096 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.15281206369400024, loss=0.018174948170781136
I0306 01:51:08.839494 140113230440192 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.14486326277256012, loss=0.01853882521390915
I0306 01:51:40.579111 140105330788096 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.14570723474025726, loss=0.01798543892800808
I0306 01:52:12.676462 140113230440192 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.1669674962759018, loss=0.020990366116166115
I0306 01:52:44.859304 140105330788096 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.14251627027988434, loss=0.01749267615377903
I0306 01:53:17.155372 140113230440192 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.1480187177658081, loss=0.01716597005724907
I0306 01:53:36.605501 140274064205632 spec.py:321] Evaluating on the training split.
I0306 01:55:20.798416 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 01:55:23.918662 140274064205632 spec.py:349] Evaluating on the test split.
I0306 01:55:26.957842 140274064205632 submission_runner.py:411] Time since start: 57200.05s, 	Step: 116362, 	{'train/accuracy': 0.9955711960792542, 'train/loss': 0.013825943693518639, 'train/mean_average_precision': 0.7763133112655201, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937281957738649, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754720384718899, 'test/num_examples': 43793, 'score': 37473.81154060364, 'total_duration': 57200.054525375366, 'accumulated_submission_time': 37473.81154060364, 'accumulated_eval_time': 19716.618169546127, 'accumulated_logging_time': 6.347314119338989}
I0306 01:55:26.996520 140098771031808 logging_writer.py:48] [116362] accumulated_eval_time=19716.618170, accumulated_logging_time=6.347314, accumulated_submission_time=37473.811541, global_step=116362, preemption_count=0, score=37473.811541, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275472, test/num_examples=43793, total_duration=57200.054525, train/accuracy=0.995571, train/loss=0.013826, train/mean_average_precision=0.776313, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293728, validation/num_examples=43793
I0306 01:55:39.574800 140113222047488 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.1224684864282608, loss=0.016826985403895378
I0306 01:56:12.117373 140098771031808 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.15797922015190125, loss=0.01620260253548622
I0306 01:56:44.573451 140113222047488 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.17180362343788147, loss=0.018880954012274742
I0306 01:57:17.039161 140098771031808 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.15037809312343597, loss=0.01631123572587967
I0306 01:57:49.643005 140113222047488 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.1281629502773285, loss=0.01647615246474743
I0306 01:58:22.111219 140098771031808 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.13890399038791656, loss=0.01797470822930336
I0306 01:58:54.258730 140113222047488 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.11867258697748184, loss=0.01578223891556263
I0306 01:59:26.731024 140098771031808 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.14079289138317108, loss=0.015107748098671436
I0306 01:59:27.058648 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:01:13.609646 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:01:16.668816 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:01:19.663995 140274064205632 submission_runner.py:411] Time since start: 57552.76s, 	Step: 117102, 	{'train/accuracy': 0.9955509305000305, 'train/loss': 0.013860664330422878, 'train/mean_average_precision': 0.7871036951453844, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936363325723397, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275412886815387, 'test/num_examples': 43793, 'score': 37713.840057611465, 'total_duration': 57552.76069331169, 'accumulated_submission_time': 37713.840057611465, 'accumulated_eval_time': 19829.223463773727, 'accumulated_logging_time': 6.399012088775635}
I0306 02:01:19.710732 140106748385024 logging_writer.py:48] [117102] accumulated_eval_time=19829.223464, accumulated_logging_time=6.399012, accumulated_submission_time=37713.840058, global_step=117102, preemption_count=0, score=37713.840058, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275413, test/num_examples=43793, total_duration=57552.760693, train/accuracy=0.995551, train/loss=0.013861, train/mean_average_precision=0.787104, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293636, validation/num_examples=43793
I0306 02:01:52.552303 140113230440192 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.13484954833984375, loss=0.018274936825037003
I0306 02:02:25.441011 140106748385024 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.138743057847023, loss=0.017784249037504196
I0306 02:02:57.638630 140113230440192 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.14722666144371033, loss=0.017995448783040047
I0306 02:03:29.830876 140106748385024 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.14766304194927216, loss=0.017991285771131516
I0306 02:04:02.102974 140113230440192 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.1405312716960907, loss=0.015145283192396164
I0306 02:04:34.393697 140106748385024 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.14488500356674194, loss=0.018848098814487457
I0306 02:05:06.768703 140113230440192 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.14891238510608673, loss=0.018367502838373184
I0306 02:05:19.972672 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:07:05.651420 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:07:08.720839 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:07:11.743729 140274064205632 submission_runner.py:411] Time since start: 57904.84s, 	Step: 117842, 	{'train/accuracy': 0.9955703616142273, 'train/loss': 0.013903734274208546, 'train/mean_average_precision': 0.7890036277593242, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936448231073847, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27536850325243967, 'test/num_examples': 43793, 'score': 37954.070449113846, 'total_duration': 57904.84043097496, 'accumulated_submission_time': 37954.070449113846, 'accumulated_eval_time': 19940.994478702545, 'accumulated_logging_time': 6.45639443397522}
I0306 02:07:11.784964 140070338619136 logging_writer.py:48] [117842] accumulated_eval_time=19940.994479, accumulated_logging_time=6.456394, accumulated_submission_time=37954.070449, global_step=117842, preemption_count=0, score=37954.070449, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275369, test/num_examples=43793, total_duration=57904.840431, train/accuracy=0.995570, train/loss=0.013904, train/mean_average_precision=0.789004, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293645, validation/num_examples=43793
I0306 02:07:30.896094 140098771031808 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.1509975790977478, loss=0.01928657293319702
I0306 02:08:03.117564 140070338619136 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.14434170722961426, loss=0.016688048839569092
I0306 02:08:35.489449 140098771031808 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.12711679935455322, loss=0.01503926981240511
I0306 02:09:08.187845 140070338619136 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.13509824872016907, loss=0.017347166314721107
I0306 02:09:40.683867 140098771031808 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.143055260181427, loss=0.018137963488698006
I0306 02:10:12.903303 140070338619136 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.14073406159877777, loss=0.017509102821350098
I0306 02:10:45.035889 140098771031808 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.16048523783683777, loss=0.017192522063851357
I0306 02:11:11.848896 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:13:00.970366 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:13:04.160565 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:13:07.171368 140274064205632 submission_runner.py:411] Time since start: 58260.27s, 	Step: 118584, 	{'train/accuracy': 0.9955964088439941, 'train/loss': 0.013852759264409542, 'train/mean_average_precision': 0.7802696371874123, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29365108435044857, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754307721802707, 'test/num_examples': 43793, 'score': 38194.10259127617, 'total_duration': 58260.26805996895, 'accumulated_submission_time': 38194.10259127617, 'accumulated_eval_time': 20056.31689786911, 'accumulated_logging_time': 6.509163856506348}
I0306 02:13:07.209010 140106748385024 logging_writer.py:48] [118584] accumulated_eval_time=20056.316898, accumulated_logging_time=6.509164, accumulated_submission_time=38194.102591, global_step=118584, preemption_count=0, score=38194.102591, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275431, test/num_examples=43793, total_duration=58260.268060, train/accuracy=0.995596, train/loss=0.013853, train/mean_average_precision=0.780270, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293651, validation/num_examples=43793
I0306 02:13:12.594662 140113230440192 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.16140969097614288, loss=0.018228285014629364
I0306 02:13:44.792976 140106748385024 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.13226258754730225, loss=0.016999756917357445
I0306 02:14:16.738932 140113230440192 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.14582626521587372, loss=0.017522133886814117
I0306 02:14:48.451251 140106748385024 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.1429004669189453, loss=0.01607593521475792
I0306 02:15:20.614253 140113230440192 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.15235625207424164, loss=0.01731841452419758
I0306 02:15:52.852400 140106748385024 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.14245308935642242, loss=0.015762027353048325
I0306 02:16:25.750557 140113230440192 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.13841131329536438, loss=0.016916802152991295
I0306 02:16:57.859503 140106748385024 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.13956643640995026, loss=0.016588255763053894
I0306 02:17:07.483185 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:18:53.880162 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:18:57.005160 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:19:00.041102 140274064205632 submission_runner.py:411] Time since start: 58613.14s, 	Step: 119331, 	{'train/accuracy': 0.9955704212188721, 'train/loss': 0.013761062175035477, 'train/mean_average_precision': 0.7811442251892875, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357848888389165, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2755224102040603, 'test/num_examples': 43793, 'score': 38434.34361219406, 'total_duration': 58613.13779401779, 'accumulated_submission_time': 38434.34361219406, 'accumulated_eval_time': 20168.874756336212, 'accumulated_logging_time': 6.5590479373931885}
I0306 02:19:00.080431 140070338619136 logging_writer.py:48] [119331] accumulated_eval_time=20168.874756, accumulated_logging_time=6.559048, accumulated_submission_time=38434.343612, global_step=119331, preemption_count=0, score=38434.343612, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275522, test/num_examples=43793, total_duration=58613.137794, train/accuracy=0.995570, train/loss=0.013761, train/mean_average_precision=0.781144, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293578, validation/num_examples=43793
I0306 02:19:22.491521 140105330788096 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.14281481504440308, loss=0.016587095335125923
I0306 02:19:54.421129 140070338619136 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.1393861323595047, loss=0.016097605228424072
I0306 02:20:26.720698 140105330788096 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.13433915376663208, loss=0.01709388568997383
I0306 02:20:58.912506 140070338619136 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.1478765606880188, loss=0.017237983644008636
I0306 02:21:31.401401 140105330788096 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.15468324720859528, loss=0.01875077560544014
I0306 02:22:03.911419 140070338619136 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.1552014797925949, loss=0.019287362694740295
I0306 02:22:36.776815 140105330788096 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.15122495591640472, loss=0.018132977187633514
I0306 02:23:00.307139 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:24:44.670531 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:24:47.817218 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:24:50.857756 140274064205632 submission_runner.py:411] Time since start: 58963.95s, 	Step: 120074, 	{'train/accuracy': 0.9955947399139404, 'train/loss': 0.013799689710140228, 'train/mean_average_precision': 0.7765940262969431, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29374636420944905, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27547666153161526, 'test/num_examples': 43793, 'score': 38674.53906083107, 'total_duration': 58963.954456329346, 'accumulated_submission_time': 38674.53906083107, 'accumulated_eval_time': 20279.42533969879, 'accumulated_logging_time': 6.609145879745483}
I0306 02:24:50.897206 140069608036096 logging_writer.py:48] [120074] accumulated_eval_time=20279.425340, accumulated_logging_time=6.609146, accumulated_submission_time=38674.539061, global_step=120074, preemption_count=0, score=38674.539061, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275477, test/num_examples=43793, total_duration=58963.954456, train/accuracy=0.995595, train/loss=0.013800, train/mean_average_precision=0.776594, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293746, validation/num_examples=43793
I0306 02:24:59.685965 140098771031808 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.15199361741542816, loss=0.018293891102075577
I0306 02:25:32.340520 140069608036096 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.15241551399230957, loss=0.016823390498757362
I0306 02:26:04.979789 140098771031808 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.16805855929851532, loss=0.01914801076054573
I0306 02:26:37.510969 140069608036096 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.14004820585250854, loss=0.017335059121251106
I0306 02:27:09.885124 140098771031808 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.14260758459568024, loss=0.01751449890434742
I0306 02:27:42.373196 140069608036096 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.13212399184703827, loss=0.015876533463597298
I0306 02:28:14.565851 140098771031808 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.13299153745174408, loss=0.016323858872056007
I0306 02:28:47.313937 140069608036096 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.1577938050031662, loss=0.01873316429555416
I0306 02:28:50.902199 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:30:39.791435 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:30:42.892427 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:30:47.788432 140274064205632 submission_runner.py:411] Time since start: 59320.89s, 	Step: 120812, 	{'train/accuracy': 0.9955591559410095, 'train/loss': 0.013879530131816864, 'train/mean_average_precision': 0.7845728335870632, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937915479617342, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2755423162549807, 'test/num_examples': 43793, 'score': 38914.5114774704, 'total_duration': 59320.885125637054, 'accumulated_submission_time': 38914.5114774704, 'accumulated_eval_time': 20396.31151819229, 'accumulated_logging_time': 6.660247087478638}
I0306 02:30:47.828606 140105330788096 logging_writer.py:48] [120812] accumulated_eval_time=20396.311518, accumulated_logging_time=6.660247, accumulated_submission_time=38914.511477, global_step=120812, preemption_count=0, score=38914.511477, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275542, test/num_examples=43793, total_duration=59320.885126, train/accuracy=0.995559, train/loss=0.013880, train/mean_average_precision=0.784573, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293792, validation/num_examples=43793
I0306 02:31:17.035135 140106748385024 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.14984764158725739, loss=0.018027599900960922
I0306 02:31:49.550525 140105330788096 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.16561777889728546, loss=0.018973397091031075
I0306 02:32:22.140983 140106748385024 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.14099764823913574, loss=0.018158970400691032
I0306 02:32:54.025330 140105330788096 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.16055454313755035, loss=0.02001534029841423
I0306 02:33:26.336787 140106748385024 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.146921306848526, loss=0.016415860503911972
I0306 02:33:58.409172 140105330788096 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.14844852685928345, loss=0.01604444533586502
I0306 02:34:30.761205 140106748385024 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.136457622051239, loss=0.016320569440722466
I0306 02:34:47.950137 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:36:32.124909 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:36:35.282248 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:36:38.279499 140274064205632 submission_runner.py:411] Time since start: 59671.38s, 	Step: 121554, 	{'train/accuracy': 0.9955857992172241, 'train/loss': 0.01378462091088295, 'train/mean_average_precision': 0.7895918756519975, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937423974114145, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27550329716780636, 'test/num_examples': 43793, 'score': 39154.60091519356, 'total_duration': 59671.376187086105, 'accumulated_submission_time': 39154.60091519356, 'accumulated_eval_time': 20506.640821695328, 'accumulated_logging_time': 6.712068557739258}
I0306 02:36:38.318052 140069608036096 logging_writer.py:48] [121554] accumulated_eval_time=20506.640822, accumulated_logging_time=6.712069, accumulated_submission_time=39154.600915, global_step=121554, preemption_count=0, score=39154.600915, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275503, test/num_examples=43793, total_duration=59671.376187, train/accuracy=0.995586, train/loss=0.013785, train/mean_average_precision=0.789592, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293742, validation/num_examples=43793
I0306 02:36:53.456532 140070338619136 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.1359664797782898, loss=0.0180737916380167
I0306 02:37:25.637270 140069608036096 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.13921795785427094, loss=0.017443915829062462
I0306 02:37:58.058801 140070338619136 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.13588400185108185, loss=0.017617519944906235
I0306 02:38:29.999847 140069608036096 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.14560364186763763, loss=0.019191468134522438
I0306 02:39:01.818496 140070338619136 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.1568223237991333, loss=0.01785837672650814
I0306 02:39:33.994257 140069608036096 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.15046745538711548, loss=0.018585719168186188
I0306 02:40:05.935845 140070338619136 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.1651332527399063, loss=0.022649100050330162
I0306 02:40:38.001843 140069608036096 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.13642296195030212, loss=0.01698114722967148
I0306 02:40:38.344386 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:42:26.428243 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:42:29.558560 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:42:32.627195 140274064205632 submission_runner.py:411] Time since start: 60025.72s, 	Step: 122302, 	{'train/accuracy': 0.9956145286560059, 'train/loss': 0.01379110012203455, 'train/mean_average_precision': 0.7868911213978563, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29367064979595386, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754014452639356, 'test/num_examples': 43793, 'score': 39394.5952064991, 'total_duration': 60025.72388958931, 'accumulated_submission_time': 39394.5952064991, 'accumulated_eval_time': 20620.923574447632, 'accumulated_logging_time': 6.762216567993164}
I0306 02:42:32.665695 140098771031808 logging_writer.py:48] [122302] accumulated_eval_time=20620.923574, accumulated_logging_time=6.762217, accumulated_submission_time=39394.595206, global_step=122302, preemption_count=0, score=39394.595206, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275401, test/num_examples=43793, total_duration=60025.723890, train/accuracy=0.995615, train/loss=0.013791, train/mean_average_precision=0.786891, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293671, validation/num_examples=43793
I0306 02:43:04.257816 140105330788096 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.15207579731941223, loss=0.018904415890574455
I0306 02:43:36.890054 140098771031808 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.13510334491729736, loss=0.017201468348503113
I0306 02:44:09.084281 140105330788096 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.14763666689395905, loss=0.018727876245975494
I0306 02:44:41.491225 140098771031808 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.12335696816444397, loss=0.015094277448952198
I0306 02:45:14.092746 140105330788096 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.1431664377450943, loss=0.019871149212121964
I0306 02:45:46.206380 140098771031808 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.1591668725013733, loss=0.0185401514172554
I0306 02:46:18.439574 140105330788096 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.14917023479938507, loss=0.0179836917668581
I0306 02:46:32.811069 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:48:18.639109 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:48:21.739384 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:48:24.820475 140274064205632 submission_runner.py:411] Time since start: 60377.92s, 	Step: 123046, 	{'train/accuracy': 0.9955765604972839, 'train/loss': 0.013819707557559013, 'train/mean_average_precision': 0.7784860680421057, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29358608497086836, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2755150682884237, 'test/num_examples': 43793, 'score': 39634.70795702934, 'total_duration': 60377.917174339294, 'accumulated_submission_time': 39634.70795702934, 'accumulated_eval_time': 20732.932939767838, 'accumulated_logging_time': 6.812769651412964}
I0306 02:48:24.859383 140069608036096 logging_writer.py:48] [123046] accumulated_eval_time=20732.932940, accumulated_logging_time=6.812770, accumulated_submission_time=39634.707957, global_step=123046, preemption_count=0, score=39634.707957, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275515, test/num_examples=43793, total_duration=60377.917174, train/accuracy=0.995577, train/loss=0.013820, train/mean_average_precision=0.778486, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293586, validation/num_examples=43793
I0306 02:48:42.526061 140106748385024 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.14722977578639984, loss=0.01859528385102749
I0306 02:49:14.696725 140069608036096 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.1527983695268631, loss=0.0179627425968647
I0306 02:49:46.385652 140106748385024 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.13226822018623352, loss=0.01730802096426487
I0306 02:50:19.005589 140069608036096 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.15731866657733917, loss=0.019190115854144096
I0306 02:50:51.555455 140106748385024 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.15533269941806793, loss=0.019584890455007553
I0306 02:51:24.399898 140069608036096 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.1370275467634201, loss=0.018475525081157684
I0306 02:51:56.992220 140106748385024 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.14927825331687927, loss=0.018335292115807533
I0306 02:52:24.865380 140274064205632 spec.py:321] Evaluating on the training split.
I0306 02:54:12.645331 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 02:54:15.740499 140274064205632 spec.py:349] Evaluating on the test split.
I0306 02:54:18.797458 140274064205632 submission_runner.py:411] Time since start: 60731.89s, 	Step: 123787, 	{'train/accuracy': 0.9955735206604004, 'train/loss': 0.013852252624928951, 'train/mean_average_precision': 0.7843652857748288, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29367525398848493, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545836011994534, 'test/num_examples': 43793, 'score': 39874.68226027489, 'total_duration': 60731.89415550232, 'accumulated_submission_time': 39874.68226027489, 'accumulated_eval_time': 20846.864969968796, 'accumulated_logging_time': 6.862906455993652}
I0306 02:54:18.837819 140070338619136 logging_writer.py:48] [123787] accumulated_eval_time=20846.864970, accumulated_logging_time=6.862906, accumulated_submission_time=39874.682260, global_step=123787, preemption_count=0, score=39874.682260, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275458, test/num_examples=43793, total_duration=60731.894156, train/accuracy=0.995574, train/loss=0.013852, train/mean_average_precision=0.784365, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293675, validation/num_examples=43793
I0306 02:54:23.383565 140105330788096 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.14827311038970947, loss=0.020046379417181015
I0306 02:54:55.427710 140070338619136 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.14174993336200714, loss=0.015962792560458183
I0306 02:55:27.799840 140105330788096 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.18137921392917633, loss=0.017890624701976776
I0306 02:56:00.121096 140070338619136 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.13306815922260284, loss=0.016353145241737366
I0306 02:56:32.512705 140105330788096 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.14566533267498016, loss=0.01714904047548771
I0306 02:57:05.301491 140070338619136 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.13217884302139282, loss=0.017020879313349724
I0306 02:57:37.959856 140105330788096 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.14447715878486633, loss=0.017015211284160614
I0306 02:58:10.475125 140070338619136 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.15095868706703186, loss=0.019069038331508636
I0306 02:58:18.857276 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:00:05.542796 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:00:08.619410 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:00:11.641223 140274064205632 submission_runner.py:411] Time since start: 61084.74s, 	Step: 124526, 	{'train/accuracy': 0.995505154132843, 'train/loss': 0.014010931365191936, 'train/mean_average_precision': 0.7773680917507209, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29378857792668367, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27544656522941235, 'test/num_examples': 43793, 'score': 40114.67047047615, 'total_duration': 61084.737924575806, 'accumulated_submission_time': 40114.67047047615, 'accumulated_eval_time': 20959.648869991302, 'accumulated_logging_time': 6.913788318634033}
I0306 03:00:11.680466 140098771031808 logging_writer.py:48] [124526] accumulated_eval_time=20959.648870, accumulated_logging_time=6.913788, accumulated_submission_time=40114.670470, global_step=124526, preemption_count=0, score=40114.670470, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275447, test/num_examples=43793, total_duration=61084.737925, train/accuracy=0.995505, train/loss=0.014011, train/mean_average_precision=0.777368, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293789, validation/num_examples=43793
I0306 03:00:36.327924 140106748385024 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.11806775629520416, loss=0.01654844731092453
I0306 03:01:09.092876 140098771031808 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.16249896585941315, loss=0.01852494664490223
I0306 03:01:41.518236 140106748385024 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.1545468270778656, loss=0.02028629742562771
I0306 03:02:14.192780 140098771031808 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.14451009035110474, loss=0.01619219407439232
I0306 03:02:46.797397 140106748385024 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.14278095960617065, loss=0.01901364140212536
I0306 03:03:19.315079 140098771031808 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.14662952721118927, loss=0.016515295952558517
I0306 03:03:51.969256 140106748385024 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.15263395011425018, loss=0.016876230016350746
I0306 03:04:11.898452 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:05:59.942462 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:06:03.159920 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:06:06.196131 140274064205632 submission_runner.py:411] Time since start: 61439.29s, 	Step: 125262, 	{'train/accuracy': 0.9956053495407104, 'train/loss': 0.013742011971771717, 'train/mean_average_precision': 0.7890075856031153, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936749697469681, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753858426724248, 'test/num_examples': 43793, 'score': 40354.857283592224, 'total_duration': 61439.29281306267, 'accumulated_submission_time': 40354.857283592224, 'accumulated_eval_time': 21073.94648528099, 'accumulated_logging_time': 6.9637815952301025}
I0306 03:06:06.238004 140070338619136 logging_writer.py:48] [125262] accumulated_eval_time=21073.946485, accumulated_logging_time=6.963782, accumulated_submission_time=40354.857284, global_step=125262, preemption_count=0, score=40354.857284, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275386, test/num_examples=43793, total_duration=61439.292813, train/accuracy=0.995605, train/loss=0.013742, train/mean_average_precision=0.789008, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293675, validation/num_examples=43793
I0306 03:06:18.994877 140105330788096 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.13828398287296295, loss=0.016622917726635933
I0306 03:06:50.860657 140070338619136 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.1513543426990509, loss=0.019166478887200356
I0306 03:07:23.974633 140105330788096 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.15964637696743011, loss=0.018471289426088333
I0306 03:07:57.293757 140070338619136 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.1501702517271042, loss=0.01866099238395691
I0306 03:08:29.788335 140105330788096 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.1393979787826538, loss=0.018590660765767097
I0306 03:09:02.078032 140070338619136 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.1342094987630844, loss=0.01785700023174286
I0306 03:09:33.894863 140105330788096 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.15252487361431122, loss=0.017503155395388603
I0306 03:10:06.342558 140070338619136 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.15754298865795135, loss=0.014996763318777084
I0306 03:10:06.347659 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:11:53.711655 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:11:56.894455 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:11:59.954654 140274064205632 submission_runner.py:411] Time since start: 61793.05s, 	Step: 126001, 	{'train/accuracy': 0.9956063628196716, 'train/loss': 0.013819137588143349, 'train/mean_average_precision': 0.7756586723562215, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937366296262176, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2756016566885009, 'test/num_examples': 43793, 'score': 40594.93407726288, 'total_duration': 61793.051352500916, 'accumulated_submission_time': 40594.93407726288, 'accumulated_eval_time': 21187.55341076851, 'accumulated_logging_time': 7.017248630523682}
I0306 03:11:59.996176 140069608036096 logging_writer.py:48] [126001] accumulated_eval_time=21187.553411, accumulated_logging_time=7.017249, accumulated_submission_time=40594.934077, global_step=126001, preemption_count=0, score=40594.934077, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275602, test/num_examples=43793, total_duration=61793.051353, train/accuracy=0.995606, train/loss=0.013819, train/mean_average_precision=0.775659, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293737, validation/num_examples=43793
I0306 03:12:32.232168 140098771031808 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.15818148851394653, loss=0.01760786771774292
I0306 03:13:04.527129 140069608036096 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.1586218774318695, loss=0.01860109344124794
I0306 03:13:36.782314 140098771031808 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.142850860953331, loss=0.019553059712052345
I0306 03:14:08.900314 140069608036096 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.1482061743736267, loss=0.01669536530971527
I0306 03:14:41.256915 140098771031808 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.13273029029369354, loss=0.017978759482502937
I0306 03:15:13.272530 140069608036096 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.15747100114822388, loss=0.019856171682476997
I0306 03:15:45.548384 140098771031808 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.15029653906822205, loss=0.01760697178542614
I0306 03:15:59.977609 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:17:48.432365 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:17:51.503707 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:17:54.545969 140274064205632 submission_runner.py:411] Time since start: 62147.64s, 	Step: 126746, 	{'train/accuracy': 0.9955751299858093, 'train/loss': 0.013775649480521679, 'train/mean_average_precision': 0.7885662798229311, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29369742883210237, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27555574478509565, 'test/num_examples': 43793, 'score': 40834.8824300766, 'total_duration': 62147.64252591133, 'accumulated_submission_time': 40834.8824300766, 'accumulated_eval_time': 21302.12158560753, 'accumulated_logging_time': 7.071530342102051}
I0306 03:17:54.584391 140070338619136 logging_writer.py:48] [126746] accumulated_eval_time=21302.121586, accumulated_logging_time=7.071530, accumulated_submission_time=40834.882430, global_step=126746, preemption_count=0, score=40834.882430, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275556, test/num_examples=43793, total_duration=62147.642526, train/accuracy=0.995575, train/loss=0.013776, train/mean_average_precision=0.788566, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293697, validation/num_examples=43793
I0306 03:18:12.322479 140106748385024 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.1657075434923172, loss=0.018946407362818718
I0306 03:18:44.238249 140070338619136 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.16203267872333527, loss=0.018805721774697304
I0306 03:19:16.105225 140106748385024 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.14828179776668549, loss=0.01793181151151657
I0306 03:19:48.037077 140070338619136 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.15682849287986755, loss=0.020026907324790955
I0306 03:20:19.992162 140106748385024 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.1558944582939148, loss=0.017622146755456924
I0306 03:20:51.594106 140070338619136 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.158842995762825, loss=0.017794974148273468
I0306 03:21:23.612991 140106748385024 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.16423945128917694, loss=0.018847689032554626
I0306 03:21:54.844506 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:23:38.129823 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:23:41.237906 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:23:44.269395 140274064205632 submission_runner.py:411] Time since start: 62497.37s, 	Step: 127499, 	{'train/accuracy': 0.9956125020980835, 'train/loss': 0.013762624002993107, 'train/mean_average_precision': 0.7903543820948858, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935955037536901, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27544351533606226, 'test/num_examples': 43793, 'score': 41075.1099421978, 'total_duration': 62497.366092681885, 'accumulated_submission_time': 41075.1099421978, 'accumulated_eval_time': 21411.546427965164, 'accumulated_logging_time': 7.12211012840271}
I0306 03:23:44.309840 140098771031808 logging_writer.py:48] [127499] accumulated_eval_time=21411.546428, accumulated_logging_time=7.122110, accumulated_submission_time=41075.109942, global_step=127499, preemption_count=0, score=41075.109942, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275444, test/num_examples=43793, total_duration=62497.366093, train/accuracy=0.995613, train/loss=0.013763, train/mean_average_precision=0.790354, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293596, validation/num_examples=43793
I0306 03:23:44.991950 140105330788096 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.12831327319145203, loss=0.016775866970419884
I0306 03:24:17.361140 140098771031808 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.12980398535728455, loss=0.015238966792821884
I0306 03:24:49.620268 140105330788096 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.14909769594669342, loss=0.01737474650144577
I0306 03:25:21.636287 140098771031808 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.13920825719833374, loss=0.015558467246592045
I0306 03:25:53.699121 140105330788096 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.14797547459602356, loss=0.01771448366343975
I0306 03:26:26.189814 140098771031808 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.1430288404226303, loss=0.016523689031600952
I0306 03:26:57.926854 140105330788096 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.15015894174575806, loss=0.0173068568110466
I0306 03:27:29.537976 140098771031808 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.1480688750743866, loss=0.01718556508421898
I0306 03:27:44.387011 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:29:32.665048 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:29:35.751413 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:29:38.834225 140274064205632 submission_runner.py:411] Time since start: 62851.93s, 	Step: 128248, 	{'train/accuracy': 0.9954938292503357, 'train/loss': 0.013969525694847107, 'train/mean_average_precision': 0.771295520144599, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937692486438858, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754312230509766, 'test/num_examples': 43793, 'score': 41315.15427827835, 'total_duration': 62851.930923223495, 'accumulated_submission_time': 41315.15427827835, 'accumulated_eval_time': 21525.99359869957, 'accumulated_logging_time': 7.175124883651733}
I0306 03:29:38.874650 140070338619136 logging_writer.py:48] [128248] accumulated_eval_time=21525.993599, accumulated_logging_time=7.175125, accumulated_submission_time=41315.154278, global_step=128248, preemption_count=0, score=41315.154278, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275431, test/num_examples=43793, total_duration=62851.930923, train/accuracy=0.995494, train/loss=0.013970, train/mean_average_precision=0.771296, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293769, validation/num_examples=43793
I0306 03:29:55.846965 140106748385024 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.14221002161502838, loss=0.01827535405755043
I0306 03:30:28.266978 140070338619136 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.15351496636867523, loss=0.01911119371652603
I0306 03:31:00.678707 140106748385024 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.15106253325939178, loss=0.016934486106038094
I0306 03:31:32.804532 140070338619136 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.16326318681240082, loss=0.018867969512939453
I0306 03:32:05.098585 140106748385024 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.1376940757036209, loss=0.01708020456135273
I0306 03:32:37.400518 140070338619136 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.14120912551879883, loss=0.017418459057807922
I0306 03:33:09.987023 140106748385024 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.15552204847335815, loss=0.01821635290980339
I0306 03:33:39.106628 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:35:20.703284 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:35:23.771530 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:35:26.807624 140274064205632 submission_runner.py:411] Time since start: 63199.90s, 	Step: 128991, 	{'train/accuracy': 0.9955839514732361, 'train/loss': 0.013854644261300564, 'train/mean_average_precision': 0.7902642742906548, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29376465723257367, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27536277871109277, 'test/num_examples': 43793, 'score': 41555.35489320755, 'total_duration': 63199.904326200485, 'accumulated_submission_time': 41555.35489320755, 'accumulated_eval_time': 21633.69455242157, 'accumulated_logging_time': 7.226402759552002}
I0306 03:35:26.847640 140098771031808 logging_writer.py:48] [128991] accumulated_eval_time=21633.694552, accumulated_logging_time=7.226403, accumulated_submission_time=41555.354893, global_step=128991, preemption_count=0, score=41555.354893, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275363, test/num_examples=43793, total_duration=63199.904326, train/accuracy=0.995584, train/loss=0.013855, train/mean_average_precision=0.790264, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293765, validation/num_examples=43793
I0306 03:35:30.325864 140105330788096 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.156887948513031, loss=0.019725626334547997
I0306 03:36:02.926884 140098771031808 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.15159305930137634, loss=0.019137518480420113
I0306 03:36:35.147160 140105330788096 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.1457679271697998, loss=0.018327344208955765
I0306 03:37:07.178498 140098771031808 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.15485461056232452, loss=0.018585892394185066
I0306 03:37:39.685615 140105330788096 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.12667608261108398, loss=0.01744300313293934
I0306 03:38:12.050800 140098771031808 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.14145894348621368, loss=0.017456920817494392
I0306 03:38:44.480369 140105330788096 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.14740291237831116, loss=0.015391100198030472
I0306 03:39:16.749840 140098771031808 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.15159952640533447, loss=0.016233444213867188
I0306 03:39:27.033928 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:41:09.002420 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:41:12.159150 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:41:15.166185 140274064205632 submission_runner.py:411] Time since start: 63548.26s, 	Step: 129733, 	{'train/accuracy': 0.9956200122833252, 'train/loss': 0.01367952860891819, 'train/mean_average_precision': 0.7835308102014059, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29358548436822185, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2755378317641422, 'test/num_examples': 43793, 'score': 41795.50968170166, 'total_duration': 63548.26286840439, 'accumulated_submission_time': 41795.50968170166, 'accumulated_eval_time': 21741.826742887497, 'accumulated_logging_time': 7.277673959732056}
I0306 03:41:15.205875 140070338619136 logging_writer.py:48] [129733] accumulated_eval_time=21741.826743, accumulated_logging_time=7.277674, accumulated_submission_time=41795.509682, global_step=129733, preemption_count=0, score=41795.509682, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275538, test/num_examples=43793, total_duration=63548.262868, train/accuracy=0.995620, train/loss=0.013680, train/mean_average_precision=0.783531, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293585, validation/num_examples=43793
I0306 03:41:36.981050 140106748385024 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.12505744397640228, loss=0.015092183835804462
I0306 03:42:08.997582 140070338619136 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.13601945340633392, loss=0.016512880101799965
I0306 03:42:41.501496 140106748385024 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.14091335237026215, loss=0.016702620312571526
I0306 03:43:13.746847 140070338619136 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.14343130588531494, loss=0.018897565081715584
I0306 03:43:45.641640 140106748385024 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.16867417097091675, loss=0.018547700718045235
I0306 03:44:18.033813 140070338619136 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.13637781143188477, loss=0.017484527081251144
I0306 03:44:50.343067 140106748385024 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.12958672642707825, loss=0.018151383846998215
I0306 03:45:15.301953 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:46:58.412473 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:47:01.596607 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:47:04.616270 140274064205632 submission_runner.py:411] Time since start: 63897.71s, 	Step: 130478, 	{'train/accuracy': 0.995602548122406, 'train/loss': 0.013836919330060482, 'train/mean_average_precision': 0.7875081898725586, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936325389844466, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27544095072648866, 'test/num_examples': 43793, 'score': 42035.57419133186, 'total_duration': 63897.712966918945, 'accumulated_submission_time': 42035.57419133186, 'accumulated_eval_time': 21851.141013383865, 'accumulated_logging_time': 7.32834792137146}
I0306 03:47:04.656443 140069608036096 logging_writer.py:48] [130478] accumulated_eval_time=21851.141013, accumulated_logging_time=7.328348, accumulated_submission_time=42035.574191, global_step=130478, preemption_count=0, score=42035.574191, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275441, test/num_examples=43793, total_duration=63897.712967, train/accuracy=0.995603, train/loss=0.013837, train/mean_average_precision=0.787508, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293633, validation/num_examples=43793
I0306 03:47:11.969722 140098771031808 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.12951022386550903, loss=0.01515138614922762
I0306 03:47:44.156202 140069608036096 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.1360270380973816, loss=0.016752084717154503
I0306 03:48:16.215406 140098771031808 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.15684780478477478, loss=0.01945575699210167
I0306 03:48:48.326208 140069608036096 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.14854466915130615, loss=0.018062854185700417
I0306 03:49:19.992055 140098771031808 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.14844919741153717, loss=0.016359249129891396
I0306 03:49:52.033607 140069608036096 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.15636026859283447, loss=0.018213702365756035
I0306 03:50:24.044645 140098771031808 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.13160043954849243, loss=0.018541937693953514
I0306 03:50:55.839757 140069608036096 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.1514369249343872, loss=0.018519509583711624
I0306 03:51:04.617591 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:52:48.230717 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:52:51.304569 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:52:54.296919 140274064205632 submission_runner.py:411] Time since start: 64247.39s, 	Step: 131227, 	{'train/accuracy': 0.9955888390541077, 'train/loss': 0.013772375881671906, 'train/mean_average_precision': 0.7732472724957443, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935959974947021, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27528763746334267, 'test/num_examples': 43793, 'score': 42275.503985881805, 'total_duration': 64247.39361643791, 'accumulated_submission_time': 42275.503985881805, 'accumulated_eval_time': 21960.82028913498, 'accumulated_logging_time': 7.3794004917144775}
I0306 03:52:54.339301 140070338619136 logging_writer.py:48] [131227] accumulated_eval_time=21960.820289, accumulated_logging_time=7.379400, accumulated_submission_time=42275.503986, global_step=131227, preemption_count=0, score=42275.503986, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275288, test/num_examples=43793, total_duration=64247.393616, train/accuracy=0.995589, train/loss=0.013772, train/mean_average_precision=0.773247, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293596, validation/num_examples=43793
I0306 03:53:18.584719 140105330788096 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.13658875226974487, loss=0.01618320867419243
I0306 03:53:50.795289 140070338619136 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.1508757770061493, loss=0.017314540222287178
I0306 03:54:23.276879 140105330788096 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.139338418841362, loss=0.01872541941702366
I0306 03:54:55.545550 140070338619136 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.13697567582130432, loss=0.01766297221183777
I0306 03:55:27.713901 140105330788096 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.14227575063705444, loss=0.01707966811954975
I0306 03:56:00.268491 140070338619136 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.13779684901237488, loss=0.0170359518378973
I0306 03:56:32.826020 140105330788096 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.1272510439157486, loss=0.016005517914891243
I0306 03:56:54.529108 140274064205632 spec.py:321] Evaluating on the training split.
I0306 03:58:40.850601 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 03:58:43.883989 140274064205632 spec.py:349] Evaluating on the test split.
I0306 03:58:46.999601 140274064205632 submission_runner.py:411] Time since start: 64600.10s, 	Step: 131969, 	{'train/accuracy': 0.9955713152885437, 'train/loss': 0.013815264217555523, 'train/mean_average_precision': 0.7818256717297449, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29364612410320456, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545812414523846, 'test/num_examples': 43793, 'score': 42515.66183280945, 'total_duration': 64600.09630036354, 'accumulated_submission_time': 42515.66183280945, 'accumulated_eval_time': 22073.290736675262, 'accumulated_logging_time': 7.432871103286743}
I0306 03:58:47.041092 140069608036096 logging_writer.py:48] [131969] accumulated_eval_time=22073.290737, accumulated_logging_time=7.432871, accumulated_submission_time=42515.661833, global_step=131969, preemption_count=0, score=42515.661833, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275458, test/num_examples=43793, total_duration=64600.096300, train/accuracy=0.995571, train/loss=0.013815, train/mean_average_precision=0.781826, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293646, validation/num_examples=43793
I0306 03:58:57.449315 140106748385024 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.1378300040960312, loss=0.016856634989380836
I0306 03:59:29.177662 140069608036096 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.1360453963279724, loss=0.01640288718044758
I0306 04:00:00.918926 140106748385024 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.14693892002105713, loss=0.017048627138137817
I0306 04:00:32.924112 140069608036096 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.14518201351165771, loss=0.017310069873929024
I0306 04:01:04.591757 140106748385024 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.14180268347263336, loss=0.018417460843920708
I0306 04:01:36.455498 140069608036096 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.14243827760219574, loss=0.016329903155565262
I0306 04:02:08.196799 140106748385024 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.16074402630329132, loss=0.01799817755818367
I0306 04:02:40.415748 140069608036096 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.15426069498062134, loss=0.01735379360616207
I0306 04:02:47.190758 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:04:31.164553 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:04:34.226905 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:04:37.238678 140274064205632 submission_runner.py:411] Time since start: 64950.34s, 	Step: 132722, 	{'train/accuracy': 0.9955384135246277, 'train/loss': 0.013938110321760178, 'train/mean_average_precision': 0.7809306588978733, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29380634166648867, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27542396694242016, 'test/num_examples': 43793, 'score': 42755.7799987793, 'total_duration': 64950.33537364006, 'accumulated_submission_time': 42755.7799987793, 'accumulated_eval_time': 22183.3386054039, 'accumulated_logging_time': 7.485567808151245}
I0306 04:04:37.278604 140098771031808 logging_writer.py:48] [132722] accumulated_eval_time=22183.338605, accumulated_logging_time=7.485568, accumulated_submission_time=42755.779999, global_step=132722, preemption_count=0, score=42755.779999, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275424, test/num_examples=43793, total_duration=64950.335374, train/accuracy=0.995538, train/loss=0.013938, train/mean_average_precision=0.780931, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293806, validation/num_examples=43793
I0306 04:05:02.786715 140105330788096 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.16859889030456543, loss=0.01766008697450161
I0306 04:05:34.491630 140098771031808 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.13992206752300262, loss=0.018227623775601387
I0306 04:06:06.639076 140105330788096 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.13502119481563568, loss=0.016674349084496498
I0306 04:06:38.095367 140098771031808 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.17403656244277954, loss=0.01911194436252117
I0306 04:07:10.111402 140105330788096 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.16037322580814362, loss=0.01906302012503147
I0306 04:07:41.990772 140098771031808 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.1491919755935669, loss=0.01819099672138691
I0306 04:08:13.940893 140105330788096 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.1415373682975769, loss=0.01654781959950924
I0306 04:08:37.493590 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:10:19.011547 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:10:22.190401 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:10:25.186517 140274064205632 submission_runner.py:411] Time since start: 65298.28s, 	Step: 133475, 	{'train/accuracy': 0.9956251978874207, 'train/loss': 0.01368892751634121, 'train/mean_average_precision': 0.7866224635436307, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2939696343293295, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27542172741490273, 'test/num_examples': 43793, 'score': 42995.96339964867, 'total_duration': 65298.283219099045, 'accumulated_submission_time': 42995.96339964867, 'accumulated_eval_time': 22291.031487464905, 'accumulated_logging_time': 7.536278247833252}
I0306 04:10:25.227804 140069608036096 logging_writer.py:48] [133475] accumulated_eval_time=22291.031487, accumulated_logging_time=7.536278, accumulated_submission_time=42995.963400, global_step=133475, preemption_count=0, score=42995.963400, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275422, test/num_examples=43793, total_duration=65298.283219, train/accuracy=0.995625, train/loss=0.013689, train/mean_average_precision=0.786622, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293970, validation/num_examples=43793
I0306 04:10:33.456785 140070338619136 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.17709840834140778, loss=0.0184335857629776
I0306 04:11:05.549364 140069608036096 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.14056386053562164, loss=0.016929438337683678
I0306 04:11:37.241409 140070338619136 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.14274877309799194, loss=0.01738772913813591
I0306 04:12:09.410012 140069608036096 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.14433832466602325, loss=0.01748792454600334
I0306 04:12:41.373337 140070338619136 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.12901943922042847, loss=0.016910269856452942
I0306 04:13:13.438954 140069608036096 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.14362893998622894, loss=0.016170479357242584
I0306 04:13:45.447432 140070338619136 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.15579119324684143, loss=0.018486592918634415
I0306 04:14:18.159248 140069608036096 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.15128912031650543, loss=0.018795153126120567
I0306 04:14:25.401597 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:16:06.674765 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:16:09.857762 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:16:13.214690 140274064205632 submission_runner.py:411] Time since start: 65646.31s, 	Step: 134223, 	{'train/accuracy': 0.9955657124519348, 'train/loss': 0.013913251459598541, 'train/mean_average_precision': 0.785629117844441, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937110096408846, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545909816177305, 'test/num_examples': 43793, 'score': 43236.10581612587, 'total_duration': 65646.3113667965, 'accumulated_submission_time': 43236.10581612587, 'accumulated_eval_time': 22398.84451198578, 'accumulated_logging_time': 7.58860969543457}
I0306 04:16:13.266864 140098771031808 logging_writer.py:48] [134223] accumulated_eval_time=22398.844512, accumulated_logging_time=7.588610, accumulated_submission_time=43236.105816, global_step=134223, preemption_count=0, score=43236.105816, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275459, test/num_examples=43793, total_duration=65646.311367, train/accuracy=0.995566, train/loss=0.013913, train/mean_average_precision=0.785629, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293711, validation/num_examples=43793
I0306 04:16:38.979451 140105330788096 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.1258227378129959, loss=0.016715850681066513
I0306 04:17:11.659932 140098771031808 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.14407667517662048, loss=0.017797617241740227
I0306 04:17:43.944860 140105330788096 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.14029817283153534, loss=0.017594892531633377
I0306 04:18:16.206175 140098771031808 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.152931347489357, loss=0.01931101642549038
I0306 04:18:48.721086 140105330788096 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.15537984669208527, loss=0.020135633647441864
I0306 04:19:21.373891 140098771031808 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.12489704042673111, loss=0.01668531447649002
I0306 04:19:54.140669 140105330788096 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.13030971586704254, loss=0.016259120777249336
I0306 04:20:13.527740 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:22:01.383396 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:22:04.845548 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:22:08.168974 140274064205632 submission_runner.py:411] Time since start: 66001.27s, 	Step: 134960, 	{'train/accuracy': 0.9955911636352539, 'train/loss': 0.013804896734654903, 'train/mean_average_precision': 0.7821396458418919, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935877750821879, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754662314445237, 'test/num_examples': 43793, 'score': 43476.32939147949, 'total_duration': 66001.26565551758, 'accumulated_submission_time': 43476.32939147949, 'accumulated_eval_time': 22513.485692977905, 'accumulated_logging_time': 7.6529457569122314}
I0306 04:22:08.215265 140069608036096 logging_writer.py:48] [134960] accumulated_eval_time=22513.485693, accumulated_logging_time=7.652946, accumulated_submission_time=43476.329391, global_step=134960, preemption_count=0, score=43476.329391, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275466, test/num_examples=43793, total_duration=66001.265656, train/accuracy=0.995591, train/loss=0.013805, train/mean_average_precision=0.782140, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293588, validation/num_examples=43793
I0306 04:22:21.407278 140106748385024 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.14499437808990479, loss=0.019420744851231575
I0306 04:22:53.436021 140069608036096 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.14769943058490753, loss=0.018480923026800156
I0306 04:23:25.759731 140106748385024 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.1495177000761032, loss=0.018065404146909714
I0306 04:23:57.858783 140069608036096 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.1356743723154068, loss=0.01759922131896019
I0306 04:24:30.101457 140106748385024 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.14765042066574097, loss=0.018394529819488525
I0306 04:25:02.214846 140069608036096 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.13866829872131348, loss=0.016274956986308098
I0306 04:25:34.467725 140106748385024 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.15752968192100525, loss=0.019633490592241287
I0306 04:26:07.095455 140069608036096 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.13159513473510742, loss=0.018154868856072426
I0306 04:26:08.169261 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:27:51.647309 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:27:54.700251 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:27:57.766124 140274064205632 submission_runner.py:411] Time since start: 66350.86s, 	Step: 135704, 	{'train/accuracy': 0.9955680966377258, 'train/loss': 0.013828875496983528, 'train/mean_average_precision': 0.7725323100158961, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938210133629313, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27557567554154977, 'test/num_examples': 43793, 'score': 43716.24765229225, 'total_duration': 66350.86281728745, 'accumulated_submission_time': 43716.24765229225, 'accumulated_eval_time': 22623.082501411438, 'accumulated_logging_time': 7.711076021194458}
I0306 04:27:57.806845 140098771031808 logging_writer.py:48] [135704] accumulated_eval_time=22623.082501, accumulated_logging_time=7.711076, accumulated_submission_time=43716.247652, global_step=135704, preemption_count=0, score=43716.247652, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275576, test/num_examples=43793, total_duration=66350.862817, train/accuracy=0.995568, train/loss=0.013829, train/mean_average_precision=0.772532, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293821, validation/num_examples=43793
I0306 04:28:28.904770 140105330788096 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.15404802560806274, loss=0.01710209995508194
I0306 04:29:01.373424 140098771031808 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.1342693716287613, loss=0.015471306629478931
I0306 04:29:33.392697 140105330788096 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.134457066655159, loss=0.017815740779042244
I0306 04:30:05.259117 140098771031808 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.13475967943668365, loss=0.018773041665554047
I0306 04:30:37.278503 140105330788096 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.14823079109191895, loss=0.01767738349735737
I0306 04:31:09.213179 140098771031808 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.13553547859191895, loss=0.016083447262644768
I0306 04:31:41.202776 140105330788096 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.1490003913640976, loss=0.01788133569061756
I0306 04:31:57.823994 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:33:43.046884 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:33:46.154109 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:33:49.196840 140274064205632 submission_runner.py:411] Time since start: 66702.29s, 	Step: 136453, 	{'train/accuracy': 0.9955561757087708, 'train/loss': 0.013866182416677475, 'train/mean_average_precision': 0.7835748448083539, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936474347516125, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753951118399199, 'test/num_examples': 43793, 'score': 43956.23304724693, 'total_duration': 66702.2934141159, 'accumulated_submission_time': 43956.23304724693, 'accumulated_eval_time': 22734.455174207687, 'accumulated_logging_time': 7.762632131576538}
I0306 04:33:49.239068 140069608036096 logging_writer.py:48] [136453] accumulated_eval_time=22734.455174, accumulated_logging_time=7.762632, accumulated_submission_time=43956.233047, global_step=136453, preemption_count=0, score=43956.233047, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275395, test/num_examples=43793, total_duration=66702.293414, train/accuracy=0.995556, train/loss=0.013866, train/mean_average_precision=0.783575, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293647, validation/num_examples=43793
I0306 04:34:04.687100 140070338619136 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.14913488924503326, loss=0.018028048798441887
I0306 04:34:36.702471 140069608036096 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.145656555891037, loss=0.01783965528011322
I0306 04:35:08.841707 140070338619136 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.13278080523014069, loss=0.01718393713235855
I0306 04:35:40.636982 140069608036096 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.14772288501262665, loss=0.01654094271361828
I0306 04:36:12.588868 140070338619136 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.13175010681152344, loss=0.015128460712730885
I0306 04:36:44.746598 140069608036096 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.15081898868083954, loss=0.019060485064983368
I0306 04:37:16.799396 140070338619136 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.15989090502262115, loss=0.017788834869861603
I0306 04:37:49.046015 140069608036096 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.17104877531528473, loss=0.018297119066119194
I0306 04:37:49.363722 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:39:33.512603 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:39:36.588918 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:39:39.660376 140274064205632 submission_runner.py:411] Time since start: 67052.76s, 	Step: 137202, 	{'train/accuracy': 0.9955800175666809, 'train/loss': 0.01379280723631382, 'train/mean_average_precision': 0.7885792245747008, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936389094944352, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27549574004320715, 'test/num_examples': 43793, 'score': 44196.325745821, 'total_duration': 67052.75707435608, 'accumulated_submission_time': 44196.325745821, 'accumulated_eval_time': 22844.751775741577, 'accumulated_logging_time': 7.815975904464722}
I0306 04:39:39.702985 140105330788096 logging_writer.py:48] [137202] accumulated_eval_time=22844.751776, accumulated_logging_time=7.815976, accumulated_submission_time=44196.325746, global_step=137202, preemption_count=0, score=44196.325746, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275496, test/num_examples=43793, total_duration=67052.757074, train/accuracy=0.995580, train/loss=0.013793, train/mean_average_precision=0.788579, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293639, validation/num_examples=43793
I0306 04:40:11.732938 140106748385024 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.16377301514148712, loss=0.017202705144882202
I0306 04:40:44.086977 140105330788096 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.1409020721912384, loss=0.01596122980117798
I0306 04:41:16.270833 140106748385024 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.1476655900478363, loss=0.019248461350798607
I0306 04:41:48.278977 140105330788096 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.15950454771518707, loss=0.015439349226653576
I0306 04:42:20.415037 140106748385024 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.13437628746032715, loss=0.01637035422027111
I0306 04:42:52.680590 140105330788096 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.15159320831298828, loss=0.0176065843552351
I0306 04:43:25.370002 140106748385024 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.1446205973625183, loss=0.016807595267891884
I0306 04:43:39.753034 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:45:23.694030 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:45:26.799467 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:45:29.851274 140274064205632 submission_runner.py:411] Time since start: 67402.95s, 	Step: 137945, 	{'train/accuracy': 0.9956175088882446, 'train/loss': 0.01379086822271347, 'train/mean_average_precision': 0.7859297351781429, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29364455355712793, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754197051375271, 'test/num_examples': 43793, 'score': 44436.34388566017, 'total_duration': 67402.94797158241, 'accumulated_submission_time': 44436.34388566017, 'accumulated_eval_time': 22954.84996652603, 'accumulated_logging_time': 7.869782209396362}
I0306 04:45:29.894552 140069608036096 logging_writer.py:48] [137945] accumulated_eval_time=22954.849967, accumulated_logging_time=7.869782, accumulated_submission_time=44436.343886, global_step=137945, preemption_count=0, score=44436.343886, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275420, test/num_examples=43793, total_duration=67402.947972, train/accuracy=0.995618, train/loss=0.013791, train/mean_average_precision=0.785930, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293645, validation/num_examples=43793
I0306 04:45:48.256201 140098771031808 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.16228091716766357, loss=0.019061638042330742
I0306 04:46:20.421061 140069608036096 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.15679284930229187, loss=0.019643118605017662
I0306 04:46:52.415149 140098771031808 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.1405126005411148, loss=0.016621552407741547
I0306 04:47:24.568869 140069608036096 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.16141954064369202, loss=0.01982394978404045
I0306 04:47:56.222800 140098771031808 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.15490704774856567, loss=0.017412152141332626
I0306 04:48:28.432278 140069608036096 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.14973071217536926, loss=0.018192101269960403
I0306 04:49:00.496069 140098771031808 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.15012072026729584, loss=0.016523879021406174
I0306 04:49:30.094545 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:51:14.377376 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:51:17.463235 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:51:20.498009 140274064205632 submission_runner.py:411] Time since start: 67753.59s, 	Step: 138693, 	{'train/accuracy': 0.9955361485481262, 'train/loss': 0.013945689424872398, 'train/mean_average_precision': 0.7821278781897788, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938877070849096, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2755277708514029, 'test/num_examples': 43793, 'score': 44676.512374162674, 'total_duration': 67753.59470915794, 'accumulated_submission_time': 44676.512374162674, 'accumulated_eval_time': 23065.253385066986, 'accumulated_logging_time': 7.923959970474243}
I0306 04:51:20.539425 140105330788096 logging_writer.py:48] [138693] accumulated_eval_time=23065.253385, accumulated_logging_time=7.923960, accumulated_submission_time=44676.512374, global_step=138693, preemption_count=0, score=44676.512374, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275528, test/num_examples=43793, total_duration=67753.594709, train/accuracy=0.995536, train/loss=0.013946, train/mean_average_precision=0.782128, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293888, validation/num_examples=43793
I0306 04:51:23.088842 140106748385024 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.13367204368114471, loss=0.018233131617307663
I0306 04:51:55.156645 140105330788096 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.1545678675174713, loss=0.017022723332047462
I0306 04:52:27.453865 140106748385024 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.142213836312294, loss=0.017651459202170372
I0306 04:52:59.488818 140105330788096 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.1416502743959427, loss=0.016624843701720238
I0306 04:53:31.507492 140106748385024 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.12488316744565964, loss=0.01593123748898506
I0306 04:54:03.640629 140105330788096 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.1438729614019394, loss=0.01893506944179535
I0306 04:54:35.714320 140106748385024 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.16752709448337555, loss=0.01975288800895214
I0306 04:55:07.835952 140105330788096 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.14651715755462646, loss=0.01831534132361412
I0306 04:55:20.556133 140274064205632 spec.py:321] Evaluating on the training split.
I0306 04:57:04.143536 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 04:57:07.252224 140274064205632 spec.py:349] Evaluating on the test split.
I0306 04:57:10.287987 140274064205632 submission_runner.py:411] Time since start: 68103.38s, 	Step: 139441, 	{'train/accuracy': 0.9956095814704895, 'train/loss': 0.013687281869351864, 'train/mean_average_precision': 0.7837841741459687, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937139301409759, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754072478578473, 'test/num_examples': 43793, 'score': 44916.496895074844, 'total_duration': 68103.3846859932, 'accumulated_submission_time': 44916.496895074844, 'accumulated_eval_time': 23174.985191106796, 'accumulated_logging_time': 7.976691007614136}
I0306 04:57:10.331213 140069608036096 logging_writer.py:48] [139441] accumulated_eval_time=23174.985191, accumulated_logging_time=7.976691, accumulated_submission_time=44916.496895, global_step=139441, preemption_count=0, score=44916.496895, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275407, test/num_examples=43793, total_duration=68103.384686, train/accuracy=0.995610, train/loss=0.013687, train/mean_average_precision=0.783784, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293714, validation/num_examples=43793
I0306 04:57:29.745116 140098771031808 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.135707825422287, loss=0.016098205000162125
I0306 04:58:01.542299 140069608036096 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.13858287036418915, loss=0.016757715493440628
I0306 04:58:33.626184 140098771031808 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.15559867024421692, loss=0.01733565516769886
I0306 04:59:06.038815 140069608036096 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.17564615607261658, loss=0.0186927430331707
I0306 04:59:37.798666 140098771031808 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.16361573338508606, loss=0.019106648862361908
I0306 05:00:09.910049 140069608036096 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.1563975214958191, loss=0.01890946365892887
I0306 05:00:42.270704 140098771031808 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.1542663425207138, loss=0.02005692943930626
I0306 05:01:10.380958 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:02:56.700777 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:03:00.065056 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:03:03.578471 140274064205632 submission_runner.py:411] Time since start: 68456.68s, 	Step: 140189, 	{'train/accuracy': 0.9955382347106934, 'train/loss': 0.013940371572971344, 'train/mean_average_precision': 0.7720245902031437, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935594829764194, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27535955131986983, 'test/num_examples': 43793, 'score': 45156.51519060135, 'total_duration': 68456.67514777184, 'accumulated_submission_time': 45156.51519060135, 'accumulated_eval_time': 23288.182639360428, 'accumulated_logging_time': 8.0308096408844}
I0306 05:03:03.628686 140105330788096 logging_writer.py:48] [140189] accumulated_eval_time=23288.182639, accumulated_logging_time=8.030810, accumulated_submission_time=45156.515191, global_step=140189, preemption_count=0, score=45156.515191, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275360, test/num_examples=43793, total_duration=68456.675148, train/accuracy=0.995538, train/loss=0.013940, train/mean_average_precision=0.772025, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293559, validation/num_examples=43793
I0306 05:03:07.602424 140106748385024 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.1414903700351715, loss=0.018028108403086662
I0306 05:03:40.202290 140105330788096 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.1466233730316162, loss=0.01782754808664322
I0306 05:04:13.146575 140106748385024 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.1529221534729004, loss=0.021209916099905968
I0306 05:04:46.020808 140105330788096 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.1431712806224823, loss=0.017449598759412766
I0306 05:05:18.531119 140106748385024 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.14508172869682312, loss=0.01924443058669567
I0306 05:05:51.213760 140105330788096 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.1507955640554428, loss=0.017789097502827644
I0306 05:06:24.375316 140106748385024 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.14583095908164978, loss=0.017806438729166985
I0306 05:06:57.333477 140105330788096 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.13763456046581268, loss=0.016599532216787338
I0306 05:07:03.655882 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:08:48.195254 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:08:51.587494 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:08:54.988155 140274064205632 submission_runner.py:411] Time since start: 68808.08s, 	Step: 140920, 	{'train/accuracy': 0.9955587983131409, 'train/loss': 0.013841639272868633, 'train/mean_average_precision': 0.7846010270148847, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938899450060268, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545348421146504, 'test/num_examples': 43793, 'score': 45396.50525188446, 'total_duration': 68808.08483695984, 'accumulated_submission_time': 45396.50525188446, 'accumulated_eval_time': 23399.51488018036, 'accumulated_logging_time': 8.09301209449768}
I0306 05:08:55.033957 140069608036096 logging_writer.py:48] [140920] accumulated_eval_time=23399.514880, accumulated_logging_time=8.093012, accumulated_submission_time=45396.505252, global_step=140920, preemption_count=0, score=45396.505252, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275453, test/num_examples=43793, total_duration=68808.084837, train/accuracy=0.995559, train/loss=0.013842, train/mean_average_precision=0.784601, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293890, validation/num_examples=43793
I0306 05:09:21.300462 140070338619136 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.1450442373752594, loss=0.01742497645318508
I0306 05:09:53.813066 140069608036096 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.13822931051254272, loss=0.017183266580104828
I0306 05:10:26.524507 140070338619136 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.15013976395130157, loss=0.016877537593245506
I0306 05:10:58.422736 140069608036096 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.16268226504325867, loss=0.017683804035186768
I0306 05:11:30.381296 140070338619136 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.18048067390918732, loss=0.022051168605685234
I0306 05:12:02.947378 140069608036096 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.1597268134355545, loss=0.017813604325056076
I0306 05:12:35.653534 140070338619136 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.14549876749515533, loss=0.018725458532571793
I0306 05:12:55.059343 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:14:38.917554 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:14:41.992461 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:14:44.995887 140274064205632 submission_runner.py:411] Time since start: 69158.09s, 	Step: 141660, 	{'train/accuracy': 0.9956005215644836, 'train/loss': 0.013840829953551292, 'train/mean_average_precision': 0.7835544456428272, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357281894132325, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754512949348238, 'test/num_examples': 43793, 'score': 45636.494839668274, 'total_duration': 69158.09258151054, 'accumulated_submission_time': 45636.494839668274, 'accumulated_eval_time': 23509.45137310028, 'accumulated_logging_time': 8.151569604873657}
I0306 05:14:45.037847 140105330788096 logging_writer.py:48] [141660] accumulated_eval_time=23509.451373, accumulated_logging_time=8.151570, accumulated_submission_time=45636.494840, global_step=141660, preemption_count=0, score=45636.494840, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275451, test/num_examples=43793, total_duration=69158.092582, train/accuracy=0.995601, train/loss=0.013841, train/mean_average_precision=0.783554, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293573, validation/num_examples=43793
I0306 05:14:57.963415 140106748385024 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.14105145633220673, loss=0.017449351027607918
I0306 05:15:29.856721 140105330788096 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.13712668418884277, loss=0.01686703786253929
I0306 05:16:01.488117 140106748385024 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.14014932513237, loss=0.01683228276669979
I0306 05:16:33.171675 140105330788096 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.1519339382648468, loss=0.016971390694379807
I0306 05:17:05.044860 140106748385024 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.15088026225566864, loss=0.018723418936133385
I0306 05:17:37.345116 140105330788096 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.1508234441280365, loss=0.019449997693300247
I0306 05:18:09.800724 140106748385024 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.13407635688781738, loss=0.01663806475698948
I0306 05:18:41.570292 140105330788096 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.17914415895938873, loss=0.0200805701315403
I0306 05:18:45.147749 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:20:25.885312 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:20:28.959883 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:20:32.016841 140274064205632 submission_runner.py:411] Time since start: 69505.11s, 	Step: 142412, 	{'train/accuracy': 0.9955870509147644, 'train/loss': 0.013802574947476387, 'train/mean_average_precision': 0.7837377125709595, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2938052738879229, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754350355381713, 'test/num_examples': 43793, 'score': 45876.57214689255, 'total_duration': 69505.11353802681, 'accumulated_submission_time': 45876.57214689255, 'accumulated_eval_time': 23616.320420265198, 'accumulated_logging_time': 8.205451011657715}
I0306 05:20:32.060932 140069608036096 logging_writer.py:48] [142412] accumulated_eval_time=23616.320420, accumulated_logging_time=8.205451, accumulated_submission_time=45876.572147, global_step=142412, preemption_count=0, score=45876.572147, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275435, test/num_examples=43793, total_duration=69505.113538, train/accuracy=0.995587, train/loss=0.013803, train/mean_average_precision=0.783738, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293805, validation/num_examples=43793
I0306 05:21:01.329719 140098771031808 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.14247572422027588, loss=0.01737302355468273
I0306 05:21:33.365046 140069608036096 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.14368192851543427, loss=0.01752466894686222
I0306 05:22:05.481630 140098771031808 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.1590694636106491, loss=0.01787491887807846
I0306 05:22:37.211955 140069608036096 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.15056930482387543, loss=0.018939929082989693
I0306 05:23:09.349081 140098771031808 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.13890652358531952, loss=0.016623331233859062
I0306 05:23:41.007149 140069608036096 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.15316125750541687, loss=0.01850409246981144
I0306 05:24:12.950016 140098771031808 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.15501703321933746, loss=0.019073570147156715
I0306 05:24:32.325735 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:26:12.639590 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:26:15.695538 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:26:18.721051 140274064205632 submission_runner.py:411] Time since start: 69851.82s, 	Step: 143162, 	{'train/accuracy': 0.9956026077270508, 'train/loss': 0.013733400963246822, 'train/mean_average_precision': 0.7859749685301147, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29392198738272085, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2753971686265937, 'test/num_examples': 43793, 'score': 46116.80425333977, 'total_duration': 69851.81775379181, 'accumulated_submission_time': 46116.80425333977, 'accumulated_eval_time': 23722.715693950653, 'accumulated_logging_time': 8.261404752731323}
I0306 05:26:18.762859 140070338619136 logging_writer.py:48] [143162] accumulated_eval_time=23722.715694, accumulated_logging_time=8.261405, accumulated_submission_time=46116.804253, global_step=143162, preemption_count=0, score=46116.804253, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275397, test/num_examples=43793, total_duration=69851.817754, train/accuracy=0.995603, train/loss=0.013733, train/mean_average_precision=0.785975, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293922, validation/num_examples=43793
I0306 05:26:31.254644 140105330788096 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.14912854135036469, loss=0.01845661550760269
I0306 05:27:03.164017 140070338619136 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.15152710676193237, loss=0.017295747995376587
I0306 05:27:35.487662 140105330788096 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.13966608047485352, loss=0.016204072162508965
I0306 05:28:07.900183 140070338619136 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.14891360700130463, loss=0.01835416816174984
I0306 05:28:39.756052 140105330788096 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.15321271121501923, loss=0.018193136900663376
I0306 05:29:11.564311 140070338619136 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.15632563829421997, loss=0.020259728655219078
I0306 05:29:43.428927 140105330788096 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.13355672359466553, loss=0.01721920445561409
I0306 05:30:15.597398 140070338619136 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.13538463413715363, loss=0.0164384413510561
I0306 05:30:18.859624 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:32:01.142066 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:32:04.312486 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:32:07.355776 140274064205632 submission_runner.py:411] Time since start: 70200.45s, 	Step: 143911, 	{'train/accuracy': 0.9955673217773438, 'train/loss': 0.013866626657545567, 'train/mean_average_precision': 0.7774714317441656, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937292203462356, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754010988589716, 'test/num_examples': 43793, 'score': 46356.86944055557, 'total_duration': 70200.4524781704, 'accumulated_submission_time': 46356.86944055557, 'accumulated_eval_time': 23831.21180343628, 'accumulated_logging_time': 8.31418228149414}
I0306 05:32:07.398651 140098771031808 logging_writer.py:48] [143911] accumulated_eval_time=23831.211803, accumulated_logging_time=8.314182, accumulated_submission_time=46356.869441, global_step=143911, preemption_count=0, score=46356.869441, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275401, test/num_examples=43793, total_duration=70200.452478, train/accuracy=0.995567, train/loss=0.013867, train/mean_average_precision=0.777471, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293729, validation/num_examples=43793
I0306 05:32:36.439251 140106748385024 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.1283079832792282, loss=0.015600290149450302
I0306 05:33:08.690345 140098771031808 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.16063827276229858, loss=0.01860184222459793
I0306 05:33:40.767780 140106748385024 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.14364834129810333, loss=0.018385061994194984
I0306 05:34:12.724805 140098771031808 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.13854935765266418, loss=0.01740836910903454
I0306 05:34:44.556598 140106748385024 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.14567582309246063, loss=0.018932411447167397
I0306 05:35:16.651397 140098771031808 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.153145894408226, loss=0.01801418326795101
I0306 05:35:48.879168 140106748385024 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.1256161779165268, loss=0.01413941103965044
I0306 05:36:07.459829 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:37:53.370823 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:37:56.406131 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:37:59.382206 140274064205632 submission_runner.py:411] Time since start: 70552.48s, 	Step: 144659, 	{'train/accuracy': 0.995541512966156, 'train/loss': 0.013875114731490612, 'train/mean_average_precision': 0.7791450319607318, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937038021491862, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27547128193346315, 'test/num_examples': 43793, 'score': 46596.899097919464, 'total_duration': 70552.47891068459, 'accumulated_submission_time': 46596.899097919464, 'accumulated_eval_time': 23943.13414955139, 'accumulated_logging_time': 8.367965698242188}
I0306 05:37:59.425127 140069608036096 logging_writer.py:48] [144659] accumulated_eval_time=23943.134150, accumulated_logging_time=8.367966, accumulated_submission_time=46596.899098, global_step=144659, preemption_count=0, score=46596.899098, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275471, test/num_examples=43793, total_duration=70552.478911, train/accuracy=0.995542, train/loss=0.013875, train/mean_average_precision=0.779145, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293704, validation/num_examples=43793
I0306 05:38:12.823022 140105330788096 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.14951181411743164, loss=0.017391592264175415
I0306 05:38:44.660555 140069608036096 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.1309727132320404, loss=0.017035329714417458
I0306 05:39:16.481168 140105330788096 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.13642527163028717, loss=0.017704660072922707
I0306 05:39:48.305854 140069608036096 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.13622517883777618, loss=0.016658641397953033
I0306 05:40:20.187495 140105330788096 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.15736421942710876, loss=0.01782739721238613
I0306 05:40:52.037710 140069608036096 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.15351502597332, loss=0.016905488446354866
I0306 05:41:24.626896 140105330788096 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.14941184222698212, loss=0.016410263255238533
I0306 05:41:58.652529 140069608036096 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.1480390429496765, loss=0.018576961010694504
I0306 05:41:59.722164 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:43:39.776083 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:43:42.845048 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:43:45.943350 140274064205632 submission_runner.py:411] Time since start: 70899.04s, 	Step: 145404, 	{'train/accuracy': 0.9955787062644958, 'train/loss': 0.013851949013769627, 'train/mean_average_precision': 0.7859469751789931, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29359242689372045, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27542165930047185, 'test/num_examples': 43793, 'score': 46837.16341853142, 'total_duration': 70899.0400402546, 'accumulated_submission_time': 46837.16341853142, 'accumulated_eval_time': 24049.355276346207, 'accumulated_logging_time': 8.423321008682251}
I0306 05:43:45.985986 140070338619136 logging_writer.py:48] [145404] accumulated_eval_time=24049.355276, accumulated_logging_time=8.423321, accumulated_submission_time=46837.163419, global_step=145404, preemption_count=0, score=46837.163419, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275422, test/num_examples=43793, total_duration=70899.040040, train/accuracy=0.995579, train/loss=0.013852, train/mean_average_precision=0.785947, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293592, validation/num_examples=43793
I0306 05:44:17.162517 140106748385024 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.17600546777248383, loss=0.021012937650084496
I0306 05:44:48.729922 140070338619136 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.16191968321800232, loss=0.017499929293990135
I0306 05:45:20.750765 140106748385024 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.1392669677734375, loss=0.01525941863656044
I0306 05:45:52.682517 140070338619136 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.13902094960212708, loss=0.017547350376844406
I0306 05:46:24.796467 140106748385024 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.1417846530675888, loss=0.01563810557126999
I0306 05:46:56.628144 140070338619136 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.1320897340774536, loss=0.01549567747861147
I0306 05:47:28.179018 140106748385024 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.14525239169597626, loss=0.01677473820745945
I0306 05:47:46.171471 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:49:27.748019 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:49:30.836169 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:49:33.846739 140274064205632 submission_runner.py:411] Time since start: 71246.94s, 	Step: 146157, 	{'train/accuracy': 0.9956125020980835, 'train/loss': 0.013777256943285465, 'train/mean_average_precision': 0.7824314669110136, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937127948568798, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27543133740506254, 'test/num_examples': 43793, 'score': 47077.318110466, 'total_duration': 71246.9433298111, 'accumulated_submission_time': 47077.318110466, 'accumulated_eval_time': 24157.030386209488, 'accumulated_logging_time': 8.47667145729065}
I0306 05:49:33.889987 140098771031808 logging_writer.py:48] [146157] accumulated_eval_time=24157.030386, accumulated_logging_time=8.476671, accumulated_submission_time=47077.318110, global_step=146157, preemption_count=0, score=47077.318110, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275431, test/num_examples=43793, total_duration=71246.943330, train/accuracy=0.995613, train/loss=0.013777, train/mean_average_precision=0.782431, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293713, validation/num_examples=43793
I0306 05:49:48.016783 140105330788096 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.1275441199541092, loss=0.016486311331391335
I0306 05:50:19.999010 140098771031808 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.16603665053844452, loss=0.019511723890900612
I0306 05:50:52.194095 140105330788096 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.1529291570186615, loss=0.018061311915516853
I0306 05:51:24.155163 140098771031808 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.13296620547771454, loss=0.016970202326774597
I0306 05:51:56.238087 140105330788096 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.13916032016277313, loss=0.0162802767008543
I0306 05:52:27.956609 140098771031808 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.14211979508399963, loss=0.016934696584939957
I0306 05:52:59.896685 140105330788096 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.1473063826560974, loss=0.018628504127264023
I0306 05:53:31.744417 140098771031808 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.15246731042861938, loss=0.018704786896705627
I0306 05:53:33.971133 140274064205632 spec.py:321] Evaluating on the training split.
I0306 05:55:15.272272 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 05:55:18.367875 140274064205632 spec.py:349] Evaluating on the test split.
I0306 05:55:21.391672 140274064205632 submission_runner.py:411] Time since start: 71594.49s, 	Step: 146908, 	{'train/accuracy': 0.9955742955207825, 'train/loss': 0.013849351555109024, 'train/mean_average_precision': 0.7841609706569808, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29368617660047164, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27549886536208884, 'test/num_examples': 43793, 'score': 47317.36692094803, 'total_duration': 71594.48837161064, 'accumulated_submission_time': 47317.36692094803, 'accumulated_eval_time': 24264.45087337494, 'accumulated_logging_time': 8.5319983959198}
I0306 05:55:21.436571 140070338619136 logging_writer.py:48] [146908] accumulated_eval_time=24264.450873, accumulated_logging_time=8.531998, accumulated_submission_time=47317.366921, global_step=146908, preemption_count=0, score=47317.366921, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275499, test/num_examples=43793, total_duration=71594.488372, train/accuracy=0.995574, train/loss=0.013849, train/mean_average_precision=0.784161, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293686, validation/num_examples=43793
I0306 05:55:51.222135 140106748385024 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.1578514128923416, loss=0.016735197976231575
I0306 05:56:23.843946 140070338619136 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.13642379641532898, loss=0.016811853274703026
I0306 05:56:56.855627 140106748385024 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.15127044916152954, loss=0.01779880002140999
I0306 05:57:29.324292 140070338619136 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.147178515791893, loss=0.01831887662410736
I0306 05:58:01.512238 140106748385024 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.13159504532814026, loss=0.01634105108678341
I0306 05:58:33.369988 140070338619136 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.14334793388843536, loss=0.017388006672263145
I0306 05:59:05.416076 140106748385024 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.14116433262825012, loss=0.01872044801712036
I0306 05:59:21.577806 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:01:07.314003 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:01:10.459810 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:01:13.488656 140274064205632 submission_runner.py:411] Time since start: 71946.59s, 	Step: 147650, 	{'train/accuracy': 0.9956088066101074, 'train/loss': 0.013758858665823936, 'train/mean_average_precision': 0.7803652558375634, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936537150876402, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27549868397283156, 'test/num_examples': 43793, 'score': 47557.4769847393, 'total_duration': 71946.58534765244, 'accumulated_submission_time': 47557.4769847393, 'accumulated_eval_time': 24376.361690044403, 'accumulated_logging_time': 8.58792233467102}
I0306 06:01:13.532018 140098771031808 logging_writer.py:48] [147650] accumulated_eval_time=24376.361690, accumulated_logging_time=8.587922, accumulated_submission_time=47557.476985, global_step=147650, preemption_count=0, score=47557.476985, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275499, test/num_examples=43793, total_duration=71946.585348, train/accuracy=0.995609, train/loss=0.013759, train/mean_average_precision=0.780365, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293654, validation/num_examples=43793
I0306 06:01:30.070455 140105330788096 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.15073464810848236, loss=0.017237674444913864
I0306 06:02:02.588753 140098771031808 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.13100245594978333, loss=0.01780228689312935
I0306 06:02:34.849882 140105330788096 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.12189997732639313, loss=0.016054391860961914
I0306 06:03:07.025469 140098771031808 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.14056800305843353, loss=0.019639115780591965
I0306 06:03:38.704349 140105330788096 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.14188750088214874, loss=0.016368355602025986
I0306 06:04:11.029666 140098771031808 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.1669386327266693, loss=0.01875166967511177
I0306 06:04:43.235889 140105330788096 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.1433071345090866, loss=0.017014948651194572
I0306 06:05:13.712001 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:06:53.232777 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:06:56.348872 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:06:59.379822 140274064205632 submission_runner.py:411] Time since start: 72292.48s, 	Step: 148396, 	{'train/accuracy': 0.9955193400382996, 'train/loss': 0.013936838135123253, 'train/mean_average_precision': 0.7758290701438132, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29371317474723335, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754836519611033, 'test/num_examples': 43793, 'score': 47797.62417554855, 'total_duration': 72292.47641682625, 'accumulated_submission_time': 47797.62417554855, 'accumulated_eval_time': 24482.029361486435, 'accumulated_logging_time': 8.643598794937134}
I0306 06:06:59.423527 140070338619136 logging_writer.py:48] [148396] accumulated_eval_time=24482.029361, accumulated_logging_time=8.643599, accumulated_submission_time=47797.624176, global_step=148396, preemption_count=0, score=47797.624176, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275484, test/num_examples=43793, total_duration=72292.476417, train/accuracy=0.995519, train/loss=0.013937, train/mean_average_precision=0.775829, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293713, validation/num_examples=43793
I0306 06:07:01.093698 140106748385024 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.1358851045370102, loss=0.01705705001950264
I0306 06:07:33.120753 140070338619136 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.14638441801071167, loss=0.016961555927991867
I0306 06:08:05.082465 140106748385024 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.12873272597789764, loss=0.01565999537706375
I0306 06:08:37.351620 140070338619136 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.1527457982301712, loss=0.017536811530590057
I0306 06:09:09.068504 140106748385024 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.15302295982837677, loss=0.017716098576784134
I0306 06:09:40.760590 140070338619136 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.14822173118591309, loss=0.017798684537410736
I0306 06:10:12.331356 140106748385024 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.14772199094295502, loss=0.016972480341792107
I0306 06:10:43.969721 140070338619136 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.14627501368522644, loss=0.017452022060751915
I0306 06:10:59.447082 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:12:41.612474 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:12:44.716642 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:12:47.711651 140274064205632 submission_runner.py:411] Time since start: 72640.81s, 	Step: 149150, 	{'train/accuracy': 0.9955896735191345, 'train/loss': 0.01373444963246584, 'train/mean_average_precision': 0.7878894749972761, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29357858805323184, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27541762620872334, 'test/num_examples': 43793, 'score': 48037.61570096016, 'total_duration': 72640.80835223198, 'accumulated_submission_time': 48037.61570096016, 'accumulated_eval_time': 24590.29389500618, 'accumulated_logging_time': 8.698626518249512}
I0306 06:12:47.755274 140069608036096 logging_writer.py:48] [149150] accumulated_eval_time=24590.293895, accumulated_logging_time=8.698627, accumulated_submission_time=48037.615701, global_step=149150, preemption_count=0, score=48037.615701, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275418, test/num_examples=43793, total_duration=72640.808352, train/accuracy=0.995590, train/loss=0.013734, train/mean_average_precision=0.787889, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293579, validation/num_examples=43793
I0306 06:13:04.220631 140098771031808 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.1397499293088913, loss=0.016679158434271812
I0306 06:13:36.039053 140069608036096 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.14193302392959595, loss=0.01691948063671589
I0306 06:14:08.375253 140098771031808 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.14708444476127625, loss=0.017839152365922928
I0306 06:14:39.952856 140069608036096 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.14880472421646118, loss=0.0185789056122303
I0306 06:15:11.824779 140098771031808 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.12844760715961456, loss=0.01575062796473503
I0306 06:15:43.540774 140069608036096 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.1718050092458725, loss=0.019102443009614944
I0306 06:16:15.265476 140098771031808 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.1542683243751526, loss=0.019783293828368187
I0306 06:16:46.994483 140069608036096 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.14896909892559052, loss=0.017377180978655815
I0306 06:16:47.935845 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:18:28.090978 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:18:31.197570 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:18:34.240108 140274064205632 submission_runner.py:411] Time since start: 72987.34s, 	Step: 149904, 	{'train/accuracy': 0.9956307411193848, 'train/loss': 0.013773547485470772, 'train/mean_average_precision': 0.7877537249143864, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29370767761027056, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2754609112657465, 'test/num_examples': 43793, 'score': 48277.7650783062, 'total_duration': 72987.3367960453, 'accumulated_submission_time': 48277.7650783062, 'accumulated_eval_time': 24696.59809613228, 'accumulated_logging_time': 8.752968549728394}
I0306 06:18:34.283601 140070338619136 logging_writer.py:48] [149904] accumulated_eval_time=24696.598096, accumulated_logging_time=8.752969, accumulated_submission_time=48277.765078, global_step=149904, preemption_count=0, score=48277.765078, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275461, test/num_examples=43793, total_duration=72987.336796, train/accuracy=0.995631, train/loss=0.013774, train/mean_average_precision=0.787754, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293708, validation/num_examples=43793
I0306 06:19:05.105975 140105330788096 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.14621882140636444, loss=0.018297133967280388
I0306 06:19:36.757642 140070338619136 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.16746844351291656, loss=0.018356498330831528
I0306 06:20:08.702750 140105330788096 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.14874012768268585, loss=0.01788235642015934
I0306 06:20:40.511029 140070338619136 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.13569249212741852, loss=0.016637425869703293
I0306 06:21:12.097109 140105330788096 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.1268843412399292, loss=0.01677422970533371
I0306 06:21:43.562073 140070338619136 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.1423858255147934, loss=0.016972918063402176
I0306 06:22:15.551949 140105330788096 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.14475585520267487, loss=0.01728031411767006
I0306 06:22:34.435953 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:24:19.890807 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:24:23.013602 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:24:26.066476 140274064205632 submission_runner.py:411] Time since start: 73339.16s, 	Step: 150660, 	{'train/accuracy': 0.9955337047576904, 'train/loss': 0.013952258042991161, 'train/mean_average_precision': 0.7795977649702918, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937253985372344, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27538938403001906, 'test/num_examples': 43793, 'score': 48517.886543273926, 'total_duration': 73339.16317415237, 'accumulated_submission_time': 48517.886543273926, 'accumulated_eval_time': 24808.22857093811, 'accumulated_logging_time': 8.807228088378906}
I0306 06:24:26.110090 140098771031808 logging_writer.py:48] [150660] accumulated_eval_time=24808.228571, accumulated_logging_time=8.807228, accumulated_submission_time=48517.886543, global_step=150660, preemption_count=0, score=48517.886543, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275389, test/num_examples=43793, total_duration=73339.163174, train/accuracy=0.995534, train/loss=0.013952, train/mean_average_precision=0.779598, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293725, validation/num_examples=43793
I0306 06:24:39.488301 140106748385024 logging_writer.py:48] [150700] global_step=150700, grad_norm=0.14568129181861877, loss=0.017097482457756996
I0306 06:25:11.657547 140098771031808 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.14854459464550018, loss=0.017201421782374382
I0306 06:25:43.946009 140106748385024 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.13788816332817078, loss=0.015863187611103058
I0306 06:26:16.617314 140098771031808 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.13696318864822388, loss=0.01658787578344345
I0306 06:26:48.355364 140106748385024 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.14096170663833618, loss=0.01768760196864605
I0306 06:27:20.768193 140098771031808 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.14758814871311188, loss=0.015015143901109695
I0306 06:27:52.630350 140106748385024 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.12447024881839752, loss=0.015351679176092148
I0306 06:28:24.828128 140098771031808 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.1651347577571869, loss=0.01882290653884411
I0306 06:28:26.082694 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:30:07.364770 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:30:10.456675 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:30:13.556975 140274064205632 submission_runner.py:411] Time since start: 73686.65s, 	Step: 151405, 	{'train/accuracy': 0.9956150054931641, 'train/loss': 0.01366223581135273, 'train/mean_average_precision': 0.7890784196348725, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29369649556453126, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754052718551408, 'test/num_examples': 43793, 'score': 48757.82743740082, 'total_duration': 73686.6535577774, 'accumulated_submission_time': 48757.82743740082, 'accumulated_eval_time': 24915.70268535614, 'accumulated_logging_time': 8.862153768539429}
I0306 06:30:13.606067 140070338619136 logging_writer.py:48] [151405] accumulated_eval_time=24915.702685, accumulated_logging_time=8.862154, accumulated_submission_time=48757.827437, global_step=151405, preemption_count=0, score=48757.827437, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275405, test/num_examples=43793, total_duration=73686.653558, train/accuracy=0.995615, train/loss=0.013662, train/mean_average_precision=0.789078, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293696, validation/num_examples=43793
I0306 06:30:44.434872 140105330788096 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.1340917944908142, loss=0.01679212972521782
I0306 06:31:16.369229 140070338619136 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.14528019726276398, loss=0.01638229750096798
I0306 06:31:47.691367 140105330788096 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.1407487392425537, loss=0.01718108542263508
I0306 06:32:19.316050 140070338619136 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.15215396881103516, loss=0.017767461016774178
I0306 06:32:50.885230 140105330788096 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.1457435041666031, loss=0.017966555431485176
I0306 06:33:22.196727 140070338619136 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.13793639838695526, loss=0.017183277755975723
I0306 06:33:53.736183 140105330788096 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.14439883828163147, loss=0.017067350447177887
I0306 06:34:13.686849 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:35:58.555010 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:36:01.749504 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:36:04.741220 140274064205632 submission_runner.py:411] Time since start: 74037.84s, 	Step: 152164, 	{'train/accuracy': 0.995550274848938, 'train/loss': 0.013902689330279827, 'train/mean_average_precision': 0.7697975941964679, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937655585787352, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.27540996332355727, 'test/num_examples': 43793, 'score': 48997.56856369972, 'total_duration': 74037.83790445328, 'accumulated_submission_time': 48997.56856369972, 'accumulated_eval_time': 25026.75702905655, 'accumulated_logging_time': 9.230566263198853}
I0306 06:36:04.786157 140069608036096 logging_writer.py:48] [152164] accumulated_eval_time=25026.757029, accumulated_logging_time=9.230566, accumulated_submission_time=48997.568564, global_step=152164, preemption_count=0, score=48997.568564, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275410, test/num_examples=43793, total_duration=74037.837904, train/accuracy=0.995550, train/loss=0.013903, train/mean_average_precision=0.769798, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293766, validation/num_examples=43793
I0306 06:36:16.693936 140098771031808 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.16483137011528015, loss=0.019553933292627335
I0306 06:36:48.687961 140069608036096 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.1413196474313736, loss=0.016775013878941536
I0306 06:37:20.992593 140098771031808 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.1465339958667755, loss=0.01812243089079857
I0306 06:37:53.190163 140069608036096 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.14114858210086823, loss=0.019268857315182686
I0306 06:38:24.986661 140098771031808 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.1557741016149521, loss=0.019940802827477455
I0306 06:38:56.776008 140069608036096 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.13759872317314148, loss=0.017129618674516678
I0306 06:39:28.660105 140098771031808 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.1335708498954773, loss=0.017048297449946404
I0306 06:40:00.070437 140069608036096 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.12678374350070953, loss=0.01602308265864849
I0306 06:40:04.813173 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:41:49.578114 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:41:52.697173 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:41:55.737229 140274064205632 submission_runner.py:411] Time since start: 74388.83s, 	Step: 152916, 	{'train/accuracy': 0.995542049407959, 'train/loss': 0.013924401253461838, 'train/mean_average_precision': 0.7826677908654057, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936824436732723, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27544598526293385, 'test/num_examples': 43793, 'score': 49237.56392478943, 'total_duration': 74388.833922863, 'accumulated_submission_time': 49237.56392478943, 'accumulated_eval_time': 25137.68102812767, 'accumulated_logging_time': 9.286834478378296}
I0306 06:41:55.781197 140105330788096 logging_writer.py:48] [152916] accumulated_eval_time=25137.681028, accumulated_logging_time=9.286834, accumulated_submission_time=49237.563925, global_step=152916, preemption_count=0, score=49237.563925, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275446, test/num_examples=43793, total_duration=74388.833923, train/accuracy=0.995542, train/loss=0.013924, train/mean_average_precision=0.782668, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293682, validation/num_examples=43793
I0306 06:42:22.749185 140106748385024 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.15616737306118011, loss=0.018189752474427223
I0306 06:42:54.368533 140105330788096 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.1648440957069397, loss=0.0178889911621809
I0306 06:43:26.100307 140106748385024 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.14335690438747406, loss=0.01911400444805622
I0306 06:43:57.901713 140105330788096 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.14715974032878876, loss=0.016566202044487
I0306 06:44:29.712221 140106748385024 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.17338037490844727, loss=0.019658390432596207
I0306 06:45:01.192844 140105330788096 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.14416319131851196, loss=0.018651552498340607
I0306 06:45:33.008516 140106748385024 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.16090163588523865, loss=0.016772497445344925
I0306 06:45:55.944423 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:47:37.636227 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:47:40.719507 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:47:43.688621 140274064205632 submission_runner.py:411] Time since start: 74736.79s, 	Step: 153672, 	{'train/accuracy': 0.9956010580062866, 'train/loss': 0.013804982416331768, 'train/mean_average_precision': 0.7902975906992156, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29370899465701694, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753901263563581, 'test/num_examples': 43793, 'score': 49477.695827007294, 'total_duration': 74736.78531956673, 'accumulated_submission_time': 49477.695827007294, 'accumulated_eval_time': 25245.425184965134, 'accumulated_logging_time': 9.341866731643677}
I0306 06:47:43.732849 140070338619136 logging_writer.py:48] [153672] accumulated_eval_time=25245.425185, accumulated_logging_time=9.341867, accumulated_submission_time=49477.695827, global_step=153672, preemption_count=0, score=49477.695827, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275390, test/num_examples=43793, total_duration=74736.785320, train/accuracy=0.995601, train/loss=0.013805, train/mean_average_precision=0.790298, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293709, validation/num_examples=43793
I0306 06:47:53.244259 140098771031808 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.12525109946727753, loss=0.015135165303945541
I0306 06:48:25.046504 140070338619136 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.1330515742301941, loss=0.017937811091542244
I0306 06:48:56.602101 140098771031808 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.1405949890613556, loss=0.01815369911491871
I0306 06:49:28.152681 140070338619136 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.15140636265277863, loss=0.018566591665148735
I0306 06:49:59.522817 140098771031808 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.1472155898809433, loss=0.017401760444045067
I0306 06:50:31.100209 140070338619136 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.15939375758171082, loss=0.02018202841281891
I0306 06:51:02.573990 140098771031808 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.1357181966304779, loss=0.015552585944533348
I0306 06:51:34.090356 140070338619136 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.14868603646755219, loss=0.01813853718340397
I0306 06:51:43.748658 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:53:20.687643 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:53:23.794149 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:53:26.796540 140274064205632 submission_runner.py:411] Time since start: 75079.89s, 	Step: 154432, 	{'train/accuracy': 0.9955745935440063, 'train/loss': 0.013863848522305489, 'train/mean_average_precision': 0.7789048942518366, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29363508562743035, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.27532541955223416, 'test/num_examples': 43793, 'score': 49717.68036675453, 'total_duration': 75079.89323925972, 'accumulated_submission_time': 49717.68036675453, 'accumulated_eval_time': 25348.473020792007, 'accumulated_logging_time': 9.397128582000732}
I0306 06:53:26.841175 140069608036096 logging_writer.py:48] [154432] accumulated_eval_time=25348.473021, accumulated_logging_time=9.397129, accumulated_submission_time=49717.680367, global_step=154432, preemption_count=0, score=49717.680367, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275325, test/num_examples=43793, total_duration=75079.893239, train/accuracy=0.995575, train/loss=0.013864, train/mean_average_precision=0.778905, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293635, validation/num_examples=43793
I0306 06:53:49.724633 140105330788096 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.13919299840927124, loss=0.01585385389626026
I0306 06:54:21.632496 140069608036096 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.1601637899875641, loss=0.017393222078680992
I0306 06:54:53.560517 140105330788096 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.14702217280864716, loss=0.018198151141405106
I0306 06:55:25.777372 140069608036096 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.1299980878829956, loss=0.015733953565359116
I0306 06:55:57.830364 140105330788096 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.14231576025485992, loss=0.01816823147237301
I0306 06:56:29.921855 140069608036096 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.14061874151229858, loss=0.01684555783867836
I0306 06:57:01.845407 140105330788096 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.13609038293361664, loss=0.01728004775941372
I0306 06:57:26.868632 140274064205632 spec.py:321] Evaluating on the training split.
I0306 06:59:11.336640 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 06:59:14.430269 140274064205632 spec.py:349] Evaluating on the test split.
I0306 06:59:17.429991 140274064205632 submission_runner.py:411] Time since start: 75430.53s, 	Step: 155180, 	{'train/accuracy': 0.995599091053009, 'train/loss': 0.013690123334527016, 'train/mean_average_precision': 0.7854078741192276, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29372967307398046, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27545196921004517, 'test/num_examples': 43793, 'score': 49957.67692351341, 'total_duration': 75430.52669262886, 'accumulated_submission_time': 49957.67692351341, 'accumulated_eval_time': 25459.034336566925, 'accumulated_logging_time': 9.452574968338013}
I0306 06:59:17.475433 140098771031808 logging_writer.py:48] [155180] accumulated_eval_time=25459.034337, accumulated_logging_time=9.452575, accumulated_submission_time=49957.676924, global_step=155180, preemption_count=0, score=49957.676924, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275452, test/num_examples=43793, total_duration=75430.526693, train/accuracy=0.995599, train/loss=0.013690, train/mean_average_precision=0.785408, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293730, validation/num_examples=43793
I0306 06:59:24.243008 140106748385024 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.1528446227312088, loss=0.018989814445376396
I0306 06:59:56.192838 140098771031808 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.1319834142923355, loss=0.016296232119202614
I0306 07:00:28.077650 140106748385024 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.15297570824623108, loss=0.017716214060783386
I0306 07:00:59.939157 140098771031808 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.1359654664993286, loss=0.015171492472290993
I0306 07:01:32.014086 140106748385024 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.14705722033977509, loss=0.01714445650577545
I0306 07:02:04.239806 140098771031808 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.1478986293077469, loss=0.018220843747258186
I0306 07:02:36.085149 140106748385024 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.15092648565769196, loss=0.01695135049521923
I0306 07:03:07.732106 140098771031808 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.1417546272277832, loss=0.016632787883281708
I0306 07:03:17.491415 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:05:04.352535 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:05:07.429160 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:05:10.427584 140274064205632 submission_runner.py:411] Time since start: 75783.52s, 	Step: 155931, 	{'train/accuracy': 0.9955906271934509, 'train/loss': 0.013853933662176132, 'train/mean_average_precision': 0.773038709091723, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29372625054724166, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754514664735991, 'test/num_examples': 43793, 'score': 50197.66117596626, 'total_duration': 75783.52428460121, 'accumulated_submission_time': 50197.66117596626, 'accumulated_eval_time': 25571.970460414886, 'accumulated_logging_time': 9.509182453155518}
I0306 07:05:10.472769 140069608036096 logging_writer.py:48] [155931] accumulated_eval_time=25571.970460, accumulated_logging_time=9.509182, accumulated_submission_time=50197.661176, global_step=155931, preemption_count=0, score=50197.661176, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275451, test/num_examples=43793, total_duration=75783.524285, train/accuracy=0.995591, train/loss=0.013854, train/mean_average_precision=0.773039, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293726, validation/num_examples=43793
I0306 07:05:33.166008 140105330788096 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.15067367255687714, loss=0.019854364916682243
I0306 07:06:05.088409 140069608036096 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.13017144799232483, loss=0.016410939395427704
I0306 07:06:37.010053 140105330788096 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.14723464846611023, loss=0.017117921262979507
I0306 07:07:08.525084 140069608036096 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.13329912722110748, loss=0.015131775289773941
I0306 07:07:40.923682 140105330788096 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.15445055067539215, loss=0.018612539395689964
I0306 07:08:13.550253 140069608036096 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.16139298677444458, loss=0.01872006244957447
I0306 07:08:46.101358 140105330788096 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.15118400752544403, loss=0.018211565911769867
I0306 07:09:10.684159 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:10:50.906411 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:10:53.926468 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:10:56.911924 140274064205632 submission_runner.py:411] Time since start: 76130.01s, 	Step: 156677, 	{'train/accuracy': 0.9955081939697266, 'train/loss': 0.013971901498734951, 'train/mean_average_precision': 0.777331023418733, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29366832340797644, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27538891994642595, 'test/num_examples': 43793, 'score': 50437.83835029602, 'total_duration': 76130.00862145424, 'accumulated_submission_time': 50437.83835029602, 'accumulated_eval_time': 25678.198181152344, 'accumulated_logging_time': 9.566497564315796}
I0306 07:10:56.958290 140098771031808 logging_writer.py:48] [156677] accumulated_eval_time=25678.198181, accumulated_logging_time=9.566498, accumulated_submission_time=50437.838350, global_step=156677, preemption_count=0, score=50437.838350, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275389, test/num_examples=43793, total_duration=76130.008621, train/accuracy=0.995508, train/loss=0.013972, train/mean_average_precision=0.777331, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293668, validation/num_examples=43793
I0306 07:11:04.735735 140106748385024 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.15127822756767273, loss=0.019013535231351852
I0306 07:11:36.432292 140098771031808 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.14436587691307068, loss=0.018160874024033546
I0306 07:12:08.082221 140106748385024 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.14253458380699158, loss=0.01838650368154049
I0306 07:12:39.914135 140098771031808 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.1564725637435913, loss=0.01895434781908989
I0306 07:13:11.960041 140106748385024 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.14646190404891968, loss=0.017956815659999847
I0306 07:13:43.534764 140098771031808 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.13937821984291077, loss=0.018451442942023277
I0306 07:14:15.338377 140106748385024 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.14564085006713867, loss=0.018107276409864426
I0306 07:14:46.833003 140098771031808 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.13179351389408112, loss=0.016655638813972473
I0306 07:14:56.949992 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:16:36.145895 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:16:39.215717 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:16:42.335134 140274064205632 submission_runner.py:411] Time since start: 76475.43s, 	Step: 157433, 	{'train/accuracy': 0.9956135749816895, 'train/loss': 0.013793391175568104, 'train/mean_average_precision': 0.7892423439032692, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29371795474800005, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753973492261922, 'test/num_examples': 43793, 'score': 50677.798704624176, 'total_duration': 76475.43181467056, 'accumulated_submission_time': 50677.798704624176, 'accumulated_eval_time': 25783.583258152008, 'accumulated_logging_time': 9.623881101608276}
I0306 07:16:42.381656 140070338619136 logging_writer.py:48] [157433] accumulated_eval_time=25783.583258, accumulated_logging_time=9.623881, accumulated_submission_time=50677.798705, global_step=157433, preemption_count=0, score=50677.798705, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275397, test/num_examples=43793, total_duration=76475.431815, train/accuracy=0.995614, train/loss=0.013793, train/mean_average_precision=0.789242, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293718, validation/num_examples=43793
I0306 07:17:04.154094 140105330788096 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.14866872131824493, loss=0.019681118428707123
I0306 07:17:35.737629 140070338619136 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.14618882536888123, loss=0.01964709535241127
I0306 07:18:07.612060 140105330788096 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.13859109580516815, loss=0.017218735069036484
I0306 07:18:40.015786 140070338619136 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.14537972211837769, loss=0.01884552836418152
I0306 07:19:12.273587 140105330788096 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.16321082413196564, loss=0.01896301656961441
I0306 07:19:44.459580 140070338619136 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.126762256026268, loss=0.017641430720686913
I0306 07:20:16.624552 140105330788096 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.1522006392478943, loss=0.018656181171536446
I0306 07:20:42.584479 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:22:26.359397 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:22:29.503337 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:22:32.539126 140274064205632 submission_runner.py:411] Time since start: 76825.64s, 	Step: 158182, 	{'train/accuracy': 0.9956179261207581, 'train/loss': 0.013740847818553448, 'train/mean_average_precision': 0.7845530195903894, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936882202342486, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275407186043599, 'test/num_examples': 43793, 'score': 50917.96872997284, 'total_duration': 76825.63581681252, 'accumulated_submission_time': 50917.96872997284, 'accumulated_eval_time': 25893.5378510952, 'accumulated_logging_time': 9.682755708694458}
I0306 07:22:32.583765 140098771031808 logging_writer.py:48] [158182] accumulated_eval_time=25893.537851, accumulated_logging_time=9.682756, accumulated_submission_time=50917.968730, global_step=158182, preemption_count=0, score=50917.968730, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275407, test/num_examples=43793, total_duration=76825.635817, train/accuracy=0.995618, train/loss=0.013741, train/mean_average_precision=0.784553, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293688, validation/num_examples=43793
I0306 07:22:38.690680 140106748385024 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.14736631512641907, loss=0.018445566296577454
I0306 07:23:10.779095 140098771031808 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.1412786841392517, loss=0.016413521021604538
I0306 07:23:42.687685 140106748385024 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.13915587961673737, loss=0.01595446467399597
I0306 07:24:15.124637 140098771031808 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.13356421887874603, loss=0.015393604524433613
I0306 07:24:47.165688 140106748385024 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.14614659547805786, loss=0.017685474827885628
I0306 07:25:19.482582 140098771031808 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.1446995586156845, loss=0.015443931333720684
I0306 07:25:51.505938 140106748385024 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.14748337864875793, loss=0.0184146948158741
I0306 07:26:23.824358 140098771031808 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.1275523453950882, loss=0.01789667457342148
I0306 07:26:32.864050 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:28:15.590266 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:28:18.627229 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:28:21.628446 140274064205632 submission_runner.py:411] Time since start: 77174.73s, 	Step: 158929, 	{'train/accuracy': 0.9955463409423828, 'train/loss': 0.013887043111026287, 'train/mean_average_precision': 0.7822689544969321, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936670853993187, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754564632069592, 'test/num_examples': 43793, 'score': 51158.216289520264, 'total_duration': 77174.72514629364, 'accumulated_submission_time': 51158.216289520264, 'accumulated_eval_time': 26002.302206754684, 'accumulated_logging_time': 9.739495277404785}
I0306 07:28:21.673974 140070338619136 logging_writer.py:48] [158929] accumulated_eval_time=26002.302207, accumulated_logging_time=9.739495, accumulated_submission_time=51158.216290, global_step=158929, preemption_count=0, score=51158.216290, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275456, test/num_examples=43793, total_duration=77174.725146, train/accuracy=0.995546, train/loss=0.013887, train/mean_average_precision=0.782269, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293667, validation/num_examples=43793
I0306 07:28:44.753085 140105330788096 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.13571122288703918, loss=0.01577143929898739
I0306 07:29:16.888286 140070338619136 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.15661849081516266, loss=0.018584538251161575
I0306 07:29:48.632999 140105330788096 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.1410420536994934, loss=0.015384017489850521
I0306 07:30:20.441370 140070338619136 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.1503860205411911, loss=0.018724989145994186
I0306 07:30:51.820355 140105330788096 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.16289347410202026, loss=0.01864599995315075
I0306 07:31:23.530061 140070338619136 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.13729186356067657, loss=0.017612038180232048
I0306 07:31:55.184937 140105330788096 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.15052160620689392, loss=0.018789708614349365
I0306 07:32:21.779123 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:34:07.311809 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:34:10.365988 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:34:13.336746 140274064205632 submission_runner.py:411] Time since start: 77526.43s, 	Step: 159684, 	{'train/accuracy': 0.9955745339393616, 'train/loss': 0.013808333314955235, 'train/mean_average_precision': 0.7875565170609959, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936776762052396, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754188429540947, 'test/num_examples': 43793, 'score': 51398.28981542587, 'total_duration': 77526.43344187737, 'accumulated_submission_time': 51398.28981542587, 'accumulated_eval_time': 26113.859795093536, 'accumulated_logging_time': 9.795924425125122}
I0306 07:34:13.383262 140069608036096 logging_writer.py:48] [159684] accumulated_eval_time=26113.859795, accumulated_logging_time=9.795924, accumulated_submission_time=51398.289815, global_step=159684, preemption_count=0, score=51398.289815, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275419, test/num_examples=43793, total_duration=77526.433442, train/accuracy=0.995575, train/loss=0.013808, train/mean_average_precision=0.787557, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293678, validation/num_examples=43793
I0306 07:34:18.924499 140106748385024 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.1470380574464798, loss=0.016919391229748726
I0306 07:34:51.142242 140069608036096 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.15937040746212006, loss=0.0192523542791605
I0306 07:35:23.251114 140106748385024 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.13666439056396484, loss=0.01458799559623003
I0306 07:35:55.454271 140069608036096 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.1384737342596054, loss=0.017851082608103752
I0306 07:36:27.983721 140106748385024 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.15752604603767395, loss=0.017833281308412552
I0306 07:37:00.307356 140069608036096 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.15120424330234528, loss=0.017376240342855453
I0306 07:37:33.332993 140106748385024 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.1257811039686203, loss=0.018855653703212738
I0306 07:38:06.052867 140069608036096 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.14366507530212402, loss=0.016289230436086655
I0306 07:38:13.568590 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:39:52.070321 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:39:55.108401 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:39:58.117848 140274064205632 submission_runner.py:411] Time since start: 77871.21s, 	Step: 160424, 	{'train/accuracy': 0.9955351948738098, 'train/loss': 0.013888009823858738, 'train/mean_average_precision': 0.774616761796803, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936985274524972, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754478020806966, 'test/num_examples': 43793, 'score': 51638.440071582794, 'total_duration': 77871.21443009377, 'accumulated_submission_time': 51638.440071582794, 'accumulated_eval_time': 26218.408900260925, 'accumulated_logging_time': 9.85430383682251}
I0306 07:39:58.164294 140098771031808 logging_writer.py:48] [160424] accumulated_eval_time=26218.408900, accumulated_logging_time=9.854304, accumulated_submission_time=51638.440072, global_step=160424, preemption_count=0, score=51638.440072, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275448, test/num_examples=43793, total_duration=77871.214430, train/accuracy=0.995535, train/loss=0.013888, train/mean_average_precision=0.774617, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293699, validation/num_examples=43793
I0306 07:40:23.157330 140105330788096 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.13696010410785675, loss=0.016142934560775757
I0306 07:40:55.102564 140098771031808 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.13510921597480774, loss=0.01596774160861969
I0306 07:41:27.292747 140105330788096 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.136673703789711, loss=0.013510338962078094
I0306 07:41:59.436694 140098771031808 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.14997725188732147, loss=0.017680436372756958
I0306 07:42:31.383954 140105330788096 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.14165276288986206, loss=0.018332364037632942
I0306 07:43:03.080170 140098771031808 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.13902050256729126, loss=0.019276298582553864
I0306 07:43:34.992241 140105330788096 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.15468208491802216, loss=0.01957905851304531
I0306 07:43:58.345200 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:45:44.430178 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:45:47.534063 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:45:50.561681 140274064205632 submission_runner.py:411] Time since start: 78223.66s, 	Step: 161174, 	{'train/accuracy': 0.9956015944480896, 'train/loss': 0.013769547455012798, 'train/mean_average_precision': 0.7862721595759705, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936892479236363, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754757861197963, 'test/num_examples': 43793, 'score': 51878.58922076225, 'total_duration': 78223.65838193893, 'accumulated_submission_time': 51878.58922076225, 'accumulated_eval_time': 26330.625336408615, 'accumulated_logging_time': 9.911830186843872}
I0306 07:45:50.607724 140069608036096 logging_writer.py:48] [161174] accumulated_eval_time=26330.625336, accumulated_logging_time=9.911830, accumulated_submission_time=51878.589221, global_step=161174, preemption_count=0, score=51878.589221, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275476, test/num_examples=43793, total_duration=78223.658382, train/accuracy=0.995602, train/loss=0.013770, train/mean_average_precision=0.786272, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293689, validation/num_examples=43793
I0306 07:45:59.331552 140106748385024 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.1514318734407425, loss=0.017785755917429924
I0306 07:46:31.533029 140069608036096 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.1506960093975067, loss=0.01699853129684925
I0306 07:47:03.076649 140106748385024 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.15325400233268738, loss=0.0198502279818058
I0306 07:47:35.035682 140069608036096 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.15249305963516235, loss=0.017161529511213303
I0306 07:48:07.887173 140106748385024 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.13386274874210358, loss=0.016912788152694702
I0306 07:48:40.385708 140069608036096 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.15323548018932343, loss=0.019745206460356712
I0306 07:49:13.116512 140106748385024 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.1362224519252777, loss=0.017129996791481972
I0306 07:49:45.772310 140069608036096 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.14303433895111084, loss=0.018333304673433304
I0306 07:49:50.625552 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:51:32.247587 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:51:35.339719 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:51:38.394409 140274064205632 submission_runner.py:411] Time since start: 78571.49s, 	Step: 161916, 	{'train/accuracy': 0.9956266283988953, 'train/loss': 0.01375333871692419, 'train/mean_average_precision': 0.7849122736140868, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937739629217785, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27540017647514886, 'test/num_examples': 43793, 'score': 52118.57286572456, 'total_duration': 78571.4911108017, 'accumulated_submission_time': 52118.57286572456, 'accumulated_eval_time': 26438.394159078598, 'accumulated_logging_time': 9.969873428344727}
I0306 07:51:38.441618 140070338619136 logging_writer.py:48] [161916] accumulated_eval_time=26438.394159, accumulated_logging_time=9.969873, accumulated_submission_time=52118.572866, global_step=161916, preemption_count=0, score=52118.572866, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275400, test/num_examples=43793, total_duration=78571.491111, train/accuracy=0.995627, train/loss=0.013753, train/mean_average_precision=0.784912, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293774, validation/num_examples=43793
I0306 07:52:06.112273 140105330788096 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.13040146231651306, loss=0.016683943569660187
I0306 07:52:38.409634 140070338619136 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.15276622772216797, loss=0.01866653375327587
I0306 07:53:11.371547 140105330788096 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.14844264090061188, loss=0.01974797621369362
I0306 07:53:43.300466 140070338619136 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.1632084995508194, loss=0.019789384678006172
I0306 07:54:15.635620 140105330788096 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.1475849747657776, loss=0.01888692006468773
I0306 07:54:47.665019 140070338619136 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.14433333277702332, loss=0.017166022211313248
I0306 07:55:20.116122 140105330788096 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.148905411362648, loss=0.017269277945160866
I0306 07:55:38.535265 140274064205632 spec.py:321] Evaluating on the training split.
I0306 07:57:23.333270 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 07:57:26.455760 140274064205632 spec.py:349] Evaluating on the test split.
I0306 07:57:29.493730 140274064205632 submission_runner.py:411] Time since start: 78922.59s, 	Step: 162659, 	{'train/accuracy': 0.9955690503120422, 'train/loss': 0.013896035961806774, 'train/mean_average_precision': 0.7824925082203897, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29360229956577955, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27550133180921216, 'test/num_examples': 43793, 'score': 52358.634991168976, 'total_duration': 78922.59042525291, 'accumulated_submission_time': 52358.634991168976, 'accumulated_eval_time': 26549.352571725845, 'accumulated_logging_time': 10.028309345245361}
I0306 07:57:29.542397 140069608036096 logging_writer.py:48] [162659] accumulated_eval_time=26549.352572, accumulated_logging_time=10.028309, accumulated_submission_time=52358.634991, global_step=162659, preemption_count=0, score=52358.634991, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275501, test/num_examples=43793, total_duration=78922.590425, train/accuracy=0.995569, train/loss=0.013896, train/mean_average_precision=0.782493, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293602, validation/num_examples=43793
I0306 07:57:43.015509 140106748385024 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.14393459260463715, loss=0.01718628592789173
I0306 07:58:15.075106 140069608036096 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.12123145908117294, loss=0.014849284663796425
I0306 07:58:46.953231 140106748385024 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.14913266897201538, loss=0.01767551898956299
I0306 07:59:18.902892 140069608036096 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.14345745742321014, loss=0.01766221970319748
I0306 07:59:50.357821 140106748385024 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.15467694401741028, loss=0.020251894369721413
I0306 08:00:22.458036 140069608036096 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.15024758875370026, loss=0.018328780308365822
I0306 08:00:53.970099 140106748385024 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.13875606656074524, loss=0.018197549507021904
I0306 08:01:26.339118 140069608036096 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.14120249450206757, loss=0.017434846609830856
I0306 08:01:29.519453 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:03:12.007338 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:03:15.127789 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:03:18.116018 140274064205632 submission_runner.py:411] Time since start: 79271.21s, 	Step: 163411, 	{'train/accuracy': 0.9955653548240662, 'train/loss': 0.013798910193145275, 'train/mean_average_precision': 0.7842163827697879, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935796978506215, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753319317863094, 'test/num_examples': 43793, 'score': 52598.57873725891, 'total_duration': 79271.2127199173, 'accumulated_submission_time': 52598.57873725891, 'accumulated_eval_time': 26657.949086904526, 'accumulated_logging_time': 10.08947467803955}
I0306 08:03:18.162405 140070338619136 logging_writer.py:48] [163411] accumulated_eval_time=26657.949087, accumulated_logging_time=10.089475, accumulated_submission_time=52598.578737, global_step=163411, preemption_count=0, score=52598.578737, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275332, test/num_examples=43793, total_duration=79271.212720, train/accuracy=0.995565, train/loss=0.013799, train/mean_average_precision=0.784216, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293580, validation/num_examples=43793
I0306 08:03:47.161768 140098771031808 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.1467391848564148, loss=0.016771893948316574
I0306 08:04:19.202014 140070338619136 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.15558163821697235, loss=0.018804891034960747
I0306 08:04:50.854252 140098771031808 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.13901175558567047, loss=0.017344869673252106
I0306 08:05:22.733374 140070338619136 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.14428642392158508, loss=0.017589915543794632
I0306 08:05:54.449628 140098771031808 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.15102867782115936, loss=0.016162149608135223
I0306 08:06:26.602341 140070338619136 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.13540571928024292, loss=0.017013046890497208
I0306 08:06:58.510039 140098771031808 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.13361231982707977, loss=0.01692809909582138
I0306 08:07:18.214939 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:08:56.694007 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:08:59.768509 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:09:02.936761 140274064205632 submission_runner.py:411] Time since start: 79616.03s, 	Step: 164163, 	{'train/accuracy': 0.9955796599388123, 'train/loss': 0.013786263763904572, 'train/mean_average_precision': 0.7746348160789243, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935902334093814, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754401584745454, 'test/num_examples': 43793, 'score': 52838.597648859024, 'total_duration': 79616.03345918655, 'accumulated_submission_time': 52838.597648859024, 'accumulated_eval_time': 26762.67085957527, 'accumulated_logging_time': 10.14896845817566}
I0306 08:09:02.984393 140105330788096 logging_writer.py:48] [164163] accumulated_eval_time=26762.670860, accumulated_logging_time=10.148968, accumulated_submission_time=52838.597649, global_step=164163, preemption_count=0, score=52838.597649, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275440, test/num_examples=43793, total_duration=79616.033459, train/accuracy=0.995580, train/loss=0.013786, train/mean_average_precision=0.774635, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293590, validation/num_examples=43793
I0306 08:09:15.272555 140106748385024 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.12540848553180695, loss=0.017341120168566704
I0306 08:09:47.584568 140105330788096 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.1567191481590271, loss=0.02012481540441513
I0306 08:10:19.696003 140106748385024 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.14106714725494385, loss=0.018876446411013603
I0306 08:10:51.895159 140105330788096 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.15213996171951294, loss=0.017588794231414795
I0306 08:11:24.064296 140106748385024 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.13392199575901031, loss=0.016560157760977745
I0306 08:11:55.734608 140105330788096 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.14479206502437592, loss=0.01767401024699211
I0306 08:12:27.964357 140106748385024 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.1510794758796692, loss=0.02003403939306736
I0306 08:12:59.876688 140105330788096 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.16694718599319458, loss=0.018270106986165047
I0306 08:13:03.179476 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:14:40.785559 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:14:43.883857 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:14:46.889825 140274064205632 submission_runner.py:411] Time since start: 79959.99s, 	Step: 164911, 	{'train/accuracy': 0.9955300688743591, 'train/loss': 0.01398532371968031, 'train/mean_average_precision': 0.7824321755727822, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2937889285679689, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27548107815156286, 'test/num_examples': 43793, 'score': 53078.761633872986, 'total_duration': 79959.98651099205, 'accumulated_submission_time': 53078.761633872986, 'accumulated_eval_time': 26866.38114452362, 'accumulated_logging_time': 10.207342863082886}
I0306 08:14:46.936612 140069608036096 logging_writer.py:48] [164911] accumulated_eval_time=26866.381145, accumulated_logging_time=10.207343, accumulated_submission_time=53078.761634, global_step=164911, preemption_count=0, score=53078.761634, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275481, test/num_examples=43793, total_duration=79959.986511, train/accuracy=0.995530, train/loss=0.013985, train/mean_average_precision=0.782432, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293789, validation/num_examples=43793
I0306 08:15:15.834785 140098771031808 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.13873177766799927, loss=0.017439192160964012
I0306 08:15:47.489490 140069608036096 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.1386559158563614, loss=0.01583513244986534
I0306 08:16:19.789206 140098771031808 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.14539994299411774, loss=0.01731795445084572
I0306 08:16:51.182222 140069608036096 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.13537082076072693, loss=0.01672777533531189
I0306 08:17:22.710949 140098771031808 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.1611863076686859, loss=0.01689235121011734
I0306 08:17:54.478083 140069608036096 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.1417037397623062, loss=0.01804499328136444
I0306 08:18:25.845317 140098771031808 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.15022426843643188, loss=0.01947431080043316
I0306 08:18:46.962935 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:20:24.480090 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:20:27.486642 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:20:30.514596 140274064205632 submission_runner.py:411] Time since start: 80303.61s, 	Step: 165668, 	{'train/accuracy': 0.9955757260322571, 'train/loss': 0.013863793574273586, 'train/mean_average_precision': 0.7839171197252315, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936526321692164, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.275376862591533, 'test/num_examples': 43793, 'score': 53318.756410360336, 'total_duration': 80303.61129760742, 'accumulated_submission_time': 53318.756410360336, 'accumulated_eval_time': 26969.93276333809, 'accumulated_logging_time': 10.265093803405762}
I0306 08:20:30.560858 140070338619136 logging_writer.py:48] [165668] accumulated_eval_time=26969.932763, accumulated_logging_time=10.265094, accumulated_submission_time=53318.756410, global_step=165668, preemption_count=0, score=53318.756410, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275377, test/num_examples=43793, total_duration=80303.611298, train/accuracy=0.995576, train/loss=0.013864, train/mean_average_precision=0.783917, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293653, validation/num_examples=43793
I0306 08:20:41.242196 140106748385024 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.1646849811077118, loss=0.020013567060232162
I0306 08:21:13.348102 140070338619136 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.14389574527740479, loss=0.018552277237176895
I0306 08:21:45.295917 140106748385024 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.14280281960964203, loss=0.01806255429983139
I0306 08:22:17.044391 140070338619136 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.14859901368618011, loss=0.018322329968214035
I0306 08:22:48.667130 140106748385024 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.13343365490436554, loss=0.0175190307199955
I0306 08:23:20.332588 140070338619136 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.15440192818641663, loss=0.01794898696243763
I0306 08:23:52.051551 140106748385024 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.13185621798038483, loss=0.01575378142297268
I0306 08:24:23.652748 140070338619136 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.12567006051540375, loss=0.014833622612059116
I0306 08:24:30.530709 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:26:13.279551 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:26:16.342188 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:26:19.297609 140274064205632 submission_runner.py:411] Time since start: 80652.39s, 	Step: 166423, 	{'train/accuracy': 0.995619535446167, 'train/loss': 0.013759400695562363, 'train/mean_average_precision': 0.7878243548892319, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29375842270513797, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2753630251818127, 'test/num_examples': 43793, 'score': 53558.695544958115, 'total_duration': 80652.39430904388, 'accumulated_submission_time': 53558.695544958115, 'accumulated_eval_time': 27078.699613571167, 'accumulated_logging_time': 10.322208404541016}
I0306 08:26:19.344165 140069608036096 logging_writer.py:48] [166423] accumulated_eval_time=27078.699614, accumulated_logging_time=10.322208, accumulated_submission_time=53558.695545, global_step=166423, preemption_count=0, score=53558.695545, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275363, test/num_examples=43793, total_duration=80652.394309, train/accuracy=0.995620, train/loss=0.013759, train/mean_average_precision=0.787824, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293758, validation/num_examples=43793
I0306 08:26:44.620401 140105330788096 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.12993279099464417, loss=0.017792552709579468
I0306 08:27:17.371794 140069608036096 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.14757110178470612, loss=0.018228814005851746
I0306 08:27:49.936890 140105330788096 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.14593172073364258, loss=0.0199288260191679
I0306 08:28:23.111834 140069608036096 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.13111929595470428, loss=0.016794057562947273
I0306 08:28:55.786160 140105330788096 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.15383966267108917, loss=0.01783345825970173
I0306 08:29:28.901831 140069608036096 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.14835184812545776, loss=0.017481863498687744
I0306 08:30:01.494431 140105330788096 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.1287888139486313, loss=0.016949499025940895
I0306 08:30:19.561989 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:32:05.495417 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:32:08.625687 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:32:11.656392 140274064205632 submission_runner.py:411] Time since start: 81004.75s, 	Step: 167157, 	{'train/accuracy': 0.9955827593803406, 'train/loss': 0.013769716024398804, 'train/mean_average_precision': 0.7863487925670267, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2939032390369472, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.2754087925659607, 'test/num_examples': 43793, 'score': 53798.87780714035, 'total_duration': 81004.75309371948, 'accumulated_submission_time': 53798.87780714035, 'accumulated_eval_time': 27190.793970823288, 'accumulated_logging_time': 10.380990743637085}
I0306 08:32:11.703577 140070338619136 logging_writer.py:48] [167157] accumulated_eval_time=27190.793971, accumulated_logging_time=10.380991, accumulated_submission_time=53798.877807, global_step=167157, preemption_count=0, score=53798.877807, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275409, test/num_examples=43793, total_duration=81004.753094, train/accuracy=0.995583, train/loss=0.013770, train/mean_average_precision=0.786349, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293903, validation/num_examples=43793
I0306 08:32:25.893971 140106748385024 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.13350625336170197, loss=0.016693489626049995
I0306 08:32:57.887956 140070338619136 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.14556530117988586, loss=0.019213024526834488
I0306 08:33:29.650702 140106748385024 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.14323271811008453, loss=0.0169137641787529
I0306 08:34:01.314782 140070338619136 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.135702982544899, loss=0.017253920435905457
I0306 08:34:33.287068 140106748385024 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.15552881360054016, loss=0.01740254834294319
I0306 08:35:05.473315 140070338619136 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.15072594583034515, loss=0.020038489252328873
I0306 08:35:37.398004 140106748385024 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.15168684720993042, loss=0.018067918717861176
I0306 08:36:09.355124 140070338619136 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.14611071348190308, loss=0.017991263419389725
I0306 08:36:11.944419 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:37:54.389970 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:37:57.474649 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:38:00.457299 140274064205632 submission_runner.py:411] Time since start: 81353.55s, 	Step: 167909, 	{'train/accuracy': 0.9955384135246277, 'train/loss': 0.013855488039553165, 'train/mean_average_precision': 0.7785570710333689, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936441465812117, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275392875397748, 'test/num_examples': 43793, 'score': 54039.08760070801, 'total_duration': 81353.55399942398, 'accumulated_submission_time': 54039.08760070801, 'accumulated_eval_time': 27299.30680155754, 'accumulated_logging_time': 10.439002513885498}
I0306 08:38:00.504209 140069608036096 logging_writer.py:48] [167909] accumulated_eval_time=27299.306802, accumulated_logging_time=10.439003, accumulated_submission_time=54039.087601, global_step=167909, preemption_count=0, score=54039.087601, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275393, test/num_examples=43793, total_duration=81353.553999, train/accuracy=0.995538, train/loss=0.013855, train/mean_average_precision=0.778557, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293644, validation/num_examples=43793
I0306 08:38:30.023456 140098771031808 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.14359407126903534, loss=0.018002290278673172
I0306 08:39:01.821417 140069608036096 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.1482597142457962, loss=0.01748458296060562
I0306 08:39:34.143123 140098771031808 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.13438473641872406, loss=0.016840102151036263
I0306 08:40:06.034208 140069608036096 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.1567019671201706, loss=0.01750974915921688
I0306 08:40:38.582363 140098771031808 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.14935730397701263, loss=0.018728217110037804
I0306 08:41:11.589112 140069608036096 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.12900935113430023, loss=0.016628291457891464
I0306 08:41:44.384143 140098771031808 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.14608056843280792, loss=0.018354039639234543
I0306 08:42:00.498594 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:43:42.436203 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:43:45.482936 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:43:48.478069 140274064205632 submission_runner.py:411] Time since start: 81701.57s, 	Step: 168650, 	{'train/accuracy': 0.99558025598526, 'train/loss': 0.013854184187948704, 'train/mean_average_precision': 0.7801272234892287, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.29372284813345567, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.275552159012681, 'test/num_examples': 43793, 'score': 54279.04728722572, 'total_duration': 81701.57476568222, 'accumulated_submission_time': 54279.04728722572, 'accumulated_eval_time': 27407.28624010086, 'accumulated_logging_time': 10.49805474281311}
I0306 08:43:48.525732 140105330788096 logging_writer.py:48] [168650] accumulated_eval_time=27407.286240, accumulated_logging_time=10.498055, accumulated_submission_time=54279.047287, global_step=168650, preemption_count=0, score=54279.047287, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275552, test/num_examples=43793, total_duration=81701.574766, train/accuracy=0.995580, train/loss=0.013854, train/mean_average_precision=0.780127, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293723, validation/num_examples=43793
I0306 08:44:04.954447 140106748385024 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.15168987214565277, loss=0.016732702031731606
I0306 08:44:36.794657 140105330788096 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.1375780552625656, loss=0.016606928780674934
I0306 08:45:08.646186 140106748385024 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.14245152473449707, loss=0.018536455929279327
I0306 08:45:39.863414 140105330788096 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.14304675161838531, loss=0.016275284811854362
I0306 08:46:12.279014 140106748385024 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.14584840834140778, loss=0.018981408327817917
I0306 08:46:44.047171 140105330788096 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.15812651813030243, loss=0.018039604648947716
I0306 08:47:16.013518 140106748385024 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.16450612246990204, loss=0.018040485680103302
I0306 08:47:47.839930 140105330788096 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.13381808996200562, loss=0.017541799694299698
I0306 08:47:48.478839 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:49:27.416902 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:49:30.452818 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:49:33.438062 140274064205632 submission_runner.py:411] Time since start: 82046.53s, 	Step: 169403, 	{'train/accuracy': 0.9955908060073853, 'train/loss': 0.013790623284876347, 'train/mean_average_precision': 0.7917480193232054, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.293798366684791, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27550147350677934, 'test/num_examples': 43793, 'score': 54518.96765470505, 'total_duration': 82046.5347559452, 'accumulated_submission_time': 54518.96765470505, 'accumulated_eval_time': 27512.24540758133, 'accumulated_logging_time': 10.557909488677979}
I0306 08:49:33.486274 140069608036096 logging_writer.py:48] [169403] accumulated_eval_time=27512.245408, accumulated_logging_time=10.557909, accumulated_submission_time=54518.967655, global_step=169403, preemption_count=0, score=54518.967655, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275501, test/num_examples=43793, total_duration=82046.534756, train/accuracy=0.995591, train/loss=0.013791, train/mean_average_precision=0.791748, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293798, validation/num_examples=43793
I0306 08:50:04.951289 140098771031808 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.1381969302892685, loss=0.01614999584853649
I0306 08:50:36.554351 140069608036096 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.13336636126041412, loss=0.015873009338974953
I0306 08:51:08.398835 140098771031808 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.14312304556369781, loss=0.018198402598500252
I0306 08:51:39.888346 140069608036096 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.1299167424440384, loss=0.016247382387518883
I0306 08:52:11.410426 140098771031808 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.14947471022605896, loss=0.019533703103661537
I0306 08:52:43.010451 140069608036096 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.13813413679599762, loss=0.017835376784205437
I0306 08:53:14.483867 140098771031808 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.15507039427757263, loss=0.017549924552440643
I0306 08:53:33.735089 140274064205632 spec.py:321] Evaluating on the training split.
I0306 08:55:13.616861 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 08:55:16.684169 140274064205632 spec.py:349] Evaluating on the test split.
I0306 08:55:19.645673 140274064205632 submission_runner.py:411] Time since start: 82392.74s, 	Step: 170162, 	{'train/accuracy': 0.9956451058387756, 'train/loss': 0.013756964355707169, 'train/mean_average_precision': 0.7842756858193123, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2935977855388695, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27537189276030805, 'test/num_examples': 43793, 'score': 54759.18366193771, 'total_duration': 82392.74225926399, 'accumulated_submission_time': 54759.18366193771, 'accumulated_eval_time': 27618.15584230423, 'accumulated_logging_time': 10.618480205535889}
I0306 08:55:19.692617 140105330788096 logging_writer.py:48] [170162] accumulated_eval_time=27618.155842, accumulated_logging_time=10.618480, accumulated_submission_time=54759.183662, global_step=170162, preemption_count=0, score=54759.183662, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275372, test/num_examples=43793, total_duration=82392.742259, train/accuracy=0.995645, train/loss=0.013757, train/mean_average_precision=0.784276, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293598, validation/num_examples=43793
I0306 08:55:31.979479 140106748385024 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.18122829496860504, loss=0.019826192408800125
I0306 08:56:03.516310 140105330788096 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.13427352905273438, loss=0.016918135806918144
I0306 08:56:35.256556 140106748385024 logging_writer.py:48] [170400] global_step=170400, grad_norm=0.14667899906635284, loss=0.01828804425895214
I0306 08:57:06.780521 140105330788096 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.13318349421024323, loss=0.017105313017964363
I0306 08:57:38.241610 140106748385024 logging_writer.py:48] [170600] global_step=170600, grad_norm=0.16289930045604706, loss=0.018656617030501366
I0306 08:58:09.922259 140105330788096 logging_writer.py:48] [170700] global_step=170700, grad_norm=0.1414356678724289, loss=0.016175376251339912
I0306 08:58:41.371581 140106748385024 logging_writer.py:48] [170800] global_step=170800, grad_norm=0.1352517157793045, loss=0.016886333003640175
I0306 08:59:13.617582 140105330788096 logging_writer.py:48] [170900] global_step=170900, grad_norm=0.1477978229522705, loss=0.01710253767669201
I0306 08:59:19.933079 140274064205632 spec.py:321] Evaluating on the training split.
I0306 09:01:05.779218 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 09:01:08.810342 140274064205632 spec.py:349] Evaluating on the test split.
I0306 09:01:11.791150 140274064205632 submission_runner.py:411] Time since start: 82744.89s, 	Step: 170921, 	{'train/accuracy': 0.9955541491508484, 'train/loss': 0.013885637745261192, 'train/mean_average_precision': 0.7790105273746023, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936214992287673, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403660237789154, 'test/mean_average_precision': 0.2755011356766365, 'test/num_examples': 43793, 'score': 54999.39244008064, 'total_duration': 82744.88785123825, 'accumulated_submission_time': 54999.39244008064, 'accumulated_eval_time': 27730.01386666298, 'accumulated_logging_time': 10.676766395568848}
I0306 09:01:11.838191 140069608036096 logging_writer.py:48] [170921] accumulated_eval_time=27730.013867, accumulated_logging_time=10.676766, accumulated_submission_time=54999.392440, global_step=170921, preemption_count=0, score=54999.392440, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275501, test/num_examples=43793, total_duration=82744.887851, train/accuracy=0.995554, train/loss=0.013886, train/mean_average_precision=0.779011, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293621, validation/num_examples=43793
I0306 09:01:37.121797 140098771031808 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.14974796772003174, loss=0.01855805702507496
I0306 09:02:09.037020 140069608036096 logging_writer.py:48] [171100] global_step=171100, grad_norm=0.1496984213590622, loss=0.01781359501183033
I0306 09:02:40.678703 140098771031808 logging_writer.py:48] [171200] global_step=171200, grad_norm=0.1475904881954193, loss=0.01643422618508339
I0306 09:03:12.614022 140069608036096 logging_writer.py:48] [171300] global_step=171300, grad_norm=0.13219726085662842, loss=0.01750067062675953
I0306 09:03:44.605994 140098771031808 logging_writer.py:48] [171400] global_step=171400, grad_norm=0.14054301381111145, loss=0.01751871034502983
I0306 09:04:16.652564 140069608036096 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.12604232132434845, loss=0.016412632539868355
I0306 09:04:48.723906 140098771031808 logging_writer.py:48] [171600] global_step=171600, grad_norm=0.14618316292762756, loss=0.017329243943095207
I0306 09:05:12.008105 140274064205632 spec.py:321] Evaluating on the training split.
I0306 09:06:46.254425 140274064205632 spec.py:333] Evaluating on the validation split.
I0306 09:06:49.260343 140274064205632 spec.py:349] Evaluating on the test split.
I0306 09:06:52.195757 140274064205632 submission_runner.py:411] Time since start: 83085.29s, 	Step: 171674, 	{'train/accuracy': 0.9955615401268005, 'train/loss': 0.013751037418842316, 'train/mean_average_precision': 0.7800829913217441, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.050742361694574356, 'validation/mean_average_precision': 0.2936228202985142, 'validation/num_examples': 43793, 'test/accuracy': 0.9862112998962402, 'test/loss': 0.05403659865260124, 'test/mean_average_precision': 0.27539845493352894, 'test/num_examples': 43793, 'score': 55239.529722452164, 'total_duration': 83085.2924580574, 'accumulated_submission_time': 55239.529722452164, 'accumulated_eval_time': 27830.2014875412, 'accumulated_logging_time': 10.736129522323608}
I0306 09:06:52.242727 140070338619136 logging_writer.py:48] [171674] accumulated_eval_time=27830.201488, accumulated_logging_time=10.736130, accumulated_submission_time=55239.529722, global_step=171674, preemption_count=0, score=55239.529722, test/accuracy=0.986211, test/loss=0.054037, test/mean_average_precision=0.275398, test/num_examples=43793, total_duration=83085.292458, train/accuracy=0.995562, train/loss=0.013751, train/mean_average_precision=0.780083, validation/accuracy=0.986992, validation/loss=0.050742, validation/mean_average_precision=0.293623, validation/num_examples=43793
I0306 09:07:01.028897 140105330788096 logging_writer.py:48] [171700] global_step=171700, grad_norm=0.14660002291202545, loss=0.018672166392207146
I0306 09:07:33.252077 140070338619136 logging_writer.py:48] [171800] global_step=171800, grad_norm=0.14966198801994324, loss=0.016383357346057892
I0306 09:08:05.130045 140105330788096 logging_writer.py:48] [171900] global_step=171900, grad_norm=0.14250659942626953, loss=0.017979081720113754
I0306 09:08:36.720123 140070338619136 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.14651313424110413, loss=0.01905301958322525
I0306 09:09:08.615076 140105330788096 logging_writer.py:48] [172100] global_step=172100, grad_norm=0.1817970871925354, loss=0.018366189673542976
I0306 09:09:40.380242 140070338619136 logging_writer.py:48] [172200] global_step=172200, grad_norm=0.14483503997325897, loss=0.017444325610995293
I0306 09:10:03.833112 140105330788096 logging_writer.py:48] [172274] global_step=172274, preemption_count=0, score=55431.058990
I0306 09:10:03.900885 140274064205632 checkpoints.py:490] Saving checkpoint at step: 172274
I0306 09:10:04.022642 140274064205632 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1/checkpoint_172274
I0306 09:10:04.024070 140274064205632 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_0/ogbg_jax/trial_1/checkpoint_172274.
I0306 09:10:04.204463 140274064205632 submission_runner.py:676] Final ogbg score: 55431.058990478516
Dataset ogbg_molpcba downloaded and prepared to /root/data/ogbg_molpcba/0.1.3. Subsequent calls will reuse this data.
