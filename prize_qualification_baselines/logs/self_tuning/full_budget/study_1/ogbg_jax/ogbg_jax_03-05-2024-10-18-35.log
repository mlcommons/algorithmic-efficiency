python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_1 --overwrite=true --save_checkpoints=false --rng_seed=3808751163 --max_global_steps=240000 --tuning_ruleset=self 2>&1 | tee -a /logs/ogbg_jax_03-05-2024-10-18-35.log
I0305 10:18:57.474205 139937033598784 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax.
I0305 10:18:58.496968 139937033598784 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0305 10:18:58.497860 139937033598784 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0305 10:18:58.498011 139937033598784 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0305 10:18:59.377487 139937033598784 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1.
I0305 10:18:59.579292 139937033598784 submission_runner.py:206] Initializing dataset.
I0305 10:18:59.876010 139937033598784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:18:59.881165 139937033598784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 10:19:00.132417 139937033598784 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 10:19:00.193381 139937033598784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:19:00.271396 139937033598784 submission_runner.py:213] Initializing model.
I0305 10:19:05.233567 139937033598784 submission_runner.py:255] Initializing optimizer.
I0305 10:19:05.887345 139937033598784 submission_runner.py:262] Initializing metrics bundle.
I0305 10:19:05.887546 139937033598784 submission_runner.py:280] Initializing checkpoint and logger.
I0305 10:19:05.888243 139937033598784 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1 with prefix checkpoint_
I0305 10:19:05.888379 139937033598784 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1/meta_data_0.json.
I0305 10:19:05.888640 139937033598784 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0305 10:19:05.888719 139937033598784 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0305 10:19:06.218494 139937033598784 logger_utils.py:220] Unable to record git information. Continuing without it.
I0305 10:19:06.519184 139937033598784 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1/flags_0.json.
I0305 10:19:06.528948 139937033598784 submission_runner.py:314] Starting training loop.
I0305 10:19:26.041047 139771816961792 logging_writer.py:48] [0] global_step=0, grad_norm=1.9359289407730103, loss=0.7161926031112671
I0305 10:19:26.057455 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:19:26.063562 139937033598784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:19:26.068449 139937033598784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:19:26.138798 139937033598784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:21:21.061255 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:21:21.064909 139937033598784 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:21:21.069014 139937033598784 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:21:21.138729 139937033598784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:22:56.943427 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:22:57.380342 139937033598784 dataset_info.py:736] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ogbg_molpcba/0.1.3
I0305 10:22:58.752255 139937033598784 dataset_info.py:578] Load dataset info from /tmp/tmpjrpyljeltfds
I0305 10:22:58.755896 139937033598784 dataset_info.py:669] Fields info.[description, release_notes, splits, module_name] from disk and from code do not match. Keeping the one from code.
I0305 10:22:58.756337 139937033598784 dataset_builder.py:593] Generating dataset ogbg_molpcba (/root/data/ogbg_molpcba/0.1.3)
Downloading and preparing dataset 37.70 MiB (download: 37.70 MiB, generated: 822.53 MiB, total: 860.23 MiB) to /root/data/ogbg_molpcba/0.1.3...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[AI0305 10:22:59.066375 139937033598784 download_manager.py:400] Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip into /root/data/downloads/snap.stan.edu_ogb_grap_csv_mol_down_pcbapc4I82Cv1THcU-IggPHK8IHZ8qM-BJ3VDk-q_rtqrf4.zip.tmp.d54da11cc43c4692886609de8ef38f03...
Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...:   0%|          | 0/37 [00:00<?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:39,  2.76s/ MiB][ADl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]
Dl Size...:   3%|â–Ž         | 1/37 [00:02<01:39,  2.76s/ MiB][A

Extraction completed...: 0 file [00:02, ? file/s][A[A
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:49,  1.40s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:49,  1.40s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:32,  1.03 MiB/s][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   8%|â–Š         | 3/37 [00:03<00:32,  1.03 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:  11%|â–ˆ         | 4/37 [00:03<00:21,  1.55 MiB/s][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:  11%|â–ˆ         | 4/37 [00:03<00:21,  1.55 MiB/s][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:16,  1.92 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:16,  1.92 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:12,  2.53 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:12,  2.53 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:09,  3.17 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:04<00:09,  3.17 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:04<00:07,  3.80 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:04<00:07,  3.80 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:04<00:07,  3.80 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:04<00:04,  5.71 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:04<00:04,  5.71 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:04<00:04,  5.95 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:04<00:04,  5.95 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:04<00:04,  5.95 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:03,  7.70 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:03,  7.70 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:03,  7.52 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:03,  7.52 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:05<00:02,  7.52 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  9.08 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  9.08 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 17/37 [00:05<00:02,  9.08 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:05<00:02,  9.08 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/37 [00:05<00:01, 11.95 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/37 [00:05<00:01, 11.95 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:05<00:01, 11.95 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21/37 [00:05<00:01, 12.41 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21/37 [00:05<00:01, 12.41 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22/37 [00:05<00:01, 12.41 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:05<00:01, 12.78 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:05<00:01, 12.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/37 [00:05<00:01, 12.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:05<00:00, 12.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26/37 [00:05<00:00, 14.78 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26/37 [00:05<00:00, 14.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27/37 [00:05<00:00, 14.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:05<00:00, 14.78 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/37 [00:06<00:00, 16.22 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/37 [00:06<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/37 [00:06<00:00, 16.22 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [00:06<00:00, 17.25 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [00:06<00:00, 17.25 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 17.25 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [00:06<00:00, 17.25 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/37 [00:06<00:00, 17.45 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/2 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/3 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/4 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/5 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/6 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/7 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/8 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/9 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/10 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/11 [00:06<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   0%|          | 0/12 [00:06<?, ? file/s][A[A

Extraction completed...:   8%|â–Š         | 1/12 [00:06<01:13,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:   8%|â–Š         | 1/12 [00:06<01:13,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  17%|â–ˆâ–‹        | 2/12 [00:06<01:06,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:06<00:59,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:06<00:53,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:06<00:46,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:06<00:39,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:06<00:33,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:06<00:26,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:06<00:19,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:06<00:13,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:06<00:06,  6.66s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00, 17.45 MiB/s][A

Extraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:06<00:00,  6.66s/ file][A[AExtraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:06<00:00,  1.80 file/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:06<00:00,  5.55 MiB/s]
Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.67s/ url]
Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
Generating train examples...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Generating train examples...:   0%|          | 84/350343 [00:00<06:58, 837.53 examples/s][A
Generating train examples...:   0%|          | 168/350343 [00:00<07:43, 755.72 examples/s][A
Generating train examples...:   0%|          | 398/350343 [00:00<04:06, 1420.37 examples/s][A
Generating train examples...:   0%|          | 630/350343 [00:00<03:18, 1761.34 examples/s][A
Generating train examples...:   0%|          | 866/350343 [00:00<02:57, 1971.86 examples/s][A
Generating train examples...:   0%|          | 1110/350343 [00:00<02:44, 2126.94 examples/s][A
Generating train examples...:   0%|          | 1342/350343 [00:00<02:39, 2188.26 examples/s][A
Generating train examples...:   0%|          | 1583/350343 [00:00<02:34, 2257.21 examples/s][A
Generating train examples...:   1%|          | 1810/350343 [00:00<02:37, 2219.37 examples/s][A
Generating train examples...:   1%|          | 2045/350343 [00:01<02:34, 2257.93 examples/s][A
Generating train examples...:   1%|          | 2279/350343 [00:01<02:32, 2281.54 examples/s][A
Generating train examples...:   1%|          | 2513/350343 [00:01<02:31, 2298.02 examples/s][A
Generating train examples...:   1%|          | 2748/350343 [00:01<02:30, 2312.15 examples/s][A
Generating train examples...:   1%|          | 2985/350343 [00:01<02:29, 2327.08 examples/s][A
Generating train examples...:   1%|          | 3219/350343 [00:01<02:29, 2328.61 examples/s][A
Generating train examples...:   1%|          | 3458/350343 [00:01<02:27, 2344.96 examples/s][A
Generating train examples...:   1%|          | 3697/350343 [00:01<02:27, 2357.63 examples/s][A
Generating train examples...:   1%|          | 3933/350343 [00:01<02:28, 2339.74 examples/s][A
Generating train examples...:   1%|          | 4168/350343 [00:01<02:28, 2333.53 examples/s][A
Generating train examples...:   1%|â–         | 4402/350343 [00:02<02:28, 2324.32 examples/s][A
Generating train examples...:   1%|â–         | 4635/350343 [00:02<02:29, 2313.22 examples/s][A
Generating train examples...:   1%|â–         | 4867/350343 [00:02<02:33, 2248.77 examples/s][A
Generating train examples...:   1%|â–         | 5103/350343 [00:02<02:31, 2281.18 examples/s][A
Generating train examples...:   2%|â–         | 5344/350343 [00:02<02:28, 2316.91 examples/s][A
Generating train examples...:   2%|â–         | 5585/350343 [00:02<02:27, 2344.10 examples/s][A
Generating train examples...:   2%|â–         | 5820/350343 [00:02<02:27, 2341.92 examples/s][A
Generating train examples...:   2%|â–         | 6056/350343 [00:02<02:26, 2346.52 examples/s][A
Generating train examples...:   2%|â–         | 6291/350343 [00:02<02:26, 2347.34 examples/s][A
Generating train examples...:   2%|â–         | 6530/350343 [00:02<02:25, 2358.70 examples/s][A
Generating train examples...:   2%|â–         | 6766/350343 [00:03<02:26, 2346.88 examples/s][A
Generating train examples...:   2%|â–         | 7001/350343 [00:03<02:27, 2322.54 examples/s][A
Generating train examples...:   2%|â–         | 7235/350343 [00:03<02:27, 2325.99 examples/s][A
Generating train examples...:   2%|â–         | 7475/350343 [00:03<02:26, 2345.45 examples/s][A
Generating train examples...:   2%|â–         | 7717/350343 [00:03<02:24, 2365.58 examples/s][A
Generating train examples...:   2%|â–         | 7955/350343 [00:03<02:24, 2368.02 examples/s][A
Generating train examples...:   2%|â–         | 8192/350343 [00:03<02:24, 2362.68 examples/s][A
Generating train examples...:   2%|â–         | 8429/350343 [00:03<02:24, 2358.87 examples/s][A
Generating train examples...:   2%|â–         | 8665/350343 [00:03<02:29, 2278.32 examples/s][A
Generating train examples...:   3%|â–Ž         | 8904/350343 [00:03<02:27, 2310.81 examples/s][A
Generating train examples...:   3%|â–Ž         | 9136/350343 [00:04<02:27, 2308.41 examples/s][A
Generating train examples...:   3%|â–Ž         | 9375/350343 [00:04<02:26, 2330.37 examples/s][A
Generating train examples...:   3%|â–Ž         | 9614/350343 [00:04<02:25, 2346.87 examples/s][A
Generating train examples...:   3%|â–Ž         | 9849/350343 [00:04<02:26, 2328.06 examples/s][A
Generating train examples...:   3%|â–Ž         | 10082/350343 [00:04<02:26, 2319.77 examples/s][A
Generating train examples...:   3%|â–Ž         | 10324/350343 [00:04<02:24, 2347.60 examples/s][A
Generating train examples...:   3%|â–Ž         | 10560/350343 [00:04<02:24, 2349.30 examples/s][A
Generating train examples...:   3%|â–Ž         | 10796/350343 [00:04<02:25, 2341.14 examples/s][A
Generating train examples...:   3%|â–Ž         | 11036/350343 [00:04<02:23, 2357.62 examples/s][A
Generating train examples...:   3%|â–Ž         | 11273/350343 [00:04<02:23, 2360.43 examples/s][A
Generating train examples...:   3%|â–Ž         | 11510/350343 [00:05<02:23, 2361.24 examples/s][A
Generating train examples...:   3%|â–Ž         | 11747/350343 [00:05<02:24, 2349.17 examples/s][A
Generating train examples...:   3%|â–Ž         | 11982/350343 [00:05<02:29, 2267.45 examples/s][A
Generating train examples...:   3%|â–Ž         | 12210/350343 [00:05<02:32, 2215.45 examples/s][A
Generating train examples...:   4%|â–Ž         | 12436/350343 [00:05<02:31, 2225.70 examples/s][A
Generating train examples...:   4%|â–Ž         | 12668/350343 [00:05<02:29, 2253.14 examples/s][A
Generating train examples...:   4%|â–Ž         | 12901/350343 [00:05<02:28, 2274.70 examples/s][A
Generating train examples...:   4%|â–Ž         | 13131/350343 [00:05<02:27, 2281.82 examples/s][A
Generating train examples...:   4%|â–         | 13364/350343 [00:05<02:32, 2216.55 examples/s][A
Generating train examples...:   4%|â–         | 13588/350343 [00:06<02:33, 2193.91 examples/s][A
Generating train examples...:   4%|â–         | 13814/350343 [00:06<02:32, 2210.91 examples/s][A
Generating train examples...:   4%|â–         | 14040/350343 [00:06<02:31, 2225.08 examples/s][A
Generating train examples...:   4%|â–         | 14270/350343 [00:06<02:29, 2244.94 examples/s][A
Generating train examples...:   4%|â–         | 14505/350343 [00:06<02:27, 2275.84 examples/s][A
Generating train examples...:   4%|â–         | 14733/350343 [00:06<02:27, 2269.47 examples/s][A
Generating train examples...:   4%|â–         | 14969/350343 [00:06<02:26, 2295.33 examples/s][A
Generating train examples...:   4%|â–         | 15207/350343 [00:06<02:24, 2318.98 examples/s][A
Generating train examples...:   4%|â–         | 15448/350343 [00:06<02:22, 2345.74 examples/s][A
Generating train examples...:   4%|â–         | 15683/350343 [00:06<02:22, 2345.87 examples/s][A
Generating train examples...:   5%|â–         | 15922/350343 [00:07<02:21, 2357.11 examples/s][A
Generating train examples...:   5%|â–         | 16158/350343 [00:07<02:24, 2317.36 examples/s][A
Generating train examples...:   5%|â–         | 16395/350343 [00:07<02:23, 2331.89 examples/s][A
Generating train examples...:   5%|â–         | 16629/350343 [00:07<02:24, 2315.62 examples/s][A
Generating train examples...:   5%|â–         | 16867/350343 [00:07<02:23, 2331.95 examples/s][A
Generating train examples...:   5%|â–         | 17103/350343 [00:07<02:22, 2339.07 examples/s][A
Generating train examples...:   5%|â–         | 17337/350343 [00:07<02:22, 2338.05 examples/s][A
Generating train examples...:   5%|â–Œ         | 17572/350343 [00:07<02:22, 2340.02 examples/s][A
Generating train examples...:   5%|â–Œ         | 17814/350343 [00:07<02:20, 2363.75 examples/s][A
Generating train examples...:   5%|â–Œ         | 18052/350343 [00:07<02:20, 2367.22 examples/s][A
Generating train examples...:   5%|â–Œ         | 18289/350343 [00:08<02:25, 2278.73 examples/s][A
Generating train examples...:   5%|â–Œ         | 18523/350343 [00:08<02:24, 2293.63 examples/s][A
Generating train examples...:   5%|â–Œ         | 18761/350343 [00:08<02:23, 2317.57 examples/s][A
Generating train examples...:   5%|â–Œ         | 19003/350343 [00:08<02:21, 2345.25 examples/s][A
Generating train examples...:   5%|â–Œ         | 19244/350343 [00:08<02:20, 2361.49 examples/s][A
Generating train examples...:   6%|â–Œ         | 19482/350343 [00:08<02:19, 2365.07 examples/s][A
Generating train examples...:   6%|â–Œ         | 19720/350343 [00:08<02:19, 2367.30 examples/s][A
Generating train examples...:   6%|â–Œ         | 19957/350343 [00:08<02:19, 2360.70 examples/s][A
Generating train examples...:   6%|â–Œ         | 20196/350343 [00:08<02:19, 2367.59 examples/s][A
Generating train examples...:   6%|â–Œ         | 20436/350343 [00:08<02:18, 2374.71 examples/s][A
Generating train examples...:   6%|â–Œ         | 20674/350343 [00:09<02:19, 2355.78 examples/s][A
Generating train examples...:   6%|â–Œ         | 20913/350343 [00:09<02:19, 2364.26 examples/s][A
Generating train examples...:   6%|â–Œ         | 21152/350343 [00:09<02:18, 2371.56 examples/s][A
Generating train examples...:   6%|â–Œ         | 21390/350343 [00:09<02:18, 2373.90 examples/s][A
Generating train examples...:   6%|â–Œ         | 21629/350343 [00:09<02:18, 2376.21 examples/s][A
Generating train examples...:   6%|â–Œ         | 21867/350343 [00:09<02:23, 2284.13 examples/s][A
Generating train examples...:   6%|â–‹         | 22102/350343 [00:09<02:22, 2302.20 examples/s][A
Generating train examples...:   6%|â–‹         | 22335/350343 [00:09<02:21, 2310.31 examples/s][A
Generating train examples...:   6%|â–‹         | 22573/350343 [00:09<02:20, 2328.66 examples/s][A
Generating train examples...:   7%|â–‹         | 22807/350343 [00:09<02:20, 2329.77 examples/s][A
Generating train examples...:   7%|â–‹         | 23041/350343 [00:10<02:21, 2320.24 examples/s][A
Generating train examples...:   7%|â–‹         | 23274/350343 [00:10<02:22, 2298.77 examples/s][A
Generating train examples...:   7%|â–‹         | 23508/350343 [00:10<02:21, 2309.64 examples/s][A
Generating train examples...:   7%|â–‹         | 23743/350343 [00:10<02:20, 2320.17 examples/s][A
Generating train examples...:   7%|â–‹         | 23978/350343 [00:10<02:20, 2327.69 examples/s][A
Generating train examples...:   7%|â–‹         | 24221/350343 [00:10<02:18, 2356.72 examples/s][A
Generating train examples...:   7%|â–‹         | 24463/350343 [00:10<02:17, 2373.33 examples/s][A
Generating train examples...:   7%|â–‹         | 24701/350343 [00:10<02:17, 2373.91 examples/s][A
Generating train examples...:   7%|â–‹         | 24939/350343 [00:10<02:17, 2362.02 examples/s][A
Generating train examples...:   7%|â–‹         | 25176/350343 [00:10<02:18, 2354.78 examples/s][A
Generating train examples...:   7%|â–‹         | 25414/350343 [00:11<02:17, 2360.91 examples/s][A
Generating train examples...:   7%|â–‹         | 25655/350343 [00:11<02:16, 2373.08 examples/s][A
Generating train examples...:   7%|â–‹         | 25895/350343 [00:11<02:16, 2380.89 examples/s][A
Generating train examples...:   7%|â–‹         | 26134/350343 [00:11<02:21, 2293.18 examples/s][A
Generating train examples...:   8%|â–Š         | 26377/350343 [00:11<02:19, 2330.17 examples/s][A
Generating train examples...:   8%|â–Š         | 26624/350343 [00:11<02:16, 2369.13 examples/s][A
Generating train examples...:   8%|â–Š         | 26862/350343 [00:11<02:17, 2357.93 examples/s][A
Generating train examples...:   8%|â–Š         | 27109/350343 [00:11<02:15, 2388.83 examples/s][A
Generating train examples...:   8%|â–Š         | 27349/350343 [00:11<02:16, 2372.39 examples/s][A
Generating train examples...:   8%|â–Š         | 27587/350343 [00:11<02:17, 2342.55 examples/s][A
Generating train examples...:   8%|â–Š         | 27830/350343 [00:12<02:16, 2367.68 examples/s][A
Generating train examples...:   8%|â–Š         | 28067/350343 [00:12<02:16, 2360.54 examples/s][A
Generating train examples...:   8%|â–Š         | 28308/350343 [00:12<02:15, 2373.13 examples/s][A
Generating train examples...:   8%|â–Š         | 28546/350343 [00:12<02:16, 2355.96 examples/s][A
Generating train examples...:   8%|â–Š         | 28790/350343 [00:12<02:15, 2379.91 examples/s][A
Generating train examples...:   8%|â–Š         | 29030/350343 [00:12<02:14, 2385.07 examples/s][A
Generating train examples...:   8%|â–Š         | 29269/350343 [00:12<02:19, 2300.60 examples/s][A
Generating train examples...:   8%|â–Š         | 29500/350343 [00:12<02:19, 2301.46 examples/s][A
Generating train examples...:   8%|â–Š         | 29737/350343 [00:12<02:18, 2320.10 examples/s][A
Generating train examples...:   9%|â–Š         | 29970/350343 [00:13<02:18, 2316.25 examples/s][A
Generating train examples...:   9%|â–Š         | 30202/350343 [00:13<02:18, 2308.38 examples/s][A
Generating train examples...:   9%|â–Š         | 30435/350343 [00:13<02:18, 2312.45 examples/s][A
Generating train examples...:   9%|â–‰         | 30672/350343 [00:13<02:17, 2328.76 examples/s][A
Generating train examples...:   9%|â–‰         | 30905/350343 [00:13<02:17, 2326.89 examples/s][A
Generating train examples...:   9%|â–‰         | 31141/350343 [00:13<02:16, 2335.33 examples/s][A
Generating train examples...:   9%|â–‰         | 31383/350343 [00:13<02:15, 2358.32 examples/s][A
Generating train examples...:   9%|â–‰         | 31619/350343 [00:13<02:16, 2340.15 examples/s][A
Generating train examples...:   9%|â–‰         | 31854/350343 [00:13<02:16, 2328.21 examples/s][A
Generating train examples...:   9%|â–‰         | 32087/350343 [00:13<02:18, 2304.16 examples/s][A
Generating train examples...:   9%|â–‰         | 32327/350343 [00:14<02:16, 2330.49 examples/s][A
Generating train examples...:   9%|â–‰         | 32561/350343 [00:14<02:16, 2321.40 examples/s][A
Generating train examples...:   9%|â–‰         | 32794/350343 [00:14<02:16, 2320.57 examples/s][A
Generating train examples...:   9%|â–‰         | 33032/350343 [00:14<02:15, 2337.19 examples/s][A
Generating train examples...:   9%|â–‰         | 33266/350343 [00:14<02:16, 2330.99 examples/s][A
Generating train examples...:  10%|â–‰         | 33500/350343 [00:14<02:16, 2326.71 examples/s][A
Generating train examples...:  10%|â–‰         | 33733/350343 [00:14<02:21, 2235.12 examples/s][A
Generating train examples...:  10%|â–‰         | 33972/350343 [00:14<02:18, 2279.78 examples/s][A
Generating train examples...:  10%|â–‰         | 34204/350343 [00:14<02:18, 2289.53 examples/s][A
Generating train examples...:  10%|â–‰         | 34434/350343 [00:14<02:18, 2285.10 examples/s][A
Generating train examples...:  10%|â–‰         | 34667/350343 [00:15<02:17, 2297.79 examples/s][A
Generating train examples...:  10%|â–‰         | 34900/350343 [00:15<02:16, 2307.27 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35131/350343 [00:15<02:16, 2302.02 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35362/350343 [00:15<02:16, 2303.82 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35604/350343 [00:15<02:14, 2336.74 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35841/350343 [00:15<02:14, 2345.80 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36076/350343 [00:15<02:14, 2338.11 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36311/350343 [00:15<02:14, 2339.59 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36546/350343 [00:15<02:13, 2341.81 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36781/350343 [00:15<02:16, 2297.81 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37016/350343 [00:16<02:15, 2312.23 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37248/350343 [00:16<02:15, 2305.05 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37479/350343 [00:16<02:15, 2305.69 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37710/350343 [00:16<02:15, 2300.47 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37950/350343 [00:16<02:14, 2329.61 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38185/350343 [00:16<02:13, 2334.25 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38420/350343 [00:16<02:13, 2338.60 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38654/350343 [00:16<02:18, 2251.64 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38890/350343 [00:16<02:16, 2281.89 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39124/350343 [00:16<02:15, 2296.39 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39358/350343 [00:17<02:14, 2306.90 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39592/350343 [00:17<02:14, 2315.58 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39824/350343 [00:17<02:14, 2310.79 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 40057/350343 [00:17<02:14, 2314.93 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40292/350343 [00:17<02:13, 2323.52 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40527/350343 [00:17<02:12, 2331.18 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40761/350343 [00:17<02:13, 2323.86 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40997/350343 [00:17<02:12, 2334.00 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41231/350343 [00:17<02:12, 2333.79 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41465/350343 [00:17<02:13, 2320.88 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41701/350343 [00:18<02:12, 2331.34 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41935/350343 [00:18<02:12, 2332.23 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42169/350343 [00:18<02:13, 2311.91 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42401/350343 [00:18<02:13, 2311.50 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42638/350343 [00:18<02:12, 2327.16 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42871/350343 [00:18<02:12, 2319.66 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43105/350343 [00:18<02:12, 2324.35 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43342/350343 [00:18<02:11, 2336.65 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43576/350343 [00:18<02:17, 2227.46 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 43814/350343 [00:19<02:15, 2270.52 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44043/350343 [00:19<02:14, 2275.38 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44276/350343 [00:19<02:13, 2289.76 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44507/350343 [00:19<02:13, 2295.16 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44737/350343 [00:19<02:14, 2275.78 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44975/350343 [00:19<02:12, 2305.60 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45214/350343 [00:19<02:11, 2328.63 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45448/350343 [00:19<02:10, 2331.37 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45686/350343 [00:19<02:09, 2344.60 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45921/350343 [00:19<02:12, 2304.27 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46156/350343 [00:20<02:11, 2317.71 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46388/350343 [00:20<02:12, 2293.25 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46618/350343 [00:20<02:15, 2244.14 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46849/350343 [00:20<02:14, 2262.03 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 47087/350343 [00:20<02:12, 2295.98 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47320/350343 [00:20<02:11, 2305.38 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47560/350343 [00:20<02:09, 2332.30 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47797/350343 [00:20<02:09, 2341.60 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 48032/350343 [00:20<02:09, 2334.89 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48266/350343 [00:20<02:16, 2216.09 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48506/350343 [00:21<02:13, 2267.64 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48751/350343 [00:21<02:09, 2320.08 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48990/350343 [00:21<02:08, 2340.06 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49227/350343 [00:21<02:08, 2347.31 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49463/350343 [00:21<02:08, 2350.44 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49704/350343 [00:21<02:07, 2367.14 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49941/350343 [00:21<02:07, 2360.95 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50179/350343 [00:21<02:06, 2366.14 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50416/350343 [00:21<02:07, 2353.75 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50652/350343 [00:21<02:07, 2352.21 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 50893/350343 [00:22<02:06, 2368.08 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51130/350343 [00:22<02:06, 2361.52 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51370/350343 [00:22<02:06, 2372.79 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51613/350343 [00:22<02:05, 2388.89 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51852/350343 [00:22<02:09, 2307.03 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52088/350343 [00:22<02:08, 2320.35 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52325/350343 [00:22<02:07, 2334.78 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52560/350343 [00:22<02:07, 2337.52 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52795/350343 [00:22<02:08, 2312.15 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53028/350343 [00:22<02:08, 2317.07 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53269/350343 [00:23<02:06, 2342.65 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53506/350343 [00:23<02:06, 2348.92 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53744/350343 [00:23<02:05, 2355.90 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53984/350343 [00:23<02:05, 2368.20 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 54227/350343 [00:23<02:04, 2385.68 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54472/350343 [00:23<02:03, 2403.45 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54713/350343 [00:23<02:03, 2400.25 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54954/350343 [00:23<02:03, 2387.59 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55193/350343 [00:23<02:04, 2364.64 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55430/350343 [00:23<02:09, 2279.34 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55659/350343 [00:24<02:09, 2279.83 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55893/350343 [00:24<02:08, 2296.00 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56128/350343 [00:24<02:07, 2310.54 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56367/350343 [00:24<02:05, 2333.25 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56602/350343 [00:24<02:05, 2337.62 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56840/350343 [00:24<02:04, 2350.14 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57076/350343 [00:24<02:04, 2352.23 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57312/350343 [00:24<02:04, 2350.64 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57548/350343 [00:24<02:05, 2332.01 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57782/350343 [00:24<02:05, 2333.72 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58021/350343 [00:25<02:04, 2350.47 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58257/350343 [00:25<02:04, 2343.73 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58493/350343 [00:25<02:04, 2347.14 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58728/350343 [00:25<02:04, 2343.14 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58966/350343 [00:25<02:03, 2353.93 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59203/350343 [00:25<02:03, 2356.64 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59439/350343 [00:25<02:03, 2353.98 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59675/350343 [00:25<02:03, 2348.07 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59910/350343 [00:25<02:09, 2250.85 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60148/350343 [00:26<02:06, 2287.82 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60386/350343 [00:26<02:05, 2314.50 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60627/350343 [00:26<02:03, 2340.66 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60866/350343 [00:26<02:02, 2353.98 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 61109/350343 [00:26<02:01, 2375.23 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61347/350343 [00:26<02:01, 2369.69 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61588/350343 [00:26<02:01, 2379.52 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61830/350343 [00:26<02:00, 2390.95 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62070/350343 [00:26<02:00, 2388.65 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62309/350343 [00:26<02:01, 2374.14 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62555/350343 [00:27<02:00, 2397.63 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62796/350343 [00:27<01:59, 2400.80 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63037/350343 [00:27<02:04, 2299.66 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63272/350343 [00:27<02:04, 2312.70 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63504/350343 [00:27<02:04, 2309.44 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63736/350343 [00:27<02:04, 2307.25 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63974/350343 [00:27<02:03, 2327.10 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64217/350343 [00:27<02:01, 2354.99 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64453/350343 [00:27<02:01, 2351.53 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64694/350343 [00:27<02:00, 2368.50 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 64934/350343 [00:28<02:00, 2376.10 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65176/350343 [00:28<01:59, 2387.73 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65415/350343 [00:28<02:00, 2368.30 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65652/350343 [00:28<02:00, 2359.85 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 65889/350343 [00:28<02:00, 2356.09 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66127/350343 [00:28<02:00, 2361.21 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66364/350343 [00:28<02:00, 2362.78 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66601/350343 [00:28<02:00, 2348.61 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66836/350343 [00:28<02:04, 2276.35 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67080/350343 [00:28<02:01, 2322.40 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67324/350343 [00:29<02:00, 2356.17 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67565/350343 [00:29<01:59, 2370.25 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67808/350343 [00:29<01:58, 2387.60 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68047/350343 [00:29<01:58, 2387.83 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68286/350343 [00:29<01:58, 2382.97 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68525/350343 [00:29<01:58, 2381.64 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68764/350343 [00:29<01:58, 2382.05 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69003/350343 [00:29<01:58, 2367.46 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69240/350343 [00:29<01:59, 2359.77 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69477/350343 [00:29<01:59, 2349.84 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69714/350343 [00:30<01:59, 2354.63 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69955/350343 [00:30<01:58, 2369.13 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70192/350343 [00:30<02:02, 2286.18 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70431/350343 [00:30<02:00, 2314.05 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70677/350343 [00:30<01:58, 2355.73 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70917/350343 [00:30<01:58, 2367.41 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71155/350343 [00:30<02:00, 2321.51 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71388/350343 [00:30<02:02, 2278.85 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71617/350343 [00:30<02:02, 2281.64 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 71856/350343 [00:30<02:00, 2312.80 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72094/350343 [00:31<01:59, 2332.06 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72331/350343 [00:31<01:58, 2343.28 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72566/350343 [00:31<01:58, 2336.20 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72804/350343 [00:31<01:58, 2347.98 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73039/350343 [00:31<01:58, 2340.56 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73278/350343 [00:31<01:57, 2355.02 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73514/350343 [00:31<01:57, 2346.58 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73755/350343 [00:31<01:57, 2362.20 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73992/350343 [00:31<02:01, 2273.12 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74236/350343 [00:32<01:59, 2319.71 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74474/350343 [00:32<01:58, 2337.32 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74709/350343 [00:32<01:57, 2339.00 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74946/350343 [00:32<01:57, 2347.84 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 75185/350343 [00:32<01:56, 2357.90 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75426/350343 [00:32<01:55, 2371.87 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75664/350343 [00:32<01:56, 2350.74 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75900/350343 [00:32<02:02, 2246.92 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76126/350343 [00:32<02:07, 2155.21 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76347/350343 [00:32<02:06, 2170.21 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76586/350343 [00:33<02:02, 2231.27 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76829/350343 [00:33<01:59, 2287.79 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77065/350343 [00:33<01:58, 2307.38 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77307/350343 [00:33<01:56, 2337.79 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77546/350343 [00:33<01:55, 2352.67 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77787/350343 [00:33<01:55, 2368.06 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78025/350343 [00:33<01:59, 2280.82 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78257/350343 [00:33<01:58, 2290.71 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78495/350343 [00:33<01:57, 2315.24 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78730/350343 [00:33<01:56, 2324.20 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 78971/350343 [00:34<01:55, 2347.85 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79209/350343 [00:34<01:55, 2354.98 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79447/350343 [00:34<01:54, 2362.03 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79684/350343 [00:34<01:55, 2352.64 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79929/350343 [00:34<01:53, 2379.25 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80171/350343 [00:34<01:53, 2389.69 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80411/350343 [00:34<01:53, 2386.48 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80650/350343 [00:34<01:53, 2370.34 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80889/350343 [00:34<01:53, 2373.85 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81127/350343 [00:34<01:54, 2357.86 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81367/350343 [00:35<01:53, 2369.45 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81604/350343 [00:35<01:57, 2277.97 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81841/350343 [00:35<01:56, 2302.46 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82083/350343 [00:35<01:54, 2335.68 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82318/350343 [00:35<01:54, 2332.31 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82554/350343 [00:35<01:54, 2339.60 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82789/350343 [00:35<01:55, 2308.92 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 83021/350343 [00:35<01:55, 2309.40 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83253/350343 [00:35<01:55, 2311.22 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83491/350343 [00:35<01:54, 2329.28 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83726/350343 [00:36<01:54, 2333.81 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83960/350343 [00:36<01:54, 2334.17 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84194/350343 [00:36<01:53, 2334.89 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84428/350343 [00:36<01:54, 2331.38 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84666/350343 [00:36<01:53, 2344.37 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84905/350343 [00:36<01:52, 2357.62 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85141/350343 [00:36<01:53, 2342.07 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85378/350343 [00:36<01:52, 2347.71 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85616/350343 [00:36<01:52, 2357.24 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 85852/350343 [00:37<01:55, 2286.14 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86092/350343 [00:37<01:53, 2319.07 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86332/350343 [00:37<01:52, 2339.80 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86577/350343 [00:37<01:51, 2371.38 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86815/350343 [00:37<01:51, 2373.88 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87053/350343 [00:37<01:51, 2368.97 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87291/350343 [00:37<01:51, 2363.40 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87536/350343 [00:37<01:50, 2387.05 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 87775/350343 [00:37<01:49, 2387.69 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88014/350343 [00:37<01:50, 2381.59 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88253/350343 [00:38<01:50, 2379.18 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88491/350343 [00:38<01:50, 2375.52 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88729/350343 [00:38<01:50, 2373.61 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88967/350343 [00:38<01:51, 2354.34 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 89203/350343 [00:38<01:54, 2280.65 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89445/350343 [00:38<01:52, 2321.00 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89679/350343 [00:38<01:52, 2325.28 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89917/350343 [00:38<01:51, 2341.34 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90152/350343 [00:38<01:51, 2335.48 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90386/350343 [00:38<01:51, 2336.43 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90620/350343 [00:39<01:51, 2333.40 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90855/350343 [00:39<01:51, 2336.60 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91096/350343 [00:39<01:50, 2356.69 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91332/350343 [00:39<01:56, 2223.21 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91556/350343 [00:39<02:22, 1811.68 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91767/350343 [00:39<02:17, 1884.82 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 91984/350343 [00:39<02:11, 1958.53 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92220/350343 [00:39<02:04, 2067.15 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92454/350343 [00:39<02:00, 2143.00 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92692/350343 [00:40<01:56, 2210.34 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 92919/350343 [00:40<01:55, 2226.79 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93148/350343 [00:40<01:54, 2243.02 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93376/350343 [00:40<01:54, 2252.99 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93605/350343 [00:40<01:53, 2263.01 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93840/350343 [00:40<01:52, 2287.28 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94070/350343 [00:40<01:52, 2283.52 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94299/350343 [00:40<01:58, 2153.57 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94517/350343 [00:40<01:59, 2140.38 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94733/350343 [00:40<02:01, 2098.83 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94968/350343 [00:41<01:57, 2169.53 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95196/350343 [00:41<01:55, 2199.91 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95422/350343 [00:41<01:55, 2215.82 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95651/350343 [00:41<01:53, 2236.43 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95887/350343 [00:41<01:52, 2271.08 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 96120/350343 [00:41<01:51, 2288.22 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96355/350343 [00:41<01:50, 2304.47 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96586/350343 [00:41<01:54, 2222.25 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96820/350343 [00:41<01:52, 2255.36 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97054/350343 [00:41<01:51, 2278.36 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97285/350343 [00:42<01:50, 2285.83 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97518/350343 [00:42<01:49, 2298.80 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97752/350343 [00:42<01:49, 2309.55 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97984/350343 [00:42<01:49, 2297.53 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98228/350343 [00:42<01:47, 2338.40 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98464/350343 [00:42<01:47, 2344.50 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98699/350343 [00:42<01:47, 2330.27 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98933/350343 [00:42<01:48, 2306.78 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99169/350343 [00:42<01:48, 2320.35 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99402/350343 [00:42<01:48, 2312.94 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99634/350343 [00:43<01:48, 2304.21 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 99867/350343 [00:43<01:48, 2310.19 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100106/350343 [00:43<01:47, 2333.51 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100340/350343 [00:43<01:47, 2322.73 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100576/350343 [00:43<01:47, 2333.53 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 100813/350343 [00:43<01:46, 2344.03 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101048/350343 [00:43<01:46, 2342.71 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101283/350343 [00:43<01:46, 2328.17 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101516/350343 [00:43<01:50, 2248.48 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101752/350343 [00:44<01:49, 2278.71 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101990/350343 [00:44<01:47, 2306.94 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102227/350343 [00:44<01:46, 2324.03 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102465/350343 [00:44<01:45, 2340.17 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102701/350343 [00:44<01:45, 2345.07 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102936/350343 [00:44<01:45, 2345.25 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 103171/350343 [00:44<01:45, 2345.90 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103406/350343 [00:44<01:46, 2326.50 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103639/350343 [00:44<01:46, 2316.59 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103875/350343 [00:44<01:45, 2326.87 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104114/350343 [00:45<01:45, 2343.10 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104349/350343 [00:45<01:44, 2344.77 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104584/350343 [00:45<01:45, 2340.36 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104821/350343 [00:45<01:44, 2349.06 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 105056/350343 [00:45<01:44, 2341.79 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105295/350343 [00:45<01:44, 2354.04 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105531/350343 [00:45<01:44, 2348.69 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105766/350343 [00:45<01:45, 2324.95 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105999/350343 [00:45<01:49, 2240.60 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106238/350343 [00:45<01:46, 2282.26 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106467/350343 [00:46<01:49, 2218.81 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106690/350343 [00:46<01:50, 2206.82 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 106921/350343 [00:46<01:48, 2234.68 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107157/350343 [00:46<01:47, 2269.04 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107393/350343 [00:46<01:45, 2295.29 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107628/350343 [00:46<01:45, 2308.66 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107860/350343 [00:46<01:45, 2308.04 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108091/350343 [00:46<01:45, 2291.57 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108326/350343 [00:46<01:44, 2308.34 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108559/350343 [00:46<01:44, 2312.51 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108792/350343 [00:47<01:44, 2317.53 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109024/350343 [00:47<01:44, 2311.49 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109261/350343 [00:47<01:43, 2326.84 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109494/350343 [00:47<01:43, 2319.46 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109727/350343 [00:47<01:43, 2320.05 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109964/350343 [00:47<01:43, 2333.72 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110198/350343 [00:47<01:43, 2326.30 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110431/350343 [00:47<01:43, 2307.21 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110662/350343 [00:47<01:44, 2298.61 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110897/350343 [00:47<01:43, 2312.70 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111130/350343 [00:48<01:43, 2317.40 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111362/350343 [00:48<01:46, 2245.00 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111601/350343 [00:48<01:44, 2284.44 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111841/350343 [00:48<01:42, 2316.60 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112074/350343 [00:48<01:43, 2294.13 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112304/350343 [00:48<01:43, 2294.80 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112537/350343 [00:48<01:43, 2304.12 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112768/350343 [00:48<01:43, 2289.74 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113003/350343 [00:48<01:42, 2305.23 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113240/350343 [00:48<01:42, 2323.72 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113476/350343 [00:49<01:41, 2332.48 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113710/350343 [00:49<01:41, 2320.33 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 113943/350343 [00:49<01:41, 2317.66 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114177/350343 [00:49<01:41, 2322.34 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114419/350343 [00:49<01:40, 2349.39 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114657/350343 [00:49<01:39, 2357.07 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114893/350343 [00:49<01:39, 2356.09 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115129/350343 [00:49<01:41, 2327.40 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115362/350343 [00:49<01:41, 2309.61 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115598/350343 [00:49<01:41, 2323.36 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115832/350343 [00:50<01:44, 2251.26 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116062/350343 [00:50<01:43, 2263.72 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116289/350343 [00:50<01:45, 2225.62 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116512/350343 [00:50<01:45, 2213.04 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116741/350343 [00:50<01:44, 2235.49 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116969/350343 [00:50<01:43, 2246.47 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117196/350343 [00:50<01:43, 2250.63 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117422/350343 [00:50<01:43, 2242.83 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117654/350343 [00:50<01:42, 2264.40 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117883/350343 [00:51<01:42, 2270.61 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 118116/350343 [00:51<01:41, 2287.16 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118358/350343 [00:51<01:39, 2325.68 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118591/350343 [00:51<01:39, 2321.01 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118828/350343 [00:51<01:39, 2333.96 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119062/350343 [00:51<01:39, 2332.69 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119296/350343 [00:51<01:39, 2328.60 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119529/350343 [00:51<01:39, 2312.32 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119764/350343 [00:51<01:39, 2322.37 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119997/350343 [00:51<01:39, 2323.31 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120235/350343 [00:52<01:38, 2339.72 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120469/350343 [00:52<01:38, 2328.37 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120702/350343 [00:52<01:38, 2322.85 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 120935/350343 [00:52<01:38, 2321.45 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121175/350343 [00:52<01:37, 2343.29 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121410/350343 [00:52<01:37, 2338.95 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121646/350343 [00:52<01:37, 2344.88 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121881/350343 [00:52<01:42, 2225.22 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122118/350343 [00:52<01:40, 2266.39 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122349/350343 [00:52<01:40, 2278.68 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122585/350343 [00:53<01:38, 2300.75 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122820/350343 [00:53<01:38, 2313.69 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123053/350343 [00:53<01:38, 2318.15 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123286/350343 [00:53<01:39, 2292.52 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123516/350343 [00:53<01:39, 2289.65 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123748/350343 [00:53<01:38, 2298.40 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123978/350343 [00:53<01:38, 2295.40 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 124208/350343 [00:53<01:39, 2280.13 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124442/350343 [00:53<01:38, 2296.86 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124673/350343 [00:53<01:38, 2298.79 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124911/350343 [00:54<01:37, 2321.85 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125144/350343 [00:54<01:37, 2313.36 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125378/350343 [00:54<01:36, 2319.51 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125612/350343 [00:54<01:36, 2323.65 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125845/350343 [00:54<01:36, 2321.42 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126085/350343 [00:54<01:35, 2343.37 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126320/350343 [00:54<01:35, 2341.18 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126555/350343 [00:54<01:36, 2322.60 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126791/350343 [00:54<01:39, 2246.73 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127031/350343 [00:54<01:37, 2289.11 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127270/350343 [00:55<01:36, 2314.18 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127503/350343 [00:55<01:36, 2316.64 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127735/350343 [00:55<01:36, 2311.65 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 127973/350343 [00:55<01:35, 2331.49 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128207/350343 [00:55<01:35, 2321.98 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128440/350343 [00:55<01:35, 2317.49 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128672/350343 [00:55<01:36, 2302.34 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128903/350343 [00:55<01:36, 2287.79 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129135/350343 [00:55<01:36, 2295.75 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129367/350343 [00:55<01:36, 2300.84 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129602/350343 [00:56<01:35, 2313.07 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129836/350343 [00:56<01:35, 2319.17 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130073/350343 [00:56<01:34, 2332.09 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130307/350343 [00:56<01:34, 2326.80 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130540/350343 [00:56<01:34, 2315.43 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130778/350343 [00:56<01:34, 2334.04 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131012/350343 [00:56<01:33, 2333.48 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131246/350343 [00:56<01:34, 2325.41 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131480/350343 [00:56<01:33, 2329.69 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131718/350343 [00:56<01:33, 2342.05 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131953/350343 [00:57<01:36, 2261.21 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132192/350343 [00:57<01:34, 2297.51 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132429/350343 [00:57<01:34, 2317.72 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132662/350343 [00:57<01:34, 2312.52 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132898/350343 [00:57<01:33, 2326.02 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133133/350343 [00:57<01:33, 2332.69 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133367/350343 [00:57<01:33, 2323.17 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133600/350343 [00:57<01:33, 2318.94 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133834/350343 [00:57<01:33, 2322.92 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134068/350343 [00:58<01:32, 2327.65 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134301/350343 [00:58<01:33, 2313.97 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134533/350343 [00:58<01:33, 2311.91 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134765/350343 [00:58<01:33, 2305.33 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 134996/350343 [00:58<01:33, 2291.99 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135227/350343 [00:58<01:33, 2295.10 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135457/350343 [00:58<01:33, 2293.51 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135687/350343 [00:58<01:34, 2271.11 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 135916/350343 [00:58<01:34, 2275.97 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136147/350343 [00:58<01:33, 2284.47 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136380/350343 [00:59<01:33, 2296.56 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136610/350343 [00:59<01:33, 2296.05 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136846/350343 [00:59<01:32, 2313.29 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137078/350343 [00:59<01:32, 2304.35 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137309/350343 [00:59<01:32, 2295.81 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137539/350343 [00:59<01:36, 2208.85 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137768/350343 [00:59<01:35, 2230.07 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137992/350343 [00:59<01:35, 2222.22 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138229/350343 [00:59<01:33, 2263.93 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138467/350343 [00:59<01:32, 2296.50 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138704/350343 [01:00<01:31, 2317.72 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138936/350343 [01:00<01:31, 2310.03 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139168/350343 [01:00<01:31, 2312.95 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139401/350343 [01:00<01:31, 2316.89 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139638/350343 [01:00<01:30, 2330.91 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139872/350343 [01:00<01:30, 2322.05 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 140113/350343 [01:00<01:29, 2345.38 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140348/350343 [01:00<01:29, 2334.34 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140582/350343 [01:00<01:29, 2335.31 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140817/350343 [01:00<01:29, 2337.34 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141056/350343 [01:01<01:28, 2352.54 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141293/350343 [01:01<01:28, 2357.02 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141529/350343 [01:01<01:28, 2354.94 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141765/350343 [01:01<01:29, 2341.98 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142000/350343 [01:01<01:29, 2326.15 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142233/350343 [01:01<01:29, 2323.59 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142466/350343 [01:01<01:33, 2229.56 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142694/350343 [01:01<01:32, 2242.54 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142932/350343 [01:01<01:30, 2281.39 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143168/350343 [01:01<01:29, 2303.51 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143402/350343 [01:02<01:29, 2313.69 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143634/350343 [01:02<01:29, 2315.09 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143866/350343 [01:02<01:29, 2310.73 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144102/350343 [01:02<01:28, 2323.03 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144335/350343 [01:02<01:28, 2318.84 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144567/350343 [01:02<01:28, 2318.05 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144799/350343 [01:02<01:29, 2301.19 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145030/350343 [01:02<01:30, 2276.43 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145266/350343 [01:02<01:29, 2300.41 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145506/350343 [01:02<01:27, 2328.85 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145739/350343 [01:03<01:28, 2320.49 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145975/350343 [01:03<01:27, 2330.83 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146211/350343 [01:03<01:27, 2337.84 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146448/350343 [01:03<01:26, 2344.92 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146685/350343 [01:03<01:26, 2349.96 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146921/350343 [01:03<01:26, 2341.37 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147156/350343 [01:03<01:26, 2336.93 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147390/350343 [01:03<01:30, 2251.67 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147630/350343 [01:03<01:28, 2292.49 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147861/350343 [01:03<01:28, 2297.47 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148097/350343 [01:04<01:27, 2314.33 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148329/350343 [01:04<01:27, 2309.25 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148562/350343 [01:04<01:27, 2314.85 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148794/350343 [01:04<01:27, 2312.18 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149031/350343 [01:04<01:26, 2329.35 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149265/350343 [01:04<01:26, 2321.25 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149498/350343 [01:04<01:27, 2291.47 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149734/350343 [01:04<01:26, 2309.62 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149966/350343 [01:04<01:26, 2312.29 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150202/350343 [01:04<01:26, 2325.04 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150436/350343 [01:05<01:25, 2328.40 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150672/350343 [01:05<01:25, 2335.98 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150906/350343 [01:05<01:25, 2331.93 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151140/350343 [01:05<01:25, 2318.97 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151372/350343 [01:05<01:25, 2317.48 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151609/350343 [01:05<01:25, 2332.99 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151843/350343 [01:05<01:25, 2317.38 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152075/350343 [01:05<01:25, 2311.60 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152307/350343 [01:05<01:28, 2229.79 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152539/350343 [01:06<01:27, 2254.92 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152771/350343 [01:06<01:26, 2272.51 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153007/350343 [01:06<01:25, 2298.06 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153241/350343 [01:06<01:25, 2309.28 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153475/350343 [01:06<01:24, 2317.29 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153709/350343 [01:06<01:24, 2323.46 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153947/350343 [01:06<01:23, 2338.36 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154181/350343 [01:06<01:24, 2318.17 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154414/350343 [01:06<01:24, 2321.21 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154647/350343 [01:06<01:24, 2321.24 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154880/350343 [01:07<01:24, 2317.11 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155112/350343 [01:07<01:24, 2311.20 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155344/350343 [01:07<01:24, 2302.76 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155579/350343 [01:07<01:24, 2315.74 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155811/350343 [01:07<01:24, 2310.21 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156043/350343 [01:07<01:24, 2303.85 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156274/350343 [01:07<01:24, 2296.37 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156504/350343 [01:07<01:25, 2277.38 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156737/350343 [01:07<01:24, 2290.75 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156976/350343 [01:07<01:23, 2319.44 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157208/350343 [01:08<01:23, 2317.47 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157440/350343 [01:08<01:24, 2294.19 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157670/350343 [01:08<01:24, 2286.56 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157899/350343 [01:08<01:27, 2203.40 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158135/350343 [01:08<01:25, 2247.59 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158368/350343 [01:08<01:24, 2270.14 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158600/350343 [01:08<01:23, 2283.25 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158829/350343 [01:08<01:24, 2272.37 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159057/350343 [01:08<01:26, 2219.61 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159286/350343 [01:08<01:25, 2237.66 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159517/350343 [01:09<01:24, 2257.49 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159751/350343 [01:09<01:23, 2279.60 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159985/350343 [01:09<01:22, 2294.87 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160219/350343 [01:09<01:22, 2306.96 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160450/350343 [01:09<01:22, 2306.80 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160681/350343 [01:09<01:22, 2301.38 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160915/350343 [01:09<01:21, 2311.72 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161147/350343 [01:09<01:22, 2280.92 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161382/350343 [01:09<01:22, 2300.73 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161615/350343 [01:09<01:21, 2308.08 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161848/350343 [01:10<01:21, 2312.69 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162080/350343 [01:10<01:21, 2305.55 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162318/350343 [01:10<01:20, 2327.18 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162552/350343 [01:10<01:20, 2330.16 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162788/350343 [01:10<01:20, 2337.65 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163028/350343 [01:10<01:19, 2354.73 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163268/350343 [01:10<01:19, 2366.24 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163505/350343 [01:10<01:22, 2265.46 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163736/350343 [01:10<01:21, 2276.36 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163971/350343 [01:10<01:21, 2296.64 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164210/350343 [01:11<01:20, 2324.10 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164444/350343 [01:11<01:19, 2327.18 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164685/350343 [01:11<01:18, 2350.50 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164921/350343 [01:11<01:19, 2334.86 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165155/350343 [01:11<01:19, 2331.97 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165389/350343 [01:11<01:19, 2329.72 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165623/350343 [01:11<01:20, 2304.23 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165854/350343 [01:11<01:20, 2305.61 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166087/350343 [01:11<01:19, 2311.09 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166319/350343 [01:11<01:19, 2304.86 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166551/350343 [01:12<01:19, 2308.19 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166787/350343 [01:12<01:19, 2322.12 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167020/350343 [01:12<01:19, 2318.06 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167252/350343 [01:12<01:19, 2297.07 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167491/350343 [01:12<01:18, 2323.97 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167725/350343 [01:12<01:18, 2326.06 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167958/350343 [01:12<01:19, 2301.15 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168189/350343 [01:12<01:22, 2218.16 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168425/350343 [01:12<01:20, 2257.01 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168660/350343 [01:13<01:19, 2283.14 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168893/350343 [01:13<01:19, 2295.74 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169125/350343 [01:13<01:18, 2302.12 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169356/350343 [01:13<01:18, 2304.33 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169591/350343 [01:13<01:18, 2315.70 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169825/350343 [01:13<01:17, 2321.22 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170066/350343 [01:13<01:16, 2346.59 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170301/350343 [01:13<01:17, 2322.19 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170534/350343 [01:13<01:17, 2314.80 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170770/350343 [01:13<01:17, 2326.39 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171005/350343 [01:14<01:16, 2332.00 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171239/350343 [01:14<01:16, 2332.70 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171476/350343 [01:14<01:16, 2342.36 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171711/350343 [01:14<01:16, 2331.46 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171945/350343 [01:14<01:16, 2326.60 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172181/350343 [01:14<01:16, 2335.89 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172415/350343 [01:14<01:16, 2332.56 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172649/350343 [01:14<01:17, 2303.22 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172884/350343 [01:14<01:19, 2231.39 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173113/350343 [01:14<01:18, 2247.86 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173348/350343 [01:15<01:17, 2275.46 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173581/350343 [01:15<01:17, 2288.87 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173815/350343 [01:15<01:16, 2302.62 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174047/350343 [01:15<01:16, 2305.88 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174281/350343 [01:15<01:16, 2314.77 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174513/350343 [01:15<01:15, 2316.12 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174745/350343 [01:15<01:15, 2316.87 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174977/350343 [01:15<01:16, 2306.15 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175210/350343 [01:15<01:15, 2311.50 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175442/350343 [01:15<01:15, 2313.84 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175676/350343 [01:16<01:15, 2321.24 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175909/350343 [01:16<01:15, 2321.70 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176142/350343 [01:16<01:15, 2310.13 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176374/350343 [01:16<01:15, 2295.58 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176604/350343 [01:16<01:15, 2296.70 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176840/350343 [01:16<01:14, 2313.66 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177072/350343 [01:16<01:15, 2298.67 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177302/350343 [01:16<01:15, 2289.36 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177531/350343 [01:16<01:15, 2284.07 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177768/350343 [01:16<01:14, 2309.49 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178002/350343 [01:17<01:14, 2317.24 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178238/350343 [01:17<01:13, 2328.31 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178471/350343 [01:17<01:13, 2328.64 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178704/350343 [01:17<01:16, 2241.05 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178936/350343 [01:17<01:15, 2263.95 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179175/350343 [01:17<01:14, 2299.41 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179406/350343 [01:17<01:14, 2295.80 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179636/350343 [01:17<01:14, 2290.39 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179866/350343 [01:17<01:14, 2287.45 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180100/350343 [01:17<01:13, 2302.78 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180331/350343 [01:18<01:14, 2296.35 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180565/350343 [01:18<01:13, 2307.52 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180797/350343 [01:18<01:13, 2309.31 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181028/350343 [01:18<01:13, 2305.61 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181259/350343 [01:18<01:13, 2306.40 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181496/350343 [01:18<01:12, 2325.33 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181729/350343 [01:18<01:12, 2316.34 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181963/350343 [01:18<01:12, 2323.14 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182200/350343 [01:18<01:11, 2335.76 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182436/350343 [01:18<01:11, 2342.53 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182671/350343 [01:19<01:11, 2336.62 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182905/350343 [01:19<01:11, 2334.35 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183139/350343 [01:19<01:11, 2329.29 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183372/350343 [01:19<01:11, 2327.57 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183606/350343 [01:19<01:11, 2330.73 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183840/350343 [01:19<01:11, 2320.21 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184073/350343 [01:19<01:14, 2217.05 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184304/350343 [01:19<01:14, 2243.56 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184531/350343 [01:19<01:13, 2251.12 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184762/350343 [01:20<01:13, 2267.72 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184993/350343 [01:20<01:12, 2279.50 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185228/350343 [01:20<01:11, 2298.37 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185461/350343 [01:20<01:11, 2305.61 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185692/350343 [01:20<01:11, 2295.97 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185922/350343 [01:20<01:11, 2293.76 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186156/350343 [01:20<01:11, 2305.99 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186387/350343 [01:20<01:11, 2299.30 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186617/350343 [01:20<01:11, 2297.76 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186849/350343 [01:20<01:11, 2302.28 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187080/350343 [01:21<01:11, 2297.49 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187310/350343 [01:21<01:11, 2295.66 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187540/350343 [01:21<01:11, 2282.42 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187774/350343 [01:21<01:10, 2296.85 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188004/350343 [01:21<01:10, 2295.24 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188234/350343 [01:21<01:10, 2286.09 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188464/350343 [01:21<01:10, 2289.93 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188694/350343 [01:21<01:11, 2268.32 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188925/350343 [01:21<01:10, 2279.65 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189154/350343 [01:21<01:10, 2274.98 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189387/350343 [01:22<01:10, 2289.84 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189619/350343 [01:22<01:09, 2296.50 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189849/350343 [01:22<01:09, 2294.00 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190085/350343 [01:22<01:09, 2311.46 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190319/350343 [01:22<01:09, 2318.49 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190554/350343 [01:22<01:10, 2253.21 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190792/350343 [01:22<01:09, 2289.13 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191022/350343 [01:22<01:09, 2291.52 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191259/350343 [01:22<01:08, 2312.69 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191495/350343 [01:22<01:08, 2325.25 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191728/350343 [01:23<01:08, 2320.39 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191961/350343 [01:23<01:08, 2311.89 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192193/350343 [01:23<01:08, 2301.24 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192424/350343 [01:23<01:08, 2297.06 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192654/350343 [01:23<01:08, 2288.30 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192885/350343 [01:23<01:08, 2294.23 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193115/350343 [01:23<01:08, 2292.93 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193345/350343 [01:23<01:09, 2272.69 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193575/350343 [01:23<01:08, 2278.93 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193803/350343 [01:23<01:08, 2274.68 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194031/350343 [01:24<01:08, 2275.63 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194265/350343 [01:24<01:08, 2293.01 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194501/350343 [01:24<01:07, 2311.04 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194739/350343 [01:24<01:06, 2330.43 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194973/350343 [01:24<01:06, 2326.46 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195210/350343 [01:24<01:06, 2338.71 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195446/350343 [01:24<01:06, 2344.11 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195681/350343 [01:24<01:06, 2329.57 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195916/350343 [01:24<01:06, 2333.15 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196150/350343 [01:24<01:09, 2229.09 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196377/350343 [01:25<01:08, 2240.84 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196610/350343 [01:25<01:07, 2265.60 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196845/350343 [01:25<01:07, 2288.33 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197084/350343 [01:25<01:06, 2316.72 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197322/350343 [01:25<01:05, 2335.48 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197556/350343 [01:25<01:05, 2333.50 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197790/350343 [01:25<01:05, 2335.00 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198024/350343 [01:25<01:05, 2331.94 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198260/350343 [01:25<01:05, 2338.73 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198494/350343 [01:25<01:05, 2306.65 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198726/350343 [01:26<01:05, 2309.98 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198958/350343 [01:26<01:05, 2300.72 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199189/350343 [01:26<01:05, 2293.98 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199419/350343 [01:26<01:05, 2286.84 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199650/350343 [01:26<01:05, 2291.83 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199885/350343 [01:26<01:05, 2308.16 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200116/350343 [01:26<01:05, 2297.61 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200346/350343 [01:26<01:05, 2290.26 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200579/350343 [01:26<01:05, 2299.91 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200816/350343 [01:26<01:04, 2318.95 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201048/350343 [01:27<01:04, 2309.87 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201280/350343 [01:27<01:04, 2294.57 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201510/350343 [01:27<01:05, 2288.13 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201739/350343 [01:27<01:05, 2271.92 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201967/350343 [01:27<01:07, 2184.11 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202193/350343 [01:27<01:07, 2205.75 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202416/350343 [01:27<01:06, 2211.05 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202647/350343 [01:27<01:05, 2239.73 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202882/350343 [01:27<01:04, 2270.54 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203112/350343 [01:28<01:04, 2278.36 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203342/350343 [01:28<01:04, 2282.37 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203571/350343 [01:28<01:04, 2274.14 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203810/350343 [01:28<01:03, 2307.01 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204041/350343 [01:28<01:03, 2303.16 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204274/350343 [01:28<01:03, 2308.87 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204510/350343 [01:28<01:02, 2323.26 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204743/350343 [01:28<01:03, 2292.18 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204977/350343 [01:28<01:03, 2305.69 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205208/350343 [01:28<01:02, 2306.57 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205440/350343 [01:29<01:02, 2309.54 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205672/350343 [01:29<01:03, 2290.73 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 205903/350343 [01:29<01:02, 2295.79 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206133/350343 [01:29<01:02, 2290.14 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206366/350343 [01:29<01:02, 2299.93 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206604/350343 [01:29<01:01, 2323.58 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206837/350343 [01:29<01:01, 2314.67 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207069/350343 [01:29<01:02, 2297.05 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207306/350343 [01:29<01:01, 2317.18 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207538/350343 [01:29<01:01, 2315.64 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207771/350343 [01:30<01:01, 2318.02 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208003/350343 [01:30<01:04, 2218.67 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208237/350343 [01:30<01:03, 2252.85 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208466/350343 [01:30<01:02, 2262.31 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208699/350343 [01:30<01:02, 2281.67 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208933/350343 [01:30<01:01, 2297.89 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209164/350343 [01:30<01:01, 2291.68 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209394/350343 [01:30<01:01, 2287.19 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209626/350343 [01:30<01:01, 2294.79 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209858/350343 [01:30<01:01, 2302.00 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210089/350343 [01:31<01:01, 2287.75 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210325/350343 [01:31<01:00, 2307.39 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210557/350343 [01:31<01:00, 2309.98 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210789/350343 [01:31<01:00, 2308.16 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211021/350343 [01:31<01:00, 2309.31 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211257/350343 [01:31<00:59, 2323.47 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211491/350343 [01:31<00:59, 2325.97 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211724/350343 [01:31<01:00, 2295.64 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211960/350343 [01:31<00:59, 2314.52 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212196/350343 [01:31<00:59, 2326.70 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212429/350343 [01:32<00:59, 2304.84 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212662/350343 [01:32<00:59, 2311.63 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212894/350343 [01:32<00:59, 2301.09 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213125/350343 [01:32<00:59, 2299.34 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213355/350343 [01:32<00:59, 2297.02 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213589/350343 [01:32<00:59, 2308.40 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213820/350343 [01:32<01:00, 2240.37 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214045/350343 [01:32<01:01, 2231.43 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214269/350343 [01:32<01:03, 2132.45 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214489/350343 [01:32<01:05, 2089.67 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214715/350343 [01:33<01:03, 2136.74 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214943/350343 [01:33<01:02, 2175.65 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215171/350343 [01:33<01:01, 2203.87 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215398/350343 [01:33<01:00, 2222.30 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215626/350343 [01:33<01:00, 2237.86 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215857/350343 [01:33<00:59, 2257.27 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216087/350343 [01:33<00:59, 2269.24 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216315/350343 [01:33<00:59, 2244.74 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216546/350343 [01:33<00:59, 2263.26 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216780/350343 [01:33<00:58, 2283.99 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217009/350343 [01:34<00:58, 2276.75 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217242/350343 [01:34<00:58, 2291.05 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217479/350343 [01:34<00:57, 2313.75 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217714/350343 [01:34<00:57, 2324.18 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217947/350343 [01:34<00:56, 2323.94 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218182/350343 [01:34<00:56, 2329.88 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218416/350343 [01:34<00:56, 2326.69 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218649/350343 [01:34<00:57, 2286.95 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218880/350343 [01:34<00:57, 2292.31 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219113/350343 [01:35<00:56, 2302.79 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219344/350343 [01:35<00:57, 2295.34 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219575/350343 [01:35<00:56, 2298.82 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219813/350343 [01:35<00:56, 2320.68 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220046/350343 [01:35<00:56, 2322.36 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220282/350343 [01:35<00:55, 2331.54 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220516/350343 [01:35<00:55, 2329.17 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220752/350343 [01:35<00:57, 2248.14 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220981/350343 [01:35<00:57, 2258.13 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221215/350343 [01:35<00:56, 2281.77 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221452/350343 [01:36<00:55, 2305.55 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221683/350343 [01:36<00:56, 2291.69 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221916/350343 [01:36<00:55, 2302.88 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222150/350343 [01:36<00:55, 2313.59 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222382/350343 [01:36<00:55, 2312.81 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222614/350343 [01:36<00:55, 2304.91 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222845/350343 [01:36<00:55, 2303.67 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223076/350343 [01:36<00:55, 2303.07 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223307/350343 [01:36<00:55, 2281.65 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223538/350343 [01:36<00:55, 2289.47 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223778/350343 [01:37<00:54, 2321.84 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224011/350343 [01:37<00:54, 2306.41 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224242/350343 [01:37<00:54, 2293.70 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224472/350343 [01:37<00:54, 2295.10 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224702/350343 [01:37<00:54, 2290.00 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224936/350343 [01:37<00:54, 2304.67 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225167/350343 [01:37<00:54, 2305.83 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225398/350343 [01:37<00:54, 2305.74 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225629/350343 [01:37<00:54, 2305.53 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225860/350343 [01:37<00:53, 2305.73 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226091/350343 [01:38<00:53, 2304.44 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226322/350343 [01:38<00:54, 2288.48 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226551/350343 [01:38<00:54, 2281.25 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226787/350343 [01:38<00:53, 2302.86 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227018/350343 [01:38<00:55, 2215.31 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227245/350343 [01:38<00:55, 2230.96 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227480/350343 [01:38<00:54, 2264.02 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227709/350343 [01:38<00:54, 2270.47 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 227937/350343 [01:38<00:54, 2266.18 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228167/350343 [01:38<00:53, 2275.15 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228397/350343 [01:39<00:53, 2282.16 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228626/350343 [01:39<00:54, 2235.14 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228850/350343 [01:39<00:55, 2186.97 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229081/350343 [01:39<00:54, 2221.04 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229304/350343 [01:39<00:54, 2217.67 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229527/350343 [01:39<00:59, 2034.90 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229734/350343 [01:39<01:06, 1815.01 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229951/350343 [01:39<01:03, 1906.16 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230179/350343 [01:39<00:59, 2007.42 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230410/350343 [01:40<00:57, 2090.50 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230640/350343 [01:40<00:55, 2148.42 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230871/350343 [01:40<00:54, 2194.38 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231105/350343 [01:40<00:53, 2236.34 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231340/350343 [01:40<00:52, 2267.87 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231573/350343 [01:40<00:51, 2285.63 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231808/350343 [01:40<00:51, 2304.29 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 232042/350343 [01:40<00:51, 2314.42 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232274/350343 [01:40<00:51, 2312.26 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232506/350343 [01:40<00:51, 2307.68 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232737/350343 [01:41<00:52, 2222.93 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232961/350343 [01:41<00:53, 2181.64 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233186/350343 [01:41<00:53, 2199.16 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233414/350343 [01:41<00:52, 2221.16 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233646/350343 [01:41<00:51, 2248.13 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233884/350343 [01:41<00:50, 2286.18 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234120/350343 [01:41<00:50, 2306.28 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234354/350343 [01:41<00:50, 2315.43 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234590/350343 [01:41<00:49, 2326.43 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234823/350343 [01:41<00:50, 2297.12 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235057/350343 [01:42<00:49, 2308.24 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235288/350343 [01:42<00:50, 2292.81 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235518/350343 [01:42<00:52, 2198.28 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235756/350343 [01:42<00:50, 2250.27 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235986/350343 [01:42<00:50, 2263.99 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236217/350343 [01:42<00:50, 2276.83 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236451/350343 [01:42<00:49, 2295.31 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236684/350343 [01:42<00:49, 2304.05 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236915/350343 [01:42<00:49, 2302.93 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237146/350343 [01:43<00:49, 2297.65 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237380/350343 [01:43<00:48, 2309.26 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237614/350343 [01:43<00:48, 2318.03 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237852/350343 [01:43<00:48, 2336.16 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238086/350343 [01:43<00:48, 2335.59 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238323/350343 [01:43<00:47, 2344.42 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238558/350343 [01:43<00:47, 2341.98 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238793/350343 [01:43<00:47, 2330.95 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239027/350343 [01:43<00:47, 2332.82 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239261/350343 [01:43<00:48, 2301.69 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239496/350343 [01:44<00:47, 2315.64 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239729/350343 [01:44<00:47, 2318.45 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239961/350343 [01:44<00:47, 2310.96 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240193/350343 [01:44<00:47, 2309.29 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240428/350343 [01:44<00:47, 2320.74 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240661/350343 [01:44<00:49, 2228.71 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 240894/350343 [01:44<00:48, 2257.52 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241122/350343 [01:44<00:48, 2263.16 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241356/350343 [01:44<00:47, 2283.87 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241585/350343 [01:44<00:47, 2274.70 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241818/350343 [01:45<00:47, 2290.57 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242054/350343 [01:45<00:46, 2309.67 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242286/350343 [01:45<00:47, 2289.04 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242516/350343 [01:45<00:47, 2281.54 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242750/350343 [01:45<00:46, 2298.19 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242982/350343 [01:45<00:46, 2303.38 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243216/350343 [01:45<00:46, 2312.02 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243453/350343 [01:45<00:45, 2327.80 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243686/350343 [01:45<00:45, 2318.83 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243918/350343 [01:45<00:46, 2300.29 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244151/350343 [01:46<00:46, 2306.91 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244384/350343 [01:46<00:45, 2310.77 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244616/350343 [01:46<00:45, 2300.35 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244848/350343 [01:46<00:45, 2305.72 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 245082/350343 [01:46<00:45, 2313.39 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245318/350343 [01:46<00:45, 2326.35 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245551/350343 [01:46<00:45, 2313.57 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245786/350343 [01:46<00:44, 2324.11 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246019/350343 [01:46<00:45, 2318.16 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246251/350343 [01:46<00:46, 2215.29 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246488/350343 [01:47<00:45, 2259.16 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246725/350343 [01:47<00:45, 2289.71 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246959/350343 [01:47<00:44, 2301.64 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247190/350343 [01:47<00:44, 2297.45 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247421/350343 [01:47<00:44, 2300.84 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247656/350343 [01:47<00:44, 2314.99 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247888/350343 [01:47<00:44, 2308.50 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248123/350343 [01:47<00:44, 2319.46 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248360/350343 [01:47<00:43, 2332.84 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248594/350343 [01:47<00:44, 2307.64 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248827/350343 [01:48<00:43, 2309.57 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249059/350343 [01:48<00:43, 2307.73 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249293/350343 [01:48<00:43, 2316.70 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249526/350343 [01:48<00:43, 2319.02 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249758/350343 [01:48<00:43, 2311.23 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249990/350343 [01:48<00:43, 2312.81 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250228/350343 [01:48<00:42, 2332.29 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250465/350343 [01:48<00:42, 2343.33 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250700/350343 [01:48<00:42, 2327.32 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250933/350343 [01:48<00:43, 2297.12 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251163/350343 [01:49<00:43, 2294.45 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251393/350343 [01:49<00:44, 2215.56 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251635/350343 [01:49<00:43, 2272.36 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251863/350343 [01:49<00:43, 2268.77 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252099/350343 [01:49<00:42, 2294.74 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252334/350343 [01:49<00:42, 2310.20 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252569/350343 [01:49<00:42, 2321.61 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252805/350343 [01:49<00:41, 2331.18 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253040/350343 [01:49<00:41, 2335.21 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253274/350343 [01:49<00:41, 2331.51 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253508/350343 [01:50<00:41, 2325.78 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253741/350343 [01:50<00:41, 2326.29 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253978/350343 [01:50<00:41, 2337.34 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254212/350343 [01:50<00:41, 2322.75 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254445/350343 [01:50<00:41, 2306.58 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254676/350343 [01:50<00:41, 2296.80 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254906/350343 [01:50<00:41, 2297.31 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255139/350343 [01:50<00:41, 2305.69 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255370/350343 [01:50<00:41, 2291.31 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255602/350343 [01:51<00:41, 2299.08 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255833/350343 [01:51<00:41, 2299.84 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256068/350343 [01:51<00:40, 2313.67 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256303/350343 [01:51<00:40, 2322.86 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256536/350343 [01:51<00:40, 2310.42 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256768/350343 [01:51<00:41, 2235.50 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257000/350343 [01:51<00:41, 2257.70 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257227/350343 [01:51<00:41, 2261.23 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257460/350343 [01:51<00:40, 2281.31 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257689/350343 [01:51<00:40, 2282.51 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257927/350343 [01:52<00:40, 2308.52 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 258158/350343 [01:52<00:40, 2297.29 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258392/350343 [01:52<00:39, 2309.46 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258629/350343 [01:52<00:39, 2327.30 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258862/350343 [01:52<00:39, 2321.47 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259096/350343 [01:52<00:39, 2326.21 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259331/350343 [01:52<00:39, 2331.48 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259565/350343 [01:52<00:38, 2328.49 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259800/350343 [01:52<00:38, 2334.82 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260034/350343 [01:52<00:38, 2321.25 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260267/350343 [01:53<00:38, 2319.70 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260499/350343 [01:53<00:38, 2304.01 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260730/350343 [01:53<00:38, 2299.58 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260961/350343 [01:53<00:38, 2300.17 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261197/350343 [01:53<00:38, 2317.40 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261432/350343 [01:53<00:38, 2326.19 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261665/350343 [01:53<00:38, 2314.64 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261898/350343 [01:53<00:38, 2316.99 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262130/350343 [01:53<00:39, 2231.86 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262354/350343 [01:53<00:39, 2223.46 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262582/350343 [01:54<00:39, 2239.00 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262809/350343 [01:54<00:38, 2246.75 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263034/350343 [01:54<00:38, 2243.80 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263264/350343 [01:54<00:38, 2258.31 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263496/350343 [01:54<00:38, 2276.30 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263732/350343 [01:54<00:37, 2299.14 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263963/350343 [01:54<00:37, 2302.32 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264194/350343 [01:54<00:37, 2301.47 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264425/350343 [01:54<00:37, 2288.61 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264654/350343 [01:54<00:37, 2264.99 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264889/350343 [01:55<00:37, 2288.68 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265118/350343 [01:55<00:37, 2282.67 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265347/350343 [01:55<00:37, 2284.27 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265578/350343 [01:55<00:37, 2289.95 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265809/350343 [01:55<00:36, 2295.48 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266040/350343 [01:55<00:36, 2297.59 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266270/350343 [01:55<00:36, 2280.90 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266505/350343 [01:55<00:36, 2299.92 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266737/350343 [01:55<00:36, 2303.17 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266968/350343 [01:55<00:36, 2290.72 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267200/350343 [01:56<00:36, 2299.03 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267430/350343 [01:56<00:36, 2291.74 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267661/350343 [01:56<00:36, 2295.46 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267894/350343 [01:56<00:35, 2305.64 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268127/350343 [01:56<00:35, 2311.99 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268359/350343 [01:56<00:35, 2305.99 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268596/350343 [01:56<00:35, 2323.67 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268829/350343 [01:56<00:36, 2240.23 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269065/350343 [01:56<00:35, 2273.78 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269296/350343 [01:56<00:35, 2283.80 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269537/350343 [01:57<00:34, 2318.57 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269770/350343 [01:57<00:34, 2321.35 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270005/350343 [01:57<00:34, 2328.01 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270238/350343 [01:57<00:34, 2303.80 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270473/350343 [01:57<00:34, 2316.38 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270706/350343 [01:57<00:34, 2319.68 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270939/350343 [01:57<00:34, 2316.84 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271173/350343 [01:57<00:34, 2323.18 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271407/350343 [01:57<00:33, 2327.13 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271640/350343 [01:57<00:34, 2313.06 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271877/350343 [01:58<00:33, 2326.88 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272110/350343 [01:58<00:33, 2318.07 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272342/350343 [01:58<00:33, 2314.62 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272576/350343 [01:58<00:33, 2320.12 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272814/350343 [01:58<00:33, 2335.91 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273048/350343 [01:58<00:33, 2335.26 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273282/350343 [01:58<00:33, 2323.75 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273515/350343 [01:58<00:33, 2318.12 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273749/350343 [01:58<00:32, 2323.29 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273982/350343 [01:59<00:34, 2230.28 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274206/350343 [01:59<00:34, 2222.85 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274440/350343 [01:59<00:33, 2255.81 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274670/350343 [01:59<00:33, 2268.13 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274903/350343 [01:59<00:32, 2286.08 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275139/350343 [01:59<00:32, 2305.17 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275377/350343 [01:59<00:32, 2326.70 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275610/350343 [01:59<00:32, 2317.10 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275842/350343 [01:59<00:32, 2303.16 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276075/350343 [01:59<00:32, 2310.23 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276307/350343 [02:00<00:32, 2304.07 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276538/350343 [02:00<00:32, 2299.91 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276769/350343 [02:00<00:31, 2302.85 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277000/350343 [02:00<00:31, 2300.93 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277235/350343 [02:00<00:31, 2314.54 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277475/350343 [02:00<00:31, 2338.49 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277709/350343 [02:00<00:31, 2334.81 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277943/350343 [02:00<00:31, 2314.34 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278179/350343 [02:00<00:31, 2325.21 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278412/350343 [02:00<00:31, 2316.73 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278644/350343 [02:01<00:31, 2308.33 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278877/350343 [02:01<00:30, 2312.64 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279109/350343 [02:01<00:31, 2297.13 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279339/350343 [02:01<00:30, 2293.55 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279569/350343 [02:01<00:32, 2187.35 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279799/350343 [02:01<00:31, 2218.40 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280034/350343 [02:01<00:31, 2255.09 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280267/350343 [02:01<00:30, 2275.52 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280500/350343 [02:01<00:30, 2290.14 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280731/350343 [02:01<00:30, 2295.93 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280969/350343 [02:02<00:29, 2319.53 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281202/350343 [02:02<00:29, 2311.30 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281434/350343 [02:02<00:29, 2307.26 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281665/350343 [02:02<00:29, 2306.75 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281898/350343 [02:02<00:29, 2311.27 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282133/350343 [02:02<00:29, 2320.01 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282366/350343 [02:02<00:29, 2297.24 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282598/350343 [02:02<00:29, 2302.80 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282832/350343 [02:02<00:29, 2311.47 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283064/350343 [02:02<00:29, 2288.64 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283300/350343 [02:03<00:29, 2307.33 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283534/350343 [02:03<00:28, 2315.94 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283770/350343 [02:03<00:28, 2328.64 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284003/350343 [02:03<00:28, 2324.11 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284236/350343 [02:03<00:28, 2320.65 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284469/350343 [02:03<00:28, 2306.22 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284704/350343 [02:03<00:29, 2228.86 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284935/350343 [02:03<00:29, 2251.21 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285167/350343 [02:03<00:28, 2270.71 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285395/350343 [02:03<00:28, 2263.28 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285631/350343 [02:04<00:28, 2291.77 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285863/350343 [02:04<00:28, 2300.05 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286098/350343 [02:04<00:27, 2312.42 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286330/350343 [02:04<00:27, 2300.53 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286563/350343 [02:04<00:27, 2307.66 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286797/350343 [02:04<00:27, 2315.87 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287029/350343 [02:04<00:27, 2285.92 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287258/350343 [02:04<00:27, 2273.57 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287486/350343 [02:04<00:27, 2274.09 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287714/350343 [02:04<00:27, 2268.49 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287945/350343 [02:05<00:27, 2278.84 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288176/350343 [02:05<00:27, 2286.42 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288411/350343 [02:05<00:26, 2303.24 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288643/350343 [02:05<00:26, 2306.10 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288879/350343 [02:05<00:26, 2320.61 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289113/350343 [02:05<00:26, 2325.85 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289346/350343 [02:05<00:26, 2308.96 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289578/350343 [02:05<00:26, 2311.47 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289810/350343 [02:05<00:26, 2290.33 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290040/350343 [02:05<00:26, 2279.93 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290271/350343 [02:06<00:26, 2287.57 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290503/350343 [02:06<00:26, 2296.85 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290736/350343 [02:06<00:25, 2304.91 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290967/350343 [02:06<00:26, 2212.61 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291205/350343 [02:06<00:26, 2259.57 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291441/350343 [02:06<00:25, 2287.00 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291677/350343 [02:06<00:25, 2307.45 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291914/350343 [02:06<00:25, 2324.67 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292149/350343 [02:06<00:24, 2330.56 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292383/350343 [02:07<00:24, 2320.89 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292616/350343 [02:07<00:24, 2318.44 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292849/350343 [02:07<00:24, 2321.78 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293082/350343 [02:07<00:24, 2315.16 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293315/350343 [02:07<00:24, 2318.00 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293547/350343 [02:07<00:24, 2311.82 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293782/350343 [02:07<00:24, 2320.97 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294015/350343 [02:07<00:24, 2309.84 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294251/350343 [02:07<00:24, 2324.19 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294489/350343 [02:07<00:23, 2339.26 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294723/350343 [02:08<00:23, 2322.74 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294962/350343 [02:08<00:23, 2339.90 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295197/350343 [02:08<00:23, 2326.08 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295430/350343 [02:08<00:23, 2327.03 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295663/350343 [02:08<00:24, 2217.54 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295890/350343 [02:08<00:24, 2231.10 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296118/350343 [02:08<00:24, 2245.04 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296352/350343 [02:08<00:23, 2271.69 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296589/350343 [02:08<00:23, 2299.28 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296820/350343 [02:08<00:23, 2297.41 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297051/350343 [02:09<00:23, 2299.87 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297284/350343 [02:09<00:22, 2307.79 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297521/350343 [02:09<00:22, 2323.33 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297754/350343 [02:09<00:22, 2300.68 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 297985/350343 [02:09<00:22, 2294.19 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298215/350343 [02:09<00:22, 2284.45 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298444/350343 [02:09<00:23, 2212.28 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298671/350343 [02:09<00:23, 2228.11 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298903/350343 [02:09<00:22, 2253.97 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299129/350343 [02:09<00:22, 2240.19 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299360/350343 [02:10<00:22, 2260.50 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299590/350343 [02:10<00:22, 2269.70 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299822/350343 [02:10<00:22, 2284.41 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300051/350343 [02:10<00:22, 2281.47 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300284/350343 [02:10<00:21, 2295.02 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300521/350343 [02:10<00:21, 2317.40 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300759/350343 [02:10<00:21, 2334.26 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300993/350343 [02:10<00:21, 2323.97 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301226/350343 [02:10<00:21, 2319.36 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301458/350343 [02:10<00:21, 2295.29 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301694/350343 [02:11<00:21, 2313.36 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301926/350343 [02:11<00:21, 2226.78 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 302165/350343 [02:11<00:21, 2272.76 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302398/350343 [02:11<00:20, 2287.72 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302635/350343 [02:11<00:20, 2309.51 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302871/350343 [02:11<00:20, 2323.51 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303104/350343 [02:11<00:20, 2318.03 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303341/350343 [02:11<00:20, 2331.49 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303576/350343 [02:11<00:20, 2334.76 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303810/350343 [02:11<00:20, 2311.12 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304044/350343 [02:12<00:19, 2317.77 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304277/350343 [02:12<00:19, 2319.76 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304510/350343 [02:12<00:19, 2312.90 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304742/350343 [02:12<00:19, 2314.61 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304975/350343 [02:12<00:19, 2318.67 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305207/350343 [02:12<00:19, 2317.42 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305442/350343 [02:12<00:19, 2327.09 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305675/350343 [02:12<00:19, 2327.13 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305908/350343 [02:12<00:19, 2316.47 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306140/350343 [02:13<00:19, 2286.27 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306377/350343 [02:13<00:19, 2308.72 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306608/350343 [02:13<00:18, 2306.21 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306840/350343 [02:13<00:18, 2307.78 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307071/350343 [02:13<00:19, 2209.46 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307301/350343 [02:13<00:19, 2233.58 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307531/350343 [02:13<00:19, 2251.21 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307764/350343 [02:13<00:18, 2273.82 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308000/350343 [02:13<00:18, 2297.90 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308234/350343 [02:13<00:18, 2308.50 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308466/350343 [02:14<00:18, 2304.45 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308702/350343 [02:14<00:17, 2319.28 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308935/350343 [02:14<00:17, 2315.17 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309170/350343 [02:14<00:17, 2325.52 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309403/350343 [02:14<00:17, 2322.06 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309636/350343 [02:14<00:17, 2322.31 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309869/350343 [02:14<00:17, 2322.76 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310104/350343 [02:14<00:17, 2329.61 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310337/350343 [02:14<00:17, 2317.79 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310575/350343 [02:14<00:17, 2334.72 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310809/350343 [02:15<00:17, 2318.85 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311042/350343 [02:15<00:16, 2320.44 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311275/350343 [02:15<00:16, 2319.73 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311509/350343 [02:15<00:16, 2325.15 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311742/350343 [02:15<00:16, 2319.32 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311974/350343 [02:15<00:16, 2311.59 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312206/350343 [02:15<00:16, 2312.40 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312438/350343 [02:15<00:16, 2230.45 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312667/350343 [02:15<00:16, 2247.57 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312898/350343 [02:15<00:16, 2263.83 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313125/350343 [02:16<00:16, 2246.53 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313350/350343 [02:16<00:16, 2243.58 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313581/350343 [02:16<00:16, 2261.78 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313815/350343 [02:16<00:15, 2284.25 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314048/350343 [02:16<00:15, 2297.43 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314285/350343 [02:16<00:15, 2317.21 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314519/350343 [02:16<00:15, 2323.60 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314753/350343 [02:16<00:15, 2328.26 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314986/350343 [02:16<00:15, 2314.20 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 315218/350343 [02:16<00:15, 2305.80 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315449/350343 [02:17<00:15, 2300.44 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315680/350343 [02:17<00:15, 2302.89 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315911/350343 [02:17<00:14, 2304.21 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316142/350343 [02:17<00:14, 2300.56 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316373/350343 [02:17<00:14, 2292.09 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316607/350343 [02:17<00:14, 2304.71 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316838/350343 [02:17<00:14, 2301.90 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317069/350343 [02:17<00:14, 2301.43 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317304/350343 [02:17<00:14, 2313.44 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317538/350343 [02:17<00:14, 2319.11 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317770/350343 [02:18<00:14, 2301.22 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318001/350343 [02:18<00:14, 2302.71 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318232/350343 [02:18<00:14, 2291.81 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318463/350343 [02:18<00:14, 2213.52 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318700/350343 [02:18<00:14, 2258.92 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318930/350343 [02:18<00:13, 2268.73 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319167/350343 [02:18<00:13, 2297.16 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319398/350343 [02:18<00:13, 2296.12 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319629/350343 [02:18<00:13, 2297.80 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319860/350343 [02:18<00:13, 2300.74 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320097/350343 [02:19<00:13, 2321.32 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320330/350343 [02:19<00:12, 2318.50 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320562/350343 [02:19<00:12, 2310.41 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320797/350343 [02:19<00:12, 2321.80 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321030/350343 [02:19<00:12, 2315.53 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321265/350343 [02:19<00:12, 2323.61 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321498/350343 [02:19<00:12, 2314.28 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321734/350343 [02:19<00:12, 2326.43 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321970/350343 [02:19<00:12, 2335.08 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322204/350343 [02:19<00:12, 2305.56 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322439/350343 [02:20<00:12, 2318.12 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322671/350343 [02:20<00:11, 2318.44 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322903/350343 [02:20<00:11, 2305.60 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323139/350343 [02:20<00:11, 2321.13 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323375/350343 [02:20<00:11, 2332.00 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323609/350343 [02:20<00:11, 2244.77 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323842/350343 [02:20<00:11, 2267.98 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324075/350343 [02:20<00:11, 2285.99 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324306/350343 [02:20<00:11, 2291.74 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324537/350343 [02:21<00:11, 2295.46 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324772/350343 [02:21<00:11, 2310.73 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325004/350343 [02:21<00:10, 2303.78 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325240/350343 [02:21<00:10, 2320.31 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325473/350343 [02:21<00:10, 2321.05 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325708/350343 [02:21<00:10, 2327.84 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325941/350343 [02:21<00:10, 2324.30 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326178/350343 [02:21<00:10, 2336.67 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326412/350343 [02:21<00:10, 2321.85 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326645/350343 [02:21<00:10, 2322.98 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326878/350343 [02:22<00:10, 2314.85 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327110/350343 [02:22<00:10, 2315.04 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327344/350343 [02:22<00:09, 2320.75 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327579/350343 [02:22<00:09, 2327.74 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327818/350343 [02:22<00:09, 2345.14 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328055/350343 [02:22<00:09, 2352.48 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328292/350343 [02:22<00:09, 2356.49 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328528/350343 [02:22<00:09, 2349.06 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328763/350343 [02:22<00:09, 2254.98 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328994/350343 [02:22<00:09, 2270.55 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329223/350343 [02:23<00:09, 2274.63 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329452/350343 [02:23<00:09, 2278.82 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329691/350343 [02:23<00:08, 2309.30 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329927/350343 [02:23<00:08, 2323.29 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330160/350343 [02:23<00:08, 2305.74 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330391/350343 [02:23<00:08, 2299.60 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330626/350343 [02:23<00:08, 2313.88 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330858/350343 [02:23<00:08, 2311.17 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331094/350343 [02:23<00:08, 2325.66 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331327/350343 [02:23<00:08, 2314.53 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331560/350343 [02:24<00:08, 2316.73 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331792/350343 [02:24<00:08, 2314.75 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332025/350343 [02:24<00:07, 2315.26 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332257/350343 [02:24<00:07, 2307.75 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332490/350343 [02:24<00:07, 2313.05 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332728/350343 [02:24<00:07, 2330.73 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 332962/350343 [02:24<00:07, 2325.22 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333195/350343 [02:24<00:07, 2320.64 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333436/350343 [02:24<00:07, 2345.71 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333671/350343 [02:24<00:07, 2338.21 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333905/350343 [02:25<00:07, 2232.18 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334138/350343 [02:25<00:07, 2257.80 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334375/350343 [02:25<00:06, 2288.39 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334605/350343 [02:25<00:06, 2285.49 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334834/350343 [02:25<00:06, 2272.19 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335070/350343 [02:25<00:06, 2298.11 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335305/350343 [02:25<00:06, 2311.58 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335540/350343 [02:25<00:06, 2320.28 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335779/350343 [02:25<00:06, 2340.63 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336014/350343 [02:25<00:06, 2327.37 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336254/350343 [02:26<00:06, 2346.89 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336489/350343 [02:26<00:05, 2333.96 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336727/350343 [02:26<00:05, 2347.06 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336962/350343 [02:26<00:05, 2328.40 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 337198/350343 [02:26<00:05, 2336.47 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337433/350343 [02:26<00:05, 2340.16 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337668/350343 [02:26<00:05, 2334.23 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337908/350343 [02:26<00:05, 2351.14 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338144/350343 [02:26<00:05, 2333.29 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338378/350343 [02:26<00:05, 2228.15 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338613/350343 [02:27<00:05, 2262.99 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338843/350343 [02:27<00:05, 2273.40 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339077/350343 [02:27<00:04, 2292.27 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339311/350343 [02:27<00:04, 2305.50 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339549/350343 [02:27<00:04, 2326.74 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339782/350343 [02:27<00:04, 2324.33 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340015/350343 [02:27<00:04, 2314.73 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340247/350343 [02:27<00:04, 2314.37 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340482/350343 [02:27<00:04, 2324.43 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340715/350343 [02:27<00:04, 2308.26 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340949/350343 [02:28<00:04, 2316.26 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341182/350343 [02:28<00:03, 2318.41 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341414/350343 [02:28<00:03, 2311.95 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341652/350343 [02:28<00:03, 2330.20 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341886/350343 [02:28<00:03, 2332.15 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342125/350343 [02:28<00:03, 2348.75 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342360/350343 [02:28<00:03, 2331.34 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342594/350343 [02:28<00:03, 2331.94 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342828/350343 [02:28<00:03, 2328.32 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343061/350343 [02:29<00:03, 2318.38 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343293/350343 [02:29<00:03, 2221.92 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343525/350343 [02:29<00:03, 2249.34 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343755/350343 [02:29<00:02, 2262.78 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343991/350343 [02:29<00:02, 2290.39 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344221/350343 [02:29<00:02, 2281.37 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344455/350343 [02:29<00:02, 2293.19 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344685/350343 [02:29<00:02, 2282.15 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344922/350343 [02:29<00:02, 2306.02 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345158/350343 [02:29<00:02, 2321.00 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345391/350343 [02:30<00:02, 2309.42 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345625/350343 [02:30<00:02, 2316.25 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345862/350343 [02:30<00:01, 2332.16 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346100/350343 [02:30<00:01, 2346.02 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346335/350343 [02:30<00:01, 2343.23 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346570/350343 [02:30<00:01, 2329.18 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346803/350343 [02:30<00:01, 2325.60 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347037/350343 [02:30<00:01, 2328.15 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347270/350343 [02:30<00:01, 2320.32 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347503/350343 [02:30<00:01, 2314.40 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347735/350343 [02:31<00:01, 2301.15 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347966/350343 [02:31<00:01, 2288.61 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348197/350343 [02:31<00:00, 2294.19 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348429/350343 [02:31<00:00, 2299.15 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348659/350343 [02:31<00:00, 2295.57 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348889/350343 [02:31<00:00, 2194.67 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349116/350343 [02:31<00:00, 2214.47 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349344/350343 [02:31<00:00, 2231.12 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349571/350343 [02:31<00:00, 2240.15 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349796/350343 [02:31<00:00, 2235.07 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350020/350343 [02:32<00:00, 2228.19 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350252/350343 [02:32<00:00, 2254.79 examples/s][A
                                                                                              [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:   0%|          | 1/350343 [00:00<41:15:06,  2.36 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:   3%|â–Ž         | 11748/350343 [00:00<00:11, 29610.79 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:   7%|â–‹         | 24023/350343 [00:00<00:06, 54268.51 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  10%|â–ˆ         | 36012/350343 [00:00<00:04, 72273.15 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  14%|â–ˆâ–        | 48423/350343 [00:00<00:03, 86868.23 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  17%|â–ˆâ–‹        | 60971/350343 [00:00<00:02, 97941.47 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  21%|â–ˆâ–ˆ        | 73717/350343 [00:01<00:02, 106517.38 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  25%|â–ˆâ–ˆâ–Œ       | 88792/350343 [00:01<00:02, 119492.56 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  30%|â–ˆâ–ˆâ–‰       | 103853/350343 [00:01<00:01, 128679.56 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118977/350343 [00:01<00:01, 135370.87 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134139/350343 [00:01<00:01, 140203.83 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149372/350343 [00:01<00:01, 143816.41 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164645/350343 [00:01<00:01, 146478.43 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179890/350343 [00:01<00:01, 148261.11 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195164/350343 [00:01<00:01, 149596.18 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210420/350343 [00:01<00:00, 150481.26 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225642/350343 [00:02<00:00, 150998.10 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 240979/350343 [00:02<00:00, 151705.53 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256342/350343 [00:02<00:00, 152280.12 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271644/350343 [00:02<00:00, 152495.62 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287139/350343 [00:02<00:00, 153226.84 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302510/350343 [00:02<00:00, 153367.74 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317860/350343 [00:02<00:00, 153404.05 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333315/350343 [00:02<00:00, 153744.98 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348694/350343 [00:02<00:00, 146686.93 examples/s][A
                                                                                                                                                            [AI0305 10:25:57.281830 139937033598784 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-train.tfrecord*. Number of examples: 350343 (shards: [43793, 43793, 43793, 43793, 43792, 43793, 43793, 43793])
Generating splits...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:35<05:10, 155.16s/ splits]
Generating validation examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating validation examples...:   0%|          | 108/43793 [00:00<00:48, 907.77 examples/s][A
Generating validation examples...:   1%|          | 330/43793 [00:00<00:27, 1591.56 examples/s][A
Generating validation examples...:   1%|â–         | 552/43793 [00:00<00:23, 1845.02 examples/s][A
Generating validation examples...:   2%|â–         | 774/43793 [00:00<00:21, 1985.60 examples/s][A
Generating validation examples...:   2%|â–         | 996/43793 [00:00<00:20, 2059.39 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1218/43793 [00:00<00:20, 2111.25 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1440/43793 [00:00<00:19, 2143.10 examples/s][A
Generating validation examples...:   4%|â–         | 1662/43793 [00:00<00:19, 2156.06 examples/s][A
Generating validation examples...:   4%|â–         | 1885/43793 [00:00<00:19, 2170.30 examples/s][A
Generating validation examples...:   5%|â–         | 2109/43793 [00:01<00:19, 2189.68 examples/s][A
Generating validation examples...:   5%|â–Œ         | 2329/43793 [00:01<00:18, 2189.15 examples/s][A
Generating validation examples...:   6%|â–Œ         | 2552/43793 [00:01<00:18, 2196.59 examples/s][A
Generating validation examples...:   6%|â–‹         | 2773/43793 [00:01<00:18, 2195.82 examples/s][A
Generating validation examples...:   7%|â–‹         | 2997/43793 [00:01<00:18, 2199.93 examples/s][A
Generating validation examples...:   7%|â–‹         | 3220/43793 [00:01<00:18, 2207.33 examples/s][A
Generating validation examples...:   8%|â–Š         | 3441/43793 [00:01<00:18, 2194.22 examples/s][A
Generating validation examples...:   8%|â–Š         | 3663/43793 [00:01<00:18, 2200.44 examples/s][A
Generating validation examples...:   9%|â–‰         | 3887/43793 [00:01<00:18, 2210.06 examples/s][A
Generating validation examples...:   9%|â–‰         | 4109/43793 [00:01<00:18, 2204.17 examples/s][A
Generating validation examples...:  10%|â–‰         | 4330/43793 [00:02<00:17, 2196.43 examples/s][A
Generating validation examples...:  10%|â–ˆ         | 4552/43793 [00:02<00:17, 2189.48 examples/s][A
Generating validation examples...:  11%|â–ˆ         | 4774/43793 [00:02<00:17, 2194.49 examples/s][A
Generating validation examples...:  11%|â–ˆâ–        | 4997/43793 [00:02<00:17, 2197.71 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5223/43793 [00:02<00:17, 2215.02 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5446/43793 [00:02<00:17, 2219.03 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5668/43793 [00:02<00:17, 2217.21 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5893/43793 [00:02<00:17, 2226.20 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6116/43793 [00:02<00:16, 2217.58 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6338/43793 [00:02<00:16, 2210.20 examples/s][A
Generating validation examples...:  15%|â–ˆâ–        | 6561/43793 [00:03<00:16, 2213.54 examples/s][A
Generating validation examples...:  15%|â–ˆâ–Œ        | 6786/43793 [00:03<00:16, 2221.84 examples/s][A
Generating validation examples...:  16%|â–ˆâ–Œ        | 7012/43793 [00:03<00:16, 2231.68 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7241/43793 [00:03<00:16, 2247.46 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7466/43793 [00:03<00:16, 2244.92 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7691/43793 [00:03<00:16, 2238.94 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7915/43793 [00:03<00:16, 2221.03 examples/s][A
Generating validation examples...:  19%|â–ˆâ–Š        | 8138/43793 [00:03<00:16, 2220.03 examples/s][A
Generating validation examples...:  19%|â–ˆâ–‰        | 8361/43793 [00:03<00:15, 2218.94 examples/s][A
Generating validation examples...:  20%|â–ˆâ–‰        | 8584/43793 [00:03<00:15, 2220.75 examples/s][A
Generating validation examples...:  20%|â–ˆâ–ˆ        | 8807/43793 [00:04<00:15, 2222.62 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9030/43793 [00:04<00:15, 2205.04 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9251/43793 [00:04<00:15, 2205.31 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9473/43793 [00:04<00:15, 2207.87 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9695/43793 [00:04<00:15, 2210.49 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 9917/43793 [00:04<00:15, 2213.18 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 10139/43793 [00:04<00:18, 1803.12 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–Ž       | 10338/43793 [00:04<00:18, 1774.71 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–       | 10561/43793 [00:04<00:17, 1865.35 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–       | 10783/43793 [00:05<00:17, 1939.47 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–Œ       | 11005/43793 [00:05<00:16, 1993.68 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11227/43793 [00:05<00:15, 2044.12 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11450/43793 [00:05<00:15, 2093.47 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11672/43793 [00:05<00:15, 2126.94 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11895/43793 [00:05<00:14, 2154.05 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12118/43793 [00:05<00:14, 2174.09 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12342/43793 [00:05<00:14, 2193.09 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–Š       | 12563/43793 [00:05<00:14, 2189.26 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–‰       | 12783/43793 [00:05<00:14, 2180.45 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–‰       | 13007/43793 [00:06<00:14, 2190.01 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13229/43793 [00:06<00:13, 2195.79 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13452/43793 [00:06<00:13, 2195.52 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13674/43793 [00:06<00:13, 2195.76 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13898/43793 [00:06<00:13, 2207.74 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14120/43793 [00:06<00:13, 2209.08 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14343/43793 [00:06<00:13, 2213.45 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14565/43793 [00:06<00:13, 2204.61 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14786/43793 [00:06<00:13, 2205.87 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 15009/43793 [00:06<00:13, 2202.45 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15232/43793 [00:07<00:13, 2189.05 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15454/43793 [00:07<00:13, 2171.85 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15678/43793 [00:07<00:12, 2191.02 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 15899/43793 [00:07<00:12, 2194.38 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16122/43793 [00:07<00:12, 2203.22 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16344/43793 [00:07<00:12, 2208.05 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16567/43793 [00:07<00:12, 2201.28 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16789/43793 [00:07<00:12, 2200.38 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17012/43793 [00:07<00:12, 2205.70 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17235/43793 [00:07<00:12, 2196.23 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17457/43793 [00:08<00:11, 2199.17 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17679/43793 [00:08<00:11, 2195.02 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17901/43793 [00:08<00:11, 2184.42 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18124/43793 [00:08<00:11, 2185.97 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18347/43793 [00:08<00:11, 2188.19 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18570/43793 [00:08<00:11, 2193.79 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18792/43793 [00:08<00:11, 2189.65 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19015/43793 [00:08<00:11, 2190.67 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19238/43793 [00:08<00:11, 2195.46 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19459/43793 [00:09<00:11, 2180.33 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19681/43793 [00:09<00:11, 2187.29 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19903/43793 [00:09<00:10, 2181.44 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20126/43793 [00:09<00:10, 2182.08 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20348/43793 [00:09<00:10, 2180.00 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20569/43793 [00:09<00:10, 2183.87 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20793/43793 [00:09<00:10, 2198.32 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21014/43793 [00:09<00:10, 2196.94 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21235/43793 [00:09<00:10, 2200.74 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21457/43793 [00:09<00:10, 2205.82 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21679/43793 [00:10<00:10, 2201.99 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21902/43793 [00:10<00:09, 2209.23 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22125/43793 [00:10<00:09, 2213.81 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22347/43793 [00:10<00:09, 2209.03 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22568/43793 [00:10<00:09, 2207.88 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22789/43793 [00:10<00:09, 2199.64 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23011/43793 [00:10<00:09, 2203.45 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23233/43793 [00:10<00:09, 2206.08 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23454/43793 [00:10<00:09, 2190.41 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23676/43793 [00:10<00:09, 2183.14 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23898/43793 [00:11<00:09, 2188.94 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24121/43793 [00:11<00:08, 2198.55 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24344/43793 [00:11<00:08, 2199.75 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24566/43793 [00:11<00:08, 2189.37 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24788/43793 [00:11<00:08, 2167.87 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25009/43793 [00:11<00:08, 2141.01 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25233/43793 [00:11<00:08, 2152.12 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25455/43793 [00:11<00:08, 2133.53 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25677/43793 [00:11<00:08, 2132.73 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25897/43793 [00:11<00:08, 2116.25 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26118/43793 [00:12<00:08, 2106.18 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26340/43793 [00:12<00:08, 2104.76 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26562/43793 [00:12<00:08, 2094.52 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26784/43793 [00:12<00:08, 2114.29 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27006/43793 [00:12<00:07, 2107.45 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27229/43793 [00:12<00:07, 2117.93 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27451/43793 [00:12<00:07, 2125.07 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27674/43793 [00:12<00:07, 2112.44 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27896/43793 [00:12<00:07, 2126.58 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28117/43793 [00:13<00:07, 2101.38 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28339/43793 [00:13<00:07, 2101.94 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28561/43793 [00:13<00:07, 2113.86 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28784/43793 [00:13<00:07, 2117.97 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 29006/43793 [00:13<00:06, 2119.79 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29228/43793 [00:13<00:06, 2128.62 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29450/43793 [00:13<00:06, 2103.52 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29672/43793 [00:13<00:06, 2100.91 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29895/43793 [00:13<00:06, 2103.14 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30116/43793 [00:13<00:06, 2097.44 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30339/43793 [00:14<00:06, 2101.47 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30559/43793 [00:14<00:06, 2078.36 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30782/43793 [00:14<00:06, 2079.65 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31005/43793 [00:14<00:06, 2081.29 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31227/43793 [00:14<00:06, 2090.32 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31448/43793 [00:14<00:05, 2088.36 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31669/43793 [00:14<00:05, 2082.66 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31893/43793 [00:14<00:05, 2101.70 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32115/43793 [00:14<00:05, 2104.06 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32337/43793 [00:15<00:05, 2103.67 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32560/43793 [00:15<00:05, 2110.05 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32782/43793 [00:15<00:05, 2117.59 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33004/43793 [00:15<00:05, 2094.85 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33225/43793 [00:15<00:05, 2083.88 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33447/43793 [00:15<00:04, 2093.96 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33669/43793 [00:15<00:04, 2076.33 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33891/43793 [00:15<00:04, 2093.90 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34114/43793 [00:15<00:04, 2110.69 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34336/43793 [00:15<00:04, 2117.87 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34558/43793 [00:16<00:04, 2136.02 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34781/43793 [00:16<00:04, 2135.46 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 35003/43793 [00:16<00:04, 2125.38 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35224/43793 [00:16<00:04, 2100.44 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35446/43793 [00:16<00:03, 2109.66 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35667/43793 [00:16<00:03, 2124.03 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35894/43793 [00:16<00:03, 2165.58 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36114/43793 [00:16<00:03, 2175.02 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36334/43793 [00:16<00:03, 2163.64 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36556/43793 [00:17<00:03, 2163.87 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36777/43793 [00:17<00:03, 2174.95 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37002/43793 [00:17<00:03, 2195.73 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37222/43793 [00:17<00:02, 2191.09 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37443/43793 [00:17<00:02, 2189.58 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37667/43793 [00:17<00:02, 2201.61 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37888/43793 [00:17<00:02, 2185.23 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38110/43793 [00:17<00:02, 2188.53 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38332/43793 [00:17<00:02, 2179.99 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38554/43793 [00:17<00:02, 2175.33 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38775/43793 [00:18<00:02, 2166.68 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 38997/43793 [00:18<00:02, 2166.20 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39220/43793 [00:18<00:02, 2182.55 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39441/43793 [00:18<00:01, 2189.11 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39663/43793 [00:18<00:01, 2195.89 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39888/43793 [00:18<00:01, 2210.71 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40110/43793 [00:18<00:01, 2211.82 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40334/43793 [00:18<00:01, 2219.44 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40557/43793 [00:18<00:01, 2220.03 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40780/43793 [00:18<00:01, 2202.75 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 41003/43793 [00:19<00:01, 2210.08 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41225/43793 [00:19<00:01, 2209.85 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41447/43793 [00:19<00:01, 2206.93 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41668/43793 [00:19<00:00, 2189.51 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41889/43793 [00:19<00:00, 2193.52 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42111/43793 [00:19<00:00, 2199.55 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42331/43793 [00:19<00:00, 2198.12 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42551/43793 [00:19<00:00, 2195.24 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42773/43793 [00:19<00:00, 2200.65 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42995/43793 [00:19<00:00, 2204.81 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43216/43793 [00:20<00:00, 2199.01 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43442/43793 [00:20<00:00, 2215.47 examples/s][A
Generating validation examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43664/43793 [00:20<00:00, 2215.35 examples/s][A
                                                                                                 [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-validation.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-validation.tfrecord*...:  23%|â–ˆâ–ˆâ–Ž       | 10206/43793 [00:00<00:00, 102051.82 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-validation.tfrecord*...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24696/43793 [00:00<00:00, 127246.42 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-validation.tfrecord*...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39940/43793 [00:00<00:00, 138745.20 examples/s][A
                                                                                                                                                               [AI0305 10:26:17.997063 139937033598784 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-validation.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:55<01:16, 76.03s/ splits] 
Generating test examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating test examples...:   0%|          | 153/43793 [00:00<00:28, 1527.71 examples/s][A
Generating test examples...:   1%|          | 362/43793 [00:00<00:23, 1855.22 examples/s][A
Generating test examples...:   1%|â–         | 574/43793 [00:00<00:21, 1972.85 examples/s][A
Generating test examples...:   2%|â–         | 788/43793 [00:00<00:21, 2037.64 examples/s][A
Generating test examples...:   2%|â–         | 1003/43793 [00:00<00:20, 2077.37 examples/s][A
Generating test examples...:   3%|â–Ž         | 1221/43793 [00:00<00:20, 2112.05 examples/s][A
Generating test examples...:   3%|â–Ž         | 1436/43793 [00:00<00:19, 2123.92 examples/s][A
Generating test examples...:   4%|â–         | 1659/43793 [00:00<00:19, 2156.91 examples/s][A
Generating test examples...:   4%|â–         | 1883/43793 [00:00<00:19, 2181.42 examples/s][A
Generating test examples...:   5%|â–         | 2106/43793 [00:01<00:18, 2194.13 examples/s][A
Generating test examples...:   5%|â–Œ         | 2329/43793 [00:01<00:18, 2205.05 examples/s][A
Generating test examples...:   6%|â–Œ         | 2550/43793 [00:01<00:18, 2200.90 examples/s][A
Generating test examples...:   6%|â–‹         | 2771/43793 [00:01<00:18, 2168.56 examples/s][A
Generating test examples...:   7%|â–‹         | 2990/43793 [00:01<00:18, 2174.04 examples/s][A
Generating test examples...:   7%|â–‹         | 3213/43793 [00:01<00:18, 2189.77 examples/s][A
Generating test examples...:   8%|â–Š         | 3435/43793 [00:01<00:18, 2194.42 examples/s][A
Generating test examples...:   8%|â–Š         | 3655/43793 [00:01<00:18, 2192.75 examples/s][A
Generating test examples...:   9%|â–‰         | 3876/43793 [00:01<00:18, 2196.83 examples/s][A
Generating test examples...:   9%|â–‰         | 4096/43793 [00:01<00:18, 2191.47 examples/s][A
Generating test examples...:  10%|â–‰         | 4319/43793 [00:02<00:17, 2202.68 examples/s][A
Generating test examples...:  10%|â–ˆ         | 4543/43793 [00:02<00:17, 2210.89 examples/s][A
Generating test examples...:  11%|â–ˆ         | 4767/43793 [00:02<00:17, 2219.55 examples/s][A
Generating test examples...:  11%|â–ˆâ–        | 4989/43793 [00:02<00:17, 2212.41 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5211/43793 [00:02<00:17, 2210.63 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5433/43793 [00:02<00:17, 2189.71 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5656/43793 [00:02<00:17, 2201.47 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5881/43793 [00:02<00:17, 2213.02 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6103/43793 [00:02<00:17, 2209.46 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6330/43793 [00:02<00:16, 2226.23 examples/s][A
Generating test examples...:  15%|â–ˆâ–        | 6554/43793 [00:03<00:16, 2228.33 examples/s][A
Generating test examples...:  15%|â–ˆâ–Œ        | 6777/43793 [00:03<00:16, 2208.45 examples/s][A
Generating test examples...:  16%|â–ˆâ–Œ        | 6998/43793 [00:03<00:16, 2206.75 examples/s][A
Generating test examples...:  16%|â–ˆâ–‹        | 7219/43793 [00:03<00:16, 2188.15 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7445/43793 [00:03<00:16, 2207.02 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7666/43793 [00:03<00:16, 2204.80 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7888/43793 [00:03<00:16, 2208.51 examples/s][A
Generating test examples...:  19%|â–ˆâ–Š        | 8109/43793 [00:03<00:16, 2202.36 examples/s][A
Generating test examples...:  19%|â–ˆâ–‰        | 8330/43793 [00:03<00:16, 2198.56 examples/s][A
Generating test examples...:  20%|â–ˆâ–‰        | 8551/43793 [00:03<00:16, 2199.70 examples/s][A
Generating test examples...:  20%|â–ˆâ–ˆ        | 8772/43793 [00:04<00:15, 2200.79 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 8996/43793 [00:04<00:15, 2205.68 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 9223/43793 [00:04<00:15, 2223.78 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9446/43793 [00:04<00:15, 2208.29 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9669/43793 [00:04<00:15, 2212.99 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 9891/43793 [00:04<00:15, 2213.40 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 10117/43793 [00:04<00:15, 2225.10 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–Ž       | 10342/43793 [00:04<00:15, 2230.04 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–       | 10566/43793 [00:04<00:14, 2229.70 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–       | 10789/43793 [00:04<00:14, 2222.27 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–Œ       | 11012/43793 [00:05<00:14, 2208.09 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11233/43793 [00:05<00:14, 2202.22 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11454/43793 [00:05<00:14, 2203.71 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11675/43793 [00:05<00:14, 2196.97 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11901/43793 [00:05<00:14, 2214.54 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12125/43793 [00:05<00:14, 2220.68 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12348/43793 [00:05<00:14, 2209.04 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–Š       | 12569/43793 [00:05<00:14, 2193.76 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–‰       | 12789/43793 [00:05<00:14, 2188.89 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–‰       | 13008/43793 [00:05<00:14, 2178.78 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13226/43793 [00:06<00:14, 2090.94 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13436/43793 [00:06<00:14, 2090.10 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13646/43793 [00:06<00:14, 2042.23 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13863/43793 [00:06<00:14, 2076.66 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14081/43793 [00:06<00:14, 2106.66 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14305/43793 [00:06<00:13, 2145.55 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14529/43793 [00:06<00:13, 2171.62 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 14748/43793 [00:06<00:13, 2175.77 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14969/43793 [00:06<00:13, 2183.80 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15190/43793 [00:06<00:13, 2168.99 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15410/43793 [00:07<00:13, 2175.37 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15633/43793 [00:07<00:12, 2190.53 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15854/43793 [00:07<00:12, 2192.77 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16077/43793 [00:07<00:12, 2201.85 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16298/43793 [00:07<00:12, 2187.65 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16522/43793 [00:07<00:12, 2202.70 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16744/43793 [00:07<00:12, 2206.14 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 16965/43793 [00:07<00:12, 2196.73 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17185/43793 [00:07<00:12, 2195.00 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17406/43793 [00:07<00:12, 2196.29 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17633/43793 [00:08<00:11, 2215.60 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17862/43793 [00:08<00:11, 2235.36 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18086/43793 [00:08<00:11, 2229.17 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18309/43793 [00:08<00:11, 2219.47 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18531/43793 [00:08<00:11, 2213.50 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18753/43793 [00:08<00:11, 2202.79 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18975/43793 [00:08<00:11, 2206.56 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19202/43793 [00:08<00:11, 2224.38 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19425/43793 [00:08<00:11, 2212.84 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19647/43793 [00:08<00:10, 2203.36 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19868/43793 [00:09<00:10, 2194.44 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20090/43793 [00:09<00:10, 2200.66 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20311/43793 [00:09<00:10, 2188.73 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20530/43793 [00:09<00:10, 2177.18 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20748/43793 [00:09<00:10, 2168.08 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20965/43793 [00:09<00:10, 2162.51 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21183/43793 [00:09<00:10, 2163.67 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21406/43793 [00:09<00:10, 2178.24 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21636/43793 [00:09<00:10, 2213.55 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21863/43793 [00:10<00:09, 2230.25 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22091/43793 [00:10<00:09, 2244.44 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22319/43793 [00:10<00:09, 2252.97 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22549/43793 [00:10<00:09, 2266.43 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22776/43793 [00:10<00:09, 2263.35 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23005/43793 [00:10<00:09, 2269.03 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23232/43793 [00:10<00:09, 2260.04 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23459/43793 [00:10<00:09, 2252.78 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23687/43793 [00:10<00:08, 2258.80 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23914/43793 [00:10<00:08, 2260.12 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24141/43793 [00:11<00:08, 2243.83 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24366/43793 [00:11<00:08, 2244.86 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24595/43793 [00:11<00:08, 2255.91 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24821/43793 [00:11<00:08, 2250.47 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25047/43793 [00:11<00:08, 2250.06 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25273/43793 [00:11<00:08, 2244.77 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25498/43793 [00:11<00:08, 2243.58 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25723/43793 [00:11<00:08, 2238.16 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25947/43793 [00:11<00:07, 2238.58 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26171/43793 [00:11<00:07, 2229.84 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26394/43793 [00:12<00:07, 2211.43 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26616/43793 [00:12<00:07, 2211.13 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26840/43793 [00:12<00:07, 2218.79 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27062/43793 [00:12<00:07, 2215.50 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27293/43793 [00:12<00:07, 2243.43 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27518/43793 [00:12<00:07, 2242.99 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27743/43793 [00:12<00:07, 2237.15 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27968/43793 [00:12<00:07, 2238.47 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28192/43793 [00:12<00:06, 2237.87 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28416/43793 [00:12<00:06, 2229.20 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28640/43793 [00:13<00:06, 2231.94 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28864/43793 [00:13<00:06, 2212.84 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29086/43793 [00:13<00:06, 2209.61 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29311/43793 [00:13<00:06, 2220.91 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29536/43793 [00:13<00:06, 2228.84 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29763/43793 [00:13<00:06, 2238.90 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29987/43793 [00:13<00:06, 2215.00 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30216/43793 [00:13<00:06, 2236.15 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30443/43793 [00:13<00:05, 2245.16 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30669/43793 [00:13<00:05, 2246.87 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30898/43793 [00:14<00:05, 2259.08 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31124/43793 [00:14<00:05, 2247.73 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31349/43793 [00:14<00:05, 2240.54 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31574/43793 [00:14<00:05, 2235.87 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31798/43793 [00:14<00:05, 2230.10 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32022/43793 [00:14<00:05, 2231.52 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32246/43793 [00:14<00:05, 2231.34 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32470/43793 [00:14<00:05, 2206.99 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32691/43793 [00:14<00:05, 2132.70 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32905/43793 [00:14<00:05, 2133.82 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33123/43793 [00:15<00:04, 2144.55 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33342/43793 [00:15<00:04, 2156.12 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33558/43793 [00:15<00:04, 2155.60 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33777/43793 [00:15<00:04, 2163.91 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 33997/43793 [00:15<00:04, 2174.46 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34215/43793 [00:15<00:04, 2169.70 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34433/43793 [00:15<00:04, 2165.70 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34662/43793 [00:15<00:04, 2201.60 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34883/43793 [00:15<00:04, 2195.24 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35106/43793 [00:15<00:03, 2205.31 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35333/43793 [00:16<00:03, 2223.64 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35557/43793 [00:16<00:03, 2226.51 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35780/43793 [00:16<00:03, 2219.75 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36005/43793 [00:16<00:03, 2226.07 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36229/43793 [00:16<00:03, 2228.36 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36453/43793 [00:16<00:03, 2229.88 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36678/43793 [00:16<00:03, 2235.85 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36903/43793 [00:16<00:03, 2232.80 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37127/43793 [00:16<00:02, 2232.03 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37355/43793 [00:16<00:02, 2245.41 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37582/43793 [00:17<00:02, 2249.74 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37809/43793 [00:17<00:02, 2253.71 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38037/43793 [00:17<00:02, 2260.93 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38264/43793 [00:17<00:02, 2261.61 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38491/43793 [00:17<00:02, 2256.27 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38717/43793 [00:17<00:02, 2251.13 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 38943/43793 [00:17<00:02, 2250.74 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39169/43793 [00:17<00:02, 2235.51 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39400/43793 [00:17<00:02, 2170.32 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39630/43793 [00:17<00:01, 2206.46 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39852/43793 [00:18<00:01, 2209.88 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40075/43793 [00:18<00:01, 2213.60 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40297/43793 [00:18<00:01, 2213.66 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40521/43793 [00:18<00:01, 2221.12 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40749/43793 [00:18<00:01, 2238.27 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40975/43793 [00:18<00:01, 2242.75 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41200/43793 [00:18<00:01, 2241.28 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41425/43793 [00:18<00:01, 2218.09 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41647/43793 [00:18<00:00, 2217.95 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41872/43793 [00:18<00:00, 2227.21 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42095/43793 [00:19<00:00, 2222.62 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42322/43793 [00:19<00:00, 2236.09 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42547/43793 [00:19<00:00, 2239.20 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42773/43793 [00:19<00:00, 2243.51 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42998/43793 [00:19<00:00, 2235.37 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43222/43793 [00:19<00:00, 2227.24 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43445/43793 [00:19<00:00, 2210.14 examples/s][A
Generating test examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43668/43793 [00:19<00:00, 2215.65 examples/s][A
                                                                                           [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-test.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-test.tfrecord*...:  23%|â–ˆâ–ˆâ–Ž       | 10074/43793 [00:00<00:00, 100733.85 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-test.tfrecord*...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25407/43793 [00:00<00:00, 131667.81 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-test.tfrecord*...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40890/43793 [00:00<00:00, 142239.55 examples/s][A
                                                                                                                                                         [AI0305 10:26:38.192637 139937033598784 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteQJWE9D/ogbg_molpcba-test.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:16<00:00, 50.54s/ splits]                                                                        I0305 10:26:38.277750 139937033598784 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /root/data/ogbg_molpcba/0.1.3
I0305 10:28:15.818901 139937033598784 submission_runner.py:411] Time since start: 549.29s, 	Step: 1, 	{'train/accuracy': 0.5404419302940369, 'train/loss': 0.7089899182319641, 'train/mean_average_precision': 0.022265429341623046, 'validation/accuracy': 0.5404320955276489, 'validation/loss': 0.7129108905792236, 'validation/mean_average_precision': 0.026174740504329722, 'validation/num_examples': 43793, 'test/accuracy': 0.5401853322982788, 'test/loss': 0.7146537899971008, 'test/mean_average_precision': 0.028019199135067188, 'test/num_examples': 43793, 'score': 19.528467893600464, 'total_duration': 549.2898893356323, 'accumulated_submission_time': 19.528467893600464, 'accumulated_eval_time': 529.7613813877106, 'accumulated_logging_time': 0}
I0305 10:28:15.835587 139769702573824 logging_writer.py:48] [1] accumulated_eval_time=529.761381, accumulated_logging_time=0, accumulated_submission_time=19.528468, global_step=1, preemption_count=0, score=19.528468, test/accuracy=0.540185, test/loss=0.714654, test/mean_average_precision=0.028019, test/num_examples=43793, total_duration=549.289889, train/accuracy=0.540442, train/loss=0.708990, train/mean_average_precision=0.022265, validation/accuracy=0.540432, validation/loss=0.712911, validation/mean_average_precision=0.026175, validation/num_examples=43793
I0305 10:28:47.966108 139768198641408 logging_writer.py:48] [100] global_step=100, grad_norm=0.2900456190109253, loss=0.2694230079650879
I0305 10:29:20.440170 139769702573824 logging_writer.py:48] [200] global_step=200, grad_norm=0.09319385886192322, loss=0.10455389320850372
I0305 10:29:52.353737 139768198641408 logging_writer.py:48] [300] global_step=300, grad_norm=0.02911320887506008, loss=0.06732970476150513
I0305 10:30:24.618050 139769702573824 logging_writer.py:48] [400] global_step=400, grad_norm=0.02460303157567978, loss=0.06085532531142235
I0305 10:30:56.452177 139768198641408 logging_writer.py:48] [500] global_step=500, grad_norm=0.015349899418652058, loss=0.052413929253816605
I0305 10:31:28.596601 139769702573824 logging_writer.py:48] [600] global_step=600, grad_norm=0.016219355165958405, loss=0.05722857266664505
I0305 10:32:00.941740 139768198641408 logging_writer.py:48] [700] global_step=700, grad_norm=0.015388396568596363, loss=0.05150943249464035
I0305 10:32:15.955633 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:34:08.330655 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:34:11.365741 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:34:14.355443 139937033598784 submission_runner.py:411] Time since start: 907.83s, 	Step: 748, 	{'train/accuracy': 0.986663281917572, 'train/loss': 0.051692791283130646, 'train/mean_average_precision': 0.051844076528978426, 'validation/accuracy': 0.9842145442962646, 'validation/loss': 0.06088528037071228, 'validation/mean_average_precision': 0.05126085682023725, 'validation/num_examples': 43793, 'test/accuracy': 0.983219563961029, 'test/loss': 0.06417810171842575, 'test/mean_average_precision': 0.0540834326521347, 'test/num_examples': 43793, 'score': 259.61792516708374, 'total_duration': 907.8264088630676, 'accumulated_submission_time': 259.61792516708374, 'accumulated_eval_time': 648.1611218452454, 'accumulated_logging_time': 0.02771592140197754}
I0305 10:34:14.372408 139769339950848 logging_writer.py:48] [748] accumulated_eval_time=648.161122, accumulated_logging_time=0.027716, accumulated_submission_time=259.617925, global_step=748, preemption_count=0, score=259.617925, test/accuracy=0.983220, test/loss=0.064178, test/mean_average_precision=0.054083, test/num_examples=43793, total_duration=907.826409, train/accuracy=0.986663, train/loss=0.051693, train/mean_average_precision=0.051844, validation/accuracy=0.984215, validation/loss=0.060885, validation/mean_average_precision=0.051261, validation/num_examples=43793
I0305 10:34:31.647027 139769409435392 logging_writer.py:48] [800] global_step=800, grad_norm=0.01905706338584423, loss=0.05398271232843399
I0305 10:35:03.789081 139769339950848 logging_writer.py:48] [900] global_step=900, grad_norm=0.030707862228155136, loss=0.052766088396310806
I0305 10:35:35.953899 139769409435392 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.02511122263967991, loss=0.046911198645830154
I0305 10:36:08.344576 139769339950848 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.022901609539985657, loss=0.04877399653196335
I0305 10:36:40.789675 139769409435392 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.025079237297177315, loss=0.05360197648406029
I0305 10:37:13.052980 139769339950848 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.0181962288916111, loss=0.04862166568636894
I0305 10:37:45.373646 139769409435392 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.025427088141441345, loss=0.04593616724014282
I0305 10:38:14.443621 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:40:10.354894 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:40:13.343148 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:40:16.299100 139937033598784 submission_runner.py:411] Time since start: 1269.77s, 	Step: 1491, 	{'train/accuracy': 0.9875317811965942, 'train/loss': 0.046178221702575684, 'train/mean_average_precision': 0.10258768189572806, 'validation/accuracy': 0.9847463369369507, 'validation/loss': 0.05511332303285599, 'validation/mean_average_precision': 0.10488632807550172, 'validation/num_examples': 43793, 'test/accuracy': 0.9837393164634705, 'test/loss': 0.058207038789987564, 'test/mean_average_precision': 0.10372074080947175, 'test/num_examples': 43793, 'score': 499.65687918663025, 'total_duration': 1269.7700910568237, 'accumulated_submission_time': 499.65687918663025, 'accumulated_eval_time': 770.0165569782257, 'accumulated_logging_time': 0.05682015419006348}
I0305 10:40:16.314189 139769401042688 logging_writer.py:48] [1491] accumulated_eval_time=770.016557, accumulated_logging_time=0.056820, accumulated_submission_time=499.656879, global_step=1491, preemption_count=0, score=499.656879, test/accuracy=0.983739, test/loss=0.058207, test/mean_average_precision=0.103721, test/num_examples=43793, total_duration=1269.770091, train/accuracy=0.987532, train/loss=0.046178, train/mean_average_precision=0.102588, validation/accuracy=0.984746, validation/loss=0.055113, validation/mean_average_precision=0.104886, validation/num_examples=43793
I0305 10:40:19.641653 139769702573824 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.023155827075242996, loss=0.04535238817334175
I0305 10:40:52.456906 139769401042688 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.027405789121985435, loss=0.045400939881801605
I0305 10:41:25.207779 139769702573824 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.02741003781557083, loss=0.04635751619935036
I0305 10:41:57.589232 139769401042688 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.02716505154967308, loss=0.04521745443344116
I0305 10:42:29.809841 139769702573824 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.03112332709133625, loss=0.0465080700814724
I0305 10:43:02.089810 139769401042688 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.017809376120567322, loss=0.04122832417488098
I0305 10:43:33.852213 139769702573824 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.01571742072701454, loss=0.04832804575562477
I0305 10:44:05.764580 139769401042688 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.021313374862074852, loss=0.04413110390305519
I0305 10:44:16.535148 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:46:15.356534 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:46:18.859982 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:46:22.230722 139937033598784 submission_runner.py:411] Time since start: 1635.70s, 	Step: 2235, 	{'train/accuracy': 0.9879155158996582, 'train/loss': 0.043444886803627014, 'train/mean_average_precision': 0.14224817369332332, 'validation/accuracy': 0.9850674271583557, 'validation/loss': 0.053188975900411606, 'validation/mean_average_precision': 0.13270506232441026, 'validation/num_examples': 43793, 'test/accuracy': 0.984077513217926, 'test/loss': 0.056263960897922516, 'test/mean_average_precision': 0.13207830472258855, 'test/num_examples': 43793, 'score': 739.8452451229095, 'total_duration': 1635.7016966342926, 'accumulated_submission_time': 739.8452451229095, 'accumulated_eval_time': 895.7120683193207, 'accumulated_logging_time': 0.08469104766845703}
I0305 10:46:22.248387 139769409435392 logging_writer.py:48] [2235] accumulated_eval_time=895.712068, accumulated_logging_time=0.084691, accumulated_submission_time=739.845245, global_step=2235, preemption_count=0, score=739.845245, test/accuracy=0.984078, test/loss=0.056264, test/mean_average_precision=0.132078, test/num_examples=43793, total_duration=1635.701697, train/accuracy=0.987916, train/loss=0.043445, train/mean_average_precision=0.142248, validation/accuracy=0.985067, validation/loss=0.053189, validation/mean_average_precision=0.132705, validation/num_examples=43793
I0305 10:46:44.405598 139769694181120 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.02043316140770912, loss=0.04258091375231743
I0305 10:47:17.234564 139769409435392 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.019155975431203842, loss=0.04699425399303436
I0305 10:47:49.684899 139769694181120 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.009887123480439186, loss=0.04038045182824135
I0305 10:48:22.150544 139769409435392 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.016372354701161385, loss=0.040667373687028885
I0305 10:48:54.398448 139769694181120 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.014405610971152782, loss=0.040157195180654526
I0305 10:49:27.073831 139769409435392 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.012548258528113365, loss=0.04521835222840309
I0305 10:49:59.131359 139769694181120 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.013567664660513401, loss=0.04107660427689552
I0305 10:50:22.529907 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:52:20.561886 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:52:23.624765 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:52:26.547986 139937033598784 submission_runner.py:411] Time since start: 2000.02s, 	Step: 2973, 	{'train/accuracy': 0.9879909753799438, 'train/loss': 0.04206007346510887, 'train/mean_average_precision': 0.16896406050275972, 'validation/accuracy': 0.9852550029754639, 'validation/loss': 0.05144836753606796, 'validation/mean_average_precision': 0.1546322777983081, 'validation/num_examples': 43793, 'test/accuracy': 0.9843037128448486, 'test/loss': 0.05429111048579216, 'test/mean_average_precision': 0.15676644008750057, 'test/num_examples': 43793, 'score': 980.0934472084045, 'total_duration': 2000.018956899643, 'accumulated_submission_time': 980.0934472084045, 'accumulated_eval_time': 1019.7300884723663, 'accumulated_logging_time': 0.11442422866821289}
I0305 10:52:26.563974 139769339950848 logging_writer.py:48] [2973] accumulated_eval_time=1019.730088, accumulated_logging_time=0.114424, accumulated_submission_time=980.093447, global_step=2973, preemption_count=0, score=980.093447, test/accuracy=0.984304, test/loss=0.054291, test/mean_average_precision=0.156766, test/num_examples=43793, total_duration=2000.018957, train/accuracy=0.987991, train/loss=0.042060, train/mean_average_precision=0.168964, validation/accuracy=0.985255, validation/loss=0.051448, validation/mean_average_precision=0.154632, validation/num_examples=43793
I0305 10:52:35.561418 139769401042688 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.016450172290205956, loss=0.04596365615725517
I0305 10:53:07.646242 139769339950848 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.01415194757282734, loss=0.04252088814973831
I0305 10:53:39.907346 139769401042688 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.03364652767777443, loss=0.04415648803114891
I0305 10:54:11.858364 139769339950848 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.015774480998516083, loss=0.04052051529288292
I0305 10:54:44.003024 139769401042688 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.017164606600999832, loss=0.041891708970069885
I0305 10:55:17.006023 139769339950848 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.012990170158445835, loss=0.037375517189502716
I0305 10:55:49.111797 139769401042688 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.010679212398827076, loss=0.04176541417837143
I0305 10:56:21.708150 139769339950848 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.02255251072347164, loss=0.04119892045855522
I0305 10:56:26.606542 139937033598784 spec.py:321] Evaluating on the training split.
I0305 10:58:27.096846 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 10:58:30.209144 139937033598784 spec.py:349] Evaluating on the test split.
I0305 10:58:33.159150 139937033598784 submission_runner.py:411] Time since start: 2366.63s, 	Step: 3716, 	{'train/accuracy': 0.9885411262512207, 'train/loss': 0.03975348174571991, 'train/mean_average_precision': 0.20069112878970993, 'validation/accuracy': 0.9854920506477356, 'validation/loss': 0.049590710550546646, 'validation/mean_average_precision': 0.17261044525630861, 'validation/num_examples': 43793, 'test/accuracy': 0.9846242666244507, 'test/loss': 0.05225050821900368, 'test/mean_average_precision': 0.17753255165867476, 'test/num_examples': 43793, 'score': 1220.1030368804932, 'total_duration': 2366.6301412582397, 'accumulated_submission_time': 1220.1030368804932, 'accumulated_eval_time': 1146.2826611995697, 'accumulated_logging_time': 0.14292407035827637}
I0305 10:58:33.174725 139769409435392 logging_writer.py:48] [3716] accumulated_eval_time=1146.282661, accumulated_logging_time=0.142924, accumulated_submission_time=1220.103037, global_step=3716, preemption_count=0, score=1220.103037, test/accuracy=0.984624, test/loss=0.052251, test/mean_average_precision=0.177533, test/num_examples=43793, total_duration=2366.630141, train/accuracy=0.988541, train/loss=0.039753, train/mean_average_precision=0.200691, validation/accuracy=0.985492, validation/loss=0.049591, validation/mean_average_precision=0.172610, validation/num_examples=43793
I0305 10:59:00.948179 139769694181120 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.01250628475099802, loss=0.03905614838004112
I0305 10:59:33.419802 139769409435392 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.012748895213007927, loss=0.04313632473349571
I0305 11:00:05.603644 139769694181120 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.017818409949541092, loss=0.03925071284174919
I0305 11:00:38.599553 139769409435392 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.01237140316516161, loss=0.03854256868362427
I0305 11:01:11.155135 139769694181120 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.016505315899848938, loss=0.04307299852371216
I0305 11:01:43.787137 139769409435392 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.009317643009126186, loss=0.041271474212408066
I0305 11:02:16.040263 139769694181120 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.009673785418272018, loss=0.03710586577653885
I0305 11:02:33.398898 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:04:34.403080 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:04:37.412461 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:04:40.352114 139937033598784 submission_runner.py:411] Time since start: 2733.82s, 	Step: 4455, 	{'train/accuracy': 0.9885896444320679, 'train/loss': 0.03889615088701248, 'train/mean_average_precision': 0.21338320017835863, 'validation/accuracy': 0.9857372045516968, 'validation/loss': 0.04834378510713577, 'validation/mean_average_precision': 0.18802501881821718, 'validation/num_examples': 43793, 'test/accuracy': 0.9848222136497498, 'test/loss': 0.05102759972214699, 'test/mean_average_precision': 0.18984376241954204, 'test/num_examples': 43793, 'score': 1460.2951698303223, 'total_duration': 2733.823101758957, 'accumulated_submission_time': 1460.2951698303223, 'accumulated_eval_time': 1273.235831975937, 'accumulated_logging_time': 0.16993355751037598}
I0305 11:04:40.367705 139769719359232 logging_writer.py:48] [4455] accumulated_eval_time=1273.235832, accumulated_logging_time=0.169934, accumulated_submission_time=1460.295170, global_step=4455, preemption_count=0, score=1460.295170, test/accuracy=0.984822, test/loss=0.051028, test/mean_average_precision=0.189844, test/num_examples=43793, total_duration=2733.823102, train/accuracy=0.988590, train/loss=0.038896, train/mean_average_precision=0.213383, validation/accuracy=0.985737, validation/loss=0.048344, validation/mean_average_precision=0.188025, validation/num_examples=43793
I0305 11:04:55.406208 139874938124032 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.013036689721047878, loss=0.03883565962314606
I0305 11:05:28.301536 139769719359232 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.014362093061208725, loss=0.03751413896679878
I0305 11:06:00.686437 139874938124032 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.014330366626381874, loss=0.04137805104255676
I0305 11:06:33.372530 139769719359232 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.04337085783481598, loss=0.0379231721162796
I0305 11:07:06.236278 139874938124032 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.024038083851337433, loss=0.03843137249350548
I0305 11:07:38.393537 139769719359232 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.01624244824051857, loss=0.0402037538588047
I0305 11:08:10.862701 139874938124032 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.024200906977057457, loss=0.040941327810287476
I0305 11:08:40.458027 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:10:43.185869 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:10:46.221889 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:10:49.162390 139937033598784 submission_runner.py:411] Time since start: 3102.63s, 	Step: 5192, 	{'train/accuracy': 0.9890533089637756, 'train/loss': 0.03699266165494919, 'train/mean_average_precision': 0.253568965894043, 'validation/accuracy': 0.9858285784721375, 'validation/loss': 0.04734494909644127, 'validation/mean_average_precision': 0.20399511589537805, 'validation/num_examples': 43793, 'test/accuracy': 0.9850471615791321, 'test/loss': 0.04986915364861488, 'test/mean_average_precision': 0.20442755700649914, 'test/num_examples': 43793, 'score': 1700.3518633842468, 'total_duration': 3102.6333820819855, 'accumulated_submission_time': 1700.3518633842468, 'accumulated_eval_time': 1401.9401717185974, 'accumulated_logging_time': 0.19832611083984375}
I0305 11:10:49.178903 139776159401728 logging_writer.py:48] [5192] accumulated_eval_time=1401.940172, accumulated_logging_time=0.198326, accumulated_submission_time=1700.351863, global_step=5192, preemption_count=0, score=1700.351863, test/accuracy=0.985047, test/loss=0.049869, test/mean_average_precision=0.204428, test/num_examples=43793, total_duration=3102.633382, train/accuracy=0.989053, train/loss=0.036993, train/mean_average_precision=0.253569, validation/accuracy=0.985829, validation/loss=0.047345, validation/mean_average_precision=0.203995, validation/num_examples=43793
I0305 11:10:52.107844 139875021985536 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.009794020093977451, loss=0.03704553097486496
I0305 11:11:24.407237 139776159401728 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.011190268211066723, loss=0.04043367877602577
I0305 11:11:56.913248 139875021985536 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.013759305700659752, loss=0.03891240432858467
I0305 11:12:30.262597 139776159401728 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.01337440125644207, loss=0.04072101414203644
I0305 11:13:03.462318 139875021985536 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.013094853609800339, loss=0.0415310300886631
I0305 11:13:35.962205 139776159401728 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.022229429334402084, loss=0.037934836000204086
I0305 11:14:08.579041 139875021985536 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.014440475031733513, loss=0.03891781345009804
I0305 11:14:41.007693 139776159401728 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.019353192299604416, loss=0.03785087168216705
I0305 11:14:49.399157 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:16:49.216858 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:16:52.267311 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:16:55.242974 139937033598784 submission_runner.py:411] Time since start: 3468.71s, 	Step: 5927, 	{'train/accuracy': 0.9893468022346497, 'train/loss': 0.03611397743225098, 'train/mean_average_precision': 0.27828959301648837, 'validation/accuracy': 0.9861021637916565, 'validation/loss': 0.04665938764810562, 'validation/mean_average_precision': 0.2093368453462271, 'validation/num_examples': 43793, 'test/accuracy': 0.9852859377861023, 'test/loss': 0.04916926845908165, 'test/mean_average_precision': 0.21947564350877818, 'test/num_examples': 43793, 'score': 1940.53954744339, 'total_duration': 3468.713948726654, 'accumulated_submission_time': 1940.53954744339, 'accumulated_eval_time': 1527.7839286327362, 'accumulated_logging_time': 0.2261953353881836}
I0305 11:16:55.258983 139769727751936 logging_writer.py:48] [5927] accumulated_eval_time=1527.783929, accumulated_logging_time=0.226195, accumulated_submission_time=1940.539547, global_step=5927, preemption_count=0, score=1940.539547, test/accuracy=0.985286, test/loss=0.049169, test/mean_average_precision=0.219476, test/num_examples=43793, total_duration=3468.713949, train/accuracy=0.989347, train/loss=0.036114, train/mean_average_precision=0.278290, validation/accuracy=0.986102, validation/loss=0.046659, validation/mean_average_precision=0.209337, validation/num_examples=43793
I0305 11:17:19.506969 139776167794432 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.02747536450624466, loss=0.03794378042221069
I0305 11:17:51.973761 139769727751936 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.011654960922896862, loss=0.03521409258246422
I0305 11:18:25.595924 139776167794432 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.011190326884388924, loss=0.03591468930244446
I0305 11:18:57.973343 139769727751936 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.02081114985048771, loss=0.04432821273803711
I0305 11:19:30.367393 139776167794432 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.018349140882492065, loss=0.040679506957530975
I0305 11:20:03.099285 139769727751936 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.01927914470434189, loss=0.03811388835310936
I0305 11:20:36.006707 139776167794432 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.013127054087817669, loss=0.03466857597231865
I0305 11:20:55.281205 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:22:53.824852 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:22:56.884536 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:22:59.904744 139937033598784 submission_runner.py:411] Time since start: 3833.38s, 	Step: 6660, 	{'train/accuracy': 0.9893681406974792, 'train/loss': 0.03610365465283394, 'train/mean_average_precision': 0.2615927564089334, 'validation/accuracy': 0.9861293435096741, 'validation/loss': 0.046503905206918716, 'validation/mean_average_precision': 0.2194878491913071, 'validation/num_examples': 43793, 'test/accuracy': 0.9852400422096252, 'test/loss': 0.0492401048541069, 'test/mean_average_precision': 0.22206600159797912, 'test/num_examples': 43793, 'score': 2180.5288002490997, 'total_duration': 3833.375732898712, 'accumulated_submission_time': 2180.5288002490997, 'accumulated_eval_time': 1652.4074277877808, 'accumulated_logging_time': 0.25497984886169434}
I0305 11:22:59.921692 139776159401728 logging_writer.py:48] [6660] accumulated_eval_time=1652.407428, accumulated_logging_time=0.254980, accumulated_submission_time=2180.528800, global_step=6660, preemption_count=0, score=2180.528800, test/accuracy=0.985240, test/loss=0.049240, test/mean_average_precision=0.222066, test/num_examples=43793, total_duration=3833.375733, train/accuracy=0.989368, train/loss=0.036104, train/mean_average_precision=0.261593, validation/accuracy=0.986129, validation/loss=0.046504, validation/mean_average_precision=0.219488, validation/num_examples=43793
I0305 11:23:13.466764 139874938124032 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.021764615550637245, loss=0.03503541275858879
I0305 11:23:45.907504 139776159401728 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.013218509033322334, loss=0.03757255896925926
I0305 11:24:18.431313 139874938124032 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.013352403417229652, loss=0.03537240996956825
I0305 11:24:50.797194 139776159401728 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.014349795877933502, loss=0.03339147940278053
I0305 11:25:23.622515 139874938124032 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.015666093677282333, loss=0.03923815116286278
I0305 11:25:56.319601 139776159401728 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.025414373725652695, loss=0.033605899661779404
I0305 11:26:29.263084 139874938124032 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.013976641930639744, loss=0.03637389838695526
I0305 11:27:00.162142 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:29:02.489658 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:29:06.149584 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:29:09.199566 139937033598784 submission_runner.py:411] Time since start: 4202.67s, 	Step: 7396, 	{'train/accuracy': 0.9895302057266235, 'train/loss': 0.03531371057033539, 'train/mean_average_precision': 0.3048981406299598, 'validation/accuracy': 0.9863741397857666, 'validation/loss': 0.04591713845729828, 'validation/mean_average_precision': 0.23462074591642437, 'validation/num_examples': 43793, 'test/accuracy': 0.9854978322982788, 'test/loss': 0.04843222349882126, 'test/mean_average_precision': 0.23136146816063408, 'test/num_examples': 43793, 'score': 2420.737658262253, 'total_duration': 4202.670556783676, 'accumulated_submission_time': 2420.737658262253, 'accumulated_eval_time': 1781.4448142051697, 'accumulated_logging_time': 0.283771276473999}
I0305 11:29:09.217080 139769727751936 logging_writer.py:48] [7396] accumulated_eval_time=1781.444814, accumulated_logging_time=0.283771, accumulated_submission_time=2420.737658, global_step=7396, preemption_count=0, score=2420.737658, test/accuracy=0.985498, test/loss=0.048432, test/mean_average_precision=0.231361, test/num_examples=43793, total_duration=4202.670557, train/accuracy=0.989530, train/loss=0.035314, train/mean_average_precision=0.304898, validation/accuracy=0.986374, validation/loss=0.045917, validation/mean_average_precision=0.234621, validation/num_examples=43793
I0305 11:29:10.998544 139875021985536 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.025741351768374443, loss=0.034144118428230286
I0305 11:29:46.800642 139769727751936 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.017210107296705246, loss=0.0383097343146801
I0305 11:30:20.033424 139875021985536 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01733691617846489, loss=0.03254926949739456
I0305 11:30:52.434470 139769727751936 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.019357822835445404, loss=0.03667493164539337
I0305 11:31:25.302340 139875021985536 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.014957934617996216, loss=0.038446392863988876
I0305 11:31:57.687304 139769727751936 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.019791334867477417, loss=0.032854847609996796
I0305 11:32:30.757523 139875021985536 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.03131292015314102, loss=0.032845452427864075
I0305 11:33:03.503490 139769727751936 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.016134336590766907, loss=0.03719419240951538
I0305 11:33:09.346298 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:35:11.697235 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:35:14.773211 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:35:17.772382 139937033598784 submission_runner.py:411] Time since start: 4571.24s, 	Step: 8119, 	{'train/accuracy': 0.9896426796913147, 'train/loss': 0.034786567091941833, 'train/mean_average_precision': 0.297149318468285, 'validation/accuracy': 0.9862799644470215, 'validation/loss': 0.04590030759572983, 'validation/mean_average_precision': 0.23563179738263187, 'validation/num_examples': 43793, 'test/accuracy': 0.9853870272636414, 'test/loss': 0.04841504991054535, 'test/mean_average_precision': 0.23126062169105766, 'test/num_examples': 43793, 'score': 2660.8356647491455, 'total_duration': 4571.243372917175, 'accumulated_submission_time': 2660.8356647491455, 'accumulated_eval_time': 1909.8708515167236, 'accumulated_logging_time': 0.3127274513244629}
I0305 11:35:17.788494 139776159401728 logging_writer.py:48] [8119] accumulated_eval_time=1909.870852, accumulated_logging_time=0.312727, accumulated_submission_time=2660.835665, global_step=8119, preemption_count=0, score=2660.835665, test/accuracy=0.985387, test/loss=0.048415, test/mean_average_precision=0.231261, test/num_examples=43793, total_duration=4571.243373, train/accuracy=0.989643, train/loss=0.034787, train/mean_average_precision=0.297149, validation/accuracy=0.986280, validation/loss=0.045900, validation/mean_average_precision=0.235632, validation/num_examples=43793
I0305 11:35:43.778895 139874938124032 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.025885386392474174, loss=0.03669699281454086
I0305 11:36:15.974403 139776159401728 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.01710057444870472, loss=0.03460853174328804
I0305 11:36:47.893739 139874938124032 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.02073867805302143, loss=0.03501413017511368
I0305 11:37:19.438067 139776159401728 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.017057914286851883, loss=0.038600675761699677
I0305 11:37:50.867653 139874938124032 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.013382739387452602, loss=0.03126877173781395
I0305 11:38:23.162420 139776159401728 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.02414715476334095, loss=0.03762609139084816
I0305 11:38:55.381887 139874938124032 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.02706046961247921, loss=0.03234538063406944
I0305 11:39:17.821502 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:41:21.657154 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:41:25.149469 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:41:28.613972 139937033598784 submission_runner.py:411] Time since start: 4942.08s, 	Step: 8871, 	{'train/accuracy': 0.9896151423454285, 'train/loss': 0.03447967395186424, 'train/mean_average_precision': 0.32396387883465033, 'validation/accuracy': 0.9862819910049438, 'validation/loss': 0.04544106498360634, 'validation/mean_average_precision': 0.24025667172702833, 'validation/num_examples': 43793, 'test/accuracy': 0.985410213470459, 'test/loss': 0.04792282357811928, 'test/mean_average_precision': 0.2439316950400646, 'test/num_examples': 43793, 'score': 2900.8369381427765, 'total_duration': 4942.084951400757, 'accumulated_submission_time': 2900.8369381427765, 'accumulated_eval_time': 2040.6632704734802, 'accumulated_logging_time': 0.3399653434753418}
I0305 11:41:28.633764 139769727751936 logging_writer.py:48] [8871] accumulated_eval_time=2040.663270, accumulated_logging_time=0.339965, accumulated_submission_time=2900.836938, global_step=8871, preemption_count=0, score=2900.836938, test/accuracy=0.985410, test/loss=0.047923, test/mean_average_precision=0.243932, test/num_examples=43793, total_duration=4942.084951, train/accuracy=0.989615, train/loss=0.034480, train/mean_average_precision=0.323964, validation/accuracy=0.986282, validation/loss=0.045441, validation/mean_average_precision=0.240257, validation/num_examples=43793
I0305 11:41:38.635588 139776167794432 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.01529028918594122, loss=0.03988705947995186
I0305 11:42:11.799007 139769727751936 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.023409266024827957, loss=0.03974119946360588
I0305 11:42:44.450159 139776167794432 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.025558358058333397, loss=0.03584542125463486
I0305 11:43:17.524626 139769727751936 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01674707420170307, loss=0.03829800710082054
I0305 11:43:49.945656 139776167794432 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.015229834243655205, loss=0.030830198898911476
I0305 11:44:22.761059 139769727751936 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.024016419425606728, loss=0.039808545261621475
I0305 11:44:55.437052 139776167794432 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.022371061146259308, loss=0.03492017090320587
I0305 11:45:28.236571 139769727751936 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.020677220076322556, loss=0.03521614149212837
I0305 11:45:28.904449 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:47:31.173343 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:47:34.213389 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:47:37.158042 139937033598784 submission_runner.py:411] Time since start: 5310.63s, 	Step: 9603, 	{'train/accuracy': 0.9900298118591309, 'train/loss': 0.033084604889154434, 'train/mean_average_precision': 0.339935185024856, 'validation/accuracy': 0.9865061044692993, 'validation/loss': 0.045229632407426834, 'validation/mean_average_precision': 0.24360972893887647, 'validation/num_examples': 43793, 'test/accuracy': 0.9856237769126892, 'test/loss': 0.047906260937452316, 'test/mean_average_precision': 0.2502284643444161, 'test/num_examples': 43793, 'score': 3141.071623802185, 'total_duration': 5310.628950357437, 'accumulated_submission_time': 3141.071623802185, 'accumulated_eval_time': 2168.9167437553406, 'accumulated_logging_time': 0.3717496395111084}
I0305 11:47:37.175393 139874938124032 logging_writer.py:48] [9603] accumulated_eval_time=2168.916744, accumulated_logging_time=0.371750, accumulated_submission_time=3141.071624, global_step=9603, preemption_count=0, score=3141.071624, test/accuracy=0.985624, test/loss=0.047906, test/mean_average_precision=0.250228, test/num_examples=43793, total_duration=5310.628950, train/accuracy=0.990030, train/loss=0.033085, train/mean_average_precision=0.339935, validation/accuracy=0.986506, validation/loss=0.045230, validation/mean_average_precision=0.243610, validation/num_examples=43793
I0305 11:48:08.644376 139875021985536 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.018950950354337692, loss=0.031587839126586914
I0305 11:48:40.353023 139874938124032 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.025101639330387115, loss=0.03382033109664917
I0305 11:49:12.064438 139875021985536 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.023971015587449074, loss=0.03814977407455444
I0305 11:49:43.875588 139874938124032 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.019171632826328278, loss=0.03703318163752556
I0305 11:50:15.951995 139875021985536 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.028101366013288498, loss=0.03651992231607437
I0305 11:50:47.626584 139874938124032 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.01974480226635933, loss=0.03619929030537605
I0305 11:51:20.044027 139875021985536 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.01914922706782818, loss=0.03390954062342644
I0305 11:51:37.339027 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:53:36.996802 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:53:40.040293 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:53:43.020850 139937033598784 submission_runner.py:411] Time since start: 5676.49s, 	Step: 10355, 	{'train/accuracy': 0.9901762008666992, 'train/loss': 0.032609812915325165, 'train/mean_average_precision': 0.3561634795300548, 'validation/accuracy': 0.9865044355392456, 'validation/loss': 0.04474025219678879, 'validation/mean_average_precision': 0.24903079621527494, 'validation/num_examples': 43793, 'test/accuracy': 0.985651969909668, 'test/loss': 0.04735453426837921, 'test/mean_average_precision': 0.24692030846675506, 'test/num_examples': 43793, 'score': 3381.203540802002, 'total_duration': 5676.491842985153, 'accumulated_submission_time': 3381.203540802002, 'accumulated_eval_time': 2294.598527908325, 'accumulated_logging_time': 0.40073466300964355}
I0305 11:53:43.039284 139776159401728 logging_writer.py:48] [10355] accumulated_eval_time=2294.598528, accumulated_logging_time=0.400735, accumulated_submission_time=3381.203541, global_step=10355, preemption_count=0, score=3381.203541, test/accuracy=0.985652, test/loss=0.047355, test/mean_average_precision=0.246920, test/num_examples=43793, total_duration=5676.491843, train/accuracy=0.990176, train/loss=0.032610, train/mean_average_precision=0.356163, validation/accuracy=0.986504, validation/loss=0.044740, validation/mean_average_precision=0.249031, validation/num_examples=43793
I0305 11:53:57.653396 139776167794432 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.027776207774877548, loss=0.03684805706143379
I0305 11:54:29.552438 139776159401728 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.030157653614878654, loss=0.033844947814941406
I0305 11:55:01.488298 139776167794432 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.032471004873514175, loss=0.032621581107378006
I0305 11:55:33.542529 139776159401728 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.020681463181972504, loss=0.031601231545209885
I0305 11:56:05.412099 139776167794432 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.027629971504211426, loss=0.03246708959341049
I0305 11:56:37.195355 139776159401728 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.03264613077044487, loss=0.03417300060391426
I0305 11:57:09.516399 139776167794432 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.024420365691184998, loss=0.03197057545185089
I0305 11:57:41.640929 139776159401728 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.023142272606492043, loss=0.030603786930441856
I0305 11:57:43.219519 139937033598784 spec.py:321] Evaluating on the training split.
I0305 11:59:46.789034 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 11:59:50.535059 139937033598784 spec.py:349] Evaluating on the test split.
I0305 11:59:53.499242 139937033598784 submission_runner.py:411] Time since start: 6046.97s, 	Step: 11106, 	{'train/accuracy': 0.9905160665512085, 'train/loss': 0.03145553916692734, 'train/mean_average_precision': 0.3813649471138655, 'validation/accuracy': 0.9866416454315186, 'validation/loss': 0.044462237507104874, 'validation/mean_average_precision': 0.2543336478332198, 'validation/num_examples': 43793, 'test/accuracy': 0.9858317971229553, 'test/loss': 0.04725351929664612, 'test/mean_average_precision': 0.2559278961595786, 'test/num_examples': 43793, 'score': 3621.3508100509644, 'total_duration': 6046.970125198364, 'accumulated_submission_time': 3621.3508100509644, 'accumulated_eval_time': 2424.878109931946, 'accumulated_logging_time': 0.43189120292663574}
I0305 11:59:53.516379 139769409435392 logging_writer.py:48] [11106] accumulated_eval_time=2424.878110, accumulated_logging_time=0.431891, accumulated_submission_time=3621.350810, global_step=11106, preemption_count=0, score=3621.350810, test/accuracy=0.985832, test/loss=0.047254, test/mean_average_precision=0.255928, test/num_examples=43793, total_duration=6046.970125, train/accuracy=0.990516, train/loss=0.031456, train/mean_average_precision=0.381365, validation/accuracy=0.986642, validation/loss=0.044462, validation/mean_average_precision=0.254334, validation/num_examples=43793
I0305 12:00:24.189897 139875021985536 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.024388907477259636, loss=0.03318196162581444
I0305 12:00:56.949045 139769409435392 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.02672685496509075, loss=0.0326840840280056
I0305 12:01:29.418308 139875021985536 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.02189618907868862, loss=0.03455778583884239
I0305 12:02:01.942936 139769409435392 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.02279193513095379, loss=0.03854433819651604
I0305 12:02:35.281334 139875021985536 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.028832385316491127, loss=0.032625455409288406
I0305 12:03:09.039120 139769409435392 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.02986595220863819, loss=0.03238131105899811
I0305 12:03:41.649415 139875021985536 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.030214793980121613, loss=0.034059084951877594
I0305 12:03:53.560707 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:05:55.836653 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:05:58.865270 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:06:01.929507 139937033598784 submission_runner.py:411] Time since start: 6415.40s, 	Step: 11838, 	{'train/accuracy': 0.9904680848121643, 'train/loss': 0.03142601251602173, 'train/mean_average_precision': 0.3899980857227915, 'validation/accuracy': 0.9865442514419556, 'validation/loss': 0.044839825481176376, 'validation/mean_average_precision': 0.2530719494491042, 'validation/num_examples': 43793, 'test/accuracy': 0.9857897162437439, 'test/loss': 0.047369662672281265, 'test/mean_average_precision': 0.25709630404517536, 'test/num_examples': 43793, 'score': 3861.3633439540863, 'total_duration': 6415.400445222855, 'accumulated_submission_time': 3861.3633439540863, 'accumulated_eval_time': 2553.246810436249, 'accumulated_logging_time': 0.4600224494934082}
I0305 12:06:01.965802 139776159401728 logging_writer.py:48] [11838] accumulated_eval_time=2553.246810, accumulated_logging_time=0.460022, accumulated_submission_time=3861.363344, global_step=11838, preemption_count=0, score=3861.363344, test/accuracy=0.985790, test/loss=0.047370, test/mean_average_precision=0.257096, test/num_examples=43793, total_duration=6415.400445, train/accuracy=0.990468, train/loss=0.031426, train/mean_average_precision=0.389998, validation/accuracy=0.986544, validation/loss=0.044840, validation/mean_average_precision=0.253072, validation/num_examples=43793
I0305 12:06:22.470023 139776167794432 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.03257695585489273, loss=0.03879125043749809
I0305 12:06:54.743310 139776159401728 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.033996306359767914, loss=0.034091368317604065
I0305 12:07:27.186342 139776167794432 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.027409406378865242, loss=0.03566602244973183
I0305 12:07:59.644573 139776159401728 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.022771665826439857, loss=0.03161464259028435
I0305 12:08:31.888990 139776167794432 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.03297315910458565, loss=0.034721385687589645
I0305 12:09:04.229078 139776159401728 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.035908401012420654, loss=0.0338420495390892
I0305 12:09:35.748436 139776167794432 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.024377495050430298, loss=0.03116835653781891
I0305 12:10:02.071513 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:12:06.349587 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:12:10.123617 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:12:13.095910 139937033598784 submission_runner.py:411] Time since start: 6786.57s, 	Step: 12583, 	{'train/accuracy': 0.9903304576873779, 'train/loss': 0.03185645118355751, 'train/mean_average_precision': 0.36944802478871486, 'validation/accuracy': 0.9867208003997803, 'validation/loss': 0.04447318986058235, 'validation/mean_average_precision': 0.25959510160944205, 'validation/num_examples': 43793, 'test/accuracy': 0.9858494997024536, 'test/loss': 0.04719026759266853, 'test/mean_average_precision': 0.26324093725410824, 'test/num_examples': 43793, 'score': 4101.431351184845, 'total_duration': 6786.566886663437, 'accumulated_submission_time': 4101.431351184845, 'accumulated_eval_time': 2684.271157026291, 'accumulated_logging_time': 0.5138566493988037}
I0305 12:12:13.113530 139769339950848 logging_writer.py:48] [12583] accumulated_eval_time=2684.271157, accumulated_logging_time=0.513857, accumulated_submission_time=4101.431351, global_step=12583, preemption_count=0, score=4101.431351, test/accuracy=0.985849, test/loss=0.047190, test/mean_average_precision=0.263241, test/num_examples=43793, total_duration=6786.566887, train/accuracy=0.990330, train/loss=0.031856, train/mean_average_precision=0.369448, validation/accuracy=0.986721, validation/loss=0.044473, validation/mean_average_precision=0.259595, validation/num_examples=43793
I0305 12:12:18.940682 139874938124032 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.024767428636550903, loss=0.03167421370744705
I0305 12:12:51.163685 139769339950848 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02831609733402729, loss=0.03254884481430054
I0305 12:13:23.235113 139874938124032 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.03301241248846054, loss=0.029769467189908028
I0305 12:13:55.480566 139769339950848 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.032952506095170975, loss=0.0348440483212471
I0305 12:14:27.674829 139874938124032 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.03125658631324768, loss=0.033219531178474426
I0305 12:14:59.328489 139769339950848 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.02735039032995701, loss=0.032208286225795746
I0305 12:15:31.012478 139874938124032 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.03132950887084007, loss=0.03483707830309868
I0305 12:16:03.175510 139769339950848 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.023765642195940018, loss=0.03165483474731445
I0305 12:16:13.354161 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:18:18.081998 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:18:21.085928 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:18:24.116853 139937033598784 submission_runner.py:411] Time since start: 7157.59s, 	Step: 13332, 	{'train/accuracy': 0.9905761480331421, 'train/loss': 0.03126542642712593, 'train/mean_average_precision': 0.3798342046666803, 'validation/accuracy': 0.9867805242538452, 'validation/loss': 0.043982576578855515, 'validation/mean_average_precision': 0.2672474720559514, 'validation/num_examples': 43793, 'test/accuracy': 0.9859135150909424, 'test/loss': 0.04675788804888725, 'test/mean_average_precision': 0.2592363877832236, 'test/num_examples': 43793, 'score': 4341.640905618668, 'total_duration': 7157.587838172913, 'accumulated_submission_time': 4341.640905618668, 'accumulated_eval_time': 2815.033797264099, 'accumulated_logging_time': 0.5426039695739746}
I0305 12:18:24.134569 139776159401728 logging_writer.py:48] [13332] accumulated_eval_time=2815.033797, accumulated_logging_time=0.542604, accumulated_submission_time=4341.640906, global_step=13332, preemption_count=0, score=4341.640906, test/accuracy=0.985914, test/loss=0.046758, test/mean_average_precision=0.259236, test/num_examples=43793, total_duration=7157.587838, train/accuracy=0.990576, train/loss=0.031265, train/mean_average_precision=0.379834, validation/accuracy=0.986781, validation/loss=0.043983, validation/mean_average_precision=0.267247, validation/num_examples=43793
I0305 12:18:46.107245 139875021985536 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.02633892558515072, loss=0.03264627978205681
I0305 12:19:18.305966 139776159401728 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.034086547791957855, loss=0.03655092418193817
I0305 12:19:50.878573 139875021985536 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.03389834985136986, loss=0.03427552059292793
I0305 12:20:23.126256 139776159401728 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.02919250726699829, loss=0.033601343631744385
I0305 12:20:54.909278 139875021985536 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.03300544619560242, loss=0.0341087281703949
I0305 12:21:27.207110 139776159401728 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.042147643864154816, loss=0.037886980921030045
I0305 12:21:59.009719 139875021985536 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.03995435684919357, loss=0.03655541315674782
I0305 12:22:24.447275 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:24:26.456318 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:24:29.504600 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:24:32.470673 139937033598784 submission_runner.py:411] Time since start: 7525.94s, 	Step: 14079, 	{'train/accuracy': 0.9905685782432556, 'train/loss': 0.031137695536017418, 'train/mean_average_precision': 0.37659143310165594, 'validation/accuracy': 0.9868125915527344, 'validation/loss': 0.04411666467785835, 'validation/mean_average_precision': 0.2629989339221388, 'validation/num_examples': 43793, 'test/accuracy': 0.9859122633934021, 'test/loss': 0.04691025987267494, 'test/mean_average_precision': 0.26015378641829295, 'test/num_examples': 43793, 'score': 4581.922067165375, 'total_duration': 7525.94166302681, 'accumulated_submission_time': 4581.922067165375, 'accumulated_eval_time': 2943.0571522712708, 'accumulated_logging_time': 0.571929931640625}
I0305 12:24:32.488353 139769339950848 logging_writer.py:48] [14079] accumulated_eval_time=2943.057152, accumulated_logging_time=0.571930, accumulated_submission_time=4581.922067, global_step=14079, preemption_count=0, score=4581.922067, test/accuracy=0.985912, test/loss=0.046910, test/mean_average_precision=0.260154, test/num_examples=43793, total_duration=7525.941663, train/accuracy=0.990569, train/loss=0.031138, train/mean_average_precision=0.376591, validation/accuracy=0.986813, validation/loss=0.044117, validation/mean_average_precision=0.262999, validation/num_examples=43793
I0305 12:24:39.722204 139776167794432 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.02519013173878193, loss=0.03077108785510063
I0305 12:25:12.363727 139769339950848 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.02530045248568058, loss=0.030971674248576164
I0305 12:25:44.613981 139776167794432 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.03863214701414108, loss=0.03448577970266342
I0305 12:26:17.413895 139769339950848 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.043594688177108765, loss=0.031070580706000328
I0305 12:26:50.266224 139776167794432 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03663013502955437, loss=0.03319159150123596
I0305 12:27:22.539551 139769339950848 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.033422186970710754, loss=0.02868802100419998
I0305 12:27:55.149049 139776167794432 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.03153581544756889, loss=0.03423168882727623
I0305 12:28:28.124355 139769339950848 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.032450031489133835, loss=0.030829889699816704
I0305 12:28:32.693393 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:30:37.388374 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:30:40.428323 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:30:43.394631 139937033598784 submission_runner.py:411] Time since start: 7896.87s, 	Step: 14815, 	{'train/accuracy': 0.9906119704246521, 'train/loss': 0.030856484547257423, 'train/mean_average_precision': 0.39531919776204916, 'validation/accuracy': 0.9868409633636475, 'validation/loss': 0.044138338416814804, 'validation/mean_average_precision': 0.26569194408292685, 'validation/num_examples': 43793, 'test/accuracy': 0.9859982132911682, 'test/loss': 0.04684007167816162, 'test/mean_average_precision': 0.26401741705710496, 'test/num_examples': 43793, 'score': 4822.095349788666, 'total_duration': 7896.865571737289, 'accumulated_submission_time': 4822.095349788666, 'accumulated_eval_time': 3073.758298397064, 'accumulated_logging_time': 0.6007485389709473}
I0305 12:30:43.412820 139776159401728 logging_writer.py:48] [14815] accumulated_eval_time=3073.758298, accumulated_logging_time=0.600749, accumulated_submission_time=4822.095350, global_step=14815, preemption_count=0, score=4822.095350, test/accuracy=0.985998, test/loss=0.046840, test/mean_average_precision=0.264017, test/num_examples=43793, total_duration=7896.865572, train/accuracy=0.990612, train/loss=0.030856, train/mean_average_precision=0.395319, validation/accuracy=0.986841, validation/loss=0.044138, validation/mean_average_precision=0.265692, validation/num_examples=43793
I0305 12:31:11.282608 139875021985536 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.040700916200876236, loss=0.033720191568136215
I0305 12:31:43.505137 139776159401728 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.03267021104693413, loss=0.030057312920689583
I0305 12:32:16.362735 139875021985536 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.04350271821022034, loss=0.03208988904953003
I0305 12:32:49.380317 139776159401728 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.03231177479028702, loss=0.03320789709687233
I0305 12:33:22.454306 139875021985536 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.03610553219914436, loss=0.0338556244969368
I0305 12:33:55.204314 139776159401728 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04298045113682747, loss=0.03187805041670799
I0305 12:34:28.368253 139875021985536 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.036672670394182205, loss=0.037117309868335724
I0305 12:34:43.566781 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:36:51.286095 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:36:54.270099 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:36:57.243030 139937033598784 submission_runner.py:411] Time since start: 8270.71s, 	Step: 15547, 	{'train/accuracy': 0.99070805311203, 'train/loss': 0.030294276773929596, 'train/mean_average_precision': 0.4081647168737971, 'validation/accuracy': 0.9868353009223938, 'validation/loss': 0.04443247988820076, 'validation/mean_average_precision': 0.26146169553123105, 'validation/num_examples': 43793, 'test/accuracy': 0.9859632253646851, 'test/loss': 0.047224510461091995, 'test/mean_average_precision': 0.260707375649573, 'test/num_examples': 43793, 'score': 5062.213956356049, 'total_duration': 8270.714021921158, 'accumulated_submission_time': 5062.213956356049, 'accumulated_eval_time': 3207.4345166683197, 'accumulated_logging_time': 0.6317923069000244}
I0305 12:36:57.261450 139769339950848 logging_writer.py:48] [15547] accumulated_eval_time=3207.434517, accumulated_logging_time=0.631792, accumulated_submission_time=5062.213956, global_step=15547, preemption_count=0, score=5062.213956, test/accuracy=0.985963, test/loss=0.047225, test/mean_average_precision=0.260707, test/num_examples=43793, total_duration=8270.714022, train/accuracy=0.990708, train/loss=0.030294, train/mean_average_precision=0.408165, validation/accuracy=0.986835, validation/loss=0.044432, validation/mean_average_precision=0.261462, validation/num_examples=43793
I0305 12:37:14.921671 139776167794432 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.033250950276851654, loss=0.029436318203806877
I0305 12:37:47.336488 139769339950848 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.033747050911188126, loss=0.03166384622454643
I0305 12:38:20.483938 139776167794432 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03419199585914612, loss=0.032522715628147125
I0305 12:38:53.255222 139769339950848 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.03423968702554703, loss=0.030908677726984024
I0305 12:39:26.055449 139776167794432 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03974964842200279, loss=0.032689694315195084
I0305 12:39:58.686059 139769339950848 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03329682722687721, loss=0.030003633350133896
I0305 12:40:31.540250 139776167794432 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.038123778998851776, loss=0.028327327221632004
I0305 12:40:57.356686 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:42:58.022449 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:43:00.998707 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:43:04.079023 139937033598784 submission_runner.py:411] Time since start: 8637.55s, 	Step: 16279, 	{'train/accuracy': 0.9908111095428467, 'train/loss': 0.02984723448753357, 'train/mean_average_precision': 0.415534257377386, 'validation/accuracy': 0.9867788553237915, 'validation/loss': 0.04418701305985451, 'validation/mean_average_precision': 0.2656293101641374, 'validation/num_examples': 43793, 'test/accuracy': 0.9859278798103333, 'test/loss': 0.04684547334909439, 'test/mean_average_precision': 0.26316927418512914, 'test/num_examples': 43793, 'score': 5302.277789592743, 'total_duration': 8637.550012588501, 'accumulated_submission_time': 5302.277789592743, 'accumulated_eval_time': 3334.1568129062653, 'accumulated_logging_time': 0.661334753036499}
I0305 12:43:04.097257 139874938124032 logging_writer.py:48] [16279] accumulated_eval_time=3334.156813, accumulated_logging_time=0.661335, accumulated_submission_time=5302.277790, global_step=16279, preemption_count=0, score=5302.277790, test/accuracy=0.985928, test/loss=0.046845, test/mean_average_precision=0.263169, test/num_examples=43793, total_duration=8637.550013, train/accuracy=0.990811, train/loss=0.029847, train/mean_average_precision=0.415534, validation/accuracy=0.986779, validation/loss=0.044187, validation/mean_average_precision=0.265629, validation/num_examples=43793
I0305 12:43:11.362607 139875021985536 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.04029679670929909, loss=0.0314614400267601
I0305 12:43:43.778902 139874938124032 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.03730679303407669, loss=0.030487924814224243
I0305 12:44:16.009761 139875021985536 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.03370777890086174, loss=0.02987373061478138
I0305 12:44:48.305244 139874938124032 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.042522285133600235, loss=0.03064795583486557
I0305 12:45:20.475980 139875021985536 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.05201510339975357, loss=0.032918017357587814
I0305 12:45:52.334981 139874938124032 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.03505430370569229, loss=0.028797663748264313
I0305 12:46:24.558734 139875021985536 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.034308940172195435, loss=0.029643218964338303
I0305 12:46:56.446177 139874938124032 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.03779423236846924, loss=0.0334576815366745
I0305 12:47:04.139276 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:49:06.630405 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:49:09.688627 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:49:12.650255 139937033598784 submission_runner.py:411] Time since start: 9006.12s, 	Step: 17025, 	{'train/accuracy': 0.9908833503723145, 'train/loss': 0.02958684042096138, 'train/mean_average_precision': 0.43396564478056066, 'validation/accuracy': 0.9867642521858215, 'validation/loss': 0.0441272146999836, 'validation/mean_average_precision': 0.2659126980593257, 'validation/num_examples': 43793, 'test/accuracy': 0.9859012961387634, 'test/loss': 0.0468287318944931, 'test/mean_average_precision': 0.26259719754608973, 'test/num_examples': 43793, 'score': 5542.2883632183075, 'total_duration': 9006.121246814728, 'accumulated_submission_time': 5542.2883632183075, 'accumulated_eval_time': 3462.667748451233, 'accumulated_logging_time': 0.6905078887939453}
I0305 12:49:12.668419 139776159401728 logging_writer.py:48] [17025] accumulated_eval_time=3462.667748, accumulated_logging_time=0.690508, accumulated_submission_time=5542.288363, global_step=17025, preemption_count=0, score=5542.288363, test/accuracy=0.985901, test/loss=0.046829, test/mean_average_precision=0.262597, test/num_examples=43793, total_duration=9006.121247, train/accuracy=0.990883, train/loss=0.029587, train/mean_average_precision=0.433966, validation/accuracy=0.986764, validation/loss=0.044127, validation/mean_average_precision=0.265913, validation/num_examples=43793
I0305 12:49:37.881985 139776167794432 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.0355089008808136, loss=0.029242949560284615
I0305 12:50:10.419533 139776159401728 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.04458475857973099, loss=0.03038671426475048
I0305 12:50:42.479952 139776167794432 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.03461997210979462, loss=0.02737053856253624
I0305 12:51:14.819911 139776159401728 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.0365910641849041, loss=0.03225736320018768
I0305 12:51:46.925518 139776167794432 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0394403412938118, loss=0.033724311739206314
I0305 12:52:19.474378 139776159401728 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.04289033263921738, loss=0.031774140894412994
I0305 12:52:51.716787 139776167794432 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.05787378549575806, loss=0.03122384287416935
I0305 12:53:12.779061 139937033598784 spec.py:321] Evaluating on the training split.
I0305 12:55:14.740314 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 12:55:17.798258 139937033598784 spec.py:349] Evaluating on the test split.
I0305 12:55:20.817201 139937033598784 submission_runner.py:411] Time since start: 9374.29s, 	Step: 17766, 	{'train/accuracy': 0.9913222193717957, 'train/loss': 0.02848220244050026, 'train/mean_average_precision': 0.4490868252663456, 'validation/accuracy': 0.9868288040161133, 'validation/loss': 0.04381118714809418, 'validation/mean_average_precision': 0.26824808973077696, 'validation/num_examples': 43793, 'test/accuracy': 0.9859914779663086, 'test/loss': 0.046521496027708054, 'test/mean_average_precision': 0.26056044294935565, 'test/num_examples': 43793, 'score': 5782.3681898117065, 'total_duration': 9374.288189411163, 'accumulated_submission_time': 5782.3681898117065, 'accumulated_eval_time': 3590.7058432102203, 'accumulated_logging_time': 0.7197377681732178}
I0305 12:55:20.836597 139768268125952 logging_writer.py:48] [17766] accumulated_eval_time=3590.705843, accumulated_logging_time=0.719738, accumulated_submission_time=5782.368190, global_step=17766, preemption_count=0, score=5782.368190, test/accuracy=0.985991, test/loss=0.046521, test/mean_average_precision=0.260560, test/num_examples=43793, total_duration=9374.288189, train/accuracy=0.991322, train/loss=0.028482, train/mean_average_precision=0.449087, validation/accuracy=0.986829, validation/loss=0.043811, validation/mean_average_precision=0.268248, validation/num_examples=43793
I0305 12:55:32.203133 139875021985536 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.06254015862941742, loss=0.033992841839790344
I0305 12:56:05.001667 139768268125952 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.07701027393341064, loss=0.032145146280527115
I0305 12:56:38.436578 139875021985536 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.051091697067022324, loss=0.03251256048679352
I0305 12:57:11.603462 139768268125952 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.04051539674401283, loss=0.031183110550045967
I0305 12:57:44.670332 139875021985536 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.047503937035799026, loss=0.03346845507621765
I0305 12:58:17.843914 139768268125952 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.04218420013785362, loss=0.029776841402053833
I0305 12:58:50.806150 139875021985536 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.044906239956617355, loss=0.0299549363553524
I0305 12:59:20.927804 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:01:22.806847 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:01:25.908898 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:01:28.972484 139937033598784 submission_runner.py:411] Time since start: 9742.44s, 	Step: 18490, 	{'train/accuracy': 0.9911221265792847, 'train/loss': 0.02917279303073883, 'train/mean_average_precision': 0.4280779237068718, 'validation/accuracy': 0.9867683053016663, 'validation/loss': 0.044331714510917664, 'validation/mean_average_precision': 0.26221615510662133, 'validation/num_examples': 43793, 'test/accuracy': 0.9860028028488159, 'test/loss': 0.04699481651186943, 'test/mean_average_precision': 0.26194694370496, 'test/num_examples': 43793, 'score': 6022.423826932907, 'total_duration': 9742.443471431732, 'accumulated_submission_time': 6022.423826932907, 'accumulated_eval_time': 3718.7504889965057, 'accumulated_logging_time': 0.7523410320281982}
I0305 13:01:28.992182 139769339950848 logging_writer.py:48] [18490] accumulated_eval_time=3718.750489, accumulated_logging_time=0.752341, accumulated_submission_time=6022.423827, global_step=18490, preemption_count=0, score=6022.423827, test/accuracy=0.986003, test/loss=0.046995, test/mean_average_precision=0.261947, test/num_examples=43793, total_duration=9742.443471, train/accuracy=0.991122, train/loss=0.029173, train/mean_average_precision=0.428078, validation/accuracy=0.986768, validation/loss=0.044332, validation/mean_average_precision=0.262216, validation/num_examples=43793
I0305 13:01:32.618203 139776159401728 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.04626356065273285, loss=0.031745873391628265
I0305 13:02:05.100801 139769339950848 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.04523547738790512, loss=0.027028502896428108
I0305 13:02:37.755534 139776159401728 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.21340584754943848, loss=0.03393418341875076
I0305 13:03:10.018213 139769339950848 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.042629584670066833, loss=0.029820455238223076
I0305 13:03:42.441226 139776159401728 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.038912057876586914, loss=0.030255300924181938
I0305 13:04:14.624176 139769339950848 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.046626802533864975, loss=0.028233814984560013
I0305 13:04:46.903271 139776159401728 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.03628131374716759, loss=0.028949854895472527
I0305 13:05:18.841633 139769339950848 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.03540264070034027, loss=0.028225552290678024
I0305 13:05:28.985689 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:07:31.596152 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:07:34.658704 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:07:37.718746 139937033598784 submission_runner.py:411] Time since start: 10111.19s, 	Step: 19233, 	{'train/accuracy': 0.9910096526145935, 'train/loss': 0.029334599152207375, 'train/mean_average_precision': 0.4320504665074334, 'validation/accuracy': 0.986805260181427, 'validation/loss': 0.04396289214491844, 'validation/mean_average_precision': 0.27540758405545673, 'validation/num_examples': 43793, 'test/accuracy': 0.9859328866004944, 'test/loss': 0.04676557332277298, 'test/mean_average_precision': 0.26261115649681405, 'test/num_examples': 43793, 'score': 6262.385018110275, 'total_duration': 10111.189734220505, 'accumulated_submission_time': 6262.385018110275, 'accumulated_eval_time': 3847.483499765396, 'accumulated_logging_time': 0.7836909294128418}
I0305 13:07:37.737439 139768268125952 logging_writer.py:48] [19233] accumulated_eval_time=3847.483500, accumulated_logging_time=0.783691, accumulated_submission_time=6262.385018, global_step=19233, preemption_count=0, score=6262.385018, test/accuracy=0.985933, test/loss=0.046766, test/mean_average_precision=0.262611, test/num_examples=43793, total_duration=10111.189734, train/accuracy=0.991010, train/loss=0.029335, train/mean_average_precision=0.432050, validation/accuracy=0.986805, validation/loss=0.043963, validation/mean_average_precision=0.275408, validation/num_examples=43793
I0305 13:07:59.459999 139776167794432 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.03691898658871651, loss=0.03116871975362301
I0305 13:08:32.014313 139768268125952 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.04082407057285309, loss=0.025486094877123833
I0305 13:09:04.436758 139776167794432 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.044461749494075775, loss=0.030919430777430534
I0305 13:09:36.989027 139768268125952 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04128678888082504, loss=0.029153037816286087
I0305 13:10:09.390861 139776167794432 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.04966513067483902, loss=0.032565973699092865
I0305 13:10:41.586440 139768268125952 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.0467771515250206, loss=0.030360544100403786
I0305 13:11:13.917674 139776167794432 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04704948887228966, loss=0.02947569265961647
I0305 13:11:37.875181 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:13:39.954469 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:13:43.032055 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:13:46.079292 139937033598784 submission_runner.py:411] Time since start: 10479.55s, 	Step: 19976, 	{'train/accuracy': 0.9907341599464417, 'train/loss': 0.0300019308924675, 'train/mean_average_precision': 0.42784259828669885, 'validation/accuracy': 0.9869095683097839, 'validation/loss': 0.04444120079278946, 'validation/mean_average_precision': 0.2761037932319848, 'validation/num_examples': 43793, 'test/accuracy': 0.9860508441925049, 'test/loss': 0.04723105579614639, 'test/mean_average_precision': 0.2681115070087855, 'test/num_examples': 43793, 'score': 6502.49117732048, 'total_duration': 10479.550280809402, 'accumulated_submission_time': 6502.49117732048, 'accumulated_eval_time': 3975.6875672340393, 'accumulated_logging_time': 0.813539981842041}
I0305 13:13:46.098230 139769339950848 logging_writer.py:48] [19976] accumulated_eval_time=3975.687567, accumulated_logging_time=0.813540, accumulated_submission_time=6502.491177, global_step=19976, preemption_count=0, score=6502.491177, test/accuracy=0.986051, test/loss=0.047231, test/mean_average_precision=0.268112, test/num_examples=43793, total_duration=10479.550281, train/accuracy=0.990734, train/loss=0.030002, train/mean_average_precision=0.427843, validation/accuracy=0.986910, validation/loss=0.044441, validation/mean_average_precision=0.276104, validation/num_examples=43793
I0305 13:13:54.202009 139776159401728 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04954091086983681, loss=0.030219418928027153
I0305 13:14:26.390638 139769339950848 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.042683012783527374, loss=0.02813010662794113
I0305 13:14:58.774371 139776159401728 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04398750886321068, loss=0.031536683440208435
I0305 13:15:30.794947 139769339950848 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.042661286890506744, loss=0.029495107010006905
I0305 13:16:03.167520 139776159401728 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.047525569796562195, loss=0.03336622193455696
I0305 13:16:35.648082 139769339950848 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.052521850913763046, loss=0.03161075338721275
I0305 13:17:07.369312 139776159401728 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.04158039391040802, loss=0.031732358038425446
I0305 13:17:39.481116 139769339950848 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.050435274839401245, loss=0.03278648853302002
I0305 13:17:46.311784 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:19:47.468809 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:19:50.507462 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:19:53.561034 139937033598784 submission_runner.py:411] Time since start: 10847.03s, 	Step: 20722, 	{'train/accuracy': 0.9907103776931763, 'train/loss': 0.030265728011727333, 'train/mean_average_precision': 0.40831877474501255, 'validation/accuracy': 0.9867829084396362, 'validation/loss': 0.04421905428171158, 'validation/mean_average_precision': 0.27433058277897215, 'validation/num_examples': 43793, 'test/accuracy': 0.9859228134155273, 'test/loss': 0.047040510922670364, 'test/mean_average_precision': 0.26317339950444973, 'test/num_examples': 43793, 'score': 6742.673607110977, 'total_duration': 10847.032025575638, 'accumulated_submission_time': 6742.673607110977, 'accumulated_eval_time': 4102.936810493469, 'accumulated_logging_time': 0.8434133529663086}
I0305 13:19:53.580541 139776167794432 logging_writer.py:48] [20722] accumulated_eval_time=4102.936810, accumulated_logging_time=0.843413, accumulated_submission_time=6742.673607, global_step=20722, preemption_count=0, score=6742.673607, test/accuracy=0.985923, test/loss=0.047041, test/mean_average_precision=0.263173, test/num_examples=43793, total_duration=10847.032026, train/accuracy=0.990710, train/loss=0.030266, train/mean_average_precision=0.408319, validation/accuracy=0.986783, validation/loss=0.044219, validation/mean_average_precision=0.274331, validation/num_examples=43793
I0305 13:20:19.342620 139875021985536 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.04165561497211456, loss=0.03067067265510559
I0305 13:20:51.144096 139776167794432 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.04534165933728218, loss=0.030079688876867294
I0305 13:21:23.412410 139875021985536 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.05159815773367882, loss=0.03639029711484909
I0305 13:21:55.813700 139776167794432 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04190191626548767, loss=0.02955123968422413
I0305 13:22:28.243677 139875021985536 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.05352591723203659, loss=0.03207314759492874
I0305 13:23:00.607380 139776167794432 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.04225296527147293, loss=0.03151237964630127
I0305 13:23:32.888967 139875021985536 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.04158563166856766, loss=0.029603857547044754
I0305 13:23:53.657942 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:25:56.759833 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:25:59.796072 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:26:02.939406 139937033598784 submission_runner.py:411] Time since start: 11216.41s, 	Step: 21465, 	{'train/accuracy': 0.9911373853683472, 'train/loss': 0.028706619516015053, 'train/mean_average_precision': 0.44942397627172315, 'validation/accuracy': 0.9869254231452942, 'validation/loss': 0.04398034140467644, 'validation/mean_average_precision': 0.2773801293033675, 'validation/num_examples': 43793, 'test/accuracy': 0.9860706329345703, 'test/loss': 0.04681684449315071, 'test/mean_average_precision': 0.2728187773762324, 'test/num_examples': 43793, 'score': 6982.719746828079, 'total_duration': 11216.410384893417, 'accumulated_submission_time': 6982.719746828079, 'accumulated_eval_time': 4232.218222379684, 'accumulated_logging_time': 0.8738067150115967}
I0305 13:26:02.959497 139769339950848 logging_writer.py:48] [21465] accumulated_eval_time=4232.218222, accumulated_logging_time=0.873807, accumulated_submission_time=6982.719747, global_step=21465, preemption_count=0, score=6982.719747, test/accuracy=0.986071, test/loss=0.046817, test/mean_average_precision=0.272819, test/num_examples=43793, total_duration=11216.410385, train/accuracy=0.991137, train/loss=0.028707, train/mean_average_precision=0.449424, validation/accuracy=0.986925, validation/loss=0.043980, validation/mean_average_precision=0.277380, validation/num_examples=43793
I0305 13:26:14.727991 139776159401728 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0558634027838707, loss=0.028955359011888504
I0305 13:26:46.765402 139769339950848 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.047910917550325394, loss=0.030743952840566635
I0305 13:27:18.985499 139776159401728 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.04471609368920326, loss=0.03234300762414932
I0305 13:27:50.893600 139769339950848 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.04339343681931496, loss=0.02929060161113739
I0305 13:28:23.244758 139776159401728 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.050086528062820435, loss=0.03329296410083771
I0305 13:28:55.663515 139769339950848 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.04373764619231224, loss=0.032098524272441864
I0305 13:29:27.973301 139776159401728 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.0450417585670948, loss=0.029703009873628616
I0305 13:30:00.303006 139769339950848 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.05095729976892471, loss=0.031063899397850037
I0305 13:30:03.238251 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:32:02.781857 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:32:05.802219 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:32:08.840754 139937033598784 submission_runner.py:411] Time since start: 11582.31s, 	Step: 22210, 	{'train/accuracy': 0.9912395477294922, 'train/loss': 0.028494717553257942, 'train/mean_average_precision': 0.44971815025571843, 'validation/accuracy': 0.9868494868278503, 'validation/loss': 0.04405718669295311, 'validation/mean_average_precision': 0.2787659404410005, 'validation/num_examples': 43793, 'test/accuracy': 0.985990583896637, 'test/loss': 0.04688079655170441, 'test/mean_average_precision': 0.26182727476044465, 'test/num_examples': 43793, 'score': 7222.9662935733795, 'total_duration': 11582.311628341675, 'accumulated_submission_time': 7222.9662935733795, 'accumulated_eval_time': 4357.820563793182, 'accumulated_logging_time': 0.9055871963500977}
I0305 13:32:08.860048 139768268125952 logging_writer.py:48] [22210] accumulated_eval_time=4357.820564, accumulated_logging_time=0.905587, accumulated_submission_time=7222.966294, global_step=22210, preemption_count=0, score=7222.966294, test/accuracy=0.985991, test/loss=0.046881, test/mean_average_precision=0.261827, test/num_examples=43793, total_duration=11582.311628, train/accuracy=0.991240, train/loss=0.028495, train/mean_average_precision=0.449718, validation/accuracy=0.986849, validation/loss=0.044057, validation/mean_average_precision=0.278766, validation/num_examples=43793
I0305 13:32:38.668968 139875021985536 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.05129881575703621, loss=0.033337969332933426
I0305 13:33:11.147416 139768268125952 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.042743079364299774, loss=0.02903759852051735
I0305 13:33:43.433265 139875021985536 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.06801960617303848, loss=0.03002035617828369
I0305 13:34:16.166498 139768268125952 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.044838521629571915, loss=0.03033577837049961
I0305 13:34:48.329876 139875021985536 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.04887043312191963, loss=0.03299717977643013
I0305 13:35:20.907347 139768268125952 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.054174911230802536, loss=0.02835262566804886
I0305 13:35:52.887141 139875021985536 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04359510913491249, loss=0.027395514771342278
I0305 13:36:08.913097 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:38:13.264221 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:38:16.311587 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:38:19.330735 139937033598784 submission_runner.py:411] Time since start: 11952.80s, 	Step: 22951, 	{'train/accuracy': 0.9914074540138245, 'train/loss': 0.02790139801800251, 'train/mean_average_precision': 0.4588849204040839, 'validation/accuracy': 0.9868494868278503, 'validation/loss': 0.04405561089515686, 'validation/mean_average_precision': 0.2763261030184993, 'validation/num_examples': 43793, 'test/accuracy': 0.9859901666641235, 'test/loss': 0.0469595305621624, 'test/mean_average_precision': 0.2666838640350755, 'test/num_examples': 43793, 'score': 7462.987103223801, 'total_duration': 11952.801618337631, 'accumulated_submission_time': 7462.987103223801, 'accumulated_eval_time': 4488.238070964813, 'accumulated_logging_time': 0.937244176864624}
I0305 13:38:19.350709 139769339950848 logging_writer.py:48] [22951] accumulated_eval_time=4488.238071, accumulated_logging_time=0.937244, accumulated_submission_time=7462.987103, global_step=22951, preemption_count=0, score=7462.987103, test/accuracy=0.985990, test/loss=0.046960, test/mean_average_precision=0.266684, test/num_examples=43793, total_duration=11952.801618, train/accuracy=0.991407, train/loss=0.027901, train/mean_average_precision=0.458885, validation/accuracy=0.986849, validation/loss=0.044056, validation/mean_average_precision=0.276326, validation/num_examples=43793
I0305 13:38:36.830646 139776159401728 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.05593955144286156, loss=0.029659753665328026
I0305 13:39:09.529104 139769339950848 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.05162128806114197, loss=0.030849473550915718
I0305 13:39:41.343915 139776159401728 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.05369969829916954, loss=0.030434025451540947
I0305 13:40:13.202488 139769339950848 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.044088464230298996, loss=0.02926935814321041
I0305 13:40:45.718958 139776159401728 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.05549231171607971, loss=0.034323278814554214
I0305 13:41:18.058320 139769339950848 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.05190364643931389, loss=0.03437900170683861
I0305 13:41:49.816194 139776159401728 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.06155432015657425, loss=0.02866622433066368
I0305 13:42:19.512169 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:44:21.630368 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:44:24.674696 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:44:27.745702 139937033598784 submission_runner.py:411] Time since start: 12321.22s, 	Step: 23693, 	{'train/accuracy': 0.9915399551391602, 'train/loss': 0.0275176540017128, 'train/mean_average_precision': 0.4827089102235441, 'validation/accuracy': 0.9867687225341797, 'validation/loss': 0.04405149817466736, 'validation/mean_average_precision': 0.28060430987862217, 'validation/num_examples': 43793, 'test/accuracy': 0.9859842658042908, 'test/loss': 0.046660155057907104, 'test/mean_average_precision': 0.26536505182199804, 'test/num_examples': 43793, 'score': 7703.116607427597, 'total_duration': 12321.216604471207, 'accumulated_submission_time': 7703.116607427597, 'accumulated_eval_time': 4616.471472978592, 'accumulated_logging_time': 0.9694650173187256}
I0305 13:44:27.765130 139768268125952 logging_writer.py:48] [23693] accumulated_eval_time=4616.471473, accumulated_logging_time=0.969465, accumulated_submission_time=7703.116607, global_step=23693, preemption_count=0, score=7703.116607, test/accuracy=0.985984, test/loss=0.046660, test/mean_average_precision=0.265365, test/num_examples=43793, total_duration=12321.216604, train/accuracy=0.991540, train/loss=0.027518, train/mean_average_precision=0.482709, validation/accuracy=0.986769, validation/loss=0.044051, validation/mean_average_precision=0.280604, validation/num_examples=43793
I0305 13:44:30.324278 139776167794432 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.04763127118349075, loss=0.029349125921726227
I0305 13:45:02.685504 139768268125952 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.04596472159028053, loss=0.0287450160831213
I0305 13:45:34.738209 139776167794432 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.05272861197590828, loss=0.029199479147791862
I0305 13:46:07.350581 139768268125952 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.07219887524843216, loss=0.029859226197004318
I0305 13:46:39.755567 139776167794432 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.042404450476169586, loss=0.02582956850528717
I0305 13:47:11.876665 139768268125952 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.05340007320046425, loss=0.02844175510108471
I0305 13:47:44.064585 139776167794432 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.05595835670828819, loss=0.031371280550956726
I0305 13:48:16.288173 139768268125952 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.06310640275478363, loss=0.032014213502407074
I0305 13:48:27.921836 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:50:31.948513 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:50:35.027157 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:50:38.083529 139937033598784 submission_runner.py:411] Time since start: 12691.55s, 	Step: 24437, 	{'train/accuracy': 0.9916155338287354, 'train/loss': 0.02726803719997406, 'train/mean_average_precision': 0.4807372090344972, 'validation/accuracy': 0.9868682026863098, 'validation/loss': 0.04398051276803017, 'validation/mean_average_precision': 0.2749072176084976, 'validation/num_examples': 43793, 'test/accuracy': 0.9860904216766357, 'test/loss': 0.046519577503204346, 'test/mean_average_precision': 0.27280477022823024, 'test/num_examples': 43793, 'score': 7943.241847038269, 'total_duration': 12691.554522037506, 'accumulated_submission_time': 7943.241847038269, 'accumulated_eval_time': 4746.633130073547, 'accumulated_logging_time': 1.000164270401001}
I0305 13:50:38.103417 139769339950848 logging_writer.py:48] [24437] accumulated_eval_time=4746.633130, accumulated_logging_time=1.000164, accumulated_submission_time=7943.241847, global_step=24437, preemption_count=0, score=7943.241847, test/accuracy=0.986090, test/loss=0.046520, test/mean_average_precision=0.272805, test/num_examples=43793, total_duration=12691.554522, train/accuracy=0.991616, train/loss=0.027268, train/mean_average_precision=0.480737, validation/accuracy=0.986868, validation/loss=0.043981, validation/mean_average_precision=0.274907, validation/num_examples=43793
I0305 13:50:59.022588 139875021985536 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0657092034816742, loss=0.0345127247273922
I0305 13:51:32.761913 139769339950848 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.0476958341896534, loss=0.028638048097491264
I0305 13:52:05.814233 139875021985536 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.043971866369247437, loss=0.029528144747018814
I0305 13:52:38.618776 139769339950848 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.05658549442887306, loss=0.0295041985809803
I0305 13:53:11.416788 139875021985536 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05841871723532677, loss=0.026319369673728943
I0305 13:53:44.081751 139769339950848 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.05266310274600983, loss=0.030880382284522057
I0305 13:54:17.014587 139875021985536 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.046406324952840805, loss=0.0283928532153368
I0305 13:54:38.202851 139937033598784 spec.py:321] Evaluating on the training split.
I0305 13:56:43.258877 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 13:56:46.359024 139937033598784 spec.py:349] Evaluating on the test split.
I0305 13:56:49.395513 139937033598784 submission_runner.py:411] Time since start: 13062.87s, 	Step: 25167, 	{'train/accuracy': 0.9914122819900513, 'train/loss': 0.027905769646167755, 'train/mean_average_precision': 0.4571382522024744, 'validation/accuracy': 0.9869388341903687, 'validation/loss': 0.044082775712013245, 'validation/mean_average_precision': 0.280402054904702, 'validation/num_examples': 43793, 'test/accuracy': 0.9861481189727783, 'test/loss': 0.046841852366924286, 'test/mean_average_precision': 0.2706147259221962, 'test/num_examples': 43793, 'score': 8183.310019493103, 'total_duration': 13062.866502285004, 'accumulated_submission_time': 8183.310019493103, 'accumulated_eval_time': 4877.8257603645325, 'accumulated_logging_time': 1.0311439037322998}
I0305 13:56:49.416549 139768268125952 logging_writer.py:48] [25167] accumulated_eval_time=4877.825760, accumulated_logging_time=1.031144, accumulated_submission_time=8183.310019, global_step=25167, preemption_count=0, score=8183.310019, test/accuracy=0.986148, test/loss=0.046842, test/mean_average_precision=0.270615, test/num_examples=43793, total_duration=13062.866502, train/accuracy=0.991412, train/loss=0.027906, train/mean_average_precision=0.457138, validation/accuracy=0.986939, validation/loss=0.044083, validation/mean_average_precision=0.280402, validation/num_examples=43793
I0305 13:57:00.565531 139776159401728 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.0471699982881546, loss=0.02986922673881054
I0305 13:57:33.145190 139768268125952 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.057393237948417664, loss=0.028546903282403946
I0305 13:58:05.944209 139776159401728 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.04667001590132713, loss=0.0281788669526577
I0305 13:58:39.699855 139768268125952 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.05709974840283394, loss=0.03332163393497467
I0305 13:59:12.864220 139776159401728 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.05305781587958336, loss=0.030894674360752106
I0305 13:59:45.459994 139768268125952 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.050113871693611145, loss=0.029379436746239662
I0305 14:00:18.353305 139776159401728 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.053273558616638184, loss=0.028592107817530632
I0305 14:00:49.410207 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:02:49.620331 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:02:52.744460 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:02:55.722973 139937033598784 submission_runner.py:411] Time since start: 13429.19s, 	Step: 25896, 	{'train/accuracy': 0.991398811340332, 'train/loss': 0.027916178107261658, 'train/mean_average_precision': 0.4598515014503374, 'validation/accuracy': 0.9869047403335571, 'validation/loss': 0.0441686175763607, 'validation/mean_average_precision': 0.27278324959094996, 'validation/num_examples': 43793, 'test/accuracy': 0.9862020611763, 'test/loss': 0.04671012610197067, 'test/mean_average_precision': 0.26409816049202955, 'test/num_examples': 43793, 'score': 8423.271643161774, 'total_duration': 13429.19395661354, 'accumulated_submission_time': 8423.271643161774, 'accumulated_eval_time': 5004.138481140137, 'accumulated_logging_time': 1.0636134147644043}
I0305 14:02:55.743030 139776167794432 logging_writer.py:48] [25896] accumulated_eval_time=5004.138481, accumulated_logging_time=1.063613, accumulated_submission_time=8423.271643, global_step=25896, preemption_count=0, score=8423.271643, test/accuracy=0.986202, test/loss=0.046710, test/mean_average_precision=0.264098, test/num_examples=43793, total_duration=13429.193957, train/accuracy=0.991399, train/loss=0.027916, train/mean_average_precision=0.459852, validation/accuracy=0.986905, validation/loss=0.044169, validation/mean_average_precision=0.272783, validation/num_examples=43793
I0305 14:02:57.393439 139875021985536 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.07289481163024902, loss=0.040306027978658676
I0305 14:03:30.001682 139776167794432 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.056462083011865616, loss=0.03092331252992153
I0305 14:04:02.537686 139875021985536 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.054886430501937866, loss=0.0348418764770031
I0305 14:04:34.873080 139776167794432 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.04955841228365898, loss=0.029322365298867226
I0305 14:05:07.774061 139875021985536 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.06411980837583542, loss=0.031814247369766235
I0305 14:05:40.192894 139776167794432 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.05495130270719528, loss=0.03068745695054531
I0305 14:06:12.886800 139875021985536 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.06523062288761139, loss=0.031336475163698196
I0305 14:06:45.255691 139776167794432 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.05830428749322891, loss=0.030338263139128685
I0305 14:06:55.977461 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:08:58.511819 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:09:01.695722 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:09:04.718051 139937033598784 submission_runner.py:411] Time since start: 13798.19s, 	Step: 26634, 	{'train/accuracy': 0.9914059638977051, 'train/loss': 0.027858808636665344, 'train/mean_average_precision': 0.48041487352690293, 'validation/accuracy': 0.9870743751525879, 'validation/loss': 0.04396640881896019, 'validation/mean_average_precision': 0.2818839899912088, 'validation/num_examples': 43793, 'test/accuracy': 0.9862193465232849, 'test/loss': 0.046763624995946884, 'test/mean_average_precision': 0.27700793096873644, 'test/num_examples': 43793, 'score': 8663.474416017532, 'total_duration': 13798.188932180405, 'accumulated_submission_time': 8663.474416017532, 'accumulated_eval_time': 5132.878929376602, 'accumulated_logging_time': 1.0949442386627197}
I0305 14:09:04.740950 139768268125952 logging_writer.py:48] [26634] accumulated_eval_time=5132.878929, accumulated_logging_time=1.094944, accumulated_submission_time=8663.474416, global_step=26634, preemption_count=0, score=8663.474416, test/accuracy=0.986219, test/loss=0.046764, test/mean_average_precision=0.277008, test/num_examples=43793, total_duration=13798.188932, train/accuracy=0.991406, train/loss=0.027859, train/mean_average_precision=0.480415, validation/accuracy=0.987074, validation/loss=0.043966, validation/mean_average_precision=0.281884, validation/num_examples=43793
I0305 14:09:26.411558 139769339950848 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.0642089918255806, loss=0.03153021261096001
I0305 14:09:58.522076 139768268125952 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.05739329010248184, loss=0.029014436528086662
I0305 14:10:31.148932 139769339950848 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.05757739767432213, loss=0.02770288474857807
I0305 14:11:04.143539 139768268125952 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.05751229077577591, loss=0.028305010870099068
I0305 14:11:36.777361 139769339950848 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.06942106038331985, loss=0.02680234983563423
I0305 14:12:09.580337 139768268125952 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05502503737807274, loss=0.029423315078020096
I0305 14:12:42.315319 139769339950848 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.051494039595127106, loss=0.030018523335456848
I0305 14:13:04.853001 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:15:04.862222 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:15:07.873548 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:15:10.830969 139937033598784 submission_runner.py:411] Time since start: 14164.30s, 	Step: 27370, 	{'train/accuracy': 0.9915772080421448, 'train/loss': 0.027190539985895157, 'train/mean_average_precision': 0.47435818621636394, 'validation/accuracy': 0.9870736002922058, 'validation/loss': 0.043841127306222916, 'validation/mean_average_precision': 0.28760961842778665, 'validation/num_examples': 43793, 'test/accuracy': 0.9862067103385925, 'test/loss': 0.04684111848473549, 'test/mean_average_precision': 0.27195462212427224, 'test/num_examples': 43793, 'score': 8903.555037021637, 'total_duration': 14164.301958322525, 'accumulated_submission_time': 8903.555037021637, 'accumulated_eval_time': 5258.8568522930145, 'accumulated_logging_time': 1.128938913345337}
I0305 14:15:10.851322 139776159401728 logging_writer.py:48] [27370] accumulated_eval_time=5258.856852, accumulated_logging_time=1.128939, accumulated_submission_time=8903.555037, global_step=27370, preemption_count=0, score=8903.555037, test/accuracy=0.986207, test/loss=0.046841, test/mean_average_precision=0.271955, test/num_examples=43793, total_duration=14164.301958, train/accuracy=0.991577, train/loss=0.027191, train/mean_average_precision=0.474358, validation/accuracy=0.987074, validation/loss=0.043841, validation/mean_average_precision=0.287610, validation/num_examples=43793
I0305 14:15:20.795837 139776167794432 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.0607777014374733, loss=0.03065318986773491
I0305 14:15:53.058192 139776159401728 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06109904870390892, loss=0.03221720829606056
I0305 14:16:25.469435 139776167794432 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.055863481014966965, loss=0.030063290148973465
I0305 14:16:57.987319 139776159401728 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.062019046396017075, loss=0.029036514461040497
I0305 14:17:30.447731 139776167794432 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.05561530962586403, loss=0.029795106500387192
I0305 14:18:02.882393 139776159401728 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.05075626075267792, loss=0.027041766792535782
I0305 14:18:35.059445 139776167794432 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.05069806054234505, loss=0.029713939875364304
I0305 14:19:07.367235 139776159401728 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.06326116621494293, loss=0.028043627738952637
I0305 14:19:10.925809 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:21:15.016907 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:21:18.125932 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:21:21.146356 139937033598784 submission_runner.py:411] Time since start: 14534.62s, 	Step: 28112, 	{'train/accuracy': 0.9914939999580383, 'train/loss': 0.027363203465938568, 'train/mean_average_precision': 0.47760750461065876, 'validation/accuracy': 0.9868556261062622, 'validation/loss': 0.04430204629898071, 'validation/mean_average_precision': 0.27810895250193857, 'validation/num_examples': 43793, 'test/accuracy': 0.9860523343086243, 'test/loss': 0.04709487408399582, 'test/mean_average_precision': 0.27044050598680414, 'test/num_examples': 43793, 'score': 9143.596687078476, 'total_duration': 14534.617344141006, 'accumulated_submission_time': 9143.596687078476, 'accumulated_eval_time': 5389.0773758888245, 'accumulated_logging_time': 1.1617298126220703}
I0305 14:21:21.167862 139768268125952 logging_writer.py:48] [28112] accumulated_eval_time=5389.077376, accumulated_logging_time=1.161730, accumulated_submission_time=9143.596687, global_step=28112, preemption_count=0, score=9143.596687, test/accuracy=0.986052, test/loss=0.047095, test/mean_average_precision=0.270441, test/num_examples=43793, total_duration=14534.617344, train/accuracy=0.991494, train/loss=0.027363, train/mean_average_precision=0.477608, validation/accuracy=0.986856, validation/loss=0.044302, validation/mean_average_precision=0.278109, validation/num_examples=43793
I0305 14:21:50.482586 139769339950848 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.059275925159454346, loss=0.028083838522434235
I0305 14:22:23.743720 139768268125952 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.05668152868747711, loss=0.0290320236235857
I0305 14:22:56.271768 139769339950848 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.06702686101198196, loss=0.029474610462784767
I0305 14:23:29.249470 139768268125952 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.050997938960790634, loss=0.029317421838641167
I0305 14:24:01.869125 139769339950848 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.054191954433918, loss=0.027239879593253136
I0305 14:24:34.000012 139768268125952 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.05592375993728638, loss=0.030868282541632652
I0305 14:25:06.579946 139769339950848 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.051560819149017334, loss=0.02939268760383129
I0305 14:25:21.416255 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:27:23.056364 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:27:26.124189 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:27:29.100252 139937033598784 submission_runner.py:411] Time since start: 14902.57s, 	Step: 28846, 	{'train/accuracy': 0.9917556643486023, 'train/loss': 0.026656657457351685, 'train/mean_average_precision': 0.49465622712082624, 'validation/accuracy': 0.9868446588516235, 'validation/loss': 0.04430018737912178, 'validation/mean_average_precision': 0.27669377137084444, 'validation/num_examples': 43793, 'test/accuracy': 0.9861131906509399, 'test/loss': 0.04706292226910591, 'test/mean_average_precision': 0.2691851735216722, 'test/num_examples': 43793, 'score': 9383.813615322113, 'total_duration': 14902.571242570877, 'accumulated_submission_time': 9383.813615322113, 'accumulated_eval_time': 5516.7613315582275, 'accumulated_logging_time': 1.194563627243042}
I0305 14:27:29.120971 139776167794432 logging_writer.py:48] [28846] accumulated_eval_time=5516.761332, accumulated_logging_time=1.194564, accumulated_submission_time=9383.813615, global_step=28846, preemption_count=0, score=9383.813615, test/accuracy=0.986113, test/loss=0.047063, test/mean_average_precision=0.269185, test/num_examples=43793, total_duration=14902.571243, train/accuracy=0.991756, train/loss=0.026657, train/mean_average_precision=0.494656, validation/accuracy=0.986845, validation/loss=0.044300, validation/mean_average_precision=0.276694, validation/num_examples=43793
I0305 14:27:46.934553 139875021985536 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.04836820811033249, loss=0.026419930160045624
I0305 14:28:19.528271 139776167794432 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.055071815848350525, loss=0.02715999260544777
I0305 14:28:52.346842 139875021985536 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.07968689501285553, loss=0.03684350103139877
I0305 14:29:25.432113 139776167794432 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.061580218374729156, loss=0.03149969130754471
I0305 14:29:57.875570 139875021985536 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05204382538795471, loss=0.027968619018793106
I0305 14:30:30.154348 139776167794432 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.055842477828264236, loss=0.029497237876057625
I0305 14:31:02.400973 139875021985536 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05047593638300896, loss=0.028429485857486725
I0305 14:31:29.300814 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:33:30.494847 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:33:33.535090 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:33:36.514682 139937033598784 submission_runner.py:411] Time since start: 15269.99s, 	Step: 29584, 	{'train/accuracy': 0.9919654130935669, 'train/loss': 0.025940055027604103, 'train/mean_average_precision': 0.5117402117318774, 'validation/accuracy': 0.9869948625564575, 'validation/loss': 0.04410788416862488, 'validation/mean_average_precision': 0.28803251269141766, 'validation/num_examples': 43793, 'test/accuracy': 0.9861321449279785, 'test/loss': 0.047050341963768005, 'test/mean_average_precision': 0.2701175138067016, 'test/num_examples': 43793, 'score': 9623.958882570267, 'total_duration': 15269.985672235489, 'accumulated_submission_time': 9623.958882570267, 'accumulated_eval_time': 5643.975158452988, 'accumulated_logging_time': 1.228114128112793}
I0305 14:33:36.535614 139769339950848 logging_writer.py:48] [29584] accumulated_eval_time=5643.975158, accumulated_logging_time=1.228114, accumulated_submission_time=9623.958883, global_step=29584, preemption_count=0, score=9623.958883, test/accuracy=0.986132, test/loss=0.047050, test/mean_average_precision=0.270118, test/num_examples=43793, total_duration=15269.985672, train/accuracy=0.991965, train/loss=0.025940, train/mean_average_precision=0.511740, validation/accuracy=0.986995, validation/loss=0.044108, validation/mean_average_precision=0.288033, validation/num_examples=43793
I0305 14:33:42.066803 139776159401728 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.06421216577291489, loss=0.033100590109825134
I0305 14:34:14.080730 139769339950848 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.06491846591234207, loss=0.027579637244343758
I0305 14:34:46.618943 139776159401728 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05643647909164429, loss=0.02848641388118267
I0305 14:35:19.145129 139769339950848 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.05542604997754097, loss=0.02813030406832695
I0305 14:35:51.316045 139776159401728 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.06441409885883331, loss=0.03338370472192764
I0305 14:36:23.854793 139769339950848 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.052407845854759216, loss=0.02893679402768612
I0305 14:36:56.235140 139776159401728 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.061888549476861954, loss=0.029852671548724174
I0305 14:37:28.638520 139769339950848 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.0523625873029232, loss=0.027714086696505547
I0305 14:37:36.621019 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:39:33.834027 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:39:36.903193 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:39:39.954736 139937033598784 submission_runner.py:411] Time since start: 15633.43s, 	Step: 30326, 	{'train/accuracy': 0.9920579195022583, 'train/loss': 0.025749610736966133, 'train/mean_average_precision': 0.5215447573111239, 'validation/accuracy': 0.9868953824043274, 'validation/loss': 0.043969716876745224, 'validation/mean_average_precision': 0.2858635197434587, 'validation/num_examples': 43793, 'test/accuracy': 0.9860247373580933, 'test/loss': 0.046749185770750046, 'test/mean_average_precision': 0.2688654699671372, 'test/num_examples': 43793, 'score': 9864.012751102448, 'total_duration': 15633.425725221634, 'accumulated_submission_time': 9864.012751102448, 'accumulated_eval_time': 5767.308829545975, 'accumulated_logging_time': 1.2603094577789307}
I0305 14:39:39.976596 139768268125952 logging_writer.py:48] [30326] accumulated_eval_time=5767.308830, accumulated_logging_time=1.260309, accumulated_submission_time=9864.012751, global_step=30326, preemption_count=0, score=9864.012751, test/accuracy=0.986025, test/loss=0.046749, test/mean_average_precision=0.268865, test/num_examples=43793, total_duration=15633.425725, train/accuracy=0.992058, train/loss=0.025750, train/mean_average_precision=0.521545, validation/accuracy=0.986895, validation/loss=0.043970, validation/mean_average_precision=0.285864, validation/num_examples=43793
I0305 14:40:04.248964 139776167794432 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.05169985443353653, loss=0.026705581694841385
I0305 14:40:36.570892 139768268125952 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.059421855956315994, loss=0.02733909711241722
I0305 14:41:08.747277 139776167794432 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05387137457728386, loss=0.028164157643914223
I0305 14:41:41.067019 139768268125952 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.05425622686743736, loss=0.02562091313302517
I0305 14:42:13.851231 139776167794432 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.07340335100889206, loss=0.03223828226327896
I0305 14:42:46.247410 139768268125952 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.06690443307161331, loss=0.026047561317682266
I0305 14:43:18.704069 139776167794432 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06009656935930252, loss=0.026959506794810295
I0305 14:43:40.052931 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:45:39.232186 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:45:42.264430 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:45:45.239263 139937033598784 submission_runner.py:411] Time since start: 15998.71s, 	Step: 31067, 	{'train/accuracy': 0.9918926358222961, 'train/loss': 0.026205135509371758, 'train/mean_average_precision': 0.5086030748444564, 'validation/accuracy': 0.986970067024231, 'validation/loss': 0.044068433344364166, 'validation/mean_average_precision': 0.28349563328399696, 'validation/num_examples': 43793, 'test/accuracy': 0.9861097931861877, 'test/loss': 0.04692363366484642, 'test/mean_average_precision': 0.2746272969201792, 'test/num_examples': 43793, 'score': 10104.057809352875, 'total_duration': 15998.710246562958, 'accumulated_submission_time': 10104.057809352875, 'accumulated_eval_time': 5892.495110750198, 'accumulated_logging_time': 1.2933599948883057}
I0305 14:45:45.260189 139769339950848 logging_writer.py:48] [31067] accumulated_eval_time=5892.495111, accumulated_logging_time=1.293360, accumulated_submission_time=10104.057809, global_step=31067, preemption_count=0, score=10104.057809, test/accuracy=0.986110, test/loss=0.046924, test/mean_average_precision=0.274627, test/num_examples=43793, total_duration=15998.710247, train/accuracy=0.991893, train/loss=0.026205, train/mean_average_precision=0.508603, validation/accuracy=0.986970, validation/loss=0.044068, validation/mean_average_precision=0.283496, validation/num_examples=43793
I0305 14:45:56.287726 139875021985536 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.0640241950750351, loss=0.026744341477751732
I0305 14:46:28.681519 139769339950848 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.055279817432165146, loss=0.028040606528520584
I0305 14:47:00.780995 139875021985536 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0624689906835556, loss=0.02666485868394375
I0305 14:47:32.738827 139769339950848 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.050228629261255264, loss=0.02620008960366249
I0305 14:48:05.002425 139875021985536 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.057326916605234146, loss=0.028148120269179344
I0305 14:48:36.687298 139769339950848 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.06076300889253616, loss=0.029442284256219864
I0305 14:49:08.567605 139875021985536 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.06031442806124687, loss=0.030564164742827415
I0305 14:49:40.570256 139769339950848 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.06413638591766357, loss=0.031125657260417938
I0305 14:49:45.332761 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:51:46.506007 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:51:49.542171 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:51:52.527460 139937033598784 submission_runner.py:411] Time since start: 16366.00s, 	Step: 31816, 	{'train/accuracy': 0.9916676878929138, 'train/loss': 0.026850014925003052, 'train/mean_average_precision': 0.4904087292640114, 'validation/accuracy': 0.9869644045829773, 'validation/loss': 0.04413967579603195, 'validation/mean_average_precision': 0.2862625865048266, 'validation/num_examples': 43793, 'test/accuracy': 0.9861460328102112, 'test/loss': 0.04685007035732269, 'test/mean_average_precision': 0.2760263600114768, 'test/num_examples': 43793, 'score': 10344.097812891006, 'total_duration': 16365.998442649841, 'accumulated_submission_time': 10344.097812891006, 'accumulated_eval_time': 6019.689756393433, 'accumulated_logging_time': 1.3267717361450195}
I0305 14:51:52.548968 139776159401728 logging_writer.py:48] [31816] accumulated_eval_time=6019.689756, accumulated_logging_time=1.326772, accumulated_submission_time=10344.097813, global_step=31816, preemption_count=0, score=10344.097813, test/accuracy=0.986146, test/loss=0.046850, test/mean_average_precision=0.276026, test/num_examples=43793, total_duration=16365.998443, train/accuracy=0.991668, train/loss=0.026850, train/mean_average_precision=0.490409, validation/accuracy=0.986964, validation/loss=0.044140, validation/mean_average_precision=0.286263, validation/num_examples=43793
I0305 14:52:20.038129 139776167794432 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.053424008190631866, loss=0.024149678647518158
I0305 14:52:51.966175 139776159401728 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.053871188312768936, loss=0.026007335633039474
I0305 14:53:24.187062 139776167794432 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.0517454594373703, loss=0.027344102039933205
I0305 14:53:56.418538 139776159401728 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.0619853213429451, loss=0.02869427017867565
I0305 14:54:28.596180 139776167794432 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.052045341581106186, loss=0.030274005606770515
I0305 14:55:01.016720 139776159401728 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.07143624126911163, loss=0.03336764872074127
I0305 14:55:33.576241 139776167794432 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.05293208733201027, loss=0.027515294030308723
I0305 14:55:52.854687 139937033598784 spec.py:321] Evaluating on the training split.
I0305 14:57:54.664605 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 14:57:57.736975 139937033598784 spec.py:349] Evaluating on the test split.
I0305 14:58:00.740359 139937033598784 submission_runner.py:411] Time since start: 16734.21s, 	Step: 32560, 	{'train/accuracy': 0.9917237758636475, 'train/loss': 0.026784153655171394, 'train/mean_average_precision': 0.493660033538621, 'validation/accuracy': 0.9869781732559204, 'validation/loss': 0.04385552555322647, 'validation/mean_average_precision': 0.290015872842108, 'validation/num_examples': 43793, 'test/accuracy': 0.9861759543418884, 'test/loss': 0.04649024456739426, 'test/mean_average_precision': 0.2793258645120916, 'test/num_examples': 43793, 'score': 10584.372139453888, 'total_duration': 16734.21134352684, 'accumulated_submission_time': 10584.372139453888, 'accumulated_eval_time': 6147.57537651062, 'accumulated_logging_time': 1.3592548370361328}
I0305 14:58:00.762063 139768268125952 logging_writer.py:48] [32560] accumulated_eval_time=6147.575377, accumulated_logging_time=1.359255, accumulated_submission_time=10584.372139, global_step=32560, preemption_count=0, score=10584.372139, test/accuracy=0.986176, test/loss=0.046490, test/mean_average_precision=0.279326, test/num_examples=43793, total_duration=16734.211344, train/accuracy=0.991724, train/loss=0.026784, train/mean_average_precision=0.493660, validation/accuracy=0.986978, validation/loss=0.043856, validation/mean_average_precision=0.290016, validation/num_examples=43793
I0305 14:58:15.154337 139769339950848 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.054460559040308, loss=0.026810016483068466
I0305 14:58:48.427672 139768268125952 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.08985935151576996, loss=0.03143736347556114
I0305 14:59:21.056447 139769339950848 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.08751803636550903, loss=0.029215671122074127
I0305 14:59:53.270853 139768268125952 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.05337819829583168, loss=0.027125349268317223
I0305 15:00:25.658307 139769339950848 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.05090877413749695, loss=0.029243644326925278
I0305 15:00:57.605183 139768268125952 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.057273782789707184, loss=0.02918820269405842
I0305 15:01:30.057918 139769339950848 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.054863303899765015, loss=0.02563955821096897
I0305 15:02:00.890442 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:04:02.774154 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:04:05.815552 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:04:08.840964 139937033598784 submission_runner.py:411] Time since start: 17102.31s, 	Step: 33296, 	{'train/accuracy': 0.9918603897094727, 'train/loss': 0.026210252195596695, 'train/mean_average_precision': 0.49822119583307656, 'validation/accuracy': 0.9870301485061646, 'validation/loss': 0.04416833072900772, 'validation/mean_average_precision': 0.287087614806626, 'validation/num_examples': 43793, 'test/accuracy': 0.9861931800842285, 'test/loss': 0.0470806285738945, 'test/mean_average_precision': 0.27092642396789335, 'test/num_examples': 43793, 'score': 10824.466272115707, 'total_duration': 17102.31195449829, 'accumulated_submission_time': 10824.466272115707, 'accumulated_eval_time': 6275.525854349136, 'accumulated_logging_time': 1.3940739631652832}
I0305 15:04:08.862524 139760711874304 logging_writer.py:48] [33296] accumulated_eval_time=6275.525854, accumulated_logging_time=1.394074, accumulated_submission_time=10824.466272, global_step=33296, preemption_count=0, score=10824.466272, test/accuracy=0.986193, test/loss=0.047081, test/mean_average_precision=0.270926, test/num_examples=43793, total_duration=17102.311954, train/accuracy=0.991860, train/loss=0.026210, train/mean_average_precision=0.498221, validation/accuracy=0.987030, validation/loss=0.044168, validation/mean_average_precision=0.287088, validation/num_examples=43793
I0305 15:04:10.540597 139776159401728 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.05731845274567604, loss=0.02564634382724762
I0305 15:04:43.552999 139760711874304 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06188293918967247, loss=0.025296514853835106
I0305 15:05:16.068295 139776159401728 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.05094683915376663, loss=0.02732706069946289
I0305 15:05:48.484569 139760711874304 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.06330642104148865, loss=0.03191269934177399
I0305 15:06:21.712456 139776159401728 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05420823022723198, loss=0.027589386329054832
I0305 15:06:55.020770 139760711874304 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.06159033253788948, loss=0.030421629548072815
I0305 15:07:27.673526 139776159401728 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.05509425699710846, loss=0.028373632580041885
I0305 15:07:59.705503 139760711874304 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.053679294884204865, loss=0.026272011920809746
I0305 15:08:09.010265 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:10:06.967589 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:10:09.982844 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:10:12.962138 139937033598784 submission_runner.py:411] Time since start: 17466.43s, 	Step: 34029, 	{'train/accuracy': 0.9919319152832031, 'train/loss': 0.02592024765908718, 'train/mean_average_precision': 0.5161529871779962, 'validation/accuracy': 0.9869481325149536, 'validation/loss': 0.0443512499332428, 'validation/mean_average_precision': 0.2823631179528553, 'validation/num_examples': 43793, 'test/accuracy': 0.9860681295394897, 'test/loss': 0.04691473767161369, 'test/mean_average_precision': 0.2748894485436163, 'test/num_examples': 43793, 'score': 11064.580405950546, 'total_duration': 17466.4331305027, 'accumulated_submission_time': 11064.580405950546, 'accumulated_eval_time': 6399.4776821136475, 'accumulated_logging_time': 1.4283545017242432}
I0305 15:10:12.983686 139768268125952 logging_writer.py:48] [34029] accumulated_eval_time=6399.477682, accumulated_logging_time=1.428355, accumulated_submission_time=11064.580406, global_step=34029, preemption_count=0, score=11064.580406, test/accuracy=0.986068, test/loss=0.046915, test/mean_average_precision=0.274889, test/num_examples=43793, total_duration=17466.433131, train/accuracy=0.991932, train/loss=0.025920, train/mean_average_precision=0.516153, validation/accuracy=0.986948, validation/loss=0.044351, validation/mean_average_precision=0.282363, validation/num_examples=43793
I0305 15:10:35.903002 139769339950848 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.05662228539586067, loss=0.027684001252055168
I0305 15:11:07.996252 139768268125952 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.060968682169914246, loss=0.02886870875954628
I0305 15:11:39.760792 139769339950848 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06312648952007294, loss=0.028674103319644928
I0305 15:12:11.731742 139768268125952 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.061782483011484146, loss=0.028921423479914665
I0305 15:12:43.844278 139769339950848 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.05855909734964371, loss=0.02871144376695156
I0305 15:13:15.821623 139768268125952 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.056649643927812576, loss=0.026148561388254166
I0305 15:13:48.037077 139769339950848 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.07698580622673035, loss=0.03330247476696968
I0305 15:14:13.158492 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:16:16.941296 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:16:20.017403 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:16:23.052923 139937033598784 submission_runner.py:411] Time since start: 17836.52s, 	Step: 34779, 	{'train/accuracy': 0.9920884966850281, 'train/loss': 0.02547481469810009, 'train/mean_average_precision': 0.5129617949169412, 'validation/accuracy': 0.9869903922080994, 'validation/loss': 0.04402710497379303, 'validation/mean_average_precision': 0.29156286018675115, 'validation/num_examples': 43793, 'test/accuracy': 0.9861776232719421, 'test/loss': 0.04699322208762169, 'test/mean_average_precision': 0.2721970894810648, 'test/num_examples': 43793, 'score': 11304.724274158478, 'total_duration': 17836.523913621902, 'accumulated_submission_time': 11304.724274158478, 'accumulated_eval_time': 6529.37206697464, 'accumulated_logging_time': 1.4609463214874268}
I0305 15:16:23.074900 139760711874304 logging_writer.py:48] [34779] accumulated_eval_time=6529.372067, accumulated_logging_time=1.460946, accumulated_submission_time=11304.724274, global_step=34779, preemption_count=0, score=11304.724274, test/accuracy=0.986178, test/loss=0.046993, test/mean_average_precision=0.272197, test/num_examples=43793, total_duration=17836.523914, train/accuracy=0.992088, train/loss=0.025475, train/mean_average_precision=0.512962, validation/accuracy=0.986990, validation/loss=0.044027, validation/mean_average_precision=0.291563, validation/num_examples=43793
I0305 15:16:30.225370 139776167794432 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06456572562456131, loss=0.026122665032744408
I0305 15:17:02.374808 139760711874304 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.0630042552947998, loss=0.031016187742352486
I0305 15:17:34.534955 139776167794432 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.05138947069644928, loss=0.02430310659110546
I0305 15:18:07.134381 139760711874304 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.06598121672868729, loss=0.02587731182575226
I0305 15:18:39.754849 139776167794432 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.056133903563022614, loss=0.027241932228207588
I0305 15:19:13.263034 139760711874304 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.05954492837190628, loss=0.026613766327500343
I0305 15:19:46.979125 139776167794432 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.05791090056300163, loss=0.02503790333867073
I0305 15:20:20.655864 139760711874304 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06649895012378693, loss=0.028482628986239433
I0305 15:20:23.333633 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:22:24.044254 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:22:27.118038 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:22:30.141001 139937033598784 submission_runner.py:411] Time since start: 18203.61s, 	Step: 35509, 	{'train/accuracy': 0.9922428131103516, 'train/loss': 0.024744657799601555, 'train/mean_average_precision': 0.5505906445570861, 'validation/accuracy': 0.9870585799217224, 'validation/loss': 0.04413862153887749, 'validation/mean_average_precision': 0.29079843071591815, 'validation/num_examples': 43793, 'test/accuracy': 0.9862454533576965, 'test/loss': 0.04701567813754082, 'test/mean_average_precision': 0.27535762609257186, 'test/num_examples': 43793, 'score': 11544.948768615723, 'total_duration': 18203.61198425293, 'accumulated_submission_time': 11544.948768615723, 'accumulated_eval_time': 6656.179391384125, 'accumulated_logging_time': 1.4955766201019287}
I0305 15:22:30.163102 139768268125952 logging_writer.py:48] [35509] accumulated_eval_time=6656.179391, accumulated_logging_time=1.495577, accumulated_submission_time=11544.948769, global_step=35509, preemption_count=0, score=11544.948769, test/accuracy=0.986245, test/loss=0.047016, test/mean_average_precision=0.275358, test/num_examples=43793, total_duration=18203.611984, train/accuracy=0.992243, train/loss=0.024745, train/mean_average_precision=0.550591, validation/accuracy=0.987059, validation/loss=0.044139, validation/mean_average_precision=0.290798, validation/num_examples=43793
I0305 15:23:00.092277 139769339950848 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.05473177507519722, loss=0.025498757138848305
I0305 15:23:32.974649 139768268125952 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.06145589053630829, loss=0.0285994503647089
I0305 15:24:05.464872 139769339950848 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.07210765033960342, loss=0.024908116087317467
I0305 15:24:37.637258 139768268125952 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.07803512364625931, loss=0.02630477398633957
I0305 15:25:09.805723 139769339950848 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.07930269837379456, loss=0.027754995971918106
I0305 15:25:42.124710 139768268125952 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.05907003954052925, loss=0.02619875967502594
I0305 15:26:14.588341 139769339950848 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.07884062826633453, loss=0.028884705156087875
I0305 15:26:30.220038 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:28:32.928521 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:28:36.004155 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:28:39.033097 139937033598784 submission_runner.py:411] Time since start: 18572.50s, 	Step: 36250, 	{'train/accuracy': 0.9923936724662781, 'train/loss': 0.024478988721966743, 'train/mean_average_precision': 0.5408486038115388, 'validation/accuracy': 0.9870277047157288, 'validation/loss': 0.04410884156823158, 'validation/mean_average_precision': 0.29081139592891014, 'validation/num_examples': 43793, 'test/accuracy': 0.9862197637557983, 'test/loss': 0.046796780079603195, 'test/mean_average_precision': 0.2766800615256882, 'test/num_examples': 43793, 'score': 11784.974064826965, 'total_duration': 18572.50408053398, 'accumulated_submission_time': 11784.974064826965, 'accumulated_eval_time': 6784.992396831512, 'accumulated_logging_time': 1.5288910865783691}
I0305 15:28:39.055364 139760711874304 logging_writer.py:48] [36250] accumulated_eval_time=6784.992397, accumulated_logging_time=1.528891, accumulated_submission_time=11784.974065, global_step=36250, preemption_count=0, score=11784.974065, test/accuracy=0.986220, test/loss=0.046797, test/mean_average_precision=0.276680, test/num_examples=43793, total_duration=18572.504081, train/accuracy=0.992394, train/loss=0.024479, train/mean_average_precision=0.540849, validation/accuracy=0.987028, validation/loss=0.044109, validation/mean_average_precision=0.290811, validation/num_examples=43793
I0305 15:28:55.329615 139776159401728 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.06797824800014496, loss=0.032011453062295914
I0305 15:29:27.524778 139760711874304 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.08456826210021973, loss=0.03179336339235306
I0305 15:29:59.747493 139776159401728 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.06397566199302673, loss=0.030541373416781425
I0305 15:30:32.050423 139760711874304 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.08065389096736908, loss=0.02775612287223339
I0305 15:31:04.957133 139776159401728 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.061528805643320084, loss=0.028639260679483414
I0305 15:31:37.263824 139760711874304 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.0746082216501236, loss=0.029515709728002548
I0305 15:32:09.394266 139776159401728 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.056640785187482834, loss=0.024703197181224823
I0305 15:32:39.088944 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:34:41.689283 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:34:44.705098 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:34:47.702772 139937033598784 submission_runner.py:411] Time since start: 18941.17s, 	Step: 36993, 	{'train/accuracy': 0.9923135042190552, 'train/loss': 0.024678941816091537, 'train/mean_average_precision': 0.5443136465959396, 'validation/accuracy': 0.9869603514671326, 'validation/loss': 0.04452231526374817, 'validation/mean_average_precision': 0.28639239563183544, 'validation/num_examples': 43793, 'test/accuracy': 0.9861136078834534, 'test/loss': 0.04725019633769989, 'test/mean_average_precision': 0.2734903307389538, 'test/num_examples': 43793, 'score': 12024.976420640945, 'total_duration': 18941.173763275146, 'accumulated_submission_time': 12024.976420640945, 'accumulated_eval_time': 6913.606185436249, 'accumulated_logging_time': 1.5621047019958496}
I0305 15:34:47.725129 139769339950848 logging_writer.py:48] [36993] accumulated_eval_time=6913.606185, accumulated_logging_time=1.562105, accumulated_submission_time=12024.976421, global_step=36993, preemption_count=0, score=12024.976421, test/accuracy=0.986114, test/loss=0.047250, test/mean_average_precision=0.273490, test/num_examples=43793, total_duration=18941.173763, train/accuracy=0.992314, train/loss=0.024679, train/mean_average_precision=0.544314, validation/accuracy=0.986960, validation/loss=0.044522, validation/mean_average_precision=0.286392, validation/num_examples=43793
I0305 15:34:50.252246 139776167794432 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.058728594332933426, loss=0.024406716227531433
I0305 15:35:22.599347 139769339950848 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.057402580976486206, loss=0.026250645518302917
I0305 15:35:54.673811 139776167794432 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06594786792993546, loss=0.027880584821105003
I0305 15:36:27.892838 139769339950848 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.059933241456747055, loss=0.025549113750457764
I0305 15:37:01.419019 139776167794432 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06191692128777504, loss=0.025709809735417366
I0305 15:37:35.021425 139769339950848 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.05745137855410576, loss=0.025585874915122986
I0305 15:38:09.943507 139776167794432 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.06095191463828087, loss=0.0282183475792408
I0305 15:38:42.979847 139769339950848 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.06675859540700912, loss=0.024995682761073112
I0305 15:38:47.987058 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:40:50.457209 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:40:53.511069 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:40:56.534317 139937033598784 submission_runner.py:411] Time since start: 19310.01s, 	Step: 37716, 	{'train/accuracy': 0.9922302961349487, 'train/loss': 0.025072799995541573, 'train/mean_average_precision': 0.5250435594834874, 'validation/accuracy': 0.9870094656944275, 'validation/loss': 0.04443740099668503, 'validation/mean_average_precision': 0.2870718545969231, 'validation/num_examples': 43793, 'test/accuracy': 0.9861860275268555, 'test/loss': 0.04708565026521683, 'test/mean_average_precision': 0.2764250250297177, 'test/num_examples': 43793, 'score': 12265.204212665558, 'total_duration': 19310.005308389664, 'accumulated_submission_time': 12265.204212665558, 'accumulated_eval_time': 7042.153408050537, 'accumulated_logging_time': 1.5955908298492432}
I0305 15:40:56.557291 139760711874304 logging_writer.py:48] [37716] accumulated_eval_time=7042.153408, accumulated_logging_time=1.595591, accumulated_submission_time=12265.204213, global_step=37716, preemption_count=0, score=12265.204213, test/accuracy=0.986186, test/loss=0.047086, test/mean_average_precision=0.276425, test/num_examples=43793, total_duration=19310.005308, train/accuracy=0.992230, train/loss=0.025073, train/mean_average_precision=0.525044, validation/accuracy=0.987009, validation/loss=0.044437, validation/mean_average_precision=0.287072, validation/num_examples=43793
I0305 15:41:23.884010 139776159401728 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.11824077367782593, loss=0.0331585519015789
I0305 15:41:56.970173 139760711874304 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.06683032959699631, loss=0.02528263069689274
I0305 15:42:29.768900 139776159401728 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07923122495412827, loss=0.0282745324075222
I0305 15:43:02.399862 139760711874304 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.06268689036369324, loss=0.02690913900732994
I0305 15:43:35.191447 139776159401728 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.06680170446634293, loss=0.031077915802598
I0305 15:44:07.779050 139760711874304 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07648269832134247, loss=0.032277293503284454
I0305 15:44:40.322714 139776159401728 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.06447134166955948, loss=0.027896728366613388
I0305 15:44:56.655271 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:46:55.754632 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:46:58.740723 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:47:01.830405 139937033598784 submission_runner.py:411] Time since start: 19675.30s, 	Step: 38451, 	{'train/accuracy': 0.9921622276306152, 'train/loss': 0.02522260881960392, 'train/mean_average_precision': 0.5285684492729031, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.04441230744123459, 'validation/mean_average_precision': 0.28773676333624504, 'validation/num_examples': 43793, 'test/accuracy': 0.9861224293708801, 'test/loss': 0.047294896095991135, 'test/mean_average_precision': 0.27052527385967584, 'test/num_examples': 43793, 'score': 12505.271298408508, 'total_duration': 19675.301374912262, 'accumulated_submission_time': 12505.271298408508, 'accumulated_eval_time': 7167.328474283218, 'accumulated_logging_time': 1.6293635368347168}
I0305 15:47:01.858572 139769339950848 logging_writer.py:48] [38451] accumulated_eval_time=7167.328474, accumulated_logging_time=1.629364, accumulated_submission_time=12505.271298, global_step=38451, preemption_count=0, score=12505.271298, test/accuracy=0.986122, test/loss=0.047295, test/mean_average_precision=0.270525, test/num_examples=43793, total_duration=19675.301375, train/accuracy=0.992162, train/loss=0.025223, train/mean_average_precision=0.528568, validation/accuracy=0.987021, validation/loss=0.044412, validation/mean_average_precision=0.287737, validation/num_examples=43793
I0305 15:47:17.798986 139776167794432 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.06598525494337082, loss=0.027022073045372963
I0305 15:47:50.218968 139769339950848 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06205838546156883, loss=0.02986272983253002
I0305 15:48:22.547487 139776167794432 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.06492321938276291, loss=0.027095725759863853
I0305 15:48:56.020938 139769339950848 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.0698280781507492, loss=0.027948135510087013
I0305 15:49:28.692892 139776167794432 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06703294813632965, loss=0.028534287586808205
I0305 15:50:01.301630 139769339950848 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.06218850240111351, loss=0.026508523151278496
I0305 15:50:33.833291 139776167794432 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.12251967936754227, loss=0.0310998372733593
I0305 15:51:02.136709 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:53:01.888423 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:53:04.929542 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:53:07.913875 139937033598784 submission_runner.py:411] Time since start: 20041.38s, 	Step: 39188, 	{'train/accuracy': 0.9923495650291443, 'train/loss': 0.024515537545084953, 'train/mean_average_precision': 0.5353314789084255, 'validation/accuracy': 0.9869980812072754, 'validation/loss': 0.04469664394855499, 'validation/mean_average_precision': 0.28809073456755274, 'validation/num_examples': 43793, 'test/accuracy': 0.9861502647399902, 'test/loss': 0.04768317937850952, 'test/mean_average_precision': 0.2739721380819767, 'test/num_examples': 43793, 'score': 12745.515764474869, 'total_duration': 20041.384852409363, 'accumulated_submission_time': 12745.515764474869, 'accumulated_eval_time': 7293.105636358261, 'accumulated_logging_time': 1.6696193218231201}
I0305 15:53:07.936759 139760711874304 logging_writer.py:48] [39188] accumulated_eval_time=7293.105636, accumulated_logging_time=1.669619, accumulated_submission_time=12745.515764, global_step=39188, preemption_count=0, score=12745.515764, test/accuracy=0.986150, test/loss=0.047683, test/mean_average_precision=0.273972, test/num_examples=43793, total_duration=20041.384852, train/accuracy=0.992350, train/loss=0.024516, train/mean_average_precision=0.535331, validation/accuracy=0.986998, validation/loss=0.044697, validation/mean_average_precision=0.288091, validation/num_examples=43793
I0305 15:53:12.378999 139776159401728 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.07545386254787445, loss=0.02968878485262394
I0305 15:53:47.066411 139760711874304 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.05556681007146835, loss=0.023978564888238907
I0305 15:54:19.766598 139776159401728 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06651264429092407, loss=0.027351023629307747
I0305 15:54:52.566016 139760711874304 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.07252562046051025, loss=0.027911191806197166
I0305 15:55:25.039138 139776159401728 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07309991866350174, loss=0.029088612645864487
I0305 15:55:57.236631 139760711874304 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.07023008167743683, loss=0.024758238345384598
I0305 15:56:29.550930 139776159401728 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.06577436625957489, loss=0.025943057611584663
I0305 15:57:01.448023 139760711874304 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.075019970536232, loss=0.027727505192160606
I0305 15:57:07.925199 139937033598784 spec.py:321] Evaluating on the training split.
I0305 15:59:12.234234 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 15:59:15.756858 139937033598784 spec.py:349] Evaluating on the test split.
I0305 15:59:19.253828 139937033598784 submission_runner.py:411] Time since start: 20412.72s, 	Step: 39921, 	{'train/accuracy': 0.9920818209648132, 'train/loss': 0.025075718760490417, 'train/mean_average_precision': 0.5387114399975329, 'validation/accuracy': 0.9870151281356812, 'validation/loss': 0.04456315189599991, 'validation/mean_average_precision': 0.29374497719057996, 'validation/num_examples': 43793, 'test/accuracy': 0.9862037301063538, 'test/loss': 0.047264937311410904, 'test/mean_average_precision': 0.2791759747694861, 'test/num_examples': 43793, 'score': 12985.472758769989, 'total_duration': 20412.724797964096, 'accumulated_submission_time': 12985.472758769989, 'accumulated_eval_time': 7424.434202432632, 'accumulated_logging_time': 1.7036066055297852}
I0305 15:59:19.282721 139769339950848 logging_writer.py:48] [39921] accumulated_eval_time=7424.434202, accumulated_logging_time=1.703607, accumulated_submission_time=12985.472759, global_step=39921, preemption_count=0, score=12985.472759, test/accuracy=0.986204, test/loss=0.047265, test/mean_average_precision=0.279176, test/num_examples=43793, total_duration=20412.724798, train/accuracy=0.992082, train/loss=0.025076, train/mean_average_precision=0.538711, validation/accuracy=0.987015, validation/loss=0.044563, validation/mean_average_precision=0.293745, validation/num_examples=43793
I0305 15:59:46.565141 139776167794432 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.06535902619361877, loss=0.0256129689514637
I0305 16:00:20.032885 139769339950848 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.06827081739902496, loss=0.025871966034173965
I0305 16:00:53.138422 139776167794432 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.07883137464523315, loss=0.028765812516212463
I0305 16:01:26.488921 139769339950848 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.06342818588018417, loss=0.026496483013033867
I0305 16:01:59.173828 139776167794432 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.06292657554149628, loss=0.025745080783963203
I0305 16:02:31.534847 139769339950848 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0642322301864624, loss=0.023110926151275635
I0305 16:03:03.892204 139776167794432 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.08466143906116486, loss=0.029558168724179268
I0305 16:03:19.544527 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:05:19.152308 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:05:22.278335 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:05:25.382877 139937033598784 submission_runner.py:411] Time since start: 20778.85s, 	Step: 40650, 	{'train/accuracy': 0.992318868637085, 'train/loss': 0.02440461702644825, 'train/mean_average_precision': 0.5562079477709713, 'validation/accuracy': 0.9870167374610901, 'validation/loss': 0.044446494430303574, 'validation/mean_average_precision': 0.29095187136323525, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.04713471233844757, 'test/mean_average_precision': 0.2772578004381141, 'test/num_examples': 43793, 'score': 13225.699823617935, 'total_duration': 20778.853857278824, 'accumulated_submission_time': 13225.699823617935, 'accumulated_eval_time': 7550.27251458168, 'accumulated_logging_time': 1.7443230152130127}
I0305 16:05:25.406380 139768268125952 logging_writer.py:48] [40650] accumulated_eval_time=7550.272515, accumulated_logging_time=1.744323, accumulated_submission_time=13225.699824, global_step=40650, preemption_count=0, score=13225.699824, test/accuracy=0.986192, test/loss=0.047135, test/mean_average_precision=0.277258, test/num_examples=43793, total_duration=20778.853857, train/accuracy=0.992319, train/loss=0.024405, train/mean_average_precision=0.556208, validation/accuracy=0.987017, validation/loss=0.044446, validation/mean_average_precision=0.290952, validation/num_examples=43793
I0305 16:05:41.959472 139776159401728 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.06840242445468903, loss=0.02980605885386467
I0305 16:06:14.588895 139768268125952 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.07153527438640594, loss=0.026619091629981995
I0305 16:06:46.864844 139776159401728 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.07391200959682465, loss=0.026127561926841736
I0305 16:07:19.543689 139768268125952 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.08125177770853043, loss=0.02627144381403923
I0305 16:07:51.977125 139776159401728 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07271081954240799, loss=0.024672124534845352
I0305 16:08:24.270053 139768268125952 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.09090445190668106, loss=0.02795586548745632
I0305 16:08:57.126808 139776159401728 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.08244576305150986, loss=0.02783217281103134
I0305 16:09:25.521524 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:11:26.522177 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:11:29.658035 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:11:32.682159 139937033598784 submission_runner.py:411] Time since start: 21146.15s, 	Step: 41388, 	{'train/accuracy': 0.9926362037658691, 'train/loss': 0.02336835116147995, 'train/mean_average_precision': 0.5768032113155684, 'validation/accuracy': 0.9870049953460693, 'validation/loss': 0.04476245865225792, 'validation/mean_average_precision': 0.28294332548482565, 'validation/num_examples': 43793, 'test/accuracy': 0.9861556887626648, 'test/loss': 0.04761787876486778, 'test/mean_average_precision': 0.27095401447383227, 'test/num_examples': 43793, 'score': 13465.783110141754, 'total_duration': 21146.153143405914, 'accumulated_submission_time': 13465.783110141754, 'accumulated_eval_time': 7677.433129072189, 'accumulated_logging_time': 1.7788498401641846}
I0305 16:11:32.705642 139760711874304 logging_writer.py:48] [41388] accumulated_eval_time=7677.433129, accumulated_logging_time=1.778850, accumulated_submission_time=13465.783110, global_step=41388, preemption_count=0, score=13465.783110, test/accuracy=0.986156, test/loss=0.047618, test/mean_average_precision=0.270954, test/num_examples=43793, total_duration=21146.153143, train/accuracy=0.992636, train/loss=0.023368, train/mean_average_precision=0.576803, validation/accuracy=0.987005, validation/loss=0.044762, validation/mean_average_precision=0.282943, validation/num_examples=43793
I0305 16:11:36.980990 139776167794432 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.08310200273990631, loss=0.028063591569662094
I0305 16:12:10.052701 139760711874304 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07239602506160736, loss=0.02658374235033989
I0305 16:12:42.604935 139776167794432 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.08530803769826889, loss=0.030500955879688263
I0305 16:13:15.637685 139760711874304 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.08692914247512817, loss=0.026684315875172615
I0305 16:13:49.014088 139776167794432 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.07295118272304535, loss=0.025525111705064774
I0305 16:14:22.157519 139760711874304 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.07212521880865097, loss=0.026640139520168304
I0305 16:14:55.026477 139776167794432 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.07627527415752411, loss=0.025389205664396286
I0305 16:15:27.980117 139760711874304 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07183554768562317, loss=0.02549031935632229
I0305 16:15:32.928962 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:17:37.472741 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:17:40.549783 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:17:43.583293 139937033598784 submission_runner.py:411] Time since start: 21517.05s, 	Step: 42116, 	{'train/accuracy': 0.992948591709137, 'train/loss': 0.02259449101984501, 'train/mean_average_precision': 0.5889403009136849, 'validation/accuracy': 0.9868965744972229, 'validation/loss': 0.044822003692388535, 'validation/mean_average_precision': 0.28701092672601175, 'validation/num_examples': 43793, 'test/accuracy': 0.986143946647644, 'test/loss': 0.04759962484240532, 'test/mean_average_precision': 0.2726043926098506, 'test/num_examples': 43793, 'score': 13705.972064971924, 'total_duration': 21517.054282665253, 'accumulated_submission_time': 13705.972064971924, 'accumulated_eval_time': 7808.087415456772, 'accumulated_logging_time': 1.8151464462280273}
I0305 16:17:43.607897 139768268125952 logging_writer.py:48] [42116] accumulated_eval_time=7808.087415, accumulated_logging_time=1.815146, accumulated_submission_time=13705.972065, global_step=42116, preemption_count=0, score=13705.972065, test/accuracy=0.986144, test/loss=0.047600, test/mean_average_precision=0.272604, test/num_examples=43793, total_duration=21517.054283, train/accuracy=0.992949, train/loss=0.022594, train/mean_average_precision=0.588940, validation/accuracy=0.986897, validation/loss=0.044822, validation/mean_average_precision=0.287011, validation/num_examples=43793
I0305 16:18:11.543901 139769339950848 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07410889118909836, loss=0.028331778943538666
I0305 16:18:44.374048 139768268125952 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.07778997719287872, loss=0.024431465193629265
I0305 16:19:16.744098 139769339950848 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.08373824506998062, loss=0.029697135090827942
I0305 16:19:49.481545 139768268125952 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07001161575317383, loss=0.0255484189838171
I0305 16:20:21.718305 139769339950848 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.06739424914121628, loss=0.026586461812257767
I0305 16:20:53.551147 139768268125952 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.07230254262685776, loss=0.025058437138795853
I0305 16:21:25.788335 139769339950848 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.08407361060380936, loss=0.02830914780497551
I0305 16:21:43.831444 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:23:41.767861 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:23:44.843689 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:23:47.879367 139937033598784 submission_runner.py:411] Time since start: 21881.35s, 	Step: 42857, 	{'train/accuracy': 0.9928516745567322, 'train/loss': 0.022742001339793205, 'train/mean_average_precision': 0.5831679997609343, 'validation/accuracy': 0.9870119094848633, 'validation/loss': 0.044940680265426636, 'validation/mean_average_precision': 0.2870644899198072, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.04781138151884079, 'test/mean_average_precision': 0.277432456781912, 'test/num_examples': 43793, 'score': 13946.163924694061, 'total_duration': 21881.350362062454, 'accumulated_submission_time': 13946.163924694061, 'accumulated_eval_time': 7932.13530087471, 'accumulated_logging_time': 1.8508188724517822}
I0305 16:23:47.903010 139760711874304 logging_writer.py:48] [42857] accumulated_eval_time=7932.135301, accumulated_logging_time=1.850819, accumulated_submission_time=13946.163925, global_step=42857, preemption_count=0, score=13946.163925, test/accuracy=0.986235, test/loss=0.047811, test/mean_average_precision=0.277432, test/num_examples=43793, total_duration=21881.350362, train/accuracy=0.992852, train/loss=0.022742, train/mean_average_precision=0.583168, validation/accuracy=0.987012, validation/loss=0.044941, validation/mean_average_precision=0.287064, validation/num_examples=43793
I0305 16:24:02.206852 139776167794432 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.07206130772829056, loss=0.024609532207250595
I0305 16:24:34.713148 139760711874304 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.06579773873090744, loss=0.024487927556037903
I0305 16:25:07.125984 139776167794432 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.0814523994922638, loss=0.02738339826464653
I0305 16:25:39.226344 139760711874304 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.07448095083236694, loss=0.025785576552152634
I0305 16:26:11.957741 139776167794432 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.07682888209819794, loss=0.02665708027780056
I0305 16:26:44.272191 139760711874304 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.0752953365445137, loss=0.025342820212244987
I0305 16:27:16.697430 139776167794432 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0783696249127388, loss=0.02500142715871334
I0305 16:27:48.046591 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:29:47.584740 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:29:50.684046 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:29:53.744349 139937033598784 submission_runner.py:411] Time since start: 22247.22s, 	Step: 43598, 	{'train/accuracy': 0.9924556016921997, 'train/loss': 0.024139532819390297, 'train/mean_average_precision': 0.5471580974598048, 'validation/accuracy': 0.9868945479393005, 'validation/loss': 0.04462670162320137, 'validation/mean_average_precision': 0.2904814723725562, 'validation/num_examples': 43793, 'test/accuracy': 0.9860474467277527, 'test/loss': 0.047526296228170395, 'test/mean_average_precision': 0.27703340131503995, 'test/num_examples': 43793, 'score': 14186.274589061737, 'total_duration': 22247.215339899063, 'accumulated_submission_time': 14186.274589061737, 'accumulated_eval_time': 8057.833017587662, 'accumulated_logging_time': 1.8867771625518799}
I0305 16:29:53.768153 139768268125952 logging_writer.py:48] [43598] accumulated_eval_time=8057.833018, accumulated_logging_time=1.886777, accumulated_submission_time=14186.274589, global_step=43598, preemption_count=0, score=14186.274589, test/accuracy=0.986047, test/loss=0.047526, test/mean_average_precision=0.277033, test/num_examples=43793, total_duration=22247.215340, train/accuracy=0.992456, train/loss=0.024140, train/mean_average_precision=0.547158, validation/accuracy=0.986895, validation/loss=0.044627, validation/mean_average_precision=0.290481, validation/num_examples=43793
I0305 16:29:54.793125 139769339950848 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07162002474069595, loss=0.02446114271879196
I0305 16:30:28.762321 139768268125952 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.07888270169496536, loss=0.025470005348324776
I0305 16:31:01.548824 139769339950848 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.07331113517284393, loss=0.025287805125117302
I0305 16:31:34.555186 139768268125952 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.0668100118637085, loss=0.02346455678343773
I0305 16:32:07.317114 139769339950848 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0695500448346138, loss=0.023440131917595863
I0305 16:32:40.074807 139768268125952 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07631208747625351, loss=0.026429787278175354
I0305 16:33:12.577235 139769339950848 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.06803202629089355, loss=0.02347634546458721
I0305 16:33:45.295373 139768268125952 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.06827130168676376, loss=0.0217413492500782
I0305 16:33:53.756535 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:35:52.491715 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:35:55.599050 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:35:58.646132 139937033598784 submission_runner.py:411] Time since start: 22612.12s, 	Step: 44327, 	{'train/accuracy': 0.9924808740615845, 'train/loss': 0.023937957361340523, 'train/mean_average_precision': 0.5512886909498305, 'validation/accuracy': 0.9869850873947144, 'validation/loss': 0.045232512056827545, 'validation/mean_average_precision': 0.28951077331403036, 'validation/num_examples': 43793, 'test/accuracy': 0.9861001372337341, 'test/loss': 0.048157885670661926, 'test/mean_average_precision': 0.27532767913089246, 'test/num_examples': 43793, 'score': 14426.232063055038, 'total_duration': 22612.117126464844, 'accumulated_submission_time': 14426.232063055038, 'accumulated_eval_time': 8182.722589492798, 'accumulated_logging_time': 1.921675443649292}
I0305 16:35:58.670168 139776159401728 logging_writer.py:48] [44327] accumulated_eval_time=8182.722589, accumulated_logging_time=1.921675, accumulated_submission_time=14426.232063, global_step=44327, preemption_count=0, score=14426.232063, test/accuracy=0.986100, test/loss=0.048158, test/mean_average_precision=0.275328, test/num_examples=43793, total_duration=22612.117126, train/accuracy=0.992481, train/loss=0.023938, train/mean_average_precision=0.551289, validation/accuracy=0.986985, validation/loss=0.045233, validation/mean_average_precision=0.289511, validation/num_examples=43793
I0305 16:36:23.182975 139776167794432 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.09164939820766449, loss=0.02514740265905857
I0305 16:36:56.005944 139776159401728 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0720965638756752, loss=0.02472532168030739
I0305 16:37:28.509478 139776167794432 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.07976112514734268, loss=0.02566605433821678
I0305 16:38:00.707574 139776159401728 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.0713946670293808, loss=0.027348745614290237
I0305 16:38:33.329135 139776167794432 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08823275566101074, loss=0.02979886159300804
I0305 16:39:05.836214 139776159401728 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.07738234847784042, loss=0.025954999029636383
I0305 16:39:38.366699 139776167794432 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.08327954262495041, loss=0.022504672408103943
I0305 16:39:58.690472 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:41:59.733746 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:42:02.927047 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:42:05.980844 139937033598784 submission_runner.py:411] Time since start: 22979.45s, 	Step: 45063, 	{'train/accuracy': 0.9927317500114441, 'train/loss': 0.023160690441727638, 'train/mean_average_precision': 0.5747661718707735, 'validation/accuracy': 0.9868564009666443, 'validation/loss': 0.0449821837246418, 'validation/mean_average_precision': 0.28365360157518, 'validation/num_examples': 43793, 'test/accuracy': 0.9860609769821167, 'test/loss': 0.04770909994840622, 'test/mean_average_precision': 0.27662479491846903, 'test/num_examples': 43793, 'score': 14666.217765569687, 'total_duration': 22979.45182442665, 'accumulated_submission_time': 14666.217765569687, 'accumulated_eval_time': 8310.012906312943, 'accumulated_logging_time': 1.9601173400878906}
I0305 16:42:06.005669 139768268125952 logging_writer.py:48] [45063] accumulated_eval_time=8310.012906, accumulated_logging_time=1.960117, accumulated_submission_time=14666.217766, global_step=45063, preemption_count=0, score=14666.217766, test/accuracy=0.986061, test/loss=0.047709, test/mean_average_precision=0.276625, test/num_examples=43793, total_duration=22979.451824, train/accuracy=0.992732, train/loss=0.023161, train/mean_average_precision=0.574766, validation/accuracy=0.986856, validation/loss=0.044982, validation/mean_average_precision=0.283654, validation/num_examples=43793
I0305 16:42:18.410289 139769339950848 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.0709756389260292, loss=0.02160085365176201
I0305 16:42:51.229650 139768268125952 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.07275940477848053, loss=0.025438914075493813
I0305 16:43:24.277161 139769339950848 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.07719141989946365, loss=0.026513932272791862
I0305 16:43:57.307913 139768268125952 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.0710296630859375, loss=0.024439143016934395
I0305 16:44:30.359151 139769339950848 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.09334257245063782, loss=0.029310205951333046
I0305 16:45:03.110951 139768268125952 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.076779805123806, loss=0.025301283225417137
I0305 16:45:35.712249 139769339950848 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.08030528575181961, loss=0.026553213596343994
I0305 16:46:06.059555 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:48:09.322433 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:48:12.409335 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:48:15.448668 139937033598784 submission_runner.py:411] Time since start: 23348.92s, 	Step: 45795, 	{'train/accuracy': 0.9927511811256409, 'train/loss': 0.022958293557167053, 'train/mean_average_precision': 0.586343797232631, 'validation/accuracy': 0.9871523380279541, 'validation/loss': 0.04534516483545303, 'validation/mean_average_precision': 0.2902990328130586, 'validation/num_examples': 43793, 'test/accuracy': 0.9862298369407654, 'test/loss': 0.04823103919625282, 'test/mean_average_precision': 0.27773459655586924, 'test/num_examples': 43793, 'score': 14906.239748001099, 'total_duration': 23348.919644355774, 'accumulated_submission_time': 14906.239748001099, 'accumulated_eval_time': 8439.401960372925, 'accumulated_logging_time': 1.9960315227508545}
I0305 16:48:15.472961 139776159401728 logging_writer.py:48] [45795] accumulated_eval_time=8439.401960, accumulated_logging_time=1.996032, accumulated_submission_time=14906.239748, global_step=45795, preemption_count=0, score=14906.239748, test/accuracy=0.986230, test/loss=0.048231, test/mean_average_precision=0.277735, test/num_examples=43793, total_duration=23348.919644, train/accuracy=0.992751, train/loss=0.022958, train/mean_average_precision=0.586344, validation/accuracy=0.987152, validation/loss=0.045345, validation/mean_average_precision=0.290299, validation/num_examples=43793
I0305 16:48:17.445347 139776167794432 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07005532085895538, loss=0.02481250837445259
I0305 16:48:49.784687 139776159401728 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.08032841235399246, loss=0.02691061981022358
I0305 16:49:22.039367 139776167794432 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.07293872535228729, loss=0.024537183344364166
I0305 16:49:54.716360 139776159401728 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.08853698521852493, loss=0.028610892593860626
I0305 16:50:27.247730 139776167794432 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.07776402682065964, loss=0.025750624015927315
I0305 16:50:59.590787 139776159401728 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.0867055356502533, loss=0.027886955067515373
I0305 16:51:32.275566 139776167794432 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.0843992531299591, loss=0.026267902925610542
I0305 16:52:04.604368 139776159401728 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08622479438781738, loss=0.025426490232348442
I0305 16:52:15.600193 139937033598784 spec.py:321] Evaluating on the training split.
I0305 16:54:18.080457 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 16:54:21.140135 139937033598784 spec.py:349] Evaluating on the test split.
I0305 16:54:24.140868 139937033598784 submission_runner.py:411] Time since start: 23717.61s, 	Step: 46535, 	{'train/accuracy': 0.9927822351455688, 'train/loss': 0.0226614810526371, 'train/mean_average_precision': 0.5853062788070533, 'validation/accuracy': 0.9870321750640869, 'validation/loss': 0.04562389478087425, 'validation/mean_average_precision': 0.288616217569316, 'validation/num_examples': 43793, 'test/accuracy': 0.98613041639328, 'test/loss': 0.04851200059056282, 'test/mean_average_precision': 0.2745446524126901, 'test/num_examples': 43793, 'score': 15146.333956003189, 'total_duration': 23717.611845493317, 'accumulated_submission_time': 15146.333956003189, 'accumulated_eval_time': 8567.942576169968, 'accumulated_logging_time': 2.0327556133270264}
I0305 16:54:24.165549 139768268125952 logging_writer.py:48] [46535] accumulated_eval_time=8567.942576, accumulated_logging_time=2.032756, accumulated_submission_time=15146.333956, global_step=46535, preemption_count=0, score=15146.333956, test/accuracy=0.986130, test/loss=0.048512, test/mean_average_precision=0.274545, test/num_examples=43793, total_duration=23717.611845, train/accuracy=0.992782, train/loss=0.022661, train/mean_average_precision=0.585306, validation/accuracy=0.987032, validation/loss=0.045624, validation/mean_average_precision=0.288616, validation/num_examples=43793
I0305 16:54:45.700552 139769339950848 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.06769856810569763, loss=0.026177335530519485
I0305 16:55:18.424199 139768268125952 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.08771644532680511, loss=0.028499484062194824
I0305 16:55:50.957562 139769339950848 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.08196509629487991, loss=0.02443169243633747
I0305 16:56:23.733274 139768268125952 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07611636817455292, loss=0.024341773241758347
I0305 16:56:56.237258 139769339950848 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.09310828149318695, loss=0.02543877251446247
I0305 16:57:28.859100 139768268125952 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08263814449310303, loss=0.02817254513502121
I0305 16:58:01.514831 139769339950848 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.09165748953819275, loss=0.02603653073310852
I0305 16:58:24.192417 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:00:26.000151 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:00:29.085889 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:00:32.119624 139937033598784 submission_runner.py:411] Time since start: 24085.59s, 	Step: 47272, 	{'train/accuracy': 0.9932552576065063, 'train/loss': 0.02135457657277584, 'train/mean_average_precision': 0.6282544798621549, 'validation/accuracy': 0.9870585799217224, 'validation/loss': 0.04514016583561897, 'validation/mean_average_precision': 0.2918916130401043, 'validation/num_examples': 43793, 'test/accuracy': 0.9861814379692078, 'test/loss': 0.048135723918676376, 'test/mean_average_precision': 0.2751556729047205, 'test/num_examples': 43793, 'score': 15386.32931470871, 'total_duration': 24085.590614795685, 'accumulated_submission_time': 15386.32931470871, 'accumulated_eval_time': 8695.869742393494, 'accumulated_logging_time': 2.068514823913574}
I0305 17:00:32.144356 139760711874304 logging_writer.py:48] [47272] accumulated_eval_time=8695.869742, accumulated_logging_time=2.068515, accumulated_submission_time=15386.329315, global_step=47272, preemption_count=0, score=15386.329315, test/accuracy=0.986181, test/loss=0.048136, test/mean_average_precision=0.275156, test/num_examples=43793, total_duration=24085.590615, train/accuracy=0.993255, train/loss=0.021355, train/mean_average_precision=0.628254, validation/accuracy=0.987059, validation/loss=0.045140, validation/mean_average_precision=0.291892, validation/num_examples=43793
I0305 17:00:41.484289 139776159401728 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.07766961306333542, loss=0.02696525864303112
I0305 17:01:13.817562 139760711874304 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.0810149759054184, loss=0.024253813549876213
I0305 17:01:46.093507 139776159401728 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.08109145611524582, loss=0.025641433894634247
I0305 17:02:18.291574 139760711874304 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08075495064258575, loss=0.024302279576659203
I0305 17:02:50.930238 139776159401728 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.0859149843454361, loss=0.02748146280646324
I0305 17:03:23.257110 139760711874304 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.08943144977092743, loss=0.026448961347341537
I0305 17:03:55.204852 139776159401728 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.10118798911571503, loss=0.026937047019600868
I0305 17:04:27.508611 139760711874304 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.08519481122493744, loss=0.0256973784416914
I0305 17:04:32.412201 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:06:27.825256 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:06:30.903866 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:06:33.993196 139937033598784 submission_runner.py:411] Time since start: 24447.46s, 	Step: 48016, 	{'train/accuracy': 0.9932361245155334, 'train/loss': 0.02131892554461956, 'train/mean_average_precision': 0.6125954411107427, 'validation/accuracy': 0.9870853424072266, 'validation/loss': 0.045618414878845215, 'validation/mean_average_precision': 0.2917766721293825, 'validation/num_examples': 43793, 'test/accuracy': 0.9862509369850159, 'test/loss': 0.04852047935128212, 'test/mean_average_precision': 0.2787191662258837, 'test/num_examples': 43793, 'score': 15626.565325260162, 'total_duration': 24447.46418762207, 'accumulated_submission_time': 15626.565325260162, 'accumulated_eval_time': 8817.450694084167, 'accumulated_logging_time': 2.1042933464050293}
I0305 17:06:34.017914 139768268125952 logging_writer.py:48] [48016] accumulated_eval_time=8817.450694, accumulated_logging_time=2.104293, accumulated_submission_time=15626.565325, global_step=48016, preemption_count=0, score=15626.565325, test/accuracy=0.986251, test/loss=0.048520, test/mean_average_precision=0.278719, test/num_examples=43793, total_duration=24447.464188, train/accuracy=0.993236, train/loss=0.021319, train/mean_average_precision=0.612595, validation/accuracy=0.987085, validation/loss=0.045618, validation/mean_average_precision=0.291777, validation/num_examples=43793
I0305 17:07:01.716850 139776167794432 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.0836745947599411, loss=0.025319915264844894
I0305 17:07:34.151753 139768268125952 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.08622264862060547, loss=0.02533469721674919
I0305 17:08:06.466298 139776167794432 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.0839366763830185, loss=0.02313958667218685
I0305 17:08:38.824370 139768268125952 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.07601315528154373, loss=0.024743320420384407
I0305 17:09:11.208658 139776167794432 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.09933490306138992, loss=0.025271782651543617
I0305 17:09:43.431377 139768268125952 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.0884222760796547, loss=0.02371487021446228
I0305 17:10:15.972236 139776167794432 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.08639203011989594, loss=0.024422964081168175
I0305 17:10:34.269667 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:12:35.252962 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:12:38.387979 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:12:41.450300 139937033598784 submission_runner.py:411] Time since start: 24814.92s, 	Step: 48757, 	{'train/accuracy': 0.9933009743690491, 'train/loss': 0.021241290494799614, 'train/mean_average_precision': 0.6215911847182629, 'validation/accuracy': 0.9870471954345703, 'validation/loss': 0.04555674269795418, 'validation/mean_average_precision': 0.2858171389788816, 'validation/num_examples': 43793, 'test/accuracy': 0.9862517714500427, 'test/loss': 0.048460401594638824, 'test/mean_average_precision': 0.275624097328915, 'test/num_examples': 43793, 'score': 15866.78537106514, 'total_duration': 24814.92128801346, 'accumulated_submission_time': 15866.78537106514, 'accumulated_eval_time': 8944.631282806396, 'accumulated_logging_time': 2.1402597427368164}
I0305 17:12:41.475723 139760711874304 logging_writer.py:48] [48757] accumulated_eval_time=8944.631283, accumulated_logging_time=2.140260, accumulated_submission_time=15866.785371, global_step=48757, preemption_count=0, score=15866.785371, test/accuracy=0.986252, test/loss=0.048460, test/mean_average_precision=0.275624, test/num_examples=43793, total_duration=24814.921288, train/accuracy=0.993301, train/loss=0.021241, train/mean_average_precision=0.621591, validation/accuracy=0.987047, validation/loss=0.045557, validation/mean_average_precision=0.285817, validation/num_examples=43793
I0305 17:12:55.816569 139769339950848 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.09156648069620132, loss=0.023009413853287697
I0305 17:13:28.158290 139760711874304 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.08408981561660767, loss=0.02498970739543438
I0305 17:14:00.757231 139769339950848 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08764921128749847, loss=0.026519617065787315
I0305 17:14:33.847638 139760711874304 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.09559110552072525, loss=0.027203291654586792
I0305 17:15:06.569402 139769339950848 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.09070899337530136, loss=0.025600692257285118
I0305 17:15:38.713201 139760711874304 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.10075139999389648, loss=0.0287757720798254
I0305 17:16:11.474992 139769339950848 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.08391576260328293, loss=0.022701876237988472
I0305 17:16:41.717904 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:18:39.544055 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:18:42.648020 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:18:45.727786 139937033598784 submission_runner.py:411] Time since start: 25179.20s, 	Step: 49494, 	{'train/accuracy': 0.9931593537330627, 'train/loss': 0.021586205810308456, 'train/mean_average_precision': 0.6026874616431173, 'validation/accuracy': 0.9870740175247192, 'validation/loss': 0.045936163514852524, 'validation/mean_average_precision': 0.2909390577597649, 'validation/num_examples': 43793, 'test/accuracy': 0.9862500429153442, 'test/loss': 0.049010585993528366, 'test/mean_average_precision': 0.27325066083232546, 'test/num_examples': 43793, 'score': 16106.996109962463, 'total_duration': 25179.198776960373, 'accumulated_submission_time': 16106.996109962463, 'accumulated_eval_time': 9068.64112663269, 'accumulated_logging_time': 2.176506996154785}
I0305 17:18:45.753027 139768268125952 logging_writer.py:48] [49494] accumulated_eval_time=9068.641127, accumulated_logging_time=2.176507, accumulated_submission_time=16106.996110, global_step=49494, preemption_count=0, score=16106.996110, test/accuracy=0.986250, test/loss=0.049011, test/mean_average_precision=0.273251, test/num_examples=43793, total_duration=25179.198777, train/accuracy=0.993159, train/loss=0.021586, train/mean_average_precision=0.602687, validation/accuracy=0.987074, validation/loss=0.045936, validation/mean_average_precision=0.290939, validation/num_examples=43793
I0305 17:18:48.046606 139776167794432 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.08331667631864548, loss=0.025609953328967094
I0305 17:19:21.173840 139768268125952 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.07768939435482025, loss=0.022675693035125732
I0305 17:19:54.457158 139776167794432 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.08390727639198303, loss=0.023901065811514854
I0305 17:20:27.966095 139768268125952 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.08988595008850098, loss=0.02460317872464657
I0305 17:21:01.198561 139776167794432 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.07949456572532654, loss=0.023631740361452103
I0305 17:21:34.190347 139768268125952 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.09243243932723999, loss=0.02285665273666382
I0305 17:22:07.365707 139776167794432 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.08513662219047546, loss=0.02801154926419258
I0305 17:22:39.592801 139768268125952 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.08749696612358093, loss=0.024600038304924965
I0305 17:22:45.909891 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:24:48.582528 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:24:51.689499 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:24:54.791526 139937033598784 submission_runner.py:411] Time since start: 25548.26s, 	Step: 50221, 	{'train/accuracy': 0.9930923581123352, 'train/loss': 0.02172422595322132, 'train/mean_average_precision': 0.6127489989284732, 'validation/accuracy': 0.9870272874832153, 'validation/loss': 0.04575221240520477, 'validation/mean_average_precision': 0.28912171972509265, 'validation/num_examples': 43793, 'test/accuracy': 0.9861556887626648, 'test/loss': 0.048608217388391495, 'test/mean_average_precision': 0.27869557829531116, 'test/num_examples': 43793, 'score': 16347.118661403656, 'total_duration': 25548.262518405914, 'accumulated_submission_time': 16347.118661403656, 'accumulated_eval_time': 9197.522719144821, 'accumulated_logging_time': 2.2130091190338135}
I0305 17:24:54.817182 139760711874304 logging_writer.py:48] [50221] accumulated_eval_time=9197.522719, accumulated_logging_time=2.213009, accumulated_submission_time=16347.118661, global_step=50221, preemption_count=0, score=16347.118661, test/accuracy=0.986156, test/loss=0.048608, test/mean_average_precision=0.278696, test/num_examples=43793, total_duration=25548.262518, train/accuracy=0.993092, train/loss=0.021724, train/mean_average_precision=0.612749, validation/accuracy=0.987027, validation/loss=0.045752, validation/mean_average_precision=0.289122, validation/num_examples=43793
I0305 17:25:21.095999 139776159401728 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.10724113881587982, loss=0.0240793339908123
I0305 17:25:53.872368 139760711874304 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.08925256878137589, loss=0.025858880952000618
I0305 17:26:26.390941 139776159401728 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.08503678441047668, loss=0.023785391822457314
I0305 17:26:58.634614 139760711874304 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09141618013381958, loss=0.023049339652061462
I0305 17:27:30.896204 139776159401728 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.08842772990465164, loss=0.022968878969550133
I0305 17:28:03.158050 139760711874304 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09438220411539078, loss=0.02496129833161831
I0305 17:28:36.058355 139776159401728 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.08475488424301147, loss=0.023864852264523506
I0305 17:28:54.807220 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:30:57.536347 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:31:01.046020 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:31:04.174969 139937033598784 submission_runner.py:411] Time since start: 25917.65s, 	Step: 50958, 	{'train/accuracy': 0.9932236075401306, 'train/loss': 0.021330006420612335, 'train/mean_average_precision': 0.6119174869729177, 'validation/accuracy': 0.9870419502258301, 'validation/loss': 0.045783400535583496, 'validation/mean_average_precision': 0.29525234805391753, 'validation/num_examples': 43793, 'test/accuracy': 0.9862281680107117, 'test/loss': 0.04873315244913101, 'test/mean_average_precision': 0.27593724371249134, 'test/num_examples': 43793, 'score': 16587.07647919655, 'total_duration': 25917.645943164825, 'accumulated_submission_time': 16587.07647919655, 'accumulated_eval_time': 9326.890407800674, 'accumulated_logging_time': 2.2503864765167236}
I0305 17:31:04.200598 139769339950848 logging_writer.py:48] [50958] accumulated_eval_time=9326.890408, accumulated_logging_time=2.250386, accumulated_submission_time=16587.076479, global_step=50958, preemption_count=0, score=16587.076479, test/accuracy=0.986228, test/loss=0.048733, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=25917.645943, train/accuracy=0.993224, train/loss=0.021330, train/mean_average_precision=0.611917, validation/accuracy=0.987042, validation/loss=0.045783, validation/mean_average_precision=0.295252, validation/num_examples=43793
I0305 17:31:18.456213 139776167794432 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.09466232359409332, loss=0.024203823879361153
I0305 17:31:51.714943 139769339950848 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.08315764367580414, loss=0.02289656363427639
I0305 17:32:24.167809 139776167794432 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.09057500958442688, loss=0.022420745342969894
I0305 17:32:56.141030 139769339950848 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.1006118431687355, loss=0.025443468242883682
I0305 17:33:28.753839 139776167794432 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08661250025033951, loss=0.022852212190628052
I0305 17:34:01.265095 139769339950848 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.1053999736905098, loss=0.025158487260341644
I0305 17:34:34.241439 139776167794432 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.0787607878446579, loss=0.023202992975711823
I0305 17:35:04.396695 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:37:04.098089 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:37:07.176632 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:37:10.274596 139937033598784 submission_runner.py:411] Time since start: 26283.75s, 	Step: 51693, 	{'train/accuracy': 0.9932354092597961, 'train/loss': 0.021341772750020027, 'train/mean_average_precision': 0.6205512641982079, 'validation/accuracy': 0.987038254737854, 'validation/loss': 0.04589930921792984, 'validation/mean_average_precision': 0.2932043078510703, 'validation/num_examples': 43793, 'test/accuracy': 0.9862563610076904, 'test/loss': 0.04878285527229309, 'test/mean_average_precision': 0.28098633098323733, 'test/num_examples': 43793, 'score': 16827.240026474, 'total_duration': 26283.745587825775, 'accumulated_submission_time': 16827.240026474, 'accumulated_eval_time': 9452.768271923065, 'accumulated_logging_time': 2.288546562194824}
I0305 17:37:10.300405 139768268125952 logging_writer.py:48] [51693] accumulated_eval_time=9452.768272, accumulated_logging_time=2.288547, accumulated_submission_time=16827.240026, global_step=51693, preemption_count=0, score=16827.240026, test/accuracy=0.986256, test/loss=0.048783, test/mean_average_precision=0.280986, test/num_examples=43793, total_duration=26283.745588, train/accuracy=0.993235, train/loss=0.021342, train/mean_average_precision=0.620551, validation/accuracy=0.987038, validation/loss=0.045899, validation/mean_average_precision=0.293204, validation/num_examples=43793
I0305 17:37:12.924228 139776159401728 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09457076340913773, loss=0.024755412712693214
I0305 17:37:45.260614 139768268125952 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.08810257166624069, loss=0.02320663258433342
I0305 17:38:17.925508 139776159401728 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.08743738383054733, loss=0.023909250274300575
I0305 17:38:50.599508 139768268125952 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.10836627334356308, loss=0.025275612249970436
I0305 17:39:22.961315 139776159401728 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.08385966718196869, loss=0.019046610221266747
I0305 17:39:55.129679 139768268125952 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.08433134108781815, loss=0.02329539880156517
I0305 17:40:27.688168 139776159401728 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.09797684103250504, loss=0.024084245786070824
I0305 17:41:00.410763 139768268125952 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09667109698057175, loss=0.02601131796836853
I0305 17:41:10.275717 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:43:07.217314 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:43:10.259274 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:43:13.292986 139937033598784 submission_runner.py:411] Time since start: 26646.76s, 	Step: 52431, 	{'train/accuracy': 0.9933454394340515, 'train/loss': 0.020886698737740517, 'train/mean_average_precision': 0.6233633128448957, 'validation/accuracy': 0.9870431423187256, 'validation/loss': 0.04637245461344719, 'validation/mean_average_precision': 0.2947568615806708, 'validation/num_examples': 43793, 'test/accuracy': 0.9862008094787598, 'test/loss': 0.049467142671346664, 'test/mean_average_precision': 0.275198819814396, 'test/num_examples': 43793, 'score': 17067.183010816574, 'total_duration': 26646.763977766037, 'accumulated_submission_time': 17067.183010816574, 'accumulated_eval_time': 9575.785498142242, 'accumulated_logging_time': 2.326702117919922}
I0305 17:43:13.317989 139760711874304 logging_writer.py:48] [52431] accumulated_eval_time=9575.785498, accumulated_logging_time=2.326702, accumulated_submission_time=17067.183011, global_step=52431, preemption_count=0, score=17067.183011, test/accuracy=0.986201, test/loss=0.049467, test/mean_average_precision=0.275199, test/num_examples=43793, total_duration=26646.763978, train/accuracy=0.993345, train/loss=0.020887, train/mean_average_precision=0.623363, validation/accuracy=0.987043, validation/loss=0.046372, validation/mean_average_precision=0.294757, validation/num_examples=43793
I0305 17:43:35.937708 139769339950848 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.08247241377830505, loss=0.022663358598947525
I0305 17:44:08.509171 139760711874304 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.11075063794851303, loss=0.025402454659342766
I0305 17:44:40.475977 139769339950848 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.09683341532945633, loss=0.02533615380525589
I0305 17:45:12.561142 139760711874304 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.11799471825361252, loss=0.02483534812927246
I0305 17:45:44.631587 139769339950848 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.08159507066011429, loss=0.02160874754190445
I0305 17:46:17.726955 139760711874304 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.10054516047239304, loss=0.025432277470827103
I0305 17:46:50.207040 139769339950848 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.09265454858541489, loss=0.021554408594965935
I0305 17:47:13.334394 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:49:13.865485 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:49:16.891064 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:49:19.926578 139937033598784 submission_runner.py:411] Time since start: 27013.40s, 	Step: 53174, 	{'train/accuracy': 0.993765652179718, 'train/loss': 0.01982664316892624, 'train/mean_average_precision': 0.648274235699503, 'validation/accuracy': 0.9869290590286255, 'validation/loss': 0.04618347808718681, 'validation/mean_average_precision': 0.293072159487718, 'validation/num_examples': 43793, 'test/accuracy': 0.9861982464790344, 'test/loss': 0.04907945916056633, 'test/mean_average_precision': 0.27787689710626606, 'test/num_examples': 43793, 'score': 17307.16687989235, 'total_duration': 27013.397568941116, 'accumulated_submission_time': 17307.16687989235, 'accumulated_eval_time': 9702.37763953209, 'accumulated_logging_time': 2.363882303237915}
I0305 17:49:19.952616 139768268125952 logging_writer.py:48] [53174] accumulated_eval_time=9702.377640, accumulated_logging_time=2.363882, accumulated_submission_time=17307.166880, global_step=53174, preemption_count=0, score=17307.166880, test/accuracy=0.986198, test/loss=0.049079, test/mean_average_precision=0.277877, test/num_examples=43793, total_duration=27013.397569, train/accuracy=0.993766, train/loss=0.019827, train/mean_average_precision=0.648274, validation/accuracy=0.986929, validation/loss=0.046183, validation/mean_average_precision=0.293072, validation/num_examples=43793
I0305 17:49:28.578691 139776159401728 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.0962255597114563, loss=0.021537993103265762
I0305 17:50:01.004665 139768268125952 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.09155438840389252, loss=0.022268354892730713
I0305 17:50:34.044158 139776159401728 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09735198318958282, loss=0.023086346685886383
I0305 17:51:07.097253 139768268125952 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.09760653972625732, loss=0.023046161979436874
I0305 17:51:40.368543 139776159401728 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.0997319146990776, loss=0.02330930531024933
I0305 17:52:13.927574 139768268125952 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.07935494184494019, loss=0.019994007423520088
I0305 17:52:46.976942 139776159401728 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.1238197386264801, loss=0.02391485497355461
I0305 17:53:19.894619 139768268125952 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10925676673650742, loss=0.024492455646395683
I0305 17:53:20.216361 139937033598784 spec.py:321] Evaluating on the training split.
I0305 17:55:20.788328 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 17:55:23.870328 139937033598784 spec.py:349] Evaluating on the test split.
I0305 17:55:26.918866 139937033598784 submission_runner.py:411] Time since start: 27380.39s, 	Step: 53902, 	{'train/accuracy': 0.9939457774162292, 'train/loss': 0.01909765414893627, 'train/mean_average_precision': 0.6707530227995426, 'validation/accuracy': 0.9869185090065002, 'validation/loss': 0.046229638159275055, 'validation/mean_average_precision': 0.2894399170703837, 'validation/num_examples': 43793, 'test/accuracy': 0.9861683249473572, 'test/loss': 0.049258239567279816, 'test/mean_average_precision': 0.27287261416855957, 'test/num_examples': 43793, 'score': 17547.396492242813, 'total_duration': 27380.38984155655, 'accumulated_submission_time': 17547.396492242813, 'accumulated_eval_time': 9829.080079555511, 'accumulated_logging_time': 2.401202440261841}
I0305 17:55:26.945382 139760711874304 logging_writer.py:48] [53902] accumulated_eval_time=9829.080080, accumulated_logging_time=2.401202, accumulated_submission_time=17547.396492, global_step=53902, preemption_count=0, score=17547.396492, test/accuracy=0.986168, test/loss=0.049258, test/mean_average_precision=0.272873, test/num_examples=43793, total_duration=27380.389842, train/accuracy=0.993946, train/loss=0.019098, train/mean_average_precision=0.670753, validation/accuracy=0.986919, validation/loss=0.046230, validation/mean_average_precision=0.289440, validation/num_examples=43793
I0305 17:55:59.534559 139776167794432 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10256201773881912, loss=0.022598987445235252
I0305 17:56:31.882987 139760711874304 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.10461977869272232, loss=0.02515794150531292
I0305 17:57:04.511948 139776167794432 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.0994994267821312, loss=0.02207155153155327
I0305 17:57:37.265418 139760711874304 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.08834690600633621, loss=0.021734638139605522
I0305 17:58:09.761264 139776167794432 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.10445400327444077, loss=0.023584365844726562
I0305 17:58:41.939295 139760711874304 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.10366485267877579, loss=0.022819021716713905
I0305 17:59:14.112624 139776167794432 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.08488290756940842, loss=0.02273639477789402
I0305 17:59:27.200367 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:01:32.001874 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:01:35.425587 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:01:38.871835 139937033598784 submission_runner.py:411] Time since start: 27752.34s, 	Step: 54639, 	{'train/accuracy': 0.9939642548561096, 'train/loss': 0.01908794231712818, 'train/mean_average_precision': 0.6836719989171522, 'validation/accuracy': 0.986976146697998, 'validation/loss': 0.046673987060785294, 'validation/mean_average_precision': 0.2931826954710649, 'validation/num_examples': 43793, 'test/accuracy': 0.9862428903579712, 'test/loss': 0.049737349152565, 'test/mean_average_precision': 0.275510165065686, 'test/num_examples': 43793, 'score': 17787.61997103691, 'total_duration': 27752.34281229973, 'accumulated_submission_time': 17787.61997103691, 'accumulated_eval_time': 9960.751490354538, 'accumulated_logging_time': 2.438985824584961}
I0305 18:01:38.902120 139769339950848 logging_writer.py:48] [54639] accumulated_eval_time=9960.751490, accumulated_logging_time=2.438986, accumulated_submission_time=17787.619971, global_step=54639, preemption_count=0, score=17787.619971, test/accuracy=0.986243, test/loss=0.049737, test/mean_average_precision=0.275510, test/num_examples=43793, total_duration=27752.342812, train/accuracy=0.993964, train/loss=0.019088, train/mean_average_precision=0.683672, validation/accuracy=0.986976, validation/loss=0.046674, validation/mean_average_precision=0.293183, validation/num_examples=43793
I0305 18:01:59.604000 139776159401728 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.1012517437338829, loss=0.020011918619275093
I0305 18:02:32.941274 139769339950848 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.09535916149616241, loss=0.022550923749804497
I0305 18:03:05.831782 139776159401728 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.10289985686540604, loss=0.023320473730564117
I0305 18:03:37.654829 139769339950848 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.1116965040564537, loss=0.02355952374637127
I0305 18:04:09.630315 139776159401728 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.10497625917196274, loss=0.022249169647693634
I0305 18:04:42.047211 139769339950848 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.09502898901700974, loss=0.02146175689995289
I0305 18:05:14.212228 139776159401728 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.09833744168281555, loss=0.02037208154797554
I0305 18:05:38.953834 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:07:41.192901 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:07:44.265321 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:07:47.281661 139937033598784 submission_runner.py:411] Time since start: 28120.75s, 	Step: 55378, 	{'train/accuracy': 0.9935755729675293, 'train/loss': 0.02005753666162491, 'train/mean_average_precision': 0.6444034424776388, 'validation/accuracy': 0.9869863390922546, 'validation/loss': 0.046782467514276505, 'validation/mean_average_precision': 0.2931136241225678, 'validation/num_examples': 43793, 'test/accuracy': 0.9861915111541748, 'test/loss': 0.049739427864551544, 'test/mean_average_precision': 0.27628710007371576, 'test/num_examples': 43793, 'score': 18027.63859820366, 'total_duration': 28120.752645730972, 'accumulated_submission_time': 18027.63859820366, 'accumulated_eval_time': 10089.079269647598, 'accumulated_logging_time': 2.4811816215515137}
I0305 18:07:47.307427 139760711874304 logging_writer.py:48] [55378] accumulated_eval_time=10089.079270, accumulated_logging_time=2.481182, accumulated_submission_time=18027.638598, global_step=55378, preemption_count=0, score=18027.638598, test/accuracy=0.986192, test/loss=0.049739, test/mean_average_precision=0.276287, test/num_examples=43793, total_duration=28120.752646, train/accuracy=0.993576, train/loss=0.020058, train/mean_average_precision=0.644403, validation/accuracy=0.986986, validation/loss=0.046782, validation/mean_average_precision=0.293114, validation/num_examples=43793
I0305 18:07:54.832587 139768268125952 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.09368225187063217, loss=0.021304981783032417
I0305 18:08:27.388664 139760711874304 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.11320193856954575, loss=0.023885248228907585
I0305 18:08:59.684527 139768268125952 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.11545751243829727, loss=0.020545009523630142
I0305 18:09:31.925712 139760711874304 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.09689078480005264, loss=0.023989208042621613
I0305 18:10:04.187568 139768268125952 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.11786799877882004, loss=0.022549599409103394
I0305 18:10:36.408969 139760711874304 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.10189563035964966, loss=0.02398744225502014
I0305 18:11:09.134876 139768268125952 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.09573541581630707, loss=0.021355189383029938
I0305 18:11:43.015631 139760711874304 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.10949821770191193, loss=0.023813240230083466
I0305 18:11:47.359805 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:13:49.619713 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:13:54.042884 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:13:57.088219 139937033598784 submission_runner.py:411] Time since start: 28490.56s, 	Step: 56114, 	{'train/accuracy': 0.9936255216598511, 'train/loss': 0.019968872889876366, 'train/mean_average_precision': 0.6374130133398599, 'validation/accuracy': 0.9870532751083374, 'validation/loss': 0.047129612416028976, 'validation/mean_average_precision': 0.29162892647182015, 'validation/num_examples': 43793, 'test/accuracy': 0.9862412214279175, 'test/loss': 0.050195202231407166, 'test/mean_average_precision': 0.2766432008581418, 'test/num_examples': 43793, 'score': 18267.65841269493, 'total_duration': 28490.559209108353, 'accumulated_submission_time': 18267.65841269493, 'accumulated_eval_time': 10218.807644367218, 'accumulated_logging_time': 2.518969774246216}
I0305 18:13:57.115864 139776159401728 logging_writer.py:48] [56114] accumulated_eval_time=10218.807644, accumulated_logging_time=2.518970, accumulated_submission_time=18267.658413, global_step=56114, preemption_count=0, score=18267.658413, test/accuracy=0.986241, test/loss=0.050195, test/mean_average_precision=0.276643, test/num_examples=43793, total_duration=28490.559209, train/accuracy=0.993626, train/loss=0.019969, train/mean_average_precision=0.637413, validation/accuracy=0.987053, validation/loss=0.047130, validation/mean_average_precision=0.291629, validation/num_examples=43793
I0305 18:14:25.285003 139776167794432 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.09668947756290436, loss=0.021999210119247437
I0305 18:14:57.873646 139776159401728 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10124029219150543, loss=0.021930385380983353
I0305 18:15:30.274426 139776167794432 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.10954610258340836, loss=0.02194146439433098
I0305 18:16:02.675077 139776159401728 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.11118506640195847, loss=0.02370471879839897
I0305 18:16:34.966262 139776167794432 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.10783284902572632, loss=0.02121930941939354
I0305 18:17:07.201380 139776159401728 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.10502976179122925, loss=0.02223440632224083
I0305 18:17:39.539900 139776167794432 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.10672890394926071, loss=0.019272221252322197
I0305 18:17:57.327755 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:19:57.930385 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:20:01.376860 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:20:04.503881 139937033598784 submission_runner.py:411] Time since start: 28857.97s, 	Step: 56855, 	{'train/accuracy': 0.9937307238578796, 'train/loss': 0.01955595053732395, 'train/mean_average_precision': 0.6575340063625909, 'validation/accuracy': 0.9870971441268921, 'validation/loss': 0.04700908809900284, 'validation/mean_average_precision': 0.2928508141046779, 'validation/num_examples': 43793, 'test/accuracy': 0.986143946647644, 'test/loss': 0.050165992230176926, 'test/mean_average_precision': 0.2744961808523273, 'test/num_examples': 43793, 'score': 18507.838593244553, 'total_duration': 28857.974859952927, 'accumulated_submission_time': 18507.838593244553, 'accumulated_eval_time': 10345.98371219635, 'accumulated_logging_time': 2.55792236328125}
I0305 18:20:04.530610 139760711874304 logging_writer.py:48] [56855] accumulated_eval_time=10345.983712, accumulated_logging_time=2.557922, accumulated_submission_time=18507.838593, global_step=56855, preemption_count=0, score=18507.838593, test/accuracy=0.986144, test/loss=0.050166, test/mean_average_precision=0.274496, test/num_examples=43793, total_duration=28857.974860, train/accuracy=0.993731, train/loss=0.019556, train/mean_average_precision=0.657534, validation/accuracy=0.987097, validation/loss=0.047009, validation/mean_average_precision=0.292851, validation/num_examples=43793
I0305 18:20:19.858910 139769339950848 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.10761293768882751, loss=0.02345864474773407
I0305 18:20:52.466325 139760711874304 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.10479982197284698, loss=0.02276241034269333
I0305 18:21:24.866603 139769339950848 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.12434351444244385, loss=0.022627633064985275
I0305 18:21:57.019377 139760711874304 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.1094096302986145, loss=0.021556071937084198
I0305 18:22:29.362279 139769339950848 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.10055940598249435, loss=0.02105950191617012
I0305 18:23:02.187663 139760711874304 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.10928501188755035, loss=0.022648660466074944
I0305 18:23:34.727696 139769339950848 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10346756130456924, loss=0.02230359986424446
I0305 18:24:04.707655 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:26:04.620344 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:26:07.699059 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:26:10.859896 139937033598784 submission_runner.py:411] Time since start: 29224.33s, 	Step: 57592, 	{'train/accuracy': 0.9938116073608398, 'train/loss': 0.019317833706736565, 'train/mean_average_precision': 0.6544946164639226, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.047213029116392136, 'validation/mean_average_precision': 0.2911910347651354, 'validation/num_examples': 43793, 'test/accuracy': 0.9862513542175293, 'test/loss': 0.050274401903152466, 'test/mean_average_precision': 0.2765929581524281, 'test/num_examples': 43793, 'score': 18747.98366856575, 'total_duration': 29224.3308801651, 'accumulated_submission_time': 18747.98366856575, 'accumulated_eval_time': 10472.135902881622, 'accumulated_logging_time': 2.5963222980499268}
I0305 18:26:10.886414 139768268125952 logging_writer.py:48] [57592] accumulated_eval_time=10472.135903, accumulated_logging_time=2.596322, accumulated_submission_time=18747.983669, global_step=57592, preemption_count=0, score=18747.983669, test/accuracy=0.986251, test/loss=0.050274, test/mean_average_precision=0.276593, test/num_examples=43793, total_duration=29224.330880, train/accuracy=0.993812, train/loss=0.019318, train/mean_average_precision=0.654495, validation/accuracy=0.987035, validation/loss=0.047213, validation/mean_average_precision=0.291191, validation/num_examples=43793
I0305 18:26:13.926979 139776167794432 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.10506211221218109, loss=0.018154118210077286
I0305 18:26:46.512115 139768268125952 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.10737255960702896, loss=0.022202519699931145
I0305 18:27:18.846535 139776167794432 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10715688019990921, loss=0.02153525874018669
I0305 18:27:50.917993 139768268125952 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10768279433250427, loss=0.020778682082891464
I0305 18:28:23.271647 139776167794432 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.10762067884206772, loss=0.02070259489119053
I0305 18:28:55.742053 139768268125952 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.10716347396373749, loss=0.02160072885453701
I0305 18:29:28.327094 139776167794432 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.1087421104311943, loss=0.02263874188065529
I0305 18:30:01.430318 139768268125952 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.1108638271689415, loss=0.023426389321684837
I0305 18:30:11.039566 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:32:14.617114 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:32:17.692619 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:32:20.667042 139937033598784 submission_runner.py:411] Time since start: 29594.14s, 	Step: 58329, 	{'train/accuracy': 0.9938645958900452, 'train/loss': 0.01901966892182827, 'train/mean_average_precision': 0.6740505018592804, 'validation/accuracy': 0.9870337843894958, 'validation/loss': 0.04748775064945221, 'validation/mean_average_precision': 0.2891203495535188, 'validation/num_examples': 43793, 'test/accuracy': 0.9861683249473572, 'test/loss': 0.050787247717380524, 'test/mean_average_precision': 0.2757953912061542, 'test/num_examples': 43793, 'score': 18988.104365110397, 'total_duration': 29594.138032197952, 'accumulated_submission_time': 18988.104365110397, 'accumulated_eval_time': 10601.76334810257, 'accumulated_logging_time': 2.6340818405151367}
I0305 18:32:20.695592 139769339950848 logging_writer.py:48] [58329] accumulated_eval_time=10601.763348, accumulated_logging_time=2.634082, accumulated_submission_time=18988.104365, global_step=58329, preemption_count=0, score=18988.104365, test/accuracy=0.986168, test/loss=0.050787, test/mean_average_precision=0.275795, test/num_examples=43793, total_duration=29594.138032, train/accuracy=0.993865, train/loss=0.019020, train/mean_average_precision=0.674051, validation/accuracy=0.987034, validation/loss=0.047488, validation/mean_average_precision=0.289120, validation/num_examples=43793
I0305 18:32:43.838551 139776159401728 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.11129721999168396, loss=0.020876513794064522
I0305 18:33:15.978885 139769339950848 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.10367599129676819, loss=0.019870465621352196
I0305 18:33:48.369338 139776159401728 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.11647088825702667, loss=0.02411448396742344
I0305 18:34:20.423008 139769339950848 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.11286284774541855, loss=0.02216864377260208
I0305 18:34:52.587679 139776159401728 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.11101032793521881, loss=0.021550865843892097
I0305 18:35:24.914783 139769339950848 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.10842540860176086, loss=0.019972337409853935
I0305 18:35:57.199396 139776159401728 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.13038791716098785, loss=0.022349661216139793
I0305 18:36:20.726535 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:38:15.095644 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:38:18.137148 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:38:21.138155 139937033598784 submission_runner.py:411] Time since start: 29954.61s, 	Step: 59073, 	{'train/accuracy': 0.9941751956939697, 'train/loss': 0.018044738098978996, 'train/mean_average_precision': 0.7022220962344548, 'validation/accuracy': 0.9870853424072266, 'validation/loss': 0.04760471731424332, 'validation/mean_average_precision': 0.2895204188531082, 'validation/num_examples': 43793, 'test/accuracy': 0.9862648248672485, 'test/loss': 0.05090069770812988, 'test/mean_average_precision': 0.27187857270904797, 'test/num_examples': 43793, 'score': 19228.10386610031, 'total_duration': 29954.60914540291, 'accumulated_submission_time': 19228.10386610031, 'accumulated_eval_time': 10722.174923658371, 'accumulated_logging_time': 2.673880100250244}
I0305 18:38:21.165120 139760711874304 logging_writer.py:48] [59073] accumulated_eval_time=10722.174924, accumulated_logging_time=2.673880, accumulated_submission_time=19228.103866, global_step=59073, preemption_count=0, score=19228.103866, test/accuracy=0.986265, test/loss=0.050901, test/mean_average_precision=0.271879, test/num_examples=43793, total_duration=29954.609145, train/accuracy=0.994175, train/loss=0.018045, train/mean_average_precision=0.702222, validation/accuracy=0.987085, validation/loss=0.047605, validation/mean_average_precision=0.289520, validation/num_examples=43793
I0305 18:38:30.357414 139776167794432 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.11617518216371536, loss=0.021247688680887222
I0305 18:39:03.154585 139760711874304 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.11394399404525757, loss=0.021915378049016
I0305 18:39:35.499035 139776167794432 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.1300666481256485, loss=0.022006351500749588
I0305 18:40:08.189734 139760711874304 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.10221366584300995, loss=0.021907290443778038
I0305 18:40:41.002263 139776167794432 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.12242408096790314, loss=0.021907977759838104
I0305 18:41:13.052782 139760711874304 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.13128191232681274, loss=0.024074088782072067
I0305 18:41:45.242462 139776167794432 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.10651587694883347, loss=0.019471751525998116
I0305 18:42:17.353554 139760711874304 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.1282685250043869, loss=0.023287730291485786
I0305 18:42:21.199005 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:44:23.422868 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:44:26.464341 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:44:29.509748 139937033598784 submission_runner.py:411] Time since start: 30322.98s, 	Step: 59813, 	{'train/accuracy': 0.9943927526473999, 'train/loss': 0.017461363226175308, 'train/mean_average_precision': 0.7008946711494934, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04789691045880318, 'validation/mean_average_precision': 0.28930684626621334, 'validation/num_examples': 43793, 'test/accuracy': 0.9861982464790344, 'test/loss': 0.05118045210838318, 'test/mean_average_precision': 0.2744422124499764, 'test/num_examples': 43793, 'score': 19468.106098413467, 'total_duration': 30322.980736255646, 'accumulated_submission_time': 19468.106098413467, 'accumulated_eval_time': 10850.485614299774, 'accumulated_logging_time': 2.7125213146209717}
I0305 18:44:29.536991 139768268125952 logging_writer.py:48] [59813] accumulated_eval_time=10850.485614, accumulated_logging_time=2.712521, accumulated_submission_time=19468.106098, global_step=59813, preemption_count=0, score=19468.106098, test/accuracy=0.986198, test/loss=0.051180, test/mean_average_precision=0.274442, test/num_examples=43793, total_duration=30322.980736, train/accuracy=0.994393, train/loss=0.017461, train/mean_average_precision=0.700895, validation/accuracy=0.987035, validation/loss=0.047897, validation/mean_average_precision=0.289307, validation/num_examples=43793
I0305 18:44:57.847126 139776159401728 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.11962435394525528, loss=0.021898210048675537
I0305 18:45:30.281779 139768268125952 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.10479622334241867, loss=0.020016729831695557
I0305 18:46:02.948801 139776159401728 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.13092467188835144, loss=0.021328603848814964
I0305 18:46:35.312795 139768268125952 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.1106693297624588, loss=0.020075025036931038
I0305 18:47:07.384593 139776159401728 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.11771562695503235, loss=0.01887120120227337
I0305 18:47:39.282052 139768268125952 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.11428441107273102, loss=0.021542944014072418
I0305 18:48:11.600158 139776159401728 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.12883372604846954, loss=0.020227182656526566
I0305 18:48:29.678812 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:50:22.677276 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:50:25.737052 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:50:28.782967 139937033598784 submission_runner.py:411] Time since start: 30682.25s, 	Step: 60557, 	{'train/accuracy': 0.9945353865623474, 'train/loss': 0.017185058444738388, 'train/mean_average_precision': 0.7161099193970502, 'validation/accuracy': 0.987028956413269, 'validation/loss': 0.04784898832440376, 'validation/mean_average_precision': 0.29115374831103913, 'validation/num_examples': 43793, 'test/accuracy': 0.986163318157196, 'test/loss': 0.05110668018460274, 'test/mean_average_precision': 0.2768247622583396, 'test/num_examples': 43793, 'score': 19708.216701745987, 'total_duration': 30682.25395989418, 'accumulated_submission_time': 19708.216701745987, 'accumulated_eval_time': 10969.589729309082, 'accumulated_logging_time': 2.7505152225494385}
I0305 18:50:28.810695 139760711874304 logging_writer.py:48] [60557] accumulated_eval_time=10969.589729, accumulated_logging_time=2.750515, accumulated_submission_time=19708.216702, global_step=60557, preemption_count=0, score=19708.216702, test/accuracy=0.986163, test/loss=0.051107, test/mean_average_precision=0.276825, test/num_examples=43793, total_duration=30682.253960, train/accuracy=0.994535, train/loss=0.017185, train/mean_average_precision=0.716110, validation/accuracy=0.987029, validation/loss=0.047849, validation/mean_average_precision=0.291154, validation/num_examples=43793
I0305 18:50:42.916050 139769339950848 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.12097761780023575, loss=0.023555047810077667
I0305 18:51:15.353448 139760711874304 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.1068858876824379, loss=0.018858937546610832
I0305 18:51:47.486968 139769339950848 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.1168021634221077, loss=0.021147584542632103
I0305 18:52:19.869166 139760711874304 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.11794289946556091, loss=0.021747266873717308
I0305 18:52:51.883468 139769339950848 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.12871214747428894, loss=0.02274911105632782
I0305 18:53:25.018837 139760711874304 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.13571229577064514, loss=0.023879801854491234
I0305 18:53:58.678180 139769339950848 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.11730890721082687, loss=0.020004311576485634
I0305 18:54:28.987304 139937033598784 spec.py:321] Evaluating on the training split.
I0305 18:56:24.504517 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 18:56:27.540621 139937033598784 spec.py:349] Evaluating on the test split.
I0305 18:56:30.598977 139937033598784 submission_runner.py:411] Time since start: 31044.07s, 	Step: 61294, 	{'train/accuracy': 0.9944341778755188, 'train/loss': 0.01734798401594162, 'train/mean_average_precision': 0.6998307741889802, 'validation/accuracy': 0.9870285391807556, 'validation/loss': 0.04815733805298805, 'validation/mean_average_precision': 0.29061876340343157, 'validation/num_examples': 43793, 'test/accuracy': 0.9862361550331116, 'test/loss': 0.05110859125852585, 'test/mean_average_precision': 0.27746633666337617, 'test/num_examples': 43793, 'score': 19948.361443281174, 'total_duration': 31044.06996655464, 'accumulated_submission_time': 19948.361443281174, 'accumulated_eval_time': 11091.201363563538, 'accumulated_logging_time': 2.789128541946411}
I0305 18:56:30.626287 139776159401728 logging_writer.py:48] [61294] accumulated_eval_time=11091.201364, accumulated_logging_time=2.789129, accumulated_submission_time=19948.361443, global_step=61294, preemption_count=0, score=19948.361443, test/accuracy=0.986236, test/loss=0.051109, test/mean_average_precision=0.277466, test/num_examples=43793, total_duration=31044.069967, train/accuracy=0.994434, train/loss=0.017348, train/mean_average_precision=0.699831, validation/accuracy=0.987029, validation/loss=0.048157, validation/mean_average_precision=0.290619, validation/num_examples=43793
I0305 18:56:32.954432 139776167794432 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.11323896050453186, loss=0.020026883110404015
I0305 18:57:05.737396 139776159401728 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.10823700577020645, loss=0.020705323666334152
I0305 18:57:38.417609 139776167794432 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.10427911579608917, loss=0.019152430817484856
I0305 18:58:11.614135 139776159401728 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.11930552124977112, loss=0.020824996754527092
I0305 18:58:44.389933 139776167794432 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.11036013066768646, loss=0.020233161747455597
I0305 18:59:16.994429 139776159401728 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.12362545728683472, loss=0.01803956739604473
I0305 18:59:49.339088 139776167794432 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.11138731986284256, loss=0.02094687521457672
I0305 19:00:21.773548 139776159401728 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.11204113066196442, loss=0.02091887779533863
I0305 19:00:30.798774 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:02:24.008344 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:02:27.035085 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:02:30.041277 139937033598784 submission_runner.py:411] Time since start: 31403.51s, 	Step: 62029, 	{'train/accuracy': 0.9944174885749817, 'train/loss': 0.017465436831116676, 'train/mean_average_precision': 0.7034989601227458, 'validation/accuracy': 0.9870642423629761, 'validation/loss': 0.0482059121131897, 'validation/mean_average_precision': 0.29478725429482805, 'validation/num_examples': 43793, 'test/accuracy': 0.9862399697303772, 'test/loss': 0.05142327398061752, 'test/mean_average_precision': 0.27745617807445644, 'test/num_examples': 43793, 'score': 20188.502468585968, 'total_duration': 31403.512269973755, 'accumulated_submission_time': 20188.502468585968, 'accumulated_eval_time': 11210.443829536438, 'accumulated_logging_time': 2.8275270462036133}
I0305 19:02:30.069340 139760711874304 logging_writer.py:48] [62029] accumulated_eval_time=11210.443830, accumulated_logging_time=2.827527, accumulated_submission_time=20188.502469, global_step=62029, preemption_count=0, score=20188.502469, test/accuracy=0.986240, test/loss=0.051423, test/mean_average_precision=0.277456, test/num_examples=43793, total_duration=31403.512270, train/accuracy=0.994417, train/loss=0.017465, train/mean_average_precision=0.703499, validation/accuracy=0.987064, validation/loss=0.048206, validation/mean_average_precision=0.294787, validation/num_examples=43793
I0305 19:02:53.165369 139768268125952 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.13970236480236053, loss=0.021078085526823997
I0305 19:03:25.229485 139760711874304 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.11253786832094193, loss=0.019956285133957863
I0305 19:03:57.386937 139768268125952 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.11620571464300156, loss=0.018403304740786552
I0305 19:04:29.540610 139760711874304 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.13857756555080414, loss=0.021336060017347336
I0305 19:05:01.612117 139768268125952 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.13400444388389587, loss=0.020458552986383438
I0305 19:05:33.695585 139760711874304 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.111569344997406, loss=0.020220188423991203
I0305 19:06:05.813238 139768268125952 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.11944657564163208, loss=0.018370483070611954
I0305 19:06:30.165132 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:08:23.210539 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:08:26.307111 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:08:29.403546 139937033598784 submission_runner.py:411] Time since start: 31762.87s, 	Step: 62776, 	{'train/accuracy': 0.9943106174468994, 'train/loss': 0.01743159629404545, 'train/mean_average_precision': 0.6984793166748001, 'validation/accuracy': 0.9869989156723022, 'validation/loss': 0.04834553971886635, 'validation/mean_average_precision': 0.29431165003005016, 'validation/num_examples': 43793, 'test/accuracy': 0.9862205982208252, 'test/loss': 0.051521532237529755, 'test/mean_average_precision': 0.28048553651861075, 'test/num_examples': 43793, 'score': 20428.56449484825, 'total_duration': 31762.874539613724, 'accumulated_submission_time': 20428.56449484825, 'accumulated_eval_time': 11329.68220448494, 'accumulated_logging_time': 2.8689663410186768}
I0305 19:08:29.431085 139769339950848 logging_writer.py:48] [62776] accumulated_eval_time=11329.682204, accumulated_logging_time=2.868966, accumulated_submission_time=20428.564495, global_step=62776, preemption_count=0, score=20428.564495, test/accuracy=0.986221, test/loss=0.051522, test/mean_average_precision=0.280486, test/num_examples=43793, total_duration=31762.874540, train/accuracy=0.994311, train/loss=0.017432, train/mean_average_precision=0.698479, validation/accuracy=0.986999, validation/loss=0.048346, validation/mean_average_precision=0.294312, validation/num_examples=43793
I0305 19:08:37.478324 139776159401728 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.11674138158559799, loss=0.019764291122555733
I0305 19:09:09.574768 139769339950848 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.11393487453460693, loss=0.018848294392228127
I0305 19:09:41.696712 139776159401728 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.12180241197347641, loss=0.019198451191186905
I0305 19:10:13.924307 139769339950848 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.13095040619373322, loss=0.018955744802951813
I0305 19:10:45.843273 139776159401728 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.13441455364227295, loss=0.019119184464216232
I0305 19:11:17.997485 139769339950848 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.13174030184745789, loss=0.02152911014854908
I0305 19:11:50.045461 139776159401728 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.14412955939769745, loss=0.020695984363555908
I0305 19:12:22.159601 139769339950848 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.1581123024225235, loss=0.022579072043299675
I0305 19:12:29.577736 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:14:27.108772 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:14:30.168223 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:14:33.211178 139937033598784 submission_runner.py:411] Time since start: 32126.68s, 	Step: 63524, 	{'train/accuracy': 0.9942901730537415, 'train/loss': 0.017771726474165916, 'train/mean_average_precision': 0.6904933370506032, 'validation/accuracy': 0.9868978261947632, 'validation/loss': 0.048730526119470596, 'validation/mean_average_precision': 0.28939576041569676, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.05174928531050682, 'test/mean_average_precision': 0.27606192059470375, 'test/num_examples': 43793, 'score': 20668.679448366165, 'total_duration': 32126.68216776848, 'accumulated_submission_time': 20668.679448366165, 'accumulated_eval_time': 11453.315598249435, 'accumulated_logging_time': 2.9075872898101807}
I0305 19:14:33.239126 139760711874304 logging_writer.py:48] [63524] accumulated_eval_time=11453.315598, accumulated_logging_time=2.907587, accumulated_submission_time=20668.679448, global_step=63524, preemption_count=0, score=20668.679448, test/accuracy=0.986174, test/loss=0.051749, test/mean_average_precision=0.276062, test/num_examples=43793, total_duration=32126.682168, train/accuracy=0.994290, train/loss=0.017772, train/mean_average_precision=0.690493, validation/accuracy=0.986898, validation/loss=0.048731, validation/mean_average_precision=0.289396, validation/num_examples=43793
I0305 19:14:58.072731 139768268125952 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.1320917010307312, loss=0.019472777843475342
I0305 19:15:30.043723 139760711874304 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.1278846710920334, loss=0.01780385710299015
I0305 19:16:01.849913 139768268125952 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.12626075744628906, loss=0.01884564384818077
I0305 19:16:34.522974 139760711874304 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.1319325715303421, loss=0.021442053839564323
I0305 19:17:06.579597 139768268125952 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.11076094955205917, loss=0.017061153426766396
I0305 19:17:38.520906 139760711874304 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.14410245418548584, loss=0.023600546643137932
I0305 19:18:11.620982 139768268125952 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.12712053954601288, loss=0.019439034163951874
I0305 19:18:33.487969 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:20:34.451987 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:20:37.555818 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:20:40.581416 139937033598784 submission_runner.py:411] Time since start: 32494.05s, 	Step: 64266, 	{'train/accuracy': 0.9944842457771301, 'train/loss': 0.017128201201558113, 'train/mean_average_precision': 0.7091291766287441, 'validation/accuracy': 0.987028956413269, 'validation/loss': 0.04882226884365082, 'validation/mean_average_precision': 0.29516011344492804, 'validation/num_examples': 43793, 'test/accuracy': 0.9861649870872498, 'test/loss': 0.05203349143266678, 'test/mean_average_precision': 0.27510950102848936, 'test/num_examples': 43793, 'score': 20908.896485328674, 'total_duration': 32494.052402973175, 'accumulated_submission_time': 20908.896485328674, 'accumulated_eval_time': 11580.409008979797, 'accumulated_logging_time': 2.946688413619995}
I0305 19:20:40.609290 139776159401728 logging_writer.py:48] [64266] accumulated_eval_time=11580.409009, accumulated_logging_time=2.946688, accumulated_submission_time=20908.896485, global_step=64266, preemption_count=0, score=20908.896485, test/accuracy=0.986165, test/loss=0.052033, test/mean_average_precision=0.275110, test/num_examples=43793, total_duration=32494.052403, train/accuracy=0.994484, train/loss=0.017128, train/mean_average_precision=0.709129, validation/accuracy=0.987029, validation/loss=0.048822, validation/mean_average_precision=0.295160, validation/num_examples=43793
I0305 19:20:51.927155 139776167794432 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.13668204843997955, loss=0.018911071121692657
I0305 19:21:24.075846 139776159401728 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.12146860361099243, loss=0.01928979530930519
I0305 19:21:56.401431 139776167794432 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.13720528781414032, loss=0.02084861323237419
I0305 19:22:28.694812 139776159401728 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.13824479281902313, loss=0.019067227840423584
I0305 19:23:01.045733 139776167794432 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.12384333461523056, loss=0.01947888731956482
I0305 19:23:33.113697 139776159401728 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.11960843205451965, loss=0.019269457086920738
I0305 19:24:05.070625 139776167794432 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.12473345547914505, loss=0.020951617509126663
I0305 19:24:36.890412 139776159401728 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.1442651003599167, loss=0.021093202754855156
I0305 19:24:40.751674 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:26:30.568028 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:26:34.129914 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:26:37.563266 139937033598784 submission_runner.py:411] Time since start: 32851.03s, 	Step: 65013, 	{'train/accuracy': 0.9946561455726624, 'train/loss': 0.01649261638522148, 'train/mean_average_precision': 0.7217163236524171, 'validation/accuracy': 0.9869893789291382, 'validation/loss': 0.04933054745197296, 'validation/mean_average_precision': 0.28856820638275354, 'validation/num_examples': 43793, 'test/accuracy': 0.9861902594566345, 'test/loss': 0.05240065976977348, 'test/mean_average_precision': 0.27668626551027076, 'test/num_examples': 43793, 'score': 21149.007489204407, 'total_duration': 32851.034240722656, 'accumulated_submission_time': 21149.007489204407, 'accumulated_eval_time': 11697.220544338226, 'accumulated_logging_time': 2.9854743480682373}
I0305 19:26:37.594958 139760711874304 logging_writer.py:48] [65013] accumulated_eval_time=11697.220544, accumulated_logging_time=2.985474, accumulated_submission_time=21149.007489, global_step=65013, preemption_count=0, score=21149.007489, test/accuracy=0.986190, test/loss=0.052401, test/mean_average_precision=0.276686, test/num_examples=43793, total_duration=32851.034241, train/accuracy=0.994656, train/loss=0.016493, train/mean_average_precision=0.721716, validation/accuracy=0.986989, validation/loss=0.049331, validation/mean_average_precision=0.288568, validation/num_examples=43793
I0305 19:27:07.282460 139768268125952 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.12740369141101837, loss=0.018274948000907898
I0305 19:27:39.698946 139760711874304 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.13345511257648468, loss=0.020451733842492104
I0305 19:28:11.893456 139768268125952 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.11675693839788437, loss=0.019076291471719742
I0305 19:28:44.202690 139760711874304 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.1451919674873352, loss=0.020282477140426636
I0305 19:29:16.198767 139768268125952 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.12404375523328781, loss=0.019203398376703262
I0305 19:29:48.421285 139760711874304 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.1124490350484848, loss=0.017069876194000244
I0305 19:30:20.424138 139768268125952 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.13068917393684387, loss=0.020274650305509567
I0305 19:30:37.567489 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:32:31.779403 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:32:34.864682 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:32:37.895286 139937033598784 submission_runner.py:411] Time since start: 33211.37s, 	Step: 65754, 	{'train/accuracy': 0.9949842095375061, 'train/loss': 0.015710394829511642, 'train/mean_average_precision': 0.751098217448867, 'validation/accuracy': 0.9869558811187744, 'validation/loss': 0.049155041575431824, 'validation/mean_average_precision': 0.29173840446335425, 'validation/num_examples': 43793, 'test/accuracy': 0.9861805438995361, 'test/loss': 0.05239059403538704, 'test/mean_average_precision': 0.2744711419810177, 'test/num_examples': 43793, 'score': 21388.94634079933, 'total_duration': 33211.36627578735, 'accumulated_submission_time': 21388.94634079933, 'accumulated_eval_time': 11817.548296689987, 'accumulated_logging_time': 3.0303094387054443}
I0305 19:32:37.923224 139769339950848 logging_writer.py:48] [65754] accumulated_eval_time=11817.548297, accumulated_logging_time=3.030309, accumulated_submission_time=21388.946341, global_step=65754, preemption_count=0, score=21388.946341, test/accuracy=0.986181, test/loss=0.052391, test/mean_average_precision=0.274471, test/num_examples=43793, total_duration=33211.366276, train/accuracy=0.994984, train/loss=0.015710, train/mean_average_precision=0.751098, validation/accuracy=0.986956, validation/loss=0.049155, validation/mean_average_precision=0.291738, validation/num_examples=43793
I0305 19:32:53.350171 139776167794432 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.13774345815181732, loss=0.01971413381397724
I0305 19:33:26.309865 139769339950848 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.13542453944683075, loss=0.019850151613354683
I0305 19:33:58.636981 139776167794432 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.13568739593029022, loss=0.020624300464987755
I0305 19:34:30.772223 139769339950848 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.1365344524383545, loss=0.020324504002928734
I0305 19:35:03.003561 139776167794432 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.12756960093975067, loss=0.021388642489910126
I0305 19:35:36.328854 139769339950848 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.13290223479270935, loss=0.02145039476454258
I0305 19:36:09.913145 139776167794432 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.12810344994068146, loss=0.019695745781064034
I0305 19:36:37.935196 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:38:32.229127 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:38:35.275904 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:38:38.358751 139937033598784 submission_runner.py:411] Time since start: 33571.83s, 	Step: 66488, 	{'train/accuracy': 0.9949955940246582, 'train/loss': 0.015580584295094013, 'train/mean_average_precision': 0.743294926692901, 'validation/accuracy': 0.9869229793548584, 'validation/loss': 0.04936438426375389, 'validation/mean_average_precision': 0.29297011312261717, 'validation/num_examples': 43793, 'test/accuracy': 0.9861359000205994, 'test/loss': 0.05256795883178711, 'test/mean_average_precision': 0.2745586815742383, 'test/num_examples': 43793, 'score': 21628.92591571808, 'total_duration': 33571.829740047455, 'accumulated_submission_time': 21628.92591571808, 'accumulated_eval_time': 11937.971812486649, 'accumulated_logging_time': 3.069685459136963}
I0305 19:38:38.387833 139760711874304 logging_writer.py:48] [66488] accumulated_eval_time=11937.971812, accumulated_logging_time=3.069685, accumulated_submission_time=21628.925916, global_step=66488, preemption_count=0, score=21628.925916, test/accuracy=0.986136, test/loss=0.052568, test/mean_average_precision=0.274559, test/num_examples=43793, total_duration=33571.829740, train/accuracy=0.994996, train/loss=0.015581, train/mean_average_precision=0.743295, validation/accuracy=0.986923, validation/loss=0.049364, validation/mean_average_precision=0.292970, validation/num_examples=43793
I0305 19:38:42.698041 139768268125952 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.13524195551872253, loss=0.019013170152902603
I0305 19:39:16.060313 139760711874304 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.12490437924861908, loss=0.020132984966039658
I0305 19:39:49.682202 139768268125952 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.12248541414737701, loss=0.019154734909534454
I0305 19:40:22.129034 139760711874304 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.14208348095417023, loss=0.01993190310895443
I0305 19:40:54.310942 139768268125952 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.11433100700378418, loss=0.017998334020376205
I0305 19:41:26.144438 139760711874304 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.13588808476924896, loss=0.018800053745508194
I0305 19:41:58.285658 139768268125952 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.11726904660463333, loss=0.017481252551078796
I0305 19:42:30.308616 139760711874304 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.11231855303049088, loss=0.017874233424663544
I0305 19:42:38.444195 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:44:33.855797 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:44:37.366095 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:44:40.772459 139937033598784 submission_runner.py:411] Time since start: 33934.24s, 	Step: 67226, 	{'train/accuracy': 0.9951726794242859, 'train/loss': 0.015160912647843361, 'train/mean_average_precision': 0.752292266305013, 'validation/accuracy': 0.9870151281356812, 'validation/loss': 0.04947211220860481, 'validation/mean_average_precision': 0.29351050534597134, 'validation/num_examples': 43793, 'test/accuracy': 0.9861502647399902, 'test/loss': 0.05270209535956383, 'test/mean_average_precision': 0.2758090958557493, 'test/num_examples': 43793, 'score': 21868.949313163757, 'total_duration': 33934.243431806564, 'accumulated_submission_time': 21868.949313163757, 'accumulated_eval_time': 12060.300013542175, 'accumulated_logging_time': 3.1101367473602295}
I0305 19:44:40.805376 139776159401728 logging_writer.py:48] [67226] accumulated_eval_time=12060.300014, accumulated_logging_time=3.110137, accumulated_submission_time=21868.949313, global_step=67226, preemption_count=0, score=21868.949313, test/accuracy=0.986150, test/loss=0.052702, test/mean_average_precision=0.275809, test/num_examples=43793, total_duration=33934.243432, train/accuracy=0.995173, train/loss=0.015161, train/mean_average_precision=0.752292, validation/accuracy=0.987015, validation/loss=0.049472, validation/mean_average_precision=0.293511, validation/num_examples=43793
I0305 19:45:06.049842 139776167794432 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.12332869321107864, loss=0.01835964247584343
I0305 19:45:38.597115 139776159401728 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.12562480568885803, loss=0.017170295119285583
I0305 19:46:11.681088 139776167794432 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.12541529536247253, loss=0.01860758103430271
I0305 19:46:44.068654 139776159401728 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.11873997747898102, loss=0.018343308940529823
I0305 19:47:16.128560 139776167794432 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.1372619867324829, loss=0.022129878401756287
I0305 19:47:48.689726 139776159401728 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.11763323098421097, loss=0.016576139256358147
I0305 19:48:21.119536 139776167794432 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.1318822056055069, loss=0.018955649808049202
I0305 19:48:40.818572 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:50:34.241250 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:50:37.269224 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:50:40.287202 139937033598784 submission_runner.py:411] Time since start: 34293.76s, 	Step: 67962, 	{'train/accuracy': 0.9949970245361328, 'train/loss': 0.015576251782476902, 'train/mean_average_precision': 0.7404822138643696, 'validation/accuracy': 0.9870175719261169, 'validation/loss': 0.04983331635594368, 'validation/mean_average_precision': 0.29561911460811935, 'validation/num_examples': 43793, 'test/accuracy': 0.9861688017845154, 'test/loss': 0.05303594470024109, 'test/mean_average_precision': 0.27687187333918123, 'test/num_examples': 43793, 'score': 22108.92947244644, 'total_duration': 34293.75819349289, 'accumulated_submission_time': 22108.92947244644, 'accumulated_eval_time': 12179.768604755402, 'accumulated_logging_time': 3.154653310775757}
I0305 19:50:40.316412 139760711874304 logging_writer.py:48] [67962] accumulated_eval_time=12179.768605, accumulated_logging_time=3.154653, accumulated_submission_time=22108.929472, global_step=67962, preemption_count=0, score=22108.929472, test/accuracy=0.986169, test/loss=0.053036, test/mean_average_precision=0.276872, test/num_examples=43793, total_duration=34293.758193, train/accuracy=0.994997, train/loss=0.015576, train/mean_average_precision=0.740482, validation/accuracy=0.987018, validation/loss=0.049833, validation/mean_average_precision=0.295619, validation/num_examples=43793
I0305 19:50:53.013737 139768268125952 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.12338884174823761, loss=0.02004365064203739
I0305 19:51:25.802831 139760711874304 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.11127183586359024, loss=0.016002165153622627
I0305 19:51:58.664335 139768268125952 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.1364799588918686, loss=0.02101447433233261
I0305 19:52:31.193783 139760711874304 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.12913000583648682, loss=0.018553195521235466
I0305 19:53:04.309820 139768268125952 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.13858072459697723, loss=0.019125163555145264
I0305 19:53:36.968426 139760711874304 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.1308879256248474, loss=0.017889516428112984
I0305 19:54:09.755048 139768268125952 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.12749646604061127, loss=0.017354048788547516
I0305 19:54:40.540729 139937033598784 spec.py:321] Evaluating on the training split.
I0305 19:56:39.473201 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 19:56:42.523613 139937033598784 spec.py:349] Evaluating on the test split.
I0305 19:56:45.521882 139937033598784 submission_runner.py:411] Time since start: 34658.99s, 	Step: 68695, 	{'train/accuracy': 0.9949944615364075, 'train/loss': 0.015577947720885277, 'train/mean_average_precision': 0.7456122928440716, 'validation/accuracy': 0.9869124293327332, 'validation/loss': 0.04966449737548828, 'validation/mean_average_precision': 0.29473258601113794, 'validation/num_examples': 43793, 'test/accuracy': 0.9861283302307129, 'test/loss': 0.05284103378653526, 'test/mean_average_precision': 0.27701263103651813, 'test/num_examples': 43793, 'score': 22349.121163129807, 'total_duration': 34658.99287319183, 'accumulated_submission_time': 22349.121163129807, 'accumulated_eval_time': 12304.749731063843, 'accumulated_logging_time': 3.194903612136841}
I0305 19:56:45.571854 139769339950848 logging_writer.py:48] [68695] accumulated_eval_time=12304.749731, accumulated_logging_time=3.194904, accumulated_submission_time=22349.121163, global_step=68695, preemption_count=0, score=22349.121163, test/accuracy=0.986128, test/loss=0.052841, test/mean_average_precision=0.277013, test/num_examples=43793, total_duration=34658.992873, train/accuracy=0.994994, train/loss=0.015578, train/mean_average_precision=0.745612, validation/accuracy=0.986912, validation/loss=0.049664, validation/mean_average_precision=0.294733, validation/num_examples=43793
I0305 19:56:47.723581 139776159401728 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.15409325063228607, loss=0.02239096164703369
I0305 19:57:20.168788 139769339950848 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.1317259818315506, loss=0.019711105152964592
I0305 19:57:52.514868 139776159401728 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.12153054773807526, loss=0.020217783749103546
I0305 19:58:25.099195 139769339950848 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.15947259962558746, loss=0.02092933841049671
I0305 19:58:57.102941 139776159401728 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.1837807595729828, loss=0.02040085382759571
I0305 19:59:29.547883 139769339950848 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.13581715524196625, loss=0.01882733218371868
I0305 20:00:01.613061 139776159401728 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.14078940451145172, loss=0.01745702140033245
I0305 20:00:33.606035 139769339950848 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.16152334213256836, loss=0.020606396719813347
I0305 20:00:45.816321 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:02:38.524336 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:02:41.548213 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:02:44.610176 139937033598784 submission_runner.py:411] Time since start: 35018.08s, 	Step: 69439, 	{'train/accuracy': 0.9948853850364685, 'train/loss': 0.01578059419989586, 'train/mean_average_precision': 0.7421132570454949, 'validation/accuracy': 0.9869562983512878, 'validation/loss': 0.049928996711969376, 'validation/mean_average_precision': 0.29227579414513377, 'validation/num_examples': 43793, 'test/accuracy': 0.9861435294151306, 'test/loss': 0.05311544984579086, 'test/mean_average_precision': 0.27646806536026014, 'test/num_examples': 43793, 'score': 22589.33414030075, 'total_duration': 35018.08116745949, 'accumulated_submission_time': 22589.33414030075, 'accumulated_eval_time': 12423.543547868729, 'accumulated_logging_time': 3.256009817123413}
I0305 20:02:44.639126 139760711874304 logging_writer.py:48] [69439] accumulated_eval_time=12423.543548, accumulated_logging_time=3.256010, accumulated_submission_time=22589.334140, global_step=69439, preemption_count=0, score=22589.334140, test/accuracy=0.986144, test/loss=0.053115, test/mean_average_precision=0.276468, test/num_examples=43793, total_duration=35018.081167, train/accuracy=0.994885, train/loss=0.015781, train/mean_average_precision=0.742113, validation/accuracy=0.986956, validation/loss=0.049929, validation/mean_average_precision=0.292276, validation/num_examples=43793
I0305 20:03:05.045254 139768268125952 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.12844207882881165, loss=0.014954738318920135
I0305 20:03:37.153491 139760711874304 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.14932939410209656, loss=0.019116662442684174
I0305 20:04:09.723032 139768268125952 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.12602008879184723, loss=0.01859949342906475
I0305 20:04:41.808011 139760711874304 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.14205904304981232, loss=0.01906740479171276
I0305 20:05:14.474128 139768268125952 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.14274710416793823, loss=0.021314211189746857
I0305 20:05:46.373048 139760711874304 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.11717897653579712, loss=0.01604967564344406
I0305 20:06:18.746136 139768268125952 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.11918900161981583, loss=0.01676679030060768
I0305 20:06:44.914040 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:08:45.112097 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:08:48.159398 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:08:51.160810 139937033598784 submission_runner.py:411] Time since start: 35384.63s, 	Step: 70182, 	{'train/accuracy': 0.9949990510940552, 'train/loss': 0.015497229993343353, 'train/mean_average_precision': 0.7395333602277423, 'validation/accuracy': 0.986955463886261, 'validation/loss': 0.049820426851511, 'validation/mean_average_precision': 0.29463461906907157, 'validation/num_examples': 43793, 'test/accuracy': 0.9861018061637878, 'test/loss': 0.05307406187057495, 'test/mean_average_precision': 0.2752897350878246, 'test/num_examples': 43793, 'score': 22829.576615333557, 'total_duration': 35384.631796598434, 'accumulated_submission_time': 22829.576615333557, 'accumulated_eval_time': 12549.790276288986, 'accumulated_logging_time': 3.2973122596740723}
I0305 20:08:51.189460 139769339950848 logging_writer.py:48] [70182] accumulated_eval_time=12549.790276, accumulated_logging_time=3.297312, accumulated_submission_time=22829.576615, global_step=70182, preemption_count=0, score=22829.576615, test/accuracy=0.986102, test/loss=0.053074, test/mean_average_precision=0.275290, test/num_examples=43793, total_duration=35384.631797, train/accuracy=0.994999, train/loss=0.015497, train/mean_average_precision=0.739533, validation/accuracy=0.986955, validation/loss=0.049820, validation/mean_average_precision=0.294635, validation/num_examples=43793
I0305 20:08:57.442732 139776167794432 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.15842334926128387, loss=0.02251104637980461
I0305 20:09:29.491050 139769339950848 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.13314999639987946, loss=0.01773867942392826
I0305 20:10:01.996784 139776167794432 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.14508897066116333, loss=0.020318882539868355
I0305 20:10:34.133767 139769339950848 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.13924066722393036, loss=0.018336422741413116
I0305 20:11:06.461812 139776167794432 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.15417559444904327, loss=0.021283945068717003
I0305 20:11:38.115148 139769339950848 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.13489659130573273, loss=0.018528005108237267
I0305 20:12:10.269228 139776167794432 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.13631445169448853, loss=0.018689226359128952
I0305 20:12:42.539732 139769339950848 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.14117743074893951, loss=0.017856501042842865
I0305 20:12:51.300854 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:14:44.064737 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:14:47.124577 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:14:50.181535 139937033598784 submission_runner.py:411] Time since start: 35743.65s, 	Step: 70928, 	{'train/accuracy': 0.9952114820480347, 'train/loss': 0.014923501759767532, 'train/mean_average_precision': 0.7624726762016496, 'validation/accuracy': 0.9870090484619141, 'validation/loss': 0.050180014222860336, 'validation/mean_average_precision': 0.292888755323264, 'validation/num_examples': 43793, 'test/accuracy': 0.9861287474632263, 'test/loss': 0.053613729774951935, 'test/mean_average_precision': 0.2747126367614844, 'test/num_examples': 43793, 'score': 23069.656600236893, 'total_duration': 35743.65252280235, 'accumulated_submission_time': 23069.656600236893, 'accumulated_eval_time': 12668.670906543732, 'accumulated_logging_time': 3.3372585773468018}
I0305 20:14:50.212067 139768268125952 logging_writer.py:48] [70928] accumulated_eval_time=12668.670907, accumulated_logging_time=3.337259, accumulated_submission_time=23069.656600, global_step=70928, preemption_count=0, score=23069.656600, test/accuracy=0.986129, test/loss=0.053614, test/mean_average_precision=0.274713, test/num_examples=43793, total_duration=35743.652523, train/accuracy=0.995211, train/loss=0.014924, train/mean_average_precision=0.762473, validation/accuracy=0.987009, validation/loss=0.050180, validation/mean_average_precision=0.292889, validation/num_examples=43793
I0305 20:15:14.034205 139776159401728 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.1494174748659134, loss=0.01797999069094658
I0305 20:15:46.379289 139768268125952 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.13732324540615082, loss=0.017939480021595955
I0305 20:16:19.355108 139776159401728 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.12969724833965302, loss=0.01717754639685154
I0305 20:16:51.769347 139768268125952 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.16176368296146393, loss=0.019843749701976776
I0305 20:17:24.078183 139776159401728 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.1476011574268341, loss=0.02113524079322815
I0305 20:17:56.099729 139768268125952 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.143082395195961, loss=0.019144589081406593
I0305 20:18:28.390101 139776159401728 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.1251169890165329, loss=0.01883220486342907
I0305 20:18:50.347130 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:20:49.290618 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:20:52.785453 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:20:56.144325 139937033598784 submission_runner.py:411] Time since start: 36109.62s, 	Step: 71669, 	{'train/accuracy': 0.9953161478042603, 'train/loss': 0.014587756246328354, 'train/mean_average_precision': 0.7535696656906885, 'validation/accuracy': 0.9870131015777588, 'validation/loss': 0.050116028636693954, 'validation/mean_average_precision': 0.2949806488973432, 'validation/num_examples': 43793, 'test/accuracy': 0.9861708879470825, 'test/loss': 0.05350334569811821, 'test/mean_average_precision': 0.2766858995086599, 'test/num_examples': 43793, 'score': 23309.76019668579, 'total_duration': 36109.615293741226, 'accumulated_submission_time': 23309.76019668579, 'accumulated_eval_time': 12794.468041181564, 'accumulated_logging_time': 3.3788788318634033}
I0305 20:20:56.178716 139760711874304 logging_writer.py:48] [71669] accumulated_eval_time=12794.468041, accumulated_logging_time=3.378879, accumulated_submission_time=23309.760197, global_step=71669, preemption_count=0, score=23309.760197, test/accuracy=0.986171, test/loss=0.053503, test/mean_average_precision=0.276686, test/num_examples=43793, total_duration=36109.615294, train/accuracy=0.995316, train/loss=0.014588, train/mean_average_precision=0.753570, validation/accuracy=0.987013, validation/loss=0.050116, validation/mean_average_precision=0.294981, validation/num_examples=43793
I0305 20:21:06.994306 139769339950848 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.13827843964099884, loss=0.019045542925596237
I0305 20:21:40.019867 139760711874304 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.1384039968252182, loss=0.01797233149409294
I0305 20:22:13.097408 139769339950848 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.1421619951725006, loss=0.019623711705207825
I0305 20:22:46.514612 139760711874304 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.1344076544046402, loss=0.016547858715057373
I0305 20:23:19.436205 139769339950848 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.13725903630256653, loss=0.020029524341225624
I0305 20:23:51.705753 139760711874304 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.13760016858577728, loss=0.01852581463754177
I0305 20:24:24.716123 139769339950848 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.1370438039302826, loss=0.0176385547965765
I0305 20:24:56.455152 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:26:52.230216 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:26:55.308575 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:26:58.343184 139937033598784 submission_runner.py:411] Time since start: 36471.81s, 	Step: 72398, 	{'train/accuracy': 0.9954724311828613, 'train/loss': 0.014200561679899693, 'train/mean_average_precision': 0.7746771146110315, 'validation/accuracy': 0.9869810342788696, 'validation/loss': 0.050081100314855576, 'validation/mean_average_precision': 0.2950637680379031, 'validation/num_examples': 43793, 'test/accuracy': 0.9861599206924438, 'test/loss': 0.053442567586898804, 'test/mean_average_precision': 0.27579941277242853, 'test/num_examples': 43793, 'score': 23550.001608610153, 'total_duration': 36471.81417179108, 'accumulated_submission_time': 23550.001608610153, 'accumulated_eval_time': 12916.356031417847, 'accumulated_logging_time': 3.4264605045318604}
I0305 20:26:58.374482 139768268125952 logging_writer.py:48] [72398] accumulated_eval_time=12916.356031, accumulated_logging_time=3.426461, accumulated_submission_time=23550.001609, global_step=72398, preemption_count=0, score=23550.001609, test/accuracy=0.986160, test/loss=0.053443, test/mean_average_precision=0.275799, test/num_examples=43793, total_duration=36471.814172, train/accuracy=0.995472, train/loss=0.014201, train/mean_average_precision=0.774677, validation/accuracy=0.986981, validation/loss=0.050081, validation/mean_average_precision=0.295064, validation/num_examples=43793
I0305 20:26:59.393788 139776159401728 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.14109201729297638, loss=0.01804986409842968
I0305 20:27:31.712721 139768268125952 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.12698084115982056, loss=0.01657332479953766
I0305 20:28:04.193217 139776159401728 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.13818123936653137, loss=0.01818014495074749
I0305 20:28:36.285747 139768268125952 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.15074558556079865, loss=0.019932683557271957
I0305 20:29:08.732734 139776159401728 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.15108847618103027, loss=0.020412813872098923
I0305 20:29:41.135243 139768268125952 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.14320845901966095, loss=0.018028777092695236
I0305 20:30:13.373749 139776159401728 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.14783243834972382, loss=0.01954522356390953
I0305 20:30:45.558506 139768268125952 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.142450213432312, loss=0.018734512850642204
I0305 20:30:58.496030 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:32:56.654632 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:32:59.658714 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:33:03.038284 139937033598784 submission_runner.py:411] Time since start: 36836.51s, 	Step: 73141, 	{'train/accuracy': 0.9954382181167603, 'train/loss': 0.014345245435833931, 'train/mean_average_precision': 0.7782669256073729, 'validation/accuracy': 0.9870086312294006, 'validation/loss': 0.050230972468853, 'validation/mean_average_precision': 0.29395359939610916, 'validation/num_examples': 43793, 'test/accuracy': 0.9861510992050171, 'test/loss': 0.05367853865027428, 'test/mean_average_precision': 0.2758017772485302, 'test/num_examples': 43793, 'score': 23790.09094119072, 'total_duration': 36836.50923585892, 'accumulated_submission_time': 23790.09094119072, 'accumulated_eval_time': 13040.898215293884, 'accumulated_logging_time': 3.469341993331909}
I0305 20:33:03.074481 139760711874304 logging_writer.py:48] [73141] accumulated_eval_time=13040.898215, accumulated_logging_time=3.469342, accumulated_submission_time=23790.090941, global_step=73141, preemption_count=0, score=23790.090941, test/accuracy=0.986151, test/loss=0.053679, test/mean_average_precision=0.275802, test/num_examples=43793, total_duration=36836.509236, train/accuracy=0.995438, train/loss=0.014345, train/mean_average_precision=0.778267, validation/accuracy=0.987009, validation/loss=0.050231, validation/mean_average_precision=0.293954, validation/num_examples=43793
I0305 20:33:22.857821 139769339950848 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.12288396060466766, loss=0.0168008916079998
I0305 20:33:55.851712 139760711874304 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.1426411122083664, loss=0.01873260922729969
I0305 20:34:29.093640 139769339950848 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.1413763016462326, loss=0.01755080558359623
I0305 20:35:02.148313 139760711874304 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.1350429207086563, loss=0.017662739381194115
I0305 20:35:35.305169 139769339950848 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.15460915863513947, loss=0.01973465457558632
I0305 20:36:08.559570 139760711874304 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.12811006605625153, loss=0.017222225666046143
I0305 20:36:41.762283 139769339950848 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.15337181091308594, loss=0.01881258934736252
I0305 20:37:03.066835 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:38:56.970806 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:39:00.114709 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:39:03.253399 139937033598784 submission_runner.py:411] Time since start: 37196.72s, 	Step: 73865, 	{'train/accuracy': 0.9954219460487366, 'train/loss': 0.014223325997591019, 'train/mean_average_precision': 0.77684622631777, 'validation/accuracy': 0.9870402812957764, 'validation/loss': 0.05032433196902275, 'validation/mean_average_precision': 0.29357148169121966, 'validation/num_examples': 43793, 'test/accuracy': 0.9861910939216614, 'test/loss': 0.05379839986562729, 'test/mean_average_precision': 0.2746981960824259, 'test/num_examples': 43793, 'score': 24030.046718120575, 'total_duration': 37196.72438669205, 'accumulated_submission_time': 24030.046718120575, 'accumulated_eval_time': 13161.084740161896, 'accumulated_logging_time': 3.5184857845306396}
I0305 20:39:03.283251 139768268125952 logging_writer.py:48] [73865] accumulated_eval_time=13161.084740, accumulated_logging_time=3.518486, accumulated_submission_time=24030.046718, global_step=73865, preemption_count=0, score=24030.046718, test/accuracy=0.986191, test/loss=0.053798, test/mean_average_precision=0.274698, test/num_examples=43793, total_duration=37196.724387, train/accuracy=0.995422, train/loss=0.014223, train/mean_average_precision=0.776846, validation/accuracy=0.987040, validation/loss=0.050324, validation/mean_average_precision=0.293571, validation/num_examples=43793
I0305 20:39:15.057641 139776159401728 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.14349040389060974, loss=0.019267436116933823
I0305 20:39:47.684681 139768268125952 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.13627126812934875, loss=0.017987189814448357
I0305 20:40:20.530664 139776159401728 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.14930912852287292, loss=0.017159251496195793
I0305 20:40:52.844973 139768268125952 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.1433422863483429, loss=0.018713979050517082
I0305 20:41:24.900200 139776159401728 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.14889992773532867, loss=0.019693007692694664
I0305 20:41:57.048472 139768268125952 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.14179018139839172, loss=0.017766892910003662
I0305 20:42:29.271282 139776159401728 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.1509464830160141, loss=0.02140798047184944
I0305 20:43:01.718842 139768268125952 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.12893731892108917, loss=0.017600176855921745
I0305 20:43:03.380134 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:44:57.619935 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:45:00.632521 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:45:04.016709 139937033598784 submission_runner.py:411] Time since start: 37557.49s, 	Step: 74606, 	{'train/accuracy': 0.9953264594078064, 'train/loss': 0.01450534537434578, 'train/mean_average_precision': 0.7675750118704716, 'validation/accuracy': 0.9869623780250549, 'validation/loss': 0.050269074738025665, 'validation/mean_average_precision': 0.29331648216032563, 'validation/num_examples': 43793, 'test/accuracy': 0.9861281514167786, 'test/loss': 0.053718943148851395, 'test/mean_average_precision': 0.27530949276613936, 'test/num_examples': 43793, 'score': 24270.110892534256, 'total_duration': 37557.48768091202, 'accumulated_submission_time': 24270.110892534256, 'accumulated_eval_time': 13281.721255779266, 'accumulated_logging_time': 3.5607388019561768}
I0305 20:45:04.082103 139769339950848 logging_writer.py:48] [74606] accumulated_eval_time=13281.721256, accumulated_logging_time=3.560739, accumulated_submission_time=24270.110893, global_step=74606, preemption_count=0, score=24270.110893, test/accuracy=0.986128, test/loss=0.053719, test/mean_average_precision=0.275309, test/num_examples=43793, total_duration=37557.487681, train/accuracy=0.995326, train/loss=0.014505, train/mean_average_precision=0.767575, validation/accuracy=0.986962, validation/loss=0.050269, validation/mean_average_precision=0.293316, validation/num_examples=43793
I0305 20:45:35.798336 139776167794432 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.13519620895385742, loss=0.015969933941960335
I0305 20:46:09.535073 139769339950848 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.1348458081483841, loss=0.01849852316081524
I0305 20:46:41.782109 139776167794432 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.1392541080713272, loss=0.017872007563710213
I0305 20:47:14.032802 139769339950848 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.14225466549396515, loss=0.01844256930053234
I0305 20:47:46.215653 139776167794432 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.14100252091884613, loss=0.01973610557615757
I0305 20:48:18.857209 139769339950848 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.1431647390127182, loss=0.019358759745955467
I0305 20:48:51.431860 139776167794432 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.13756868243217468, loss=0.017317315563559532
I0305 20:49:04.031863 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:50:59.910436 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:51:03.224891 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:51:06.606860 139937033598784 submission_runner.py:411] Time since start: 37920.08s, 	Step: 75339, 	{'train/accuracy': 0.9953559637069702, 'train/loss': 0.014485582709312439, 'train/mean_average_precision': 0.7559158070368784, 'validation/accuracy': 0.9869757294654846, 'validation/loss': 0.05043775215744972, 'validation/mean_average_precision': 0.2932075498164984, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05384349077939987, 'test/mean_average_precision': 0.2762096863110901, 'test/num_examples': 43793, 'score': 24510.027459144592, 'total_duration': 37920.07783651352, 'accumulated_submission_time': 24510.027459144592, 'accumulated_eval_time': 13404.296205759048, 'accumulated_logging_time': 3.6379876136779785}
I0305 20:51:06.640131 139760711874304 logging_writer.py:48] [75339] accumulated_eval_time=13404.296206, accumulated_logging_time=3.637988, accumulated_submission_time=24510.027459, global_step=75339, preemption_count=0, score=24510.027459, test/accuracy=0.986148, test/loss=0.053843, test/mean_average_precision=0.276210, test/num_examples=43793, total_duration=37920.077837, train/accuracy=0.995356, train/loss=0.014486, train/mean_average_precision=0.755916, validation/accuracy=0.986976, validation/loss=0.050438, validation/mean_average_precision=0.293208, validation/num_examples=43793
I0305 20:51:27.713630 139776159401728 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.15574029088020325, loss=0.018461016938090324
I0305 20:52:01.119500 139760711874304 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.13474561274051666, loss=0.016800884157419205
I0305 20:52:33.787453 139776159401728 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.14091791212558746, loss=0.018379876390099525
I0305 20:53:06.531569 139760711874304 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.132058247923851, loss=0.016086440533399582
I0305 20:53:39.446212 139776159401728 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.1614769846200943, loss=0.01871408149600029
I0305 20:54:12.587244 139760711874304 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.14073200523853302, loss=0.01975433900952339
I0305 20:54:44.678851 139776159401728 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.12469513714313507, loss=0.01763211004436016
I0305 20:55:06.836895 139937033598784 spec.py:321] Evaluating on the training split.
I0305 20:57:01.362782 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 20:57:04.573665 139937033598784 spec.py:349] Evaluating on the test split.
I0305 20:57:07.571051 139937033598784 submission_runner.py:411] Time since start: 38281.04s, 	Step: 76068, 	{'train/accuracy': 0.9953692555427551, 'train/loss': 0.014444767497479916, 'train/mean_average_precision': 0.7646718876340945, 'validation/accuracy': 0.9869931936264038, 'validation/loss': 0.050412535667419434, 'validation/mean_average_precision': 0.29312898943391663, 'validation/num_examples': 43793, 'test/accuracy': 0.9861472845077515, 'test/loss': 0.053802452981472015, 'test/mean_average_precision': 0.27676027420475374, 'test/num_examples': 43793, 'score': 24750.19059085846, 'total_duration': 38281.042036771774, 'accumulated_submission_time': 24750.19059085846, 'accumulated_eval_time': 13525.030313014984, 'accumulated_logging_time': 3.6831717491149902}
I0305 20:57:07.601157 139768268125952 logging_writer.py:48] [76068] accumulated_eval_time=13525.030313, accumulated_logging_time=3.683172, accumulated_submission_time=24750.190591, global_step=76068, preemption_count=0, score=24750.190591, test/accuracy=0.986147, test/loss=0.053802, test/mean_average_precision=0.276760, test/num_examples=43793, total_duration=38281.042037, train/accuracy=0.995369, train/loss=0.014445, train/mean_average_precision=0.764672, validation/accuracy=0.986993, validation/loss=0.050413, validation/mean_average_precision=0.293129, validation/num_examples=43793
I0305 20:57:18.420253 139769339950848 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.14902018010616302, loss=0.018680579960346222
I0305 20:57:50.488324 139768268125952 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.11732293665409088, loss=0.01541534811258316
I0305 20:58:22.811136 139769339950848 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.15141017735004425, loss=0.01789168268442154
I0305 20:58:55.112002 139768268125952 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.13631261885166168, loss=0.018648387864232063
I0305 20:59:27.638881 139769339950848 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.1387624591588974, loss=0.016225723549723625
I0305 21:00:00.122854 139768268125952 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.14451952278614044, loss=0.016778288409113884
I0305 21:00:32.467556 139769339950848 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.13989683985710144, loss=0.01867673359811306
I0305 21:01:04.756551 139768268125952 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.12976007163524628, loss=0.015959464013576508
I0305 21:01:07.676159 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:02:57.889045 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:03:00.968028 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:03:04.042262 139937033598784 submission_runner.py:411] Time since start: 38637.51s, 	Step: 76810, 	{'train/accuracy': 0.995473325252533, 'train/loss': 0.014102102257311344, 'train/mean_average_precision': 0.7793426664482765, 'validation/accuracy': 0.9869822263717651, 'validation/loss': 0.05040169507265091, 'validation/mean_average_precision': 0.2939869283770364, 'validation/num_examples': 43793, 'test/accuracy': 0.986127495765686, 'test/loss': 0.05386100336909294, 'test/mean_average_precision': 0.275816124039496, 'test/num_examples': 43793, 'score': 24990.2333111763, 'total_duration': 38637.51325106621, 'accumulated_submission_time': 24990.2333111763, 'accumulated_eval_time': 13641.3963701725, 'accumulated_logging_time': 3.7246971130371094}
I0305 21:03:04.072712 139760711874304 logging_writer.py:48] [76810] accumulated_eval_time=13641.396370, accumulated_logging_time=3.724697, accumulated_submission_time=24990.233311, global_step=76810, preemption_count=0, score=24990.233311, test/accuracy=0.986127, test/loss=0.053861, test/mean_average_precision=0.275816, test/num_examples=43793, total_duration=38637.513251, train/accuracy=0.995473, train/loss=0.014102, train/mean_average_precision=0.779343, validation/accuracy=0.986982, validation/loss=0.050402, validation/mean_average_precision=0.293987, validation/num_examples=43793
I0305 21:03:33.667643 139776167794432 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.1466253399848938, loss=0.017594480887055397
I0305 21:04:06.499171 139760711874304 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.14472894370555878, loss=0.018246596679091454
I0305 21:04:38.894681 139776167794432 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.15056096017360687, loss=0.019896015524864197
I0305 21:05:11.628903 139760711874304 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.12282184511423111, loss=0.014983940869569778
I0305 21:05:44.954604 139776167794432 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.15287034213542938, loss=0.02073710598051548
I0305 21:06:18.056203 139760711874304 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.15039929747581482, loss=0.01805594563484192
I0305 21:06:50.528094 139776167794432 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.13296696543693542, loss=0.015375956892967224
I0305 21:07:04.162900 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:08:56.993545 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:09:00.096808 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:09:03.217640 139937033598784 submission_runner.py:411] Time since start: 38996.69s, 	Step: 77543, 	{'train/accuracy': 0.995535135269165, 'train/loss': 0.013972028158605099, 'train/mean_average_precision': 0.7854258956826089, 'validation/accuracy': 0.9869814515113831, 'validation/loss': 0.0504111647605896, 'validation/mean_average_precision': 0.2938731668240078, 'validation/num_examples': 43793, 'test/accuracy': 0.98613041639328, 'test/loss': 0.05384191870689392, 'test/mean_average_precision': 0.2757654531108339, 'test/num_examples': 43793, 'score': 25230.289999961853, 'total_duration': 38996.68863105774, 'accumulated_submission_time': 25230.289999961853, 'accumulated_eval_time': 13760.451066493988, 'accumulated_logging_time': 3.7672665119171143}
I0305 21:09:03.247687 139769339950848 logging_writer.py:48] [77543] accumulated_eval_time=13760.451066, accumulated_logging_time=3.767267, accumulated_submission_time=25230.290000, global_step=77543, preemption_count=0, score=25230.290000, test/accuracy=0.986130, test/loss=0.053842, test/mean_average_precision=0.275765, test/num_examples=43793, total_duration=38996.688631, train/accuracy=0.995535, train/loss=0.013972, train/mean_average_precision=0.785426, validation/accuracy=0.986981, validation/loss=0.050411, validation/mean_average_precision=0.293873, validation/num_examples=43793
I0305 21:09:22.064374 139776159401728 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.14045347273349762, loss=0.017821229994297028
I0305 21:09:54.498700 139769339950848 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.152415931224823, loss=0.01938159577548504
I0305 21:10:26.525475 139776159401728 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.13169623911380768, loss=0.016601547598838806
I0305 21:10:58.894769 139769339950848 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.14742445945739746, loss=0.019743310287594795
I0305 21:11:31.315310 139776159401728 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.12992586195468903, loss=0.017335467040538788
I0305 21:12:03.785387 139769339950848 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.15369512140750885, loss=0.02136288769543171
I0305 21:12:36.629452 139776159401728 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.13108482956886292, loss=0.01713545247912407
I0305 21:13:03.401365 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:14:58.164701 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:15:01.294397 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:15:04.749689 139937033598784 submission_runner.py:411] Time since start: 39358.22s, 	Step: 78284, 	{'train/accuracy': 0.9955268502235413, 'train/loss': 0.014017085544764996, 'train/mean_average_precision': 0.7766022190074704, 'validation/accuracy': 0.9870017170906067, 'validation/loss': 0.05045250803232193, 'validation/mean_average_precision': 0.2935173090637798, 'validation/num_examples': 43793, 'test/accuracy': 0.9861502647399902, 'test/loss': 0.05388105660676956, 'test/mean_average_precision': 0.2762060660463843, 'test/num_examples': 43793, 'score': 25470.411824703217, 'total_duration': 39358.220655441284, 'accumulated_submission_time': 25470.411824703217, 'accumulated_eval_time': 13881.799325227737, 'accumulated_logging_time': 3.8089919090270996}
I0305 21:15:04.785161 139760711874304 logging_writer.py:48] [78284] accumulated_eval_time=13881.799325, accumulated_logging_time=3.808992, accumulated_submission_time=25470.411825, global_step=78284, preemption_count=0, score=25470.411825, test/accuracy=0.986150, test/loss=0.053881, test/mean_average_precision=0.276206, test/num_examples=43793, total_duration=39358.220655, train/accuracy=0.995527, train/loss=0.014017, train/mean_average_precision=0.776602, validation/accuracy=0.987002, validation/loss=0.050453, validation/mean_average_precision=0.293517, validation/num_examples=43793
I0305 21:15:10.546810 139768268125952 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.14591993391513824, loss=0.018044088035821915
I0305 21:15:43.815783 139760711874304 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.1287972331047058, loss=0.017001664265990257
I0305 21:16:17.495274 139768268125952 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.1345490962266922, loss=0.016864698380231857
I0305 21:16:51.067958 139760711874304 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.13425110280513763, loss=0.017064647749066353
I0305 21:17:25.023080 139768268125952 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.1497211754322052, loss=0.0171207282692194
I0305 21:17:58.637417 139760711874304 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.1562182605266571, loss=0.018373403698205948
I0305 21:18:31.188231 139768268125952 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.15561708807945251, loss=0.01795332133769989
I0305 21:19:03.586513 139760711874304 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.13674621284008026, loss=0.018573809415102005
I0305 21:19:04.889453 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:21:00.414907 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:21:04.149711 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:21:07.660141 139937033598784 submission_runner.py:411] Time since start: 39721.13s, 	Step: 79005, 	{'train/accuracy': 0.9955037236213684, 'train/loss': 0.01407643873244524, 'train/mean_average_precision': 0.7753496218501175, 'validation/accuracy': 0.9870216250419617, 'validation/loss': 0.05049348622560501, 'validation/mean_average_precision': 0.2931579732099656, 'validation/num_examples': 43793, 'test/accuracy': 0.9861544370651245, 'test/loss': 0.05393727496266365, 'test/mean_average_precision': 0.27604131322985426, 'test/num_examples': 43793, 'score': 25710.480096817017, 'total_duration': 39721.13111758232, 'accumulated_submission_time': 25710.480096817017, 'accumulated_eval_time': 14004.569951534271, 'accumulated_logging_time': 3.8572590351104736}
I0305 21:21:07.696011 139769339950848 logging_writer.py:48] [79005] accumulated_eval_time=14004.569952, accumulated_logging_time=3.857259, accumulated_submission_time=25710.480097, global_step=79005, preemption_count=0, score=25710.480097, test/accuracy=0.986154, test/loss=0.053937, test/mean_average_precision=0.276041, test/num_examples=43793, total_duration=39721.131118, train/accuracy=0.995504, train/loss=0.014076, train/mean_average_precision=0.775350, validation/accuracy=0.987022, validation/loss=0.050493, validation/mean_average_precision=0.293158, validation/num_examples=43793
I0305 21:21:39.753208 139776159401728 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.14245116710662842, loss=0.017523208633065224
I0305 21:22:13.112634 139769339950848 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.1573788970708847, loss=0.01777954213321209
I0305 21:22:45.295662 139776159401728 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.1462768018245697, loss=0.018890630453824997
I0305 21:23:17.963125 139769339950848 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.1382964849472046, loss=0.016644960269331932
I0305 21:23:50.163599 139776159401728 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.13058076798915863, loss=0.017824603244662285
I0305 21:24:22.239239 139769339950848 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.1277845948934555, loss=0.017488686367869377
I0305 21:24:54.272708 139776159401728 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.14804607629776, loss=0.017270363867282867
I0305 21:25:07.850134 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:26:59.095488 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:27:02.337335 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:27:05.385969 139937033598784 submission_runner.py:411] Time since start: 40078.86s, 	Step: 79743, 	{'train/accuracy': 0.9954732656478882, 'train/loss': 0.014132573269307613, 'train/mean_average_precision': 0.7680403320439355, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048912763595581, 'validation/mean_average_precision': 0.29310524775423263, 'validation/num_examples': 43793, 'test/accuracy': 0.986162006855011, 'test/loss': 0.05393175780773163, 'test/mean_average_precision': 0.27587410183709227, 'test/num_examples': 43793, 'score': 25950.60027909279, 'total_duration': 40078.85696077347, 'accumulated_submission_time': 25950.60027909279, 'accumulated_eval_time': 14122.105741977692, 'accumulated_logging_time': 3.9061312675476074}
I0305 21:27:05.416732 139760711874304 logging_writer.py:48] [79743] accumulated_eval_time=14122.105742, accumulated_logging_time=3.906131, accumulated_submission_time=25950.600279, global_step=79743, preemption_count=0, score=25950.600279, test/accuracy=0.986162, test/loss=0.053932, test/mean_average_precision=0.275874, test/num_examples=43793, total_duration=40078.856961, train/accuracy=0.995473, train/loss=0.014133, train/mean_average_precision=0.768040, validation/accuracy=0.987021, validation/loss=0.050489, validation/mean_average_precision=0.293105, validation/num_examples=43793
I0305 21:27:24.093436 139776167794432 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.13678321242332458, loss=0.01767328754067421
I0305 21:27:56.217101 139760711874304 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.1407180279493332, loss=0.018142305314540863
I0305 21:28:28.997741 139776167794432 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.14739401638507843, loss=0.017156558111310005
I0305 21:29:01.636529 139760711874304 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.16010132431983948, loss=0.019305499270558357
I0305 21:29:33.977691 139776167794432 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.1646149605512619, loss=0.02085709199309349
I0305 21:30:06.767726 139760711874304 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.17257975041866302, loss=0.017289312556385994
I0305 21:30:38.905078 139776167794432 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.13679562509059906, loss=0.0179470032453537
I0305 21:31:05.682314 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:32:57.249978 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:33:00.339596 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:33:03.455100 139937033598784 submission_runner.py:411] Time since start: 40436.93s, 	Step: 80484, 	{'train/accuracy': 0.9954727292060852, 'train/loss': 0.014098569750785828, 'train/mean_average_precision': 0.7734134501513834, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932042189287904, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760013251638327, 'test/num_examples': 43793, 'score': 26190.832530498505, 'total_duration': 40436.92608857155, 'accumulated_submission_time': 26190.832530498505, 'accumulated_eval_time': 14239.878486156464, 'accumulated_logging_time': 3.949522018432617}
I0305 21:33:03.486012 139768268125952 logging_writer.py:48] [80484] accumulated_eval_time=14239.878486, accumulated_logging_time=3.949522, accumulated_submission_time=26190.832530, global_step=80484, preemption_count=0, score=26190.832530, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276001, test/num_examples=43793, total_duration=40436.926089, train/accuracy=0.995473, train/loss=0.014099, train/mean_average_precision=0.773413, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293204, validation/num_examples=43793
I0305 21:33:09.015965 139776159401728 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.14784134924411774, loss=0.018021153286099434
I0305 21:33:41.286433 139768268125952 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.138413667678833, loss=0.018668629229068756
I0305 21:34:13.655000 139776159401728 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.14459741115570068, loss=0.01742873713374138
I0305 21:34:46.065920 139768268125952 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.13929538428783417, loss=0.01801118813455105
I0305 21:35:18.313873 139776159401728 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.13498839735984802, loss=0.01869087852537632
I0305 21:35:50.378892 139768268125952 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.1402920037508011, loss=0.017371585592627525
I0305 21:36:22.836365 139776159401728 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.1358935534954071, loss=0.01760176755487919
I0305 21:36:55.201431 139768268125952 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.14384756982326508, loss=0.018140949308872223
I0305 21:37:03.600466 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:38:53.875793 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:38:56.950926 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:39:00.054632 139937033598784 submission_runner.py:411] Time since start: 40793.53s, 	Step: 81227, 	{'train/accuracy': 0.9955683350563049, 'train/loss': 0.013863898813724518, 'train/mean_average_precision': 0.7826635520548781, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29331643558968223, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759893265669187, 'test/num_examples': 43793, 'score': 26430.913827180862, 'total_duration': 40793.525622844696, 'accumulated_submission_time': 26430.913827180862, 'accumulated_eval_time': 14356.332607507706, 'accumulated_logging_time': 3.993171215057373}
I0305 21:39:00.085479 139769339950848 logging_writer.py:48] [81227] accumulated_eval_time=14356.332608, accumulated_logging_time=3.993171, accumulated_submission_time=26430.913827, global_step=81227, preemption_count=0, score=26430.913827, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275989, test/num_examples=43793, total_duration=40793.525623, train/accuracy=0.995568, train/loss=0.013864, train/mean_average_precision=0.782664, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293316, validation/num_examples=43793
I0305 21:39:24.207299 139776167794432 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.13030558824539185, loss=0.01722715049982071
I0305 21:39:56.757747 139769339950848 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.13538430631160736, loss=0.016823353245854378
I0305 21:40:29.215229 139776167794432 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.14453570544719696, loss=0.017132040113210678
I0305 21:41:01.623907 139769339950848 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.14510202407836914, loss=0.018535640090703964
I0305 21:41:34.088469 139776167794432 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.15021254122257233, loss=0.019671805202960968
I0305 21:42:06.792278 139769339950848 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.15536203980445862, loss=0.018534168601036072
I0305 21:42:39.540096 139776167794432 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.15465863049030304, loss=0.01821240223944187
I0305 21:43:00.283039 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:44:58.594766 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:45:01.702347 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:45:04.721227 139937033598784 submission_runner.py:411] Time since start: 41158.19s, 	Step: 81964, 	{'train/accuracy': 0.9954887628555298, 'train/loss': 0.014108401723206043, 'train/mean_average_precision': 0.7777829012838969, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932682244947826, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27588795117723025, 'test/num_examples': 43793, 'score': 26671.080646276474, 'total_duration': 41158.192217350006, 'accumulated_submission_time': 26671.080646276474, 'accumulated_eval_time': 14480.77075123787, 'accumulated_logging_time': 4.034946918487549}
I0305 21:45:04.752310 139768268125952 logging_writer.py:48] [81964] accumulated_eval_time=14480.770751, accumulated_logging_time=4.034947, accumulated_submission_time=26671.080646, global_step=81964, preemption_count=0, score=26671.080646, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275888, test/num_examples=43793, total_duration=41158.192217, train/accuracy=0.995489, train/loss=0.014108, train/mean_average_precision=0.777783, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293268, validation/num_examples=43793
I0305 21:45:16.872156 139776159401728 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.1325272172689438, loss=0.01757100038230419
I0305 21:45:49.290182 139768268125952 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.13474754989147186, loss=0.018058303743600845
I0305 21:46:22.429383 139776159401728 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.13582558929920197, loss=0.017677173018455505
I0305 21:46:56.306435 139768268125952 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.14458073675632477, loss=0.018761159852147102
I0305 21:47:29.877465 139776159401728 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.14584235846996307, loss=0.01948447711765766
I0305 21:48:03.903031 139768268125952 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.13111430406570435, loss=0.01741667091846466
I0305 21:48:37.190902 139776159401728 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.12859033048152924, loss=0.017192207276821136
I0305 21:49:04.932116 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:50:57.600343 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:51:00.707069 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:51:03.820267 139937033598784 submission_runner.py:411] Time since start: 41517.29s, 	Step: 82684, 	{'train/accuracy': 0.9954996109008789, 'train/loss': 0.01404161099344492, 'train/mean_average_precision': 0.773361696380792, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29326462150505195, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27598772603870264, 'test/num_examples': 43793, 'score': 26911.22525882721, 'total_duration': 41517.2912569046, 'accumulated_submission_time': 26911.22525882721, 'accumulated_eval_time': 14599.658869504929, 'accumulated_logging_time': 4.078474044799805}
I0305 21:51:03.851373 139760711874304 logging_writer.py:48] [82684] accumulated_eval_time=14599.658870, accumulated_logging_time=4.078474, accumulated_submission_time=26911.225259, global_step=82684, preemption_count=0, score=26911.225259, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275988, test/num_examples=43793, total_duration=41517.291257, train/accuracy=0.995500, train/loss=0.014042, train/mean_average_precision=0.773362, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293265, validation/num_examples=43793
I0305 21:51:09.698732 139776167794432 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.13170714676380157, loss=0.01717180572450161
I0305 21:51:42.490204 139760711874304 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.12986081838607788, loss=0.016382021829485893
I0305 21:52:14.900865 139776167794432 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.1508103460073471, loss=0.01860651932656765
I0305 21:52:47.146737 139760711874304 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.14486125111579895, loss=0.018771080300211906
I0305 21:53:19.584367 139776167794432 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.13283297419548035, loss=0.01614772528409958
I0305 21:53:51.962205 139760711874304 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.1425694078207016, loss=0.018815720453858376
I0305 21:54:24.514415 139776167794432 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.14256651699543, loss=0.018910864368081093
I0305 21:54:56.562527 139760711874304 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.12882059812545776, loss=0.017889011651277542
I0305 21:55:04.127091 139937033598784 spec.py:321] Evaluating on the training split.
I0305 21:56:58.273137 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 21:57:01.473583 139937033598784 spec.py:349] Evaluating on the test split.
I0305 21:57:04.529305 139937033598784 submission_runner.py:411] Time since start: 41878.00s, 	Step: 83424, 	{'train/accuracy': 0.9954760074615479, 'train/loss': 0.014135552570223808, 'train/mean_average_precision': 0.7666465209970005, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931580788234526, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27596034965604593, 'test/num_examples': 43793, 'score': 27151.46985912323, 'total_duration': 41878.00029158592, 'accumulated_submission_time': 27151.46985912323, 'accumulated_eval_time': 14720.061032772064, 'accumulated_logging_time': 4.120722055435181}
I0305 21:57:04.561560 139769339950848 logging_writer.py:48] [83424] accumulated_eval_time=14720.061033, accumulated_logging_time=4.120722, accumulated_submission_time=27151.469859, global_step=83424, preemption_count=0, score=27151.469859, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275960, test/num_examples=43793, total_duration=41878.000292, train/accuracy=0.995476, train/loss=0.014136, train/mean_average_precision=0.766647, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293158, validation/num_examples=43793
I0305 21:57:29.550993 139776159401728 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.14336581528186798, loss=0.017854053527116776
I0305 21:58:02.222808 139769339950848 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.1354609876871109, loss=0.018206896260380745
I0305 21:58:34.395101 139776159401728 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.15808507800102234, loss=0.01946062594652176
I0305 21:59:06.877560 139769339950848 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.13850274682044983, loss=0.01869956962764263
I0305 21:59:39.500494 139776159401728 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.14356908202171326, loss=0.020836571231484413
I0305 22:00:12.285702 139769339950848 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.15149837732315063, loss=0.019559221342206
I0305 22:00:44.693841 139776159401728 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.13857771456241608, loss=0.01975032314658165
I0305 22:01:04.592450 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:02:57.643419 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:03:00.823963 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:03:03.922319 139937033598784 submission_runner.py:411] Time since start: 42237.39s, 	Step: 84163, 	{'train/accuracy': 0.9954543709754944, 'train/loss': 0.014166614972054958, 'train/mean_average_precision': 0.7774010441861712, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29323892383021394, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759228082006909, 'test/num_examples': 43793, 'score': 27391.469383001328, 'total_duration': 42237.39330744743, 'accumulated_submission_time': 27391.469383001328, 'accumulated_eval_time': 14839.390854358673, 'accumulated_logging_time': 4.164043664932251}
I0305 22:03:03.953588 139768268125952 logging_writer.py:48] [84163] accumulated_eval_time=14839.390854, accumulated_logging_time=4.164044, accumulated_submission_time=27391.469383, global_step=84163, preemption_count=0, score=27391.469383, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275923, test/num_examples=43793, total_duration=42237.393307, train/accuracy=0.995454, train/loss=0.014167, train/mean_average_precision=0.777401, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293239, validation/num_examples=43793
I0305 22:03:16.182648 139776167794432 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.15812577307224274, loss=0.019831769168376923
I0305 22:03:48.504148 139768268125952 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.1406126767396927, loss=0.019550934433937073
I0305 22:04:20.878056 139776167794432 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.14219681918621063, loss=0.01785862073302269
I0305 22:04:53.161936 139768268125952 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.13512219488620758, loss=0.016189174726605415
I0305 22:05:26.381828 139776167794432 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.1449674367904663, loss=0.019199034199118614
I0305 22:05:59.924207 139768268125952 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.15414944291114807, loss=0.018479259684681892
I0305 22:06:33.816655 139776167794432 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.13591088354587555, loss=0.018759794533252716
I0305 22:07:03.956991 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:08:56.741277 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:08:59.797713 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:09:02.933493 139937033598784 submission_runner.py:411] Time since start: 42596.40s, 	Step: 84890, 	{'train/accuracy': 0.9955111145973206, 'train/loss': 0.014026428572833538, 'train/mean_average_precision': 0.7820682909038227, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29331241818515863, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27595444500250044, 'test/num_examples': 43793, 'score': 27631.439783096313, 'total_duration': 42596.40437030792, 'accumulated_submission_time': 27631.439783096313, 'accumulated_eval_time': 14958.367205619812, 'accumulated_logging_time': 4.2065746784210205}
I0305 22:09:02.965182 139760711874304 logging_writer.py:48] [84890] accumulated_eval_time=14958.367206, accumulated_logging_time=4.206575, accumulated_submission_time=27631.439783, global_step=84890, preemption_count=0, score=27631.439783, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275954, test/num_examples=43793, total_duration=42596.404370, train/accuracy=0.995511, train/loss=0.014026, train/mean_average_precision=0.782068, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293312, validation/num_examples=43793
I0305 22:09:06.677342 139769339950848 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.1456514596939087, loss=0.01885366439819336
I0305 22:09:39.382311 139760711874304 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.146737739443779, loss=0.01786746457219124
I0305 22:10:12.220071 139769339950848 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.16404350101947784, loss=0.01966708153486252
I0305 22:10:44.270017 139760711874304 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.1335027664899826, loss=0.01808094047009945
I0305 22:11:16.625051 139769339950848 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.1468828022480011, loss=0.017477288842201233
I0305 22:11:48.948335 139760711874304 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.13735395669937134, loss=0.018733864650130272
I0305 22:12:22.221349 139769339950848 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.1413298100233078, loss=0.018459733575582504
I0305 22:12:56.311407 139760711874304 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.11797129362821579, loss=0.014262203127145767
I0305 22:13:03.203701 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:14:59.747303 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:15:02.996517 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:15:06.032133 139937033598784 submission_runner.py:411] Time since start: 42959.50s, 	Step: 85621, 	{'train/accuracy': 0.9955190420150757, 'train/loss': 0.013955594971776009, 'train/mean_average_precision': 0.7841828855297852, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931340940827518, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759841329736199, 'test/num_examples': 43793, 'score': 27871.646685361862, 'total_duration': 42959.503124952316, 'accumulated_submission_time': 27871.646685361862, 'accumulated_eval_time': 15081.195603370667, 'accumulated_logging_time': 4.2489824295043945}
I0305 22:15:06.064111 139776159401728 logging_writer.py:48] [85621] accumulated_eval_time=15081.195603, accumulated_logging_time=4.248982, accumulated_submission_time=27871.646685, global_step=85621, preemption_count=0, score=27871.646685, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275984, test/num_examples=43793, total_duration=42959.503125, train/accuracy=0.995519, train/loss=0.013956, train/mean_average_precision=0.784183, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293134, validation/num_examples=43793
I0305 22:15:32.538813 139776167794432 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.12437000870704651, loss=0.016746170818805695
I0305 22:16:05.550819 139776159401728 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.14901982247829437, loss=0.018064085394144058
I0305 22:16:38.157515 139776167794432 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.14603212475776672, loss=0.020196547731757164
I0305 22:17:10.788091 139776159401728 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.13461081683635712, loss=0.018071357160806656
I0305 22:17:43.660473 139776167794432 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.13848279416561127, loss=0.017685187980532646
I0305 22:18:17.267393 139776159401728 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.13961420953273773, loss=0.018240079283714294
I0305 22:18:51.368396 139776167794432 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.13884864747524261, loss=0.01925678737461567
I0305 22:19:06.089959 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:21:04.715518 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:21:08.215561 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:21:11.694694 139937033598784 submission_runner.py:411] Time since start: 43325.17s, 	Step: 86345, 	{'train/accuracy': 0.9955059885978699, 'train/loss': 0.014062725938856602, 'train/mean_average_precision': 0.772074521198249, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932729704005158, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.275911657756955, 'test/num_examples': 43793, 'score': 28111.639726161957, 'total_duration': 43325.16566514969, 'accumulated_submission_time': 28111.639726161957, 'accumulated_eval_time': 15206.80028629303, 'accumulated_logging_time': 4.29235053062439}
I0305 22:21:11.732385 139760711874304 logging_writer.py:48] [86345] accumulated_eval_time=15206.800286, accumulated_logging_time=4.292351, accumulated_submission_time=28111.639726, global_step=86345, preemption_count=0, score=28111.639726, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275912, test/num_examples=43793, total_duration=43325.165665, train/accuracy=0.995506, train/loss=0.014063, train/mean_average_precision=0.772075, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293273, validation/num_examples=43793
I0305 22:21:30.713495 139769339950848 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.14712205529212952, loss=0.01670728623867035
I0305 22:22:04.027392 139760711874304 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.1582515686750412, loss=0.01768835075199604
I0305 22:22:36.866716 139769339950848 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.12603454291820526, loss=0.01759301871061325
I0305 22:23:09.820126 139760711874304 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.12895594537258148, loss=0.01617703028023243
I0305 22:23:42.874073 139769339950848 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.15762090682983398, loss=0.01825697161257267
I0305 22:24:15.765064 139760711874304 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.14827287197113037, loss=0.017287883907556534
I0305 22:24:47.983212 139769339950848 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.14450864493846893, loss=0.016857469454407692
I0305 22:25:11.931024 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:27:03.700661 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:27:07.118516 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:27:10.568694 139937033598784 submission_runner.py:411] Time since start: 43684.04s, 	Step: 87074, 	{'train/accuracy': 0.9954939484596252, 'train/loss': 0.014098945073783398, 'train/mean_average_precision': 0.7701163354328072, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29309989003867665, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27602271256168415, 'test/num_examples': 43793, 'score': 28351.80605864525, 'total_duration': 43684.0396668911, 'accumulated_submission_time': 28351.80605864525, 'accumulated_eval_time': 15325.437908411026, 'accumulated_logging_time': 4.341654539108276}
I0305 22:27:10.605650 139768268125952 logging_writer.py:48] [87074] accumulated_eval_time=15325.437908, accumulated_logging_time=4.341655, accumulated_submission_time=28351.806059, global_step=87074, preemption_count=0, score=28351.806059, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276023, test/num_examples=43793, total_duration=43684.039667, train/accuracy=0.995494, train/loss=0.014099, train/mean_average_precision=0.770116, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293100, validation/num_examples=43793
I0305 22:27:20.341600 139776167794432 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.14955221116542816, loss=0.019141940400004387
I0305 22:27:54.313724 139768268125952 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.14328162372112274, loss=0.01874668523669243
I0305 22:28:27.629555 139776167794432 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.15977416932582855, loss=0.020932363346219063
I0305 22:29:00.378708 139768268125952 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.1288207322359085, loss=0.0177936814725399
I0305 22:29:32.941622 139776167794432 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.1631850004196167, loss=0.017636364325881004
I0305 22:30:05.615897 139768268125952 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.13484519720077515, loss=0.01769205927848816
I0305 22:30:38.228917 139776167794432 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.13871577382087708, loss=0.018912797793745995
I0305 22:31:10.743958 139768268125952 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.1413842886686325, loss=0.016079267486929893
I0305 22:31:10.749273 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:33:02.223917 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:33:05.349205 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:33:08.355036 139937033598784 submission_runner.py:411] Time since start: 44041.83s, 	Step: 87801, 	{'train/accuracy': 0.9954929947853088, 'train/loss': 0.014130521565675735, 'train/mean_average_precision': 0.771353052064392, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932041771177117, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27590603625788973, 'test/num_examples': 43793, 'score': 28591.916484594345, 'total_duration': 44041.82602286339, 'accumulated_submission_time': 28591.916484594345, 'accumulated_eval_time': 15443.043601989746, 'accumulated_logging_time': 4.391093969345093}
I0305 22:33:08.386935 139769339950848 logging_writer.py:48] [87801] accumulated_eval_time=15443.043602, accumulated_logging_time=4.391094, accumulated_submission_time=28591.916485, global_step=87801, preemption_count=0, score=28591.916485, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275906, test/num_examples=43793, total_duration=44041.826023, train/accuracy=0.995493, train/loss=0.014131, train/mean_average_precision=0.771353, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293204, validation/num_examples=43793
I0305 22:33:41.285144 139776159401728 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.13670843839645386, loss=0.0174925085157156
I0305 22:34:13.988293 139769339950848 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.14887697994709015, loss=0.01953139901161194
I0305 22:34:46.598350 139776159401728 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.14104080200195312, loss=0.018800007179379463
I0305 22:35:19.094335 139769339950848 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.15402349829673767, loss=0.020146189257502556
I0305 22:35:52.719619 139776159401728 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.15568646788597107, loss=0.018672427162528038
I0305 22:36:25.991023 139769339950848 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.12229537218809128, loss=0.017216503620147705
I0305 22:36:58.182321 139776159401728 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.14871278405189514, loss=0.018672028556466103
I0305 22:37:08.377508 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:38:55.457541 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:38:58.978890 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:39:02.444963 139937033598784 submission_runner.py:411] Time since start: 44395.92s, 	Step: 88532, 	{'train/accuracy': 0.9954800009727478, 'train/loss': 0.01409502886235714, 'train/mean_average_precision': 0.781205735501981, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29323697358572914, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759228496513186, 'test/num_examples': 43793, 'score': 28831.876116752625, 'total_duration': 44395.915930986404, 'accumulated_submission_time': 28831.876116752625, 'accumulated_eval_time': 15557.111010789871, 'accumulated_logging_time': 4.433941125869751}
I0305 22:39:02.477590 139768268125952 logging_writer.py:48] [88532] accumulated_eval_time=15557.111011, accumulated_logging_time=4.433941, accumulated_submission_time=28831.876117, global_step=88532, preemption_count=0, score=28831.876117, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275923, test/num_examples=43793, total_duration=44395.915931, train/accuracy=0.995480, train/loss=0.014095, train/mean_average_precision=0.781206, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293237, validation/num_examples=43793
I0305 22:39:24.798502 139776167794432 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.15132857859134674, loss=0.01803951896727085
I0305 22:39:56.848042 139768268125952 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.14299894869327545, loss=0.017934145405888557
I0305 22:40:28.814286 139776167794432 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.1467738151550293, loss=0.01708766631782055
I0305 22:41:01.215224 139768268125952 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.12316730618476868, loss=0.015887072309851646
I0305 22:41:33.826080 139776167794432 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.13180693984031677, loss=0.018195923417806625
I0305 22:42:06.338314 139768268125952 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.14458231627941132, loss=0.01840391755104065
I0305 22:42:38.863740 139776167794432 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.1352274864912033, loss=0.017800364643335342
I0305 22:43:02.594877 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:44:55.745880 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:44:58.921194 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:45:02.068422 139937033598784 submission_runner.py:411] Time since start: 44755.54s, 	Step: 89274, 	{'train/accuracy': 0.9955880641937256, 'train/loss': 0.013813593424856663, 'train/mean_average_precision': 0.7830603803705166, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321985759029223, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2758838902077703, 'test/num_examples': 43793, 'score': 29071.96212220192, 'total_duration': 44755.5393948555, 'accumulated_submission_time': 29071.96212220192, 'accumulated_eval_time': 15676.584495544434, 'accumulated_logging_time': 4.4775614738464355}
I0305 22:45:02.107728 139760711874304 logging_writer.py:48] [89274] accumulated_eval_time=15676.584496, accumulated_logging_time=4.477561, accumulated_submission_time=29071.962122, global_step=89274, preemption_count=0, score=29071.962122, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275884, test/num_examples=43793, total_duration=44755.539395, train/accuracy=0.995588, train/loss=0.013814, train/mean_average_precision=0.783060, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293220, validation/num_examples=43793
I0305 22:45:10.798124 139769339950848 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.13311177492141724, loss=0.017470132559537888
I0305 22:45:42.959863 139760711874304 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.1276167929172516, loss=0.017984772101044655
I0305 22:46:15.241257 139769339950848 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.13038240373134613, loss=0.017525138333439827
I0305 22:46:47.777417 139760711874304 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.1607196033000946, loss=0.019191890954971313
I0305 22:47:20.694957 139769339950848 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.12795740365982056, loss=0.015529652126133442
I0305 22:47:53.451313 139760711874304 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.15252511203289032, loss=0.020816324278712273
I0305 22:48:26.968672 139769339950848 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.151305690407753, loss=0.019274475052952766
I0305 22:49:00.790467 139760711874304 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.134888157248497, loss=0.017857573926448822
I0305 22:49:02.257588 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:50:52.362223 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:50:55.465644 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:50:58.509599 139937033598784 submission_runner.py:411] Time since start: 45111.98s, 	Step: 90005, 	{'train/accuracy': 0.9954330921173096, 'train/loss': 0.014143915846943855, 'train/mean_average_precision': 0.7768471852559291, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29322482257373067, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27589049806452043, 'test/num_examples': 43793, 'score': 29312.07736825943, 'total_duration': 45111.980585575104, 'accumulated_submission_time': 29312.07736825943, 'accumulated_eval_time': 15792.836474895477, 'accumulated_logging_time': 4.530365467071533}
I0305 22:50:58.543094 139768268125952 logging_writer.py:48] [90005] accumulated_eval_time=15792.836475, accumulated_logging_time=4.530365, accumulated_submission_time=29312.077368, global_step=90005, preemption_count=0, score=29312.077368, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275890, test/num_examples=43793, total_duration=45111.980586, train/accuracy=0.995433, train/loss=0.014144, train/mean_average_precision=0.776847, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293225, validation/num_examples=43793
I0305 22:51:29.491178 139776167794432 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.14727844297885895, loss=0.018112584948539734
I0305 22:52:01.815513 139768268125952 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.12809042632579803, loss=0.018352186307311058
I0305 22:52:33.928678 139776167794432 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.13274069130420685, loss=0.017588863149285316
I0305 22:53:05.925553 139768268125952 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.1296221762895584, loss=0.01714698038995266
I0305 22:53:37.801942 139776167794432 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.15108707547187805, loss=0.018986888229846954
I0305 22:54:10.524886 139768268125952 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.15056775510311127, loss=0.0172689501196146
I0305 22:54:44.033015 139776167794432 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.15008273720741272, loss=0.018054774031043053
I0305 22:54:58.646277 139937033598784 spec.py:321] Evaluating on the training split.
I0305 22:56:55.743687 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 22:56:59.192159 139937033598784 spec.py:349] Evaluating on the test split.
I0305 22:57:02.667365 139937033598784 submission_runner.py:411] Time since start: 45476.14s, 	Step: 90745, 	{'train/accuracy': 0.9954633712768555, 'train/loss': 0.014169501140713692, 'train/mean_average_precision': 0.7759647711457989, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2933183661124961, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27598022454586935, 'test/num_examples': 43793, 'score': 29552.14755630493, 'total_duration': 45476.13833808899, 'accumulated_submission_time': 29552.14755630493, 'accumulated_eval_time': 15916.857501029968, 'accumulated_logging_time': 4.576200008392334}
I0305 22:57:02.705488 139760711874304 logging_writer.py:48] [90745] accumulated_eval_time=15916.857501, accumulated_logging_time=4.576200, accumulated_submission_time=29552.147556, global_step=90745, preemption_count=0, score=29552.147556, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275980, test/num_examples=43793, total_duration=45476.138338, train/accuracy=0.995463, train/loss=0.014170, train/mean_average_precision=0.775965, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293318, validation/num_examples=43793
I0305 22:57:21.676541 139769339950848 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.1296587586402893, loss=0.01662474125623703
I0305 22:57:54.847698 139760711874304 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.15577061474323273, loss=0.019481239840388298
I0305 22:58:28.207489 139769339950848 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.1442810595035553, loss=0.018014082685112953
I0305 22:59:01.330280 139760711874304 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.15603983402252197, loss=0.01861381158232689
I0305 22:59:33.691936 139769339950848 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.1395702362060547, loss=0.019597316160798073
I0305 23:00:05.678995 139760711874304 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.13528724014759064, loss=0.018343087285757065
I0305 23:00:37.535598 139769339950848 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.12975484132766724, loss=0.01667661964893341
I0305 23:01:02.897592 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:02:53.157902 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:02:57.832565 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:03:00.849557 139937033598784 submission_runner.py:411] Time since start: 45834.32s, 	Step: 91480, 	{'train/accuracy': 0.9955159425735474, 'train/loss': 0.014062237925827503, 'train/mean_average_precision': 0.7617297949769014, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932435573506383, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27607639370919773, 'test/num_examples': 43793, 'score': 29792.304618120193, 'total_duration': 45834.32055068016, 'accumulated_submission_time': 29792.304618120193, 'accumulated_eval_time': 16034.809426784515, 'accumulated_logging_time': 4.627282619476318}
I0305 23:03:00.882954 139768268125952 logging_writer.py:48] [91480] accumulated_eval_time=16034.809427, accumulated_logging_time=4.627283, accumulated_submission_time=29792.304618, global_step=91480, preemption_count=0, score=29792.304618, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276076, test/num_examples=43793, total_duration=45834.320551, train/accuracy=0.995516, train/loss=0.014062, train/mean_average_precision=0.761730, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293244, validation/num_examples=43793
I0305 23:03:07.863521 139776167794432 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.15038014948368073, loss=0.01809365674853325
I0305 23:03:40.061480 139768268125952 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.13742315769195557, loss=0.01719978265464306
I0305 23:04:12.325714 139776167794432 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.1560305804014206, loss=0.016631564125418663
I0305 23:04:44.522661 139768268125952 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.14799702167510986, loss=0.01748589612543583
I0305 23:05:17.070810 139776167794432 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.12404870986938477, loss=0.01770690269768238
I0305 23:05:49.211353 139768268125952 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.13559271395206451, loss=0.01809694431722164
I0305 23:06:21.629527 139776167794432 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.13496214151382446, loss=0.016199983656406403
I0305 23:06:53.843018 139768268125952 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.14255137741565704, loss=0.018173228949308395
I0305 23:07:00.952059 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:08:45.633926 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:08:48.671415 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:08:51.669130 139937033598784 submission_runner.py:411] Time since start: 46185.14s, 	Step: 92223, 	{'train/accuracy': 0.9955058097839355, 'train/loss': 0.014057345688343048, 'train/mean_average_precision': 0.7794176027937787, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29309124887214016, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759261753171792, 'test/num_examples': 43793, 'score': 30032.342000246048, 'total_duration': 46185.14011597633, 'accumulated_submission_time': 30032.342000246048, 'accumulated_eval_time': 16145.526446580887, 'accumulated_logging_time': 4.671795606613159}
I0305 23:08:51.702559 139769339950848 logging_writer.py:48] [92223] accumulated_eval_time=16145.526447, accumulated_logging_time=4.671796, accumulated_submission_time=30032.342000, global_step=92223, preemption_count=0, score=30032.342000, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275926, test/num_examples=43793, total_duration=46185.140116, train/accuracy=0.995506, train/loss=0.014057, train/mean_average_precision=0.779418, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293091, validation/num_examples=43793
I0305 23:09:16.664779 139776159401728 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.15260367095470428, loss=0.017639823257923126
I0305 23:09:48.508167 139769339950848 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.13424181938171387, loss=0.017378132790327072
I0305 23:10:20.625880 139776159401728 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.15172244608402252, loss=0.018073247745633125
I0305 23:10:52.566774 139769339950848 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.1551806628704071, loss=0.020676063373684883
I0305 23:11:24.799625 139776159401728 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.1557525098323822, loss=0.018636582419276237
I0305 23:11:56.704414 139769339950848 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.12679706513881683, loss=0.016222339123487473
I0305 23:12:29.010245 139776159401728 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.1376492977142334, loss=0.017718326300382614
I0305 23:12:51.899374 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:14:40.176436 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:14:43.232359 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:14:46.217141 139937033598784 submission_runner.py:411] Time since start: 46539.69s, 	Step: 92972, 	{'train/accuracy': 0.9955204129219055, 'train/loss': 0.013932582922279835, 'train/mean_average_precision': 0.7852867760278921, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931630314705667, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760432220871991, 'test/num_examples': 43793, 'score': 30272.507484912872, 'total_duration': 46539.68813109398, 'accumulated_submission_time': 30272.507484912872, 'accumulated_eval_time': 16259.84417271614, 'accumulated_logging_time': 4.716129779815674}
I0305 23:14:46.249957 139760711874304 logging_writer.py:48] [92972] accumulated_eval_time=16259.844173, accumulated_logging_time=4.716130, accumulated_submission_time=30272.507485, global_step=92972, preemption_count=0, score=30272.507485, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276043, test/num_examples=43793, total_duration=46539.688131, train/accuracy=0.995520, train/loss=0.013933, train/mean_average_precision=0.785287, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293163, validation/num_examples=43793
I0305 23:14:55.521602 139768268125952 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.13732846081256866, loss=0.01827751100063324
I0305 23:15:28.115766 139760711874304 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.13403679430484772, loss=0.016133079305291176
I0305 23:16:00.148037 139768268125952 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.14637398719787598, loss=0.016468524932861328
I0305 23:16:32.256675 139760711874304 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.14683756232261658, loss=0.016549231484532356
I0305 23:17:04.611621 139768268125952 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.14060190320014954, loss=0.016001364216208458
I0305 23:17:36.615442 139760711874304 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.13209663331508636, loss=0.016748301684856415
I0305 23:18:08.802491 139768268125952 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.15236115455627441, loss=0.018969012424349785
I0305 23:18:40.808078 139760711874304 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.14240789413452148, loss=0.017064223065972328
I0305 23:18:46.218376 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:20:42.745493 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:20:45.845722 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:20:48.885900 139937033598784 submission_runner.py:411] Time since start: 46902.36s, 	Step: 93718, 	{'train/accuracy': 0.9955332279205322, 'train/loss': 0.01397078949958086, 'train/mean_average_precision': 0.7733520647228189, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29317287703299777, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27604873602505775, 'test/num_examples': 43793, 'score': 30512.441433429718, 'total_duration': 46902.35689115524, 'accumulated_submission_time': 30512.441433429718, 'accumulated_eval_time': 16382.511649608612, 'accumulated_logging_time': 4.762882471084595}
I0305 23:20:48.919019 139769339950848 logging_writer.py:48] [93718] accumulated_eval_time=16382.511650, accumulated_logging_time=4.762882, accumulated_submission_time=30512.441433, global_step=93718, preemption_count=0, score=30512.441433, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276049, test/num_examples=43793, total_duration=46902.356891, train/accuracy=0.995533, train/loss=0.013971, train/mean_average_precision=0.773352, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293173, validation/num_examples=43793
I0305 23:21:15.919459 139776167794432 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.13677456974983215, loss=0.015881607308983803
I0305 23:21:48.179063 139769339950848 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.13505125045776367, loss=0.017525564879179
I0305 23:22:20.303128 139776167794432 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.1502072513103485, loss=0.01989121362566948
I0305 23:22:52.159201 139769339950848 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.13749071955680847, loss=0.01830221153795719
I0305 23:23:25.583935 139776167794432 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.1409139782190323, loss=0.017070073634386063
I0305 23:23:58.813348 139769339950848 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.1383126676082611, loss=0.019385607913136482
I0305 23:24:31.742754 139776167794432 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.14195628464221954, loss=0.017177419736981392
I0305 23:24:49.044238 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:26:38.371744 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:26:41.505107 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:26:44.572586 139937033598784 submission_runner.py:411] Time since start: 47258.04s, 	Step: 94453, 	{'train/accuracy': 0.9954761266708374, 'train/loss': 0.01412392407655716, 'train/mean_average_precision': 0.7818585828536757, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932003254588689, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2758970474532101, 'test/num_examples': 43793, 'score': 30752.5343811512, 'total_duration': 47258.04355049133, 'accumulated_submission_time': 30752.5343811512, 'accumulated_eval_time': 16498.03994011879, 'accumulated_logging_time': 4.8067238330841064}
I0305 23:26:44.606221 139768268125952 logging_writer.py:48] [94453] accumulated_eval_time=16498.039940, accumulated_logging_time=4.806724, accumulated_submission_time=30752.534381, global_step=94453, preemption_count=0, score=30752.534381, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275897, test/num_examples=43793, total_duration=47258.043550, train/accuracy=0.995476, train/loss=0.014124, train/mean_average_precision=0.781859, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293200, validation/num_examples=43793
I0305 23:27:00.284428 139776159401728 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.1551995724439621, loss=0.01934858039021492
I0305 23:27:33.356331 139768268125952 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.13711684942245483, loss=0.017319178208708763
I0305 23:28:06.010540 139776159401728 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.1359526365995407, loss=0.01720392517745495
I0305 23:28:39.815438 139768268125952 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.1378546953201294, loss=0.016132356598973274
I0305 23:29:12.415631 139776159401728 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.1230020746588707, loss=0.015107166022062302
I0305 23:29:44.745393 139768268125952 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.13677386939525604, loss=0.01653893105685711
I0305 23:30:17.571864 139776159401728 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.14703185856342316, loss=0.01789839193224907
I0305 23:30:44.723229 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:32:31.879015 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:32:34.915050 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:32:37.890328 139937033598784 submission_runner.py:411] Time since start: 47611.36s, 	Step: 95184, 	{'train/accuracy': 0.9954842329025269, 'train/loss': 0.014087477698922157, 'train/mean_average_precision': 0.7673942179190336, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29318389003807044, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27599644478181057, 'test/num_examples': 43793, 'score': 30992.618040323257, 'total_duration': 47611.36131834984, 'accumulated_submission_time': 30992.618040323257, 'accumulated_eval_time': 16611.206999063492, 'accumulated_logging_time': 4.85191011428833}
I0305 23:32:37.923923 139769339950848 logging_writer.py:48] [95184] accumulated_eval_time=16611.206999, accumulated_logging_time=4.851910, accumulated_submission_time=30992.618040, global_step=95184, preemption_count=0, score=30992.618040, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275996, test/num_examples=43793, total_duration=47611.361318, train/accuracy=0.995484, train/loss=0.014087, train/mean_average_precision=0.767394, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293184, validation/num_examples=43793
I0305 23:32:43.470408 139776167794432 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.15986695885658264, loss=0.017734797671437263
I0305 23:33:16.137393 139769339950848 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.1340687870979309, loss=0.017658179625868797
I0305 23:33:48.405635 139776167794432 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.16389746963977814, loss=0.01642049290239811
I0305 23:34:20.768113 139769339950848 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.12794539332389832, loss=0.015988990664482117
I0305 23:34:52.906651 139776167794432 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.13235020637512207, loss=0.017266273498535156
I0305 23:35:25.333569 139769339950848 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.14820031821727753, loss=0.016727164387702942
I0305 23:35:57.854083 139776167794432 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.1381518691778183, loss=0.01666620559990406
I0305 23:36:30.788166 139769339950848 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.146476611495018, loss=0.016958089545369148
I0305 23:36:38.200716 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:38:31.485798 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:38:34.550096 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:38:37.636440 139937033598784 submission_runner.py:411] Time since start: 47971.11s, 	Step: 95924, 	{'train/accuracy': 0.9954838156700134, 'train/loss': 0.014161848463118076, 'train/mean_average_precision': 0.7696215638128405, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29318711209121273, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759322101408783, 'test/num_examples': 43793, 'score': 31232.86323785782, 'total_duration': 47971.10742545128, 'accumulated_submission_time': 31232.86323785782, 'accumulated_eval_time': 16730.642671346664, 'accumulated_logging_time': 4.896559953689575}
I0305 23:38:37.673215 139768268125952 logging_writer.py:48] [95924] accumulated_eval_time=16730.642671, accumulated_logging_time=4.896560, accumulated_submission_time=31232.863238, global_step=95924, preemption_count=0, score=31232.863238, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275932, test/num_examples=43793, total_duration=47971.107425, train/accuracy=0.995484, train/loss=0.014162, train/mean_average_precision=0.769622, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293187, validation/num_examples=43793
I0305 23:39:02.747090 139776159401728 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.14392513036727905, loss=0.01706048659980297
I0305 23:39:36.374146 139768268125952 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.13515429198741913, loss=0.01726667396724224
I0305 23:40:08.931859 139776159401728 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.12766602635383606, loss=0.016557680442929268
I0305 23:40:41.150278 139768268125952 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.13557446002960205, loss=0.018805911764502525
I0305 23:41:13.398398 139776159401728 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.1420842409133911, loss=0.018573597073554993
I0305 23:41:45.977937 139768268125952 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.13977985084056854, loss=0.01650669239461422
I0305 23:42:18.871475 139776159401728 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.1463993340730667, loss=0.01924806647002697
I0305 23:42:37.962308 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:44:28.170343 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:44:32.867432 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:44:35.870703 139937033598784 submission_runner.py:411] Time since start: 48329.34s, 	Step: 96659, 	{'train/accuracy': 0.9955300688743591, 'train/loss': 0.013971513137221336, 'train/mean_average_precision': 0.7872416582912839, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29324672647801764, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27593970033395276, 'test/num_examples': 43793, 'score': 31473.12002182007, 'total_duration': 48329.34169435501, 'accumulated_submission_time': 31473.12002182007, 'accumulated_eval_time': 16848.551023483276, 'accumulated_logging_time': 4.945436477661133}
I0305 23:44:35.905262 139760711874304 logging_writer.py:48] [96659] accumulated_eval_time=16848.551023, accumulated_logging_time=4.945436, accumulated_submission_time=31473.120022, global_step=96659, preemption_count=0, score=31473.120022, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275940, test/num_examples=43793, total_duration=48329.341694, train/accuracy=0.995530, train/loss=0.013972, train/mean_average_precision=0.787242, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293247, validation/num_examples=43793
I0305 23:44:49.687362 139776167794432 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.14820292592048645, loss=0.018967628479003906
I0305 23:45:22.284707 139760711874304 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.1753249317407608, loss=0.02232268638908863
I0305 23:45:54.678205 139776167794432 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.14090996980667114, loss=0.019194750115275383
I0305 23:46:27.660426 139760711874304 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.1340678483247757, loss=0.017279284074902534
I0305 23:46:59.853943 139776167794432 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.17205674946308136, loss=0.017465323209762573
I0305 23:47:32.687094 139760711874304 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.13716275990009308, loss=0.01923437975347042
I0305 23:48:05.368938 139776167794432 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.1446678340435028, loss=0.018028417602181435
I0305 23:48:36.044378 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:50:22.356355 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:50:25.449378 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:50:28.421910 139937033598784 submission_runner.py:411] Time since start: 48681.89s, 	Step: 97395, 	{'train/accuracy': 0.9954854249954224, 'train/loss': 0.014052127487957478, 'train/mean_average_precision': 0.7760487820006098, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932641153462439, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27603009546171875, 'test/num_examples': 43793, 'score': 31713.225883245468, 'total_duration': 48681.89289999008, 'accumulated_submission_time': 31713.225883245468, 'accumulated_eval_time': 16960.928512334824, 'accumulated_logging_time': 4.992716550827026}
I0305 23:50:28.455955 139768268125952 logging_writer.py:48] [97395] accumulated_eval_time=16960.928512, accumulated_logging_time=4.992717, accumulated_submission_time=31713.225883, global_step=97395, preemption_count=0, score=31713.225883, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276030, test/num_examples=43793, total_duration=48681.892900, train/accuracy=0.995485, train/loss=0.014052, train/mean_average_precision=0.776049, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293264, validation/num_examples=43793
I0305 23:50:30.421886 139776159401728 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.1501258909702301, loss=0.018115321174263954
I0305 23:51:02.492448 139768268125952 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.14158995449543, loss=0.018170692026615143
I0305 23:51:35.227933 139776159401728 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.13074027001857758, loss=0.017151134088635445
I0305 23:52:07.718481 139768268125952 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.12944023311138153, loss=0.015352020040154457
I0305 23:52:39.692867 139776159401728 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.13293670117855072, loss=0.017929714173078537
I0305 23:53:11.827853 139768268125952 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.1351587325334549, loss=0.01680072583258152
I0305 23:53:44.051597 139776159401728 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.13490010797977448, loss=0.01798294112086296
I0305 23:54:16.499908 139768268125952 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.14594867825508118, loss=0.017409449443221092
I0305 23:54:28.643129 139937033598784 spec.py:321] Evaluating on the training split.
I0305 23:56:19.780487 139937033598784 spec.py:333] Evaluating on the validation split.
I0305 23:56:22.819692 139937033598784 spec.py:349] Evaluating on the test split.
I0305 23:56:25.811398 139937033598784 submission_runner.py:411] Time since start: 49039.28s, 	Step: 98139, 	{'train/accuracy': 0.99550861120224, 'train/loss': 0.014036486856639385, 'train/mean_average_precision': 0.775531835162473, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29308441332238366, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759224751185829, 'test/num_examples': 43793, 'score': 31953.380586862564, 'total_duration': 49039.28237915039, 'accumulated_submission_time': 31953.380586862564, 'accumulated_eval_time': 17078.096727132797, 'accumulated_logging_time': 5.03900408744812}
I0305 23:56:25.845192 139769339950848 logging_writer.py:48] [98139] accumulated_eval_time=17078.096727, accumulated_logging_time=5.039004, accumulated_submission_time=31953.380587, global_step=98139, preemption_count=0, score=31953.380587, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275922, test/num_examples=43793, total_duration=49039.282379, train/accuracy=0.995509, train/loss=0.014036, train/mean_average_precision=0.775532, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293084, validation/num_examples=43793
I0305 23:56:45.793670 139776167794432 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.1406325399875641, loss=0.01673264242708683
I0305 23:57:17.948185 139769339950848 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.14160796999931335, loss=0.018603861331939697
I0305 23:57:49.762845 139776167794432 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.14041085541248322, loss=0.016923116520047188
I0305 23:58:21.679371 139769339950848 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.14512290060520172, loss=0.021004673093557358
I0305 23:58:53.588225 139776167794432 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.13494423031806946, loss=0.01740693859755993
I0305 23:59:25.362278 139769339950848 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.14342159032821655, loss=0.017839543521404266
I0305 23:59:57.144652 139776167794432 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.16619354486465454, loss=0.01929723098874092
I0306 00:00:25.877291 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:02:11.640340 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:02:14.699656 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:02:17.701808 139937033598784 submission_runner.py:411] Time since start: 49391.17s, 	Step: 98892, 	{'train/accuracy': 0.9955294132232666, 'train/loss': 0.013975768350064754, 'train/mean_average_precision': 0.7690663991926115, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931506027499619, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27599397121038144, 'test/num_examples': 43793, 'score': 32193.38186430931, 'total_duration': 49391.1727848053, 'accumulated_submission_time': 32193.38186430931, 'accumulated_eval_time': 17189.921184539795, 'accumulated_logging_time': 5.08355450630188}
I0306 00:02:17.736028 139760711874304 logging_writer.py:48] [98892] accumulated_eval_time=17189.921185, accumulated_logging_time=5.083555, accumulated_submission_time=32193.381864, global_step=98892, preemption_count=0, score=32193.381864, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275994, test/num_examples=43793, total_duration=49391.172785, train/accuracy=0.995529, train/loss=0.013976, train/mean_average_precision=0.769066, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293151, validation/num_examples=43793
I0306 00:02:20.615324 139776159401728 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.13981853425502777, loss=0.01753954030573368
I0306 00:02:53.141874 139760711874304 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.13390791416168213, loss=0.016036368906497955
I0306 00:03:25.162087 139776159401728 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.1255030333995819, loss=0.016189614310860634
I0306 00:03:57.343039 139760711874304 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.14880552887916565, loss=0.01937786675989628
I0306 00:04:29.929104 139776159401728 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.14164544641971588, loss=0.019030572846531868
I0306 00:05:02.215758 139760711874304 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.14277565479278564, loss=0.01818273402750492
I0306 00:05:34.494396 139776159401728 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.1314917951822281, loss=0.017873050644993782
I0306 00:06:06.658035 139760711874304 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.13369879126548767, loss=0.014960821717977524
I0306 00:06:17.861775 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:08:05.188989 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:08:08.192057 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:08:11.170390 139937033598784 submission_runner.py:411] Time since start: 49744.64s, 	Step: 99635, 	{'train/accuracy': 0.9954495429992676, 'train/loss': 0.014205018058419228, 'train/mean_average_precision': 0.770540888307726, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932130341888327, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27589823405987746, 'test/num_examples': 43793, 'score': 32433.476836681366, 'total_duration': 49744.64137840271, 'accumulated_submission_time': 32433.476836681366, 'accumulated_eval_time': 17303.229751110077, 'accumulated_logging_time': 5.12899374961853}
I0306 00:08:11.204798 139768268125952 logging_writer.py:48] [99635] accumulated_eval_time=17303.229751, accumulated_logging_time=5.128994, accumulated_submission_time=32433.476837, global_step=99635, preemption_count=0, score=32433.476837, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275898, test/num_examples=43793, total_duration=49744.641378, train/accuracy=0.995450, train/loss=0.014205, train/mean_average_precision=0.770541, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293213, validation/num_examples=43793
I0306 00:08:32.529628 139776167794432 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.14937724173069, loss=0.01803099736571312
I0306 00:09:04.244414 139768268125952 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.14504553377628326, loss=0.019355228170752525
I0306 00:09:36.131616 139776167794432 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.14368702471256256, loss=0.017650168389081955
I0306 00:10:07.759726 139768268125952 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.13398048281669617, loss=0.01806650310754776
I0306 00:10:39.656036 139776167794432 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.14445503056049347, loss=0.0182365570217371
I0306 00:11:11.461212 139768268125952 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.1473502665758133, loss=0.018265480175614357
I0306 00:11:43.129113 139776167794432 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.13689389824867249, loss=0.01800185814499855
I0306 00:12:11.232298 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:13:53.291622 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:13:56.284597 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:13:59.273432 139937033598784 submission_runner.py:411] Time since start: 50092.74s, 	Step: 100387, 	{'train/accuracy': 0.995512068271637, 'train/loss': 0.014008007012307644, 'train/mean_average_precision': 0.7855383859173153, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2930783521607571, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27593447060398973, 'test/num_examples': 43793, 'score': 32673.473799943924, 'total_duration': 50092.74442219734, 'accumulated_submission_time': 32673.473799943924, 'accumulated_eval_time': 17411.270839214325, 'accumulated_logging_time': 5.1742777824401855}
I0306 00:13:59.309030 139760711874304 logging_writer.py:48] [100387] accumulated_eval_time=17411.270839, accumulated_logging_time=5.174278, accumulated_submission_time=32673.473800, global_step=100387, preemption_count=0, score=32673.473800, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275934, test/num_examples=43793, total_duration=50092.744422, train/accuracy=0.995512, train/loss=0.014008, train/mean_average_precision=0.785538, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293078, validation/num_examples=43793
I0306 00:14:04.048061 139769339950848 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.14417661726474762, loss=0.017443114891648293
I0306 00:14:36.155488 139760711874304 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.13720297813415527, loss=0.015398591756820679
I0306 00:15:08.205727 139769339950848 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.1568397879600525, loss=0.018922312185168266
I0306 00:15:40.151290 139760711874304 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.1420167088508606, loss=0.01924024149775505
I0306 00:16:12.346868 139769339950848 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.14151571691036224, loss=0.017266064882278442
I0306 00:16:44.335000 139760711874304 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.15222249925136566, loss=0.019500093534588814
I0306 00:17:16.307491 139769339950848 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.154160737991333, loss=0.019538838416337967
I0306 00:17:48.179645 139760711874304 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.14206328988075256, loss=0.0161513090133667
I0306 00:17:59.394757 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:19:45.857900 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:19:48.903324 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:19:51.924837 139937033598784 submission_runner.py:411] Time since start: 50445.40s, 	Step: 101136, 	{'train/accuracy': 0.9954873919487, 'train/loss': 0.014025319367647171, 'train/mean_average_precision': 0.7812430634313685, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.293090200785777, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759272854940975, 'test/num_examples': 43793, 'score': 32913.52817153931, 'total_duration': 50445.39581871033, 'accumulated_submission_time': 32913.52817153931, 'accumulated_eval_time': 17523.80086541176, 'accumulated_logging_time': 5.221256732940674}
I0306 00:19:51.959446 139768268125952 logging_writer.py:48] [101136] accumulated_eval_time=17523.800865, accumulated_logging_time=5.221257, accumulated_submission_time=32913.528172, global_step=101136, preemption_count=0, score=32913.528172, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275927, test/num_examples=43793, total_duration=50445.395819, train/accuracy=0.995487, train/loss=0.014025, train/mean_average_precision=0.781243, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293090, validation/num_examples=43793
I0306 00:20:13.020648 139776159401728 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.13331057131290436, loss=0.016724420711398125
I0306 00:20:44.689631 139768268125952 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.14078402519226074, loss=0.017171626910567284
I0306 00:21:16.847791 139776159401728 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.13860578835010529, loss=0.018766839057207108
I0306 00:21:49.370630 139768268125952 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.12930399179458618, loss=0.017887718975543976
I0306 00:22:22.020891 139776159401728 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.14293022453784943, loss=0.01788441464304924
I0306 00:22:54.811660 139768268125952 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.1461057960987091, loss=0.020213553681969643
I0306 00:23:27.879546 139776159401728 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.13535533845424652, loss=0.014854290522634983
I0306 00:23:52.062655 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:25:41.812659 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:25:44.853415 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:25:47.845387 139937033598784 submission_runner.py:411] Time since start: 50801.32s, 	Step: 101875, 	{'train/accuracy': 0.9955024719238281, 'train/loss': 0.014068104326725006, 'train/mean_average_precision': 0.775914528450701, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29316560298850525, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759245002265661, 'test/num_examples': 43793, 'score': 33153.59733271599, 'total_duration': 50801.31637907028, 'accumulated_submission_time': 33153.59733271599, 'accumulated_eval_time': 17639.583562850952, 'accumulated_logging_time': 5.267125368118286}
I0306 00:25:47.880330 139760711874304 logging_writer.py:48] [101875] accumulated_eval_time=17639.583563, accumulated_logging_time=5.267125, accumulated_submission_time=33153.597333, global_step=101875, preemption_count=0, score=33153.597333, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275925, test/num_examples=43793, total_duration=50801.316379, train/accuracy=0.995502, train/loss=0.014068, train/mean_average_precision=0.775915, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293166, validation/num_examples=43793
I0306 00:25:56.197774 139769339950848 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.13113108277320862, loss=0.016688121482729912
I0306 00:26:28.660720 139760711874304 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.14229150116443634, loss=0.01799183525145054
I0306 00:27:00.992476 139769339950848 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.1376902461051941, loss=0.01706196367740631
I0306 00:27:33.274034 139760711874304 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.13921396434307098, loss=0.01634712889790535
I0306 00:28:05.448884 139769339950848 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.13599886000156403, loss=0.01676761545240879
I0306 00:28:37.480109 139760711874304 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.13322414457798004, loss=0.01831304095685482
I0306 00:29:09.629830 139769339950848 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.14237730205059052, loss=0.01969291642308235
I0306 00:29:41.728271 139760711874304 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.13917219638824463, loss=0.018049342557787895
I0306 00:29:48.123236 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:31:34.035810 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:31:37.120206 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:31:40.110302 139937033598784 submission_runner.py:411] Time since start: 51153.58s, 	Step: 102621, 	{'train/accuracy': 0.9955505728721619, 'train/loss': 0.014012372121214867, 'train/mean_average_precision': 0.776397050638031, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29332868955970287, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2758684385834484, 'test/num_examples': 43793, 'score': 33393.808596372604, 'total_duration': 51153.58129143715, 'accumulated_submission_time': 33393.808596372604, 'accumulated_eval_time': 17751.57058095932, 'accumulated_logging_time': 5.313305616378784}
I0306 00:31:40.145094 139776159401728 logging_writer.py:48] [102621] accumulated_eval_time=17751.570581, accumulated_logging_time=5.313306, accumulated_submission_time=33393.808596, global_step=102621, preemption_count=0, score=33393.808596, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275868, test/num_examples=43793, total_duration=51153.581291, train/accuracy=0.995551, train/loss=0.014012, train/mean_average_precision=0.776397, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293329, validation/num_examples=43793
I0306 00:32:06.346755 139776167794432 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.136129692196846, loss=0.01811307854950428
I0306 00:32:38.607139 139776159401728 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.14678817987442017, loss=0.019702844321727753
I0306 00:33:11.074982 139776167794432 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.14847563207149506, loss=0.01728447526693344
I0306 00:33:43.239171 139776159401728 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.1337536871433258, loss=0.01779037155210972
I0306 00:34:15.366335 139776167794432 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.14280326664447784, loss=0.017872782424092293
I0306 00:34:47.222604 139776159401728 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.12257038801908493, loss=0.016322912648320198
I0306 00:35:19.573858 139776167794432 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.14454059302806854, loss=0.019780727103352547
I0306 00:35:40.171098 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:37:29.177838 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:37:32.235185 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:37:35.218261 139937033598784 submission_runner.py:411] Time since start: 51508.69s, 	Step: 103365, 	{'train/accuracy': 0.9954778552055359, 'train/loss': 0.014079206623136997, 'train/mean_average_precision': 0.7638450164442087, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932093756277367, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27603862505552595, 'test/num_examples': 43793, 'score': 33633.803196430206, 'total_duration': 51508.68910455704, 'accumulated_submission_time': 33633.803196430206, 'accumulated_eval_time': 17866.617556095123, 'accumulated_logging_time': 5.3591132164001465}
I0306 00:37:35.254174 139768268125952 logging_writer.py:48] [103365] accumulated_eval_time=17866.617556, accumulated_logging_time=5.359113, accumulated_submission_time=33633.803196, global_step=103365, preemption_count=0, score=33633.803196, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276039, test/num_examples=43793, total_duration=51508.689105, train/accuracy=0.995478, train/loss=0.014079, train/mean_average_precision=0.763845, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293209, validation/num_examples=43793
I0306 00:37:46.599229 139769339950848 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.13669666647911072, loss=0.01857704482972622
I0306 00:38:18.438035 139768268125952 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.1440000683069229, loss=0.016988661140203476
I0306 00:38:50.072421 139769339950848 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.13454753160476685, loss=0.016361428424715996
I0306 00:39:22.052718 139768268125952 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.13374494016170502, loss=0.016851311549544334
I0306 00:39:54.881932 139769339950848 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.14509296417236328, loss=0.018222594633698463
I0306 00:40:27.482245 139768268125952 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.15388096868991852, loss=0.019524412229657173
I0306 00:40:59.427383 139769339950848 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.12247548252344131, loss=0.015885625034570694
I0306 00:41:31.072200 139768268125952 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.13117828965187073, loss=0.015126693993806839
I0306 00:41:35.296453 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:43:22.163380 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:43:25.217056 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:43:28.241724 139937033598784 submission_runner.py:411] Time since start: 51861.71s, 	Step: 104114, 	{'train/accuracy': 0.9954692721366882, 'train/loss': 0.014179518446326256, 'train/mean_average_precision': 0.7776649159084392, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321614569236537, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759427131775175, 'test/num_examples': 43793, 'score': 33873.81412649155, 'total_duration': 51861.71270442009, 'accumulated_submission_time': 33873.81412649155, 'accumulated_eval_time': 17979.562771081924, 'accumulated_logging_time': 5.4056007862091064}
I0306 00:43:28.276989 139760711874304 logging_writer.py:48] [104114] accumulated_eval_time=17979.562771, accumulated_logging_time=5.405601, accumulated_submission_time=33873.814126, global_step=104114, preemption_count=0, score=33873.814126, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275943, test/num_examples=43793, total_duration=51861.712704, train/accuracy=0.995469, train/loss=0.014180, train/mean_average_precision=0.777665, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293216, validation/num_examples=43793
I0306 00:43:56.412895 139776167794432 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.14941257238388062, loss=0.018046049401164055
I0306 00:44:28.838599 139760711874304 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.1406453400850296, loss=0.0192324947565794
I0306 00:45:01.047088 139776167794432 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.1377781480550766, loss=0.016613930463790894
I0306 00:45:34.375512 139760711874304 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.1495695859193802, loss=0.020827077329158783
I0306 00:46:07.542959 139776167794432 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.15123571455478668, loss=0.018786897882819176
I0306 00:46:40.317322 139760711874304 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.13951444625854492, loss=0.018325308337807655
I0306 00:47:12.788743 139776167794432 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.13855069875717163, loss=0.018221458420157433
I0306 00:47:28.316633 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:49:11.770258 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:49:14.788752 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:49:17.788365 139937033598784 submission_runner.py:411] Time since start: 52211.26s, 	Step: 104849, 	{'train/accuracy': 0.9955123066902161, 'train/loss': 0.01398214977234602, 'train/mean_average_precision': 0.7832667867806614, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931969650094557, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27605469166194924, 'test/num_examples': 43793, 'score': 34113.82161974907, 'total_duration': 52211.25935649872, 'accumulated_submission_time': 34113.82161974907, 'accumulated_eval_time': 18089.034460544586, 'accumulated_logging_time': 5.451803922653198}
I0306 00:49:17.823816 139768268125952 logging_writer.py:48] [104849] accumulated_eval_time=18089.034461, accumulated_logging_time=5.451804, accumulated_submission_time=34113.821620, global_step=104849, preemption_count=0, score=34113.821620, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276055, test/num_examples=43793, total_duration=52211.259356, train/accuracy=0.995512, train/loss=0.013982, train/mean_average_precision=0.783267, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293197, validation/num_examples=43793
I0306 00:49:34.573839 139769339950848 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.13941624760627747, loss=0.018156373873353004
I0306 00:50:06.753941 139768268125952 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.13915738463401794, loss=0.02010445110499859
I0306 00:50:38.580849 139769339950848 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.1449648141860962, loss=0.01990983635187149
I0306 00:51:10.784977 139768268125952 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.14032579958438873, loss=0.016725946217775345
I0306 00:51:42.273039 139769339950848 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.14364083111286163, loss=0.01799261011183262
I0306 00:52:14.286472 139768268125952 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.13226178288459778, loss=0.016309745609760284
I0306 00:52:45.979055 139769339950848 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.16151219606399536, loss=0.019722094759345055
I0306 00:53:17.610347 139768268125952 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.1504615992307663, loss=0.016966968774795532
I0306 00:53:17.971126 139937033598784 spec.py:321] Evaluating on the training split.
I0306 00:55:03.936728 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 00:55:06.956966 139937033598784 spec.py:349] Evaluating on the test split.
I0306 00:55:09.934979 139937033598784 submission_runner.py:411] Time since start: 52563.41s, 	Step: 105602, 	{'train/accuracy': 0.9954873919487, 'train/loss': 0.014040553942322731, 'train/mean_average_precision': 0.7785648148482405, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29325271544386494, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759460188977808, 'test/num_examples': 43793, 'score': 34353.93736720085, 'total_duration': 52563.40596866608, 'accumulated_submission_time': 34353.93736720085, 'accumulated_eval_time': 18200.998265981674, 'accumulated_logging_time': 5.49837327003479}
I0306 00:55:09.970419 139760711874304 logging_writer.py:48] [105602] accumulated_eval_time=18200.998266, accumulated_logging_time=5.498373, accumulated_submission_time=34353.937367, global_step=105602, preemption_count=0, score=34353.937367, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275946, test/num_examples=43793, total_duration=52563.405969, train/accuracy=0.995487, train/loss=0.014041, train/mean_average_precision=0.778565, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293253, validation/num_examples=43793
I0306 00:55:41.799173 139776159401728 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.14562490582466125, loss=0.0181000754237175
I0306 00:56:13.944966 139760711874304 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.1321239024400711, loss=0.01911550760269165
I0306 00:56:46.585188 139776159401728 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.15897615253925323, loss=0.01868429034948349
I0306 00:57:18.924550 139760711874304 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.1430869847536087, loss=0.017499826848506927
I0306 00:57:51.067251 139776159401728 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.13218486309051514, loss=0.01755332574248314
I0306 00:58:23.226452 139760711874304 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.16319529712200165, loss=0.01930798776447773
I0306 00:58:55.489592 139776159401728 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.13394460082054138, loss=0.01667957939207554
I0306 00:59:10.122159 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:00:55.297827 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:00:58.336952 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:01:01.296999 139937033598784 submission_runner.py:411] Time since start: 52914.77s, 	Step: 106346, 	{'train/accuracy': 0.9955300688743591, 'train/loss': 0.014030514284968376, 'train/mean_average_precision': 0.7752093696893039, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29314283307095546, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760089875594442, 'test/num_examples': 43793, 'score': 34594.05852675438, 'total_duration': 52914.76798701286, 'accumulated_submission_time': 34594.05852675438, 'accumulated_eval_time': 18312.173060655594, 'accumulated_logging_time': 5.544644832611084}
I0306 01:01:01.334192 139768268125952 logging_writer.py:48] [106346] accumulated_eval_time=18312.173061, accumulated_logging_time=5.544645, accumulated_submission_time=34594.058527, global_step=106346, preemption_count=0, score=34594.058527, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276009, test/num_examples=43793, total_duration=52914.767987, train/accuracy=0.995530, train/loss=0.014031, train/mean_average_precision=0.775209, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293143, validation/num_examples=43793
I0306 01:01:19.033767 139776167794432 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.14198900759220123, loss=0.016118483617901802
I0306 01:01:51.070864 139768268125952 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.13322466611862183, loss=0.01847335696220398
I0306 01:02:23.163425 139776167794432 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.12392920255661011, loss=0.01584044098854065
I0306 01:02:54.979423 139768268125952 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.12467937171459198, loss=0.015449799597263336
I0306 01:03:27.392337 139776167794432 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.14512474834918976, loss=0.016810916364192963
I0306 01:03:59.598092 139768268125952 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.1285807490348816, loss=0.016460739076137543
I0306 01:04:31.539278 139776167794432 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.12921354174613953, loss=0.015692472457885742
I0306 01:05:01.535647 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:06:46.059787 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:06:49.092310 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:06:52.135339 139937033598784 submission_runner.py:411] Time since start: 53265.61s, 	Step: 107096, 	{'train/accuracy': 0.9954977035522461, 'train/loss': 0.014016111381351948, 'train/mean_average_precision': 0.7694205666819729, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.293144991287255, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27592387520931405, 'test/num_examples': 43793, 'score': 34834.22911691666, 'total_duration': 53265.606330394745, 'accumulated_submission_time': 34834.22911691666, 'accumulated_eval_time': 18422.772712945938, 'accumulated_logging_time': 5.592383623123169}
I0306 01:06:52.171658 139760711874304 logging_writer.py:48] [107096] accumulated_eval_time=18422.772713, accumulated_logging_time=5.592384, accumulated_submission_time=34834.229117, global_step=107096, preemption_count=0, score=34834.229117, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275924, test/num_examples=43793, total_duration=53265.606330, train/accuracy=0.995498, train/loss=0.014016, train/mean_average_precision=0.769421, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293145, validation/num_examples=43793
I0306 01:06:53.912388 139769339950848 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.13240757584571838, loss=0.016904424875974655
I0306 01:07:26.353569 139760711874304 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.15537679195404053, loss=0.018127936869859695
I0306 01:07:58.386990 139769339950848 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.1569075584411621, loss=0.019033389165997505
I0306 01:08:30.619409 139760711874304 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.14671698212623596, loss=0.018169181421399117
I0306 01:09:02.854788 139769339950848 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.1488216370344162, loss=0.018450479954481125
I0306 01:09:35.326808 139760711874304 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.17824997007846832, loss=0.020810488611459732
I0306 01:10:07.815194 139769339950848 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.14487150311470032, loss=0.018747180700302124
I0306 01:10:40.754459 139760711874304 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.1373564451932907, loss=0.018339484930038452
I0306 01:10:52.310745 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:12:40.624400 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:12:43.641717 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:12:46.604770 139937033598784 submission_runner.py:411] Time since start: 53620.08s, 	Step: 107836, 	{'train/accuracy': 0.995468020439148, 'train/loss': 0.014182766899466515, 'train/mean_average_precision': 0.7754401981422524, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932577357417439, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27591354179574895, 'test/num_examples': 43793, 'score': 35074.33372759819, 'total_duration': 53620.075761795044, 'accumulated_submission_time': 35074.33372759819, 'accumulated_eval_time': 18537.066703557968, 'accumulated_logging_time': 5.642268657684326}
I0306 01:12:46.641096 139768268125952 logging_writer.py:48] [107836] accumulated_eval_time=18537.066704, accumulated_logging_time=5.642269, accumulated_submission_time=35074.333728, global_step=107836, preemption_count=0, score=35074.333728, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275914, test/num_examples=43793, total_duration=53620.075762, train/accuracy=0.995468, train/loss=0.014183, train/mean_average_precision=0.775440, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293258, validation/num_examples=43793
I0306 01:13:07.288784 139776167794432 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.14029181003570557, loss=0.01769990473985672
I0306 01:13:38.695174 139768268125952 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.16383789479732513, loss=0.0215544905513525
I0306 01:14:10.751652 139776167794432 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.13450166583061218, loss=0.017077142372727394
I0306 01:14:42.688297 139768268125952 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.14161929488182068, loss=0.015630602836608887
I0306 01:15:15.055027 139776167794432 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.13380120694637299, loss=0.01765737123787403
I0306 01:15:46.670292 139768268125952 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.13378246128559113, loss=0.018549051135778427
I0306 01:16:18.747895 139776167794432 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.13853681087493896, loss=0.018226301297545433
I0306 01:16:46.706580 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:18:33.387836 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:18:36.394194 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:18:39.349348 139937033598784 submission_runner.py:411] Time since start: 53972.82s, 	Step: 108589, 	{'train/accuracy': 0.9955382347106934, 'train/loss': 0.01395164430141449, 'train/mean_average_precision': 0.7749240428541433, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29320173589829696, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27596556967141567, 'test/num_examples': 43793, 'score': 35314.367590904236, 'total_duration': 53972.82032966614, 'accumulated_submission_time': 35314.367590904236, 'accumulated_eval_time': 18649.70942378044, 'accumulated_logging_time': 5.689713478088379}
I0306 01:18:39.384981 139760711874304 logging_writer.py:48] [108589] accumulated_eval_time=18649.709424, accumulated_logging_time=5.689713, accumulated_submission_time=35314.367591, global_step=108589, preemption_count=0, score=35314.367591, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275966, test/num_examples=43793, total_duration=53972.820330, train/accuracy=0.995538, train/loss=0.013952, train/mean_average_precision=0.774924, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293202, validation/num_examples=43793
I0306 01:18:43.562454 139776159401728 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.14697293937206268, loss=0.01683504693210125
I0306 01:19:15.758169 139760711874304 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.15583257377147675, loss=0.01858379691839218
I0306 01:19:48.003116 139776159401728 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.12412038445472717, loss=0.01805768348276615
I0306 01:20:19.983969 139760711874304 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.13805384933948517, loss=0.016082433983683586
I0306 01:20:51.857347 139776159401728 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.13201135396957397, loss=0.017439473420381546
I0306 01:21:23.676404 139760711874304 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.1468288004398346, loss=0.01852555200457573
I0306 01:21:55.629500 139776159401728 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.13911791145801544, loss=0.01873381994664669
I0306 01:22:27.431802 139760711874304 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.1481461524963379, loss=0.02007976733148098
I0306 01:22:39.415356 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:24:25.158721 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:24:28.196019 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:24:31.191998 139937033598784 submission_runner.py:411] Time since start: 54324.66s, 	Step: 109339, 	{'train/accuracy': 0.9954889416694641, 'train/loss': 0.014051767997443676, 'train/mean_average_precision': 0.783534154624986, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29306610186826576, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.275938599498735, 'test/num_examples': 43793, 'score': 35554.36666512489, 'total_duration': 54324.66298913956, 'accumulated_submission_time': 35554.36666512489, 'accumulated_eval_time': 18761.486020088196, 'accumulated_logging_time': 5.736135482788086}
I0306 01:24:31.230213 139768268125952 logging_writer.py:48] [109339] accumulated_eval_time=18761.486020, accumulated_logging_time=5.736135, accumulated_submission_time=35554.366665, global_step=109339, preemption_count=0, score=35554.366665, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275939, test/num_examples=43793, total_duration=54324.662989, train/accuracy=0.995489, train/loss=0.014052, train/mean_average_precision=0.783534, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293066, validation/num_examples=43793
I0306 01:24:51.241672 139769339950848 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.13906724750995636, loss=0.018993906676769257
I0306 01:25:23.177022 139768268125952 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.15557125210762024, loss=0.01764432154595852
I0306 01:25:55.185443 139769339950848 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.1290392130613327, loss=0.015912918373942375
I0306 01:26:27.539036 139768268125952 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.1400318741798401, loss=0.01941031776368618
I0306 01:26:59.267388 139769339950848 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.13915708661079407, loss=0.018110940232872963
I0306 01:27:31.909381 139768268125952 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.14565536379814148, loss=0.01646466739475727
I0306 01:28:03.926378 139769339950848 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.13940340280532837, loss=0.020731747150421143
I0306 01:28:31.393433 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:30:18.751699 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:30:22.241534 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:30:25.690362 139937033598784 submission_runner.py:411] Time since start: 54679.16s, 	Step: 110088, 	{'train/accuracy': 0.9954979419708252, 'train/loss': 0.014077340252697468, 'train/mean_average_precision': 0.7767405525270463, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932059522267767, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27591691751020175, 'test/num_examples': 43793, 'score': 35794.49884533882, 'total_duration': 54679.16133594513, 'accumulated_submission_time': 35794.49884533882, 'accumulated_eval_time': 18875.782890081406, 'accumulated_logging_time': 5.784965753555298}
I0306 01:30:25.732976 139760711874304 logging_writer.py:48] [110088] accumulated_eval_time=18875.782890, accumulated_logging_time=5.784966, accumulated_submission_time=35794.498845, global_step=110088, preemption_count=0, score=35794.498845, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275917, test/num_examples=43793, total_duration=54679.161336, train/accuracy=0.995498, train/loss=0.014077, train/mean_average_precision=0.776741, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293206, validation/num_examples=43793
I0306 01:30:30.088466 139776159401728 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.13362477719783783, loss=0.018170328810811043
I0306 01:31:03.293067 139760711874304 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.15104806423187256, loss=0.01683310605585575
I0306 01:31:36.301670 139776159401728 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.14018233120441437, loss=0.018509093672037125
I0306 01:32:09.201290 139760711874304 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.1531841903924942, loss=0.01740897074341774
I0306 01:32:41.562399 139776159401728 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.124478280544281, loss=0.015304405242204666
I0306 01:33:13.823844 139760711874304 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.13374637067317963, loss=0.016425836831331253
I0306 01:33:45.817336 139776159401728 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.14136892557144165, loss=0.018447130918502808
I0306 01:34:18.129288 139760711874304 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.13276360929012299, loss=0.01857180707156658
I0306 01:34:25.898298 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:36:08.266492 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:36:11.378071 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:36:14.401143 139937033598784 submission_runner.py:411] Time since start: 55027.87s, 	Step: 110825, 	{'train/accuracy': 0.9955165982246399, 'train/loss': 0.014020571485161781, 'train/mean_average_precision': 0.7701554597187098, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29331116826883313, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27591366676828855, 'test/num_examples': 43793, 'score': 36034.63081741333, 'total_duration': 55027.872133016586, 'accumulated_submission_time': 36034.63081741333, 'accumulated_eval_time': 18984.285687446594, 'accumulated_logging_time': 5.839473009109497}
I0306 01:36:14.437469 139768268125952 logging_writer.py:48] [110825] accumulated_eval_time=18984.285687, accumulated_logging_time=5.839473, accumulated_submission_time=36034.630817, global_step=110825, preemption_count=0, score=36034.630817, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275914, test/num_examples=43793, total_duration=55027.872133, train/accuracy=0.995517, train/loss=0.014021, train/mean_average_precision=0.770155, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293311, validation/num_examples=43793
I0306 01:36:39.048267 139769339950848 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.13773572444915771, loss=0.017749808728694916
I0306 01:37:11.443048 139768268125952 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.15922433137893677, loss=0.019248059019446373
I0306 01:37:43.650426 139769339950848 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.18307223916053772, loss=0.017970429733395576
I0306 01:38:15.938675 139768268125952 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.1355554163455963, loss=0.01802990399301052
I0306 01:38:47.665027 139769339950848 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.13329777121543884, loss=0.017772600054740906
I0306 01:39:19.910051 139768268125952 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.14026549458503723, loss=0.019344033673405647
I0306 01:39:51.905465 139769339950848 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.12460313737392426, loss=0.01730499044060707
I0306 01:40:14.603196 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:41:58.591863 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:42:01.628349 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:42:05.065191 139937033598784 submission_runner.py:411] Time since start: 55378.54s, 	Step: 111571, 	{'train/accuracy': 0.995445728302002, 'train/loss': 0.01419789157807827, 'train/mean_average_precision': 0.7755410798187834, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321466650476974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27589064130139934, 'test/num_examples': 43793, 'score': 36274.76571893692, 'total_duration': 55378.53616476059, 'accumulated_submission_time': 36274.76571893692, 'accumulated_eval_time': 19094.74762225151, 'accumulated_logging_time': 5.886816501617432}
I0306 01:42:05.108349 139760711874304 logging_writer.py:48] [111571] accumulated_eval_time=19094.747622, accumulated_logging_time=5.886817, accumulated_submission_time=36274.765719, global_step=111571, preemption_count=0, score=36274.765719, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275891, test/num_examples=43793, total_duration=55378.536165, train/accuracy=0.995446, train/loss=0.014198, train/mean_average_precision=0.775541, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293215, validation/num_examples=43793
I0306 01:42:15.276124 139776159401728 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.1571541428565979, loss=0.018894508481025696
I0306 01:42:49.181320 139760711874304 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.13462801277637482, loss=0.019224857911467552
I0306 01:43:22.723406 139776159401728 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.1226721927523613, loss=0.017305010929703712
I0306 01:43:55.613028 139760711874304 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.15790823101997375, loss=0.01929677277803421
I0306 01:44:28.838941 139776159401728 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.14416339993476868, loss=0.018498500809073448
I0306 01:45:01.995615 139760711874304 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.138466477394104, loss=0.017844343557953835
I0306 01:45:34.448487 139776159401728 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.14368875324726105, loss=0.017465222626924515
I0306 01:46:05.218834 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:47:47.160487 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:47:50.198919 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:47:53.273577 139937033598784 submission_runner.py:411] Time since start: 55726.74s, 	Step: 112296, 	{'train/accuracy': 0.9955244064331055, 'train/loss': 0.013996027410030365, 'train/mean_average_precision': 0.7807075584140233, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931795272519896, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759193595748794, 'test/num_examples': 43793, 'score': 36514.839708566666, 'total_duration': 55726.74456644058, 'accumulated_submission_time': 36514.839708566666, 'accumulated_eval_time': 19202.802329540253, 'accumulated_logging_time': 5.942864656448364}
I0306 01:47:53.310217 139768268125952 logging_writer.py:48] [112296] accumulated_eval_time=19202.802330, accumulated_logging_time=5.942865, accumulated_submission_time=36514.839709, global_step=112296, preemption_count=0, score=36514.839709, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275919, test/num_examples=43793, total_duration=55726.744566, train/accuracy=0.995524, train/loss=0.013996, train/mean_average_precision=0.780708, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293180, validation/num_examples=43793
I0306 01:47:55.069372 139769339950848 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.13529178500175476, loss=0.01573382504284382
I0306 01:48:27.252084 139768268125952 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.15872667729854584, loss=0.01624220609664917
I0306 01:48:59.083314 139769339950848 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.14234599471092224, loss=0.018481247127056122
I0306 01:49:31.038457 139768268125952 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.1395302563905716, loss=0.015723126009106636
I0306 01:50:03.062407 139769339950848 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.15792103111743927, loss=0.015338709577918053
I0306 01:50:35.480008 139768268125952 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.13553640246391296, loss=0.01746661774814129
I0306 01:51:07.947136 139769339950848 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.13494791090488434, loss=0.016955263912677765
I0306 01:51:40.394802 139768268125952 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.14377379417419434, loss=0.01859738491475582
I0306 01:51:53.319611 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:53:39.349866 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:53:42.686491 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:53:46.041550 139937033598784 submission_runner.py:411] Time since start: 56079.51s, 	Step: 113040, 	{'train/accuracy': 0.9955311417579651, 'train/loss': 0.013984798453748226, 'train/mean_average_precision': 0.7829461868920284, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29336350498459346, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27608841962605063, 'test/num_examples': 43793, 'score': 36754.817924022675, 'total_duration': 56079.51251745224, 'accumulated_submission_time': 36754.817924022675, 'accumulated_eval_time': 19315.524201393127, 'accumulated_logging_time': 5.990647315979004}
I0306 01:53:46.081258 139760711874304 logging_writer.py:48] [113040] accumulated_eval_time=19315.524201, accumulated_logging_time=5.990647, accumulated_submission_time=36754.817924, global_step=113040, preemption_count=0, score=36754.817924, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276088, test/num_examples=43793, total_duration=56079.512517, train/accuracy=0.995531, train/loss=0.013985, train/mean_average_precision=0.782946, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293364, validation/num_examples=43793
I0306 01:54:06.272607 139776167794432 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.1298966258764267, loss=0.017826231196522713
I0306 01:54:38.325064 139760711874304 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.14875629544258118, loss=0.018685832619667053
I0306 01:55:10.418484 139776167794432 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.1363442838191986, loss=0.01699516735970974
I0306 01:55:42.269761 139760711874304 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.1469470113515854, loss=0.017354486510157585
I0306 01:56:14.569634 139776167794432 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.13273370265960693, loss=0.017625022679567337
I0306 01:56:46.085836 139760711874304 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.13667799532413483, loss=0.015994207933545113
I0306 01:57:18.305726 139776167794432 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.12671642005443573, loss=0.017713375389575958
I0306 01:57:46.142185 139937033598784 spec.py:321] Evaluating on the training split.
I0306 01:59:27.816772 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 01:59:30.917328 139937033598784 spec.py:349] Evaluating on the test split.
I0306 01:59:33.904625 139937033598784 submission_runner.py:411] Time since start: 56427.38s, 	Step: 113788, 	{'train/accuracy': 0.9954670071601868, 'train/loss': 0.01406609546393156, 'train/mean_average_precision': 0.7718478538114525, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.293139372132737, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27591598721013394, 'test/num_examples': 43793, 'score': 36994.844656705856, 'total_duration': 56427.37561607361, 'accumulated_submission_time': 36994.844656705856, 'accumulated_eval_time': 19423.286600351334, 'accumulated_logging_time': 6.043809175491333}
I0306 01:59:33.941687 139768268125952 logging_writer.py:48] [113788] accumulated_eval_time=19423.286600, accumulated_logging_time=6.043809, accumulated_submission_time=36994.844657, global_step=113788, preemption_count=0, score=36994.844657, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275916, test/num_examples=43793, total_duration=56427.375616, train/accuracy=0.995467, train/loss=0.014066, train/mean_average_precision=0.771848, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293139, validation/num_examples=43793
I0306 01:59:38.463404 139769339950848 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.14796574413776398, loss=0.01856863498687744
I0306 02:00:10.711442 139768268125952 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.1291862577199936, loss=0.01959613524377346
I0306 02:00:42.532567 139769339950848 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.12574879825115204, loss=0.015524527058005333
I0306 02:01:14.987393 139768268125952 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.14089174568653107, loss=0.018285328522324562
I0306 02:01:47.254493 139769339950848 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.14378367364406586, loss=0.01788647286593914
I0306 02:02:19.268085 139768268125952 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.13597191870212555, loss=0.01614842377603054
I0306 02:02:51.309666 139769339950848 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.12387131154537201, loss=0.01697859913110733
I0306 02:03:23.960054 139768268125952 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.13511985540390015, loss=0.018521711230278015
I0306 02:03:34.091631 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:05:22.945450 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:05:26.001538 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:05:29.010349 139937033598784 submission_runner.py:411] Time since start: 56782.48s, 	Step: 114532, 	{'train/accuracy': 0.9955251216888428, 'train/loss': 0.014010895974934101, 'train/mean_average_precision': 0.7738367760206022, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.293188261011175, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759121370281739, 'test/num_examples': 43793, 'score': 37234.62434768677, 'total_duration': 56782.48134112358, 'accumulated_submission_time': 37234.62434768677, 'accumulated_eval_time': 19538.20528769493, 'accumulated_logging_time': 6.430257320404053}
I0306 02:05:29.047601 139760711874304 logging_writer.py:48] [114532] accumulated_eval_time=19538.205288, accumulated_logging_time=6.430257, accumulated_submission_time=37234.624348, global_step=114532, preemption_count=0, score=37234.624348, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275912, test/num_examples=43793, total_duration=56782.481341, train/accuracy=0.995525, train/loss=0.014011, train/mean_average_precision=0.773837, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293188, validation/num_examples=43793
I0306 02:05:51.104783 139776159401728 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.14371417462825775, loss=0.018816569820046425
I0306 02:06:23.645826 139760711874304 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.15208862721920013, loss=0.01866035722196102
I0306 02:06:56.316011 139776159401728 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.13845576345920563, loss=0.01654542237520218
I0306 02:07:28.715978 139760711874304 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.13966307044029236, loss=0.017298059538006783
I0306 02:08:00.869151 139776159401728 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.1384667158126831, loss=0.017094427719712257
I0306 02:08:33.328705 139760711874304 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.13391205668449402, loss=0.01657896861433983
I0306 02:09:05.616635 139776159401728 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.14478802680969238, loss=0.020235199481248856
I0306 02:09:29.238644 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:11:12.840478 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:11:16.035411 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:11:19.175544 139937033598784 submission_runner.py:411] Time since start: 57132.65s, 	Step: 115275, 	{'train/accuracy': 0.9954529404640198, 'train/loss': 0.014180625788867474, 'train/mean_average_precision': 0.7671019554487162, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932588652343885, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27592765659127594, 'test/num_examples': 43793, 'score': 37474.78272938728, 'total_duration': 57132.64653587341, 'accumulated_submission_time': 37474.78272938728, 'accumulated_eval_time': 19648.142145633698, 'accumulated_logging_time': 6.480026960372925}
I0306 02:11:19.212896 139768268125952 logging_writer.py:48] [115275] accumulated_eval_time=19648.142146, accumulated_logging_time=6.480027, accumulated_submission_time=37474.782729, global_step=115275, preemption_count=0, score=37474.782729, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275928, test/num_examples=43793, total_duration=57132.646536, train/accuracy=0.995453, train/loss=0.014181, train/mean_average_precision=0.767102, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293259, validation/num_examples=43793
I0306 02:11:27.744720 139776167794432 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.1332496553659439, loss=0.01399260014295578
I0306 02:12:00.436599 139768268125952 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.14426320791244507, loss=0.018863383680582047
I0306 02:12:33.145214 139776167794432 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.15063568949699402, loss=0.01890060119330883
I0306 02:13:05.285169 139768268125952 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.13851341605186462, loss=0.01738066039979458
I0306 02:13:37.333620 139776167794432 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.13410381972789764, loss=0.01719949021935463
I0306 02:14:09.704765 139768268125952 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.14238914847373962, loss=0.016914410516619682
I0306 02:14:41.601852 139776167794432 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.13578592240810394, loss=0.018700936809182167
I0306 02:15:13.938652 139768268125952 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.13517843186855316, loss=0.017300430685281754
I0306 02:15:19.385778 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:17:06.508092 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:17:09.634676 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:17:12.594355 139937033598784 submission_runner.py:411] Time since start: 57486.07s, 	Step: 116018, 	{'train/accuracy': 0.9955293536186218, 'train/loss': 0.014038870111107826, 'train/mean_average_precision': 0.7772930990901283, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29323764303014993, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27600052422004157, 'test/num_examples': 43793, 'score': 37714.924875974655, 'total_duration': 57486.06534719467, 'accumulated_submission_time': 37714.924875974655, 'accumulated_eval_time': 19761.3506834507, 'accumulated_logging_time': 6.528173208236694}
I0306 02:17:12.631139 139769339950848 logging_writer.py:48] [116018] accumulated_eval_time=19761.350683, accumulated_logging_time=6.528173, accumulated_submission_time=37714.924876, global_step=116018, preemption_count=0, score=37714.924876, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276001, test/num_examples=43793, total_duration=57486.065347, train/accuracy=0.995529, train/loss=0.014039, train/mean_average_precision=0.777293, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293238, validation/num_examples=43793
I0306 02:17:39.224092 139776159401728 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.12760449945926666, loss=0.015683341771364212
I0306 02:18:11.512029 139769339950848 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.14649243652820587, loss=0.01784547232091427
I0306 02:18:43.725788 139776159401728 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.13219022750854492, loss=0.016582004725933075
I0306 02:19:15.875386 139769339950848 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.14037418365478516, loss=0.01662263460457325
I0306 02:19:47.789455 139776159401728 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.14153623580932617, loss=0.019119327887892723
I0306 02:20:20.165502 139769339950848 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.13563422858715057, loss=0.01902930811047554
I0306 02:20:51.953879 139776159401728 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.14873006939888, loss=0.01936987414956093
I0306 02:21:12.604231 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:22:53.256971 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:22:56.313710 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:22:59.314165 139937033598784 submission_runner.py:411] Time since start: 57832.79s, 	Step: 116766, 	{'train/accuracy': 0.9954851865768433, 'train/loss': 0.014084882102906704, 'train/mean_average_precision': 0.7850383991852307, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931388648347254, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760248373857741, 'test/num_examples': 43793, 'score': 37954.865159749985, 'total_duration': 57832.78515815735, 'accumulated_submission_time': 37954.865159749985, 'accumulated_eval_time': 19868.060576677322, 'accumulated_logging_time': 6.577084302902222}
I0306 02:22:59.352253 139768268125952 logging_writer.py:48] [116766] accumulated_eval_time=19868.060577, accumulated_logging_time=6.577084, accumulated_submission_time=37954.865160, global_step=116766, preemption_count=0, score=37954.865160, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276025, test/num_examples=43793, total_duration=57832.785158, train/accuracy=0.995485, train/loss=0.014085, train/mean_average_precision=0.785038, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293139, validation/num_examples=43793
I0306 02:23:10.600974 139776167794432 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.13178353011608124, loss=0.018924083560705185
I0306 02:23:42.925777 139768268125952 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.14936192333698273, loss=0.018455222249031067
I0306 02:24:15.443235 139776167794432 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.13523909449577332, loss=0.01727541908621788
I0306 02:24:47.896975 139768268125952 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.1345670521259308, loss=0.017655109986662865
I0306 02:25:20.628669 139776167794432 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.15178394317626953, loss=0.018350329250097275
I0306 02:25:53.166073 139768268125952 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.13633529841899872, loss=0.017487524077296257
I0306 02:26:25.546369 139776167794432 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.16124507784843445, loss=0.019941965118050575
I0306 02:26:57.651862 139768268125952 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.1548585295677185, loss=0.016777917742729187
I0306 02:26:59.609071 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:28:47.522717 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:28:50.610087 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:28:53.616908 139937033598784 submission_runner.py:411] Time since start: 58187.09s, 	Step: 117507, 	{'train/accuracy': 0.9955120086669922, 'train/loss': 0.013983430340886116, 'train/mean_average_precision': 0.7743394761970276, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931852186260689, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759380183987922, 'test/num_examples': 43793, 'score': 38195.090396404266, 'total_duration': 58187.087896347046, 'accumulated_submission_time': 38195.090396404266, 'accumulated_eval_time': 19982.06836414337, 'accumulated_logging_time': 6.626428604125977}
I0306 02:28:53.654698 139769339950848 logging_writer.py:48] [117507] accumulated_eval_time=19982.068364, accumulated_logging_time=6.626429, accumulated_submission_time=38195.090396, global_step=117507, preemption_count=0, score=38195.090396, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275938, test/num_examples=43793, total_duration=58187.087896, train/accuracy=0.995512, train/loss=0.013983, train/mean_average_precision=0.774339, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293185, validation/num_examples=43793
I0306 02:29:24.390453 139776159401728 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.14124254882335663, loss=0.017309224233031273
I0306 02:29:56.620022 139769339950848 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.14222687482833862, loss=0.01760854385793209
I0306 02:30:28.470647 139776159401728 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.14296413958072662, loss=0.018580026924610138
I0306 02:31:00.211154 139769339950848 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.14297762513160706, loss=0.018113290891051292
I0306 02:31:32.451210 139776159401728 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.12839381396770477, loss=0.015150154009461403
I0306 02:32:04.599589 139769339950848 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.13981525599956512, loss=0.018378982320427895
I0306 02:32:36.878372 139776159401728 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.12162898480892181, loss=0.01682078279554844
I0306 02:32:53.712082 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:34:41.436046 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:34:44.866817 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:34:48.222460 139937033598784 submission_runner.py:411] Time since start: 58541.69s, 	Step: 118253, 	{'train/accuracy': 0.9955330491065979, 'train/loss': 0.013999680057168007, 'train/mean_average_precision': 0.7822424577658693, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932161305695458, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.276064940503458, 'test/num_examples': 43793, 'score': 38435.11602497101, 'total_duration': 58541.69343018532, 'accumulated_submission_time': 38435.11602497101, 'accumulated_eval_time': 20096.57867860794, 'accumulated_logging_time': 6.675387859344482}
I0306 02:34:48.268389 139760711874304 logging_writer.py:48] [118253] accumulated_eval_time=20096.578679, accumulated_logging_time=6.675388, accumulated_submission_time=38435.116025, global_step=118253, preemption_count=0, score=38435.116025, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276065, test/num_examples=43793, total_duration=58541.693430, train/accuracy=0.995533, train/loss=0.014000, train/mean_average_precision=0.782242, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293216, validation/num_examples=43793
I0306 02:35:04.203166 139768268125952 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.1310671716928482, loss=0.017218561843037605
I0306 02:35:36.490653 139760711874304 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.1432306468486786, loss=0.018147967755794525
I0306 02:36:08.646347 139768268125952 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.14074981212615967, loss=0.01676798425614834
I0306 02:36:40.979451 139760711874304 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.1286335289478302, loss=0.0186820887029171
I0306 02:37:13.204215 139768268125952 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.14147470891475677, loss=0.016772648319602013
I0306 02:37:45.076970 139760711874304 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.1502290517091751, loss=0.01705719716846943
I0306 02:38:17.105297 139768268125952 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.12963658571243286, loss=0.01721821539103985
I0306 02:38:48.338229 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:40:31.982468 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:40:35.025475 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:40:38.064480 139937033598784 submission_runner.py:411] Time since start: 58891.54s, 	Step: 118999, 	{'train/accuracy': 0.9954657554626465, 'train/loss': 0.014105337671935558, 'train/mean_average_precision': 0.7663495065328356, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931702847497467, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759337036347488, 'test/num_examples': 43793, 'score': 38675.15397500992, 'total_duration': 58891.5354681015, 'accumulated_submission_time': 38675.15397500992, 'accumulated_eval_time': 20206.304889678955, 'accumulated_logging_time': 6.732793807983398}
I0306 02:40:38.103107 139769339950848 logging_writer.py:48] [118999] accumulated_eval_time=20206.304890, accumulated_logging_time=6.732794, accumulated_submission_time=38675.153975, global_step=118999, preemption_count=0, score=38675.153975, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275934, test/num_examples=43793, total_duration=58891.535468, train/accuracy=0.995466, train/loss=0.014105, train/mean_average_precision=0.766350, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293170, validation/num_examples=43793
I0306 02:40:38.807154 139776159401728 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.14727696776390076, loss=0.017784366384148598
I0306 02:41:11.170968 139769339950848 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.12918245792388916, loss=0.016909975558519363
I0306 02:41:43.738107 139776159401728 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.1401163637638092, loss=0.019128667190670967
I0306 02:42:16.735146 139769339950848 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.13067959249019623, loss=0.017281532287597656
I0306 02:42:48.923379 139776159401728 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.15133073925971985, loss=0.02150680497288704
I0306 02:43:21.414212 139769339950848 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.1384127289056778, loss=0.017125830054283142
I0306 02:43:53.486904 139776159401728 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.14921678602695465, loss=0.019760366529226303
I0306 02:44:26.032439 139769339950848 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.14185066521167755, loss=0.02035086415708065
I0306 02:44:38.173431 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:46:20.105230 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:46:23.190792 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:46:28.027416 139937033598784 submission_runner.py:411] Time since start: 59241.50s, 	Step: 119739, 	{'train/accuracy': 0.9954808950424194, 'train/loss': 0.014174436219036579, 'train/mean_average_precision': 0.7775733664311058, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932722189513038, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2761250320439972, 'test/num_examples': 43793, 'score': 38915.19271636009, 'total_duration': 59241.49839806557, 'accumulated_submission_time': 38915.19271636009, 'accumulated_eval_time': 20316.15882253647, 'accumulated_logging_time': 6.782490015029907}
I0306 02:46:28.066689 139760711874304 logging_writer.py:48] [119739] accumulated_eval_time=20316.158823, accumulated_logging_time=6.782490, accumulated_submission_time=38915.192716, global_step=119739, preemption_count=0, score=38915.192716, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276125, test/num_examples=43793, total_duration=59241.498398, train/accuracy=0.995481, train/loss=0.014174, train/mean_average_precision=0.777573, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293272, validation/num_examples=43793
I0306 02:46:48.732282 139768268125952 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.1470654010772705, loss=0.017441412433981895
I0306 02:47:21.387596 139760711874304 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.1650051325559616, loss=0.019190452992916107
I0306 02:47:54.003890 139768268125952 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.1432027667760849, loss=0.015209718607366085
I0306 02:48:26.463028 139760711874304 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.13222111761569977, loss=0.017644817009568214
I0306 02:48:58.817077 139768268125952 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.1236279234290123, loss=0.01706911064684391
I0306 02:49:31.241601 139760711874304 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.13369865715503693, loss=0.0171003807336092
I0306 02:50:03.657791 139768268125952 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.15006354451179504, loss=0.018967702984809875
I0306 02:50:28.162461 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:52:08.883319 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:52:11.967415 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:52:14.998940 139937033598784 submission_runner.py:411] Time since start: 59588.47s, 	Step: 120477, 	{'train/accuracy': 0.9955210089683533, 'train/loss': 0.01397948618978262, 'train/mean_average_precision': 0.7815879575595298, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29307354415923226, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27592028877566144, 'test/num_examples': 43793, 'score': 39155.25708556175, 'total_duration': 59588.46991467476, 'accumulated_submission_time': 39155.25708556175, 'accumulated_eval_time': 20422.99523949623, 'accumulated_logging_time': 6.833070516586304}
I0306 02:52:15.036904 139769339950848 logging_writer.py:48] [120477] accumulated_eval_time=20422.995239, accumulated_logging_time=6.833071, accumulated_submission_time=39155.257086, global_step=120477, preemption_count=0, score=39155.257086, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275920, test/num_examples=43793, total_duration=59588.469915, train/accuracy=0.995521, train/loss=0.013979, train/mean_average_precision=0.781588, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293074, validation/num_examples=43793
I0306 02:52:23.128576 139776167794432 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.1569259911775589, loss=0.019888749346137047
I0306 02:52:56.359900 139769339950848 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.13685671985149384, loss=0.017710022628307343
I0306 02:53:28.931700 139776167794432 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.12989625334739685, loss=0.01585298217833042
I0306 02:54:01.822669 139769339950848 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.13498170673847198, loss=0.017258064821362495
I0306 02:54:33.697427 139776167794432 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.15509572625160217, loss=0.018336480483412743
I0306 02:55:05.819828 139769339950848 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.15436851978302002, loss=0.018240535631775856
I0306 02:55:38.261895 139776167794432 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.13109071552753448, loss=0.01729843206703663
I0306 02:56:10.953876 139769339950848 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.13676370680332184, loss=0.016704685986042023
I0306 02:56:15.313627 139937033598784 spec.py:321] Evaluating on the training split.
I0306 02:57:57.667036 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 02:58:00.663295 139937033598784 spec.py:349] Evaluating on the test split.
I0306 02:58:05.596192 139937033598784 submission_runner.py:411] Time since start: 59939.07s, 	Step: 121214, 	{'train/accuracy': 0.9954981207847595, 'train/loss': 0.01399257406592369, 'train/mean_average_precision': 0.7821686742341195, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931317994411526, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27596331301248744, 'test/num_examples': 43793, 'score': 39395.500725746155, 'total_duration': 59939.0671851635, 'accumulated_submission_time': 39395.500725746155, 'accumulated_eval_time': 20533.277759552002, 'accumulated_logging_time': 6.884111404418945}
I0306 02:58:05.635932 139760711874304 logging_writer.py:48] [121214] accumulated_eval_time=20533.277760, accumulated_logging_time=6.884111, accumulated_submission_time=39395.500726, global_step=121214, preemption_count=0, score=39395.500726, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275963, test/num_examples=43793, total_duration=59939.067185, train/accuracy=0.995498, train/loss=0.013993, train/mean_average_precision=0.782169, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293132, validation/num_examples=43793
I0306 02:58:33.845701 139768268125952 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.14352047443389893, loss=0.016994180157780647
I0306 02:59:06.865785 139760711874304 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.14371928572654724, loss=0.019759755581617355
I0306 02:59:39.830729 139768268125952 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.14765134453773499, loss=0.020154105499386787
I0306 03:00:12.281650 139760711874304 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.16076421737670898, loss=0.01903235912322998
I0306 03:00:44.189320 139768268125952 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.144143745303154, loss=0.017688607797026634
I0306 03:01:16.913224 139760711874304 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.14134962856769562, loss=0.016863282769918442
I0306 03:01:49.973849 139768268125952 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.15097270905971527, loss=0.01896600052714348
I0306 03:02:05.605112 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:03:49.142951 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:03:52.218119 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:03:55.208026 139937033598784 submission_runner.py:411] Time since start: 60288.68s, 	Step: 121949, 	{'train/accuracy': 0.9954849481582642, 'train/loss': 0.014108995907008648, 'train/mean_average_precision': 0.7756348087463598, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048839747905731, 'validation/mean_average_precision': 0.29329234471891386, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2758850451009829, 'test/num_examples': 43793, 'score': 39635.43482732773, 'total_duration': 60288.67901420593, 'accumulated_submission_time': 39635.43482732773, 'accumulated_eval_time': 20642.88063430786, 'accumulated_logging_time': 6.936408996582031}
I0306 03:03:55.246892 139769339950848 logging_writer.py:48] [121949] accumulated_eval_time=20642.880634, accumulated_logging_time=6.936409, accumulated_submission_time=39635.434827, global_step=121949, preemption_count=0, score=39635.434827, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275885, test/num_examples=43793, total_duration=60288.679014, train/accuracy=0.995485, train/loss=0.014109, train/mean_average_precision=0.775635, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293292, validation/num_examples=43793
I0306 03:04:12.881097 139776159401728 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.15510843694210052, loss=0.020138045772910118
I0306 03:04:45.029683 139769339950848 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.15037891268730164, loss=0.018359972164034843
I0306 03:05:17.538156 139776159401728 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.1280534565448761, loss=0.0183295626193285
I0306 03:05:49.396695 139769339950848 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.1306510865688324, loss=0.017481837421655655
I0306 03:06:21.464953 139776159401728 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.14177049696445465, loss=0.019940337166190147
I0306 03:06:53.478935 139769339950848 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.1517845243215561, loss=0.018615832552313805
I0306 03:07:25.809081 139776159401728 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.14136110246181488, loss=0.01768999733030796
I0306 03:07:55.353438 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:09:38.335865 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:09:41.397855 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:09:46.291679 139937033598784 submission_runner.py:411] Time since start: 60639.76s, 	Step: 122693, 	{'train/accuracy': 0.9955112338066101, 'train/loss': 0.01404925063252449, 'train/mean_average_precision': 0.7678146983914897, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932222266958525, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27592769802178213, 'test/num_examples': 43793, 'score': 39875.50879430771, 'total_duration': 60639.76265883446, 'accumulated_submission_time': 39875.50879430771, 'accumulated_eval_time': 20753.818819522858, 'accumulated_logging_time': 6.987528085708618}
I0306 03:09:46.332747 139768268125952 logging_writer.py:48] [122693] accumulated_eval_time=20753.818820, accumulated_logging_time=6.987528, accumulated_submission_time=39875.508794, global_step=122693, preemption_count=0, score=39875.508794, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275928, test/num_examples=43793, total_duration=60639.762659, train/accuracy=0.995511, train/loss=0.014049, train/mean_average_precision=0.767815, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293222, validation/num_examples=43793
I0306 03:09:48.976204 139776167794432 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.13559028506278992, loss=0.01659858226776123
I0306 03:10:21.429059 139768268125952 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.1630069464445114, loss=0.018841424956917763
I0306 03:10:53.639586 139776167794432 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.14703017473220825, loss=0.01846623234450817
I0306 03:11:26.076068 139768268125952 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.13613361120224, loss=0.015453362837433815
I0306 03:11:58.477246 139776167794432 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.14378368854522705, loss=0.01595267839729786
I0306 03:12:30.647020 139768268125952 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.13488584756851196, loss=0.01884484477341175
I0306 03:13:02.487801 139776167794432 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.16010412573814392, loss=0.019122527912259102
I0306 03:13:35.124134 139768268125952 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.12087178230285645, loss=0.017195409163832664
I0306 03:13:46.435449 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:15:30.938964 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:15:34.039979 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:15:37.015836 139937033598784 submission_runner.py:411] Time since start: 60990.49s, 	Step: 123435, 	{'train/accuracy': 0.9954609870910645, 'train/loss': 0.014168189838528633, 'train/mean_average_precision': 0.7764094208096587, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931615148093934, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759675452993614, 'test/num_examples': 43793, 'score': 40115.57969236374, 'total_duration': 60990.48681926727, 'accumulated_submission_time': 40115.57969236374, 'accumulated_eval_time': 20864.3991625309, 'accumulated_logging_time': 7.039609432220459}
I0306 03:15:37.055035 139760711874304 logging_writer.py:48] [123435] accumulated_eval_time=20864.399163, accumulated_logging_time=7.039609, accumulated_submission_time=40115.579692, global_step=123435, preemption_count=0, score=40115.579692, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275968, test/num_examples=43793, total_duration=60990.486819, train/accuracy=0.995461, train/loss=0.014168, train/mean_average_precision=0.776409, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293162, validation/num_examples=43793
I0306 03:15:58.430112 139769339950848 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.1495475172996521, loss=0.020875180140137672
I0306 03:16:30.698487 139760711874304 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.1478070765733719, loss=0.020427711308002472
I0306 03:17:02.649760 139769339950848 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.14330890774726868, loss=0.018985014408826828
I0306 03:17:34.328636 139760711874304 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.1471739262342453, loss=0.019068963825702667
I0306 03:18:06.413037 139769339950848 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.14556486904621124, loss=0.018520569428801537
I0306 03:18:38.868330 139760711874304 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.16497588157653809, loss=0.01998143270611763
I0306 03:19:11.486394 139769339950848 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.12636028230190277, loss=0.01622103899717331
I0306 03:19:37.157690 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:21:16.910648 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:21:19.961457 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:21:22.997047 139937033598784 submission_runner.py:411] Time since start: 61336.47s, 	Step: 124180, 	{'train/accuracy': 0.9955407977104187, 'train/loss': 0.013928944244980812, 'train/mean_average_precision': 0.7768192297472776, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29324373385876684, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760686342789627, 'test/num_examples': 43793, 'score': 40355.65077519417, 'total_duration': 61336.468036174774, 'accumulated_submission_time': 40355.65077519417, 'accumulated_eval_time': 20970.238475561142, 'accumulated_logging_time': 7.089834451675415}
I0306 03:21:23.036100 139768268125952 logging_writer.py:48] [124180] accumulated_eval_time=20970.238476, accumulated_logging_time=7.089834, accumulated_submission_time=40355.650775, global_step=124180, preemption_count=0, score=40355.650775, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276069, test/num_examples=43793, total_duration=61336.468036, train/accuracy=0.995541, train/loss=0.013929, train/mean_average_precision=0.776819, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293244, validation/num_examples=43793
I0306 03:21:29.708826 139776167794432 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.1418231725692749, loss=0.01752881519496441
I0306 03:22:01.926000 139768268125952 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.13500961661338806, loss=0.017515452578663826
I0306 03:22:34.194524 139776167794432 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.1418740451335907, loss=0.01870025135576725
I0306 03:23:06.328857 139768268125952 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.1532241255044937, loss=0.020240260288119316
I0306 03:23:38.971167 139776167794432 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.12717942893505096, loss=0.01783493533730507
I0306 03:24:11.036048 139768268125952 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.13355432450771332, loss=0.016845274716615677
I0306 03:24:42.805338 139776167794432 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.1412399858236313, loss=0.016074471175670624
I0306 03:25:14.631786 139768268125952 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.13433323800563812, loss=0.017558535560965538
I0306 03:25:23.114736 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:27:08.557586 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:27:11.593494 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:27:14.545554 139937033598784 submission_runner.py:411] Time since start: 61688.02s, 	Step: 124928, 	{'train/accuracy': 0.9954872131347656, 'train/loss': 0.014046452939510345, 'train/mean_average_precision': 0.7832406491909317, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931946410851083, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760136439954811, 'test/num_examples': 43793, 'score': 40595.69643044472, 'total_duration': 61688.01654314995, 'accumulated_submission_time': 40595.69643044472, 'accumulated_eval_time': 21081.669250011444, 'accumulated_logging_time': 7.141475677490234}
I0306 03:27:14.584041 139760711874304 logging_writer.py:48] [124928] accumulated_eval_time=21081.669250, accumulated_logging_time=7.141476, accumulated_submission_time=40595.696430, global_step=124928, preemption_count=0, score=40595.696430, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276014, test/num_examples=43793, total_duration=61688.016543, train/accuracy=0.995487, train/loss=0.014046, train/mean_average_precision=0.783241, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293195, validation/num_examples=43793
I0306 03:27:38.519263 139769339950848 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.1455697864294052, loss=0.018001480028033257
I0306 03:28:10.675190 139760711874304 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.1467915028333664, loss=0.01955958642065525
I0306 03:28:42.836404 139769339950848 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.11623795330524445, loss=0.01718088611960411
I0306 03:29:15.163830 139760711874304 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.14082714915275574, loss=0.01635156385600567
I0306 03:29:47.273513 139769339950848 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.14661255478858948, loss=0.019028907641768456
I0306 03:30:19.078353 139760711874304 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.14643235504627228, loss=0.01950654573738575
I0306 03:30:51.650620 139769339950848 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.13172736763954163, loss=0.017297782003879547
I0306 03:31:14.779232 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:33:03.641411 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:33:06.657833 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:33:09.662247 139937033598784 submission_runner.py:411] Time since start: 62043.13s, 	Step: 125672, 	{'train/accuracy': 0.9954885840415955, 'train/loss': 0.014062105678021908, 'train/mean_average_precision': 0.774091442743637, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2933094306738013, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27600472089676736, 'test/num_examples': 43793, 'score': 40835.86017179489, 'total_duration': 62043.13323545456, 'accumulated_submission_time': 40835.86017179489, 'accumulated_eval_time': 21196.552217245102, 'accumulated_logging_time': 7.191240072250366}
I0306 03:33:09.702626 139768268125952 logging_writer.py:48] [125672] accumulated_eval_time=21196.552217, accumulated_logging_time=7.191240, accumulated_submission_time=40835.860172, global_step=125672, preemption_count=0, score=40835.860172, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276005, test/num_examples=43793, total_duration=62043.133235, train/accuracy=0.995489, train/loss=0.014062, train/mean_average_precision=0.774091, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293309, validation/num_examples=43793
I0306 03:33:19.062193 139776159401728 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.14521251618862152, loss=0.016511820256710052
I0306 03:33:51.241009 139768268125952 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.1515735536813736, loss=0.017748136073350906
I0306 03:34:23.451462 139776159401728 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.1606559455394745, loss=0.016387147828936577
I0306 03:34:55.617063 139768268125952 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.14094027876853943, loss=0.01933559775352478
I0306 03:35:28.066022 139776159401728 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.1365218311548233, loss=0.018943993374705315
I0306 03:36:00.177082 139768268125952 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.11876490712165833, loss=0.017780227586627007
I0306 03:36:32.526674 139776159401728 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.14990617334842682, loss=0.019550640136003494
I0306 03:37:05.004180 139768268125952 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.14965280890464783, loss=0.01839766651391983
I0306 03:37:09.793715 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:38:53.865210 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:38:56.910373 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:38:59.966070 139937033598784 submission_runner.py:411] Time since start: 62393.44s, 	Step: 126416, 	{'train/accuracy': 0.9955195188522339, 'train/loss': 0.014060750603675842, 'train/mean_average_precision': 0.7807933473108024, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932310735617742, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760524931298125, 'test/num_examples': 43793, 'score': 41075.91949534416, 'total_duration': 62393.43706226349, 'accumulated_submission_time': 41075.91949534416, 'accumulated_eval_time': 21306.72452545166, 'accumulated_logging_time': 7.242794752120972}
I0306 03:39:00.006169 139760711874304 logging_writer.py:48] [126416] accumulated_eval_time=21306.724525, accumulated_logging_time=7.242795, accumulated_submission_time=41075.919495, global_step=126416, preemption_count=0, score=41075.919495, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276052, test/num_examples=43793, total_duration=62393.437062, train/accuracy=0.995520, train/loss=0.014061, train/mean_average_precision=0.780793, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293231, validation/num_examples=43793
I0306 03:39:27.770410 139769339950848 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.15518327057361603, loss=0.019274242222309113
I0306 03:39:59.937790 139760711874304 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.13881367444992065, loss=0.016448577865958214
I0306 03:40:32.028965 139769339950848 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.15145394206047058, loss=0.018711410462856293
I0306 03:41:04.314076 139760711874304 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.13548630475997925, loss=0.017397565767169
I0306 03:41:36.765523 139769339950848 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.14117182791233063, loss=0.018632400780916214
I0306 03:42:08.915270 139760711874304 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.1418575644493103, loss=0.019270390272140503
I0306 03:42:41.220942 139769339950848 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.1510743647813797, loss=0.01861242577433586
I0306 03:43:00.278059 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:44:42.607424 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:44:45.640243 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:44:48.617070 139937033598784 submission_runner.py:411] Time since start: 62742.09s, 	Step: 127160, 	{'train/accuracy': 0.9954697489738464, 'train/loss': 0.014137211255729198, 'train/mean_average_precision': 0.7587198449639805, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29327645431167626, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760117545460758, 'test/num_examples': 43793, 'score': 41316.159787893295, 'total_duration': 62742.088060855865, 'accumulated_submission_time': 41316.159787893295, 'accumulated_eval_time': 21415.063493013382, 'accumulated_logging_time': 7.293832540512085}
I0306 03:44:48.656838 139768268125952 logging_writer.py:48] [127160] accumulated_eval_time=21415.063493, accumulated_logging_time=7.293833, accumulated_submission_time=41316.159788, global_step=127160, preemption_count=0, score=41316.159788, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276012, test/num_examples=43793, total_duration=62742.088061, train/accuracy=0.995470, train/loss=0.014137, train/mean_average_precision=0.758720, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293276, validation/num_examples=43793
I0306 03:45:02.209667 139776159401728 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.14702774584293365, loss=0.019873352721333504
I0306 03:45:34.967262 139768268125952 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.13010649383068085, loss=0.017722228541970253
I0306 03:46:07.618087 139776159401728 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.135258749127388, loss=0.016105275601148605
I0306 03:46:39.891565 139768268125952 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.16055718064308167, loss=0.01948104240000248
I0306 03:47:11.893846 139776159401728 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.15566635131835938, loss=0.018900729715824127
I0306 03:47:43.969867 139768268125952 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.12398179620504379, loss=0.016212012618780136
I0306 03:48:16.441781 139776159401728 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.1309882253408432, loss=0.018456121906638145
I0306 03:48:48.798696 139768268125952 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.14554093778133392, loss=0.018191669136285782
I0306 03:48:48.803703 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:50:28.380477 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:50:31.440802 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:50:34.434916 139937033598784 submission_runner.py:411] Time since start: 63087.91s, 	Step: 127901, 	{'train/accuracy': 0.995512843132019, 'train/loss': 0.014034428633749485, 'train/mean_average_precision': 0.7794579884619639, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29319543604567316, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27596083265112803, 'test/num_examples': 43793, 'score': 41556.275861501694, 'total_duration': 63087.90578913689, 'accumulated_submission_time': 41556.275861501694, 'accumulated_eval_time': 21520.694525957108, 'accumulated_logging_time': 7.344229459762573}
I0306 03:50:34.474826 139769339950848 logging_writer.py:48] [127901] accumulated_eval_time=21520.694526, accumulated_logging_time=7.344229, accumulated_submission_time=41556.275862, global_step=127901, preemption_count=0, score=41556.275862, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275961, test/num_examples=43793, total_duration=63087.905789, train/accuracy=0.995513, train/loss=0.014034, train/mean_average_precision=0.779458, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293195, validation/num_examples=43793
I0306 03:51:06.583355 139776167794432 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.1520366370677948, loss=0.019539427012205124
I0306 03:51:38.577732 139769339950848 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.13766522705554962, loss=0.017811369150877
I0306 03:52:10.938283 139776167794432 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.14154256880283356, loss=0.0179374311119318
I0306 03:52:43.088558 139769339950848 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.14723965525627136, loss=0.01819636858999729
I0306 03:53:15.474928 139776167794432 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.12983788549900055, loss=0.014713643118739128
I0306 03:53:47.851611 139769339950848 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.13704343140125275, loss=0.018404321745038033
I0306 03:54:20.008272 139776167794432 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.15326449275016785, loss=0.017855050042271614
I0306 03:54:34.710534 139937033598784 spec.py:321] Evaluating on the training split.
I0306 03:56:13.872358 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 03:56:16.945732 139937033598784 spec.py:349] Evaluating on the test split.
I0306 03:56:21.920988 139937033598784 submission_runner.py:411] Time since start: 63435.39s, 	Step: 128646, 	{'train/accuracy': 0.9955059885978699, 'train/loss': 0.014027849771082401, 'train/mean_average_precision': 0.7830336621813878, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29312898490792993, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760132085099576, 'test/num_examples': 43793, 'score': 41796.47999668121, 'total_duration': 63435.391981840134, 'accumulated_submission_time': 41796.47999668121, 'accumulated_eval_time': 21627.90496778488, 'accumulated_logging_time': 7.395256757736206}
I0306 03:56:21.962163 139760711874304 logging_writer.py:48] [128646] accumulated_eval_time=21627.904968, accumulated_logging_time=7.395257, accumulated_submission_time=41796.479997, global_step=128646, preemption_count=0, score=41796.479997, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276013, test/num_examples=43793, total_duration=63435.391982, train/accuracy=0.995506, train/loss=0.014028, train/mean_average_precision=0.783034, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293129, validation/num_examples=43793
I0306 03:56:40.444448 139776159401728 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.1487572193145752, loss=0.018086669966578484
I0306 03:57:13.478063 139760711874304 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.13962946832180023, loss=0.017472688108682632
I0306 03:57:46.651169 139776159401728 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.1687477082014084, loss=0.020608972758054733
I0306 03:58:19.514819 139760711874304 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.1653793603181839, loss=0.020927058532834053
I0306 03:58:52.373654 139776159401728 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.13900160789489746, loss=0.01762116327881813
I0306 03:59:25.617258 139760711874304 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.16874751448631287, loss=0.01879001408815384
I0306 04:00:00.128112 139776159401728 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.13823449611663818, loss=0.017667874693870544
I0306 04:00:22.008272 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:02:06.465871 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:02:09.512017 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:02:12.510832 139937033598784 submission_runner.py:411] Time since start: 63785.98s, 	Step: 129368, 	{'train/accuracy': 0.9955154657363892, 'train/loss': 0.01396663673222065, 'train/mean_average_precision': 0.7766582892696063, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29319985036734203, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.276029149677099, 'test/num_examples': 43793, 'score': 42036.49334049225, 'total_duration': 63785.981810092926, 'accumulated_submission_time': 42036.49334049225, 'accumulated_eval_time': 21738.40746998787, 'accumulated_logging_time': 7.44689416885376}
I0306 04:02:12.550340 139769339950848 logging_writer.py:48] [129368] accumulated_eval_time=21738.407470, accumulated_logging_time=7.446894, accumulated_submission_time=42036.493340, global_step=129368, preemption_count=0, score=42036.493340, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276029, test/num_examples=43793, total_duration=63785.981810, train/accuracy=0.995515, train/loss=0.013967, train/mean_average_precision=0.776658, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293200, validation/num_examples=43793
I0306 04:02:23.224161 139776167794432 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.13872987031936646, loss=0.016658946871757507
I0306 04:02:55.601188 139769339950848 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.14481203258037567, loss=0.01894756406545639
I0306 04:03:28.675658 139776167794432 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.1334267407655716, loss=0.017758667469024658
I0306 04:04:01.852242 139769339950848 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.14261189103126526, loss=0.017434386536478996
I0306 04:04:33.829337 139776167794432 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.1535804122686386, loss=0.017601676285266876
I0306 04:05:06.763452 139769339950848 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.13955152034759521, loss=0.016653593629598618
I0306 04:05:38.948161 139776167794432 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.1386324167251587, loss=0.018269268795847893
I0306 04:06:11.330564 139769339950848 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.13712608814239502, loss=0.017565511167049408
I0306 04:06:12.641579 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:07:54.274850 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:07:57.346546 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:08:02.483836 139937033598784 submission_runner.py:411] Time since start: 64135.95s, 	Step: 130105, 	{'train/accuracy': 0.9955015778541565, 'train/loss': 0.01405276358127594, 'train/mean_average_precision': 0.7812440709210555, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932153395867975, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27598998109246764, 'test/num_examples': 43793, 'score': 42276.55060958862, 'total_duration': 64135.954810619354, 'accumulated_submission_time': 42276.55060958862, 'accumulated_eval_time': 21848.249663591385, 'accumulated_logging_time': 7.499475479125977}
I0306 04:08:02.530646 139768268125952 logging_writer.py:48] [130105] accumulated_eval_time=21848.249664, accumulated_logging_time=7.499475, accumulated_submission_time=42276.550610, global_step=130105, preemption_count=0, score=42276.550610, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275990, test/num_examples=43793, total_duration=64135.954811, train/accuracy=0.995502, train/loss=0.014053, train/mean_average_precision=0.781244, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293215, validation/num_examples=43793
I0306 04:08:34.383431 139776159401728 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.14325354993343353, loss=0.018466930836439133
I0306 04:09:07.282783 139768268125952 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.1488654464483261, loss=0.020057646557688713
I0306 04:09:39.552528 139776159401728 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.14343419671058655, loss=0.018058037385344505
I0306 04:10:12.299896 139768268125952 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.13815736770629883, loss=0.018767500296235085
I0306 04:10:44.561267 139776159401728 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.14806289970874786, loss=0.01714077591896057
I0306 04:11:16.909874 139768268125952 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.14288942515850067, loss=0.018148602917790413
I0306 04:11:49.611117 139776159401728 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.15100033581256866, loss=0.01722814328968525
I0306 04:12:02.496741 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:13:47.662717 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:13:50.710665 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:13:53.705226 139937033598784 submission_runner.py:411] Time since start: 64487.18s, 	Step: 130840, 	{'train/accuracy': 0.9954874515533447, 'train/loss': 0.014094805344939232, 'train/mean_average_precision': 0.7717035971463917, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2930906746882962, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759373339764978, 'test/num_examples': 43793, 'score': 42516.48381781578, 'total_duration': 64487.176218509674, 'accumulated_submission_time': 42516.48381781578, 'accumulated_eval_time': 21959.45810699463, 'accumulated_logging_time': 7.557837963104248}
I0306 04:13:53.745389 139769339950848 logging_writer.py:48] [130840] accumulated_eval_time=21959.458107, accumulated_logging_time=7.557838, accumulated_submission_time=42516.483818, global_step=130840, preemption_count=0, score=42516.483818, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=64487.176219, train/accuracy=0.995487, train/loss=0.014095, train/mean_average_precision=0.771704, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293091, validation/num_examples=43793
I0306 04:14:13.712696 139776167794432 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.14082270860671997, loss=0.01958787441253662
I0306 04:14:46.049409 139769339950848 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.14261747896671295, loss=0.01866268739104271
I0306 04:15:18.386217 139776167794432 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.12572970986366272, loss=0.01661720871925354
I0306 04:15:50.195568 139769339950848 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.13427458703517914, loss=0.01848309300839901
I0306 04:16:22.649691 139776167794432 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.12154330313205719, loss=0.014188448898494244
I0306 04:16:54.657438 139769339950848 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.14233528077602386, loss=0.018680337816476822
I0306 04:17:26.712673 139776167794432 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.15489475429058075, loss=0.01779833808541298
I0306 04:17:53.905278 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:19:33.703362 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:19:36.759323 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:19:41.710999 139937033598784 submission_runner.py:411] Time since start: 64835.18s, 	Step: 131586, 	{'train/accuracy': 0.995464563369751, 'train/loss': 0.014273915439844131, 'train/mean_average_precision': 0.7646260937715028, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321558027246314, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2758972165051127, 'test/num_examples': 43793, 'score': 42756.610813617706, 'total_duration': 64835.18199014664, 'accumulated_submission_time': 42756.610813617706, 'accumulated_eval_time': 22067.263786792755, 'accumulated_logging_time': 7.610391139984131}
I0306 04:19:41.752494 139760711874304 logging_writer.py:48] [131586] accumulated_eval_time=22067.263787, accumulated_logging_time=7.610391, accumulated_submission_time=42756.610814, global_step=131586, preemption_count=0, score=42756.610814, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275897, test/num_examples=43793, total_duration=64835.181990, train/accuracy=0.995465, train/loss=0.014274, train/mean_average_precision=0.764626, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293216, validation/num_examples=43793
I0306 04:19:46.585958 139768268125952 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.13785357773303986, loss=0.01768924854695797
I0306 04:20:18.766607 139760711874304 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.11438660323619843, loss=0.013368597254157066
I0306 04:20:51.116346 139768268125952 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.16005197167396545, loss=0.019298141822218895
I0306 04:21:23.317424 139760711874304 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.13614000380039215, loss=0.01841079071164131
I0306 04:21:55.361269 139768268125952 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.1547982394695282, loss=0.01954452134668827
I0306 04:22:27.562013 139760711874304 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.15050360560417175, loss=0.018074408173561096
I0306 04:22:59.866022 139768268125952 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.14637592434883118, loss=0.018956007435917854
I0306 04:23:31.549301 139760711874304 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.1343112736940384, loss=0.01913309283554554
I0306 04:23:41.743658 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:25:25.961561 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:25:29.114882 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:25:32.159730 139937033598784 submission_runner.py:411] Time since start: 65185.63s, 	Step: 132333, 	{'train/accuracy': 0.9955654144287109, 'train/loss': 0.013841940090060234, 'train/mean_average_precision': 0.7805771446483449, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932781435099585, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759797114188226, 'test/num_examples': 43793, 'score': 42996.56942439079, 'total_duration': 65185.630719423294, 'accumulated_submission_time': 42996.56942439079, 'accumulated_eval_time': 22177.679812192917, 'accumulated_logging_time': 7.664034605026245}
I0306 04:25:32.200730 139776159401728 logging_writer.py:48] [132333] accumulated_eval_time=22177.679812, accumulated_logging_time=7.664035, accumulated_submission_time=42996.569424, global_step=132333, preemption_count=0, score=42996.569424, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275980, test/num_examples=43793, total_duration=65185.630719, train/accuracy=0.995565, train/loss=0.013842, train/mean_average_precision=0.780577, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293278, validation/num_examples=43793
I0306 04:25:54.342840 139776167794432 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.13027805089950562, loss=0.01827041618525982
I0306 04:26:26.741143 139776159401728 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.1414700746536255, loss=0.018506869673728943
I0306 04:26:58.757457 139776167794432 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.1492900401353836, loss=0.019965430721640587
I0306 04:27:31.072609 139776159401728 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.1399271935224533, loss=0.018358195200562477
I0306 04:28:03.092613 139776167794432 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.13806505501270294, loss=0.016929470002651215
I0306 04:28:35.227492 139776159401728 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.13540010154247284, loss=0.017039941623806953
I0306 04:29:07.151933 139776167794432 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.13477884232997894, loss=0.015490217134356499
I0306 04:29:32.404095 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:31:14.923372 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:31:17.955662 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:31:20.974363 139937033598784 submission_runner.py:411] Time since start: 65534.45s, 	Step: 133080, 	{'train/accuracy': 0.9954608678817749, 'train/loss': 0.014066028408706188, 'train/mean_average_precision': 0.7814404601298929, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2933030047203722, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27595130260704476, 'test/num_examples': 43793, 'score': 43236.740758657455, 'total_duration': 65534.44535279274, 'accumulated_submission_time': 43236.740758657455, 'accumulated_eval_time': 22286.250038146973, 'accumulated_logging_time': 7.716859579086304}
I0306 04:31:21.014829 139760711874304 logging_writer.py:48] [133080] accumulated_eval_time=22286.250038, accumulated_logging_time=7.716860, accumulated_submission_time=43236.740759, global_step=133080, preemption_count=0, score=43236.740759, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275951, test/num_examples=43793, total_duration=65534.445353, train/accuracy=0.995461, train/loss=0.014066, train/mean_average_precision=0.781440, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293303, validation/num_examples=43793
I0306 04:31:27.863051 139768268125952 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.13556085526943207, loss=0.017141705378890038
I0306 04:32:00.014602 139760711874304 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.14680424332618713, loss=0.020165940746665
I0306 04:32:32.223752 139768268125952 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.12505684792995453, loss=0.017407961189746857
I0306 04:33:04.942308 139760711874304 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.14397874474525452, loss=0.017659759148955345
I0306 04:33:37.618461 139768268125952 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.13729441165924072, loss=0.018621986731886864
I0306 04:34:10.214206 139760711874304 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.14502397179603577, loss=0.0180527176707983
I0306 04:34:42.717706 139768268125952 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.13286490738391876, loss=0.016420194879174232
I0306 04:35:15.074660 139760711874304 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.1455341875553131, loss=0.017990119755268097
I0306 04:35:21.114142 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:37:07.711210 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:37:11.114110 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:37:14.512306 139937033598784 submission_runner.py:411] Time since start: 65887.98s, 	Step: 133820, 	{'train/accuracy': 0.9954729676246643, 'train/loss': 0.014144431799650192, 'train/mean_average_precision': 0.7757278511595392, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.293336474690113, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27592778583202165, 'test/num_examples': 43793, 'score': 43476.80657362938, 'total_duration': 65887.98328089714, 'accumulated_submission_time': 43476.80657362938, 'accumulated_eval_time': 22399.648137807846, 'accumulated_logging_time': 7.770319223403931}
I0306 04:37:14.560024 139776159401728 logging_writer.py:48] [133820] accumulated_eval_time=22399.648138, accumulated_logging_time=7.770319, accumulated_submission_time=43476.806574, global_step=133820, preemption_count=0, score=43476.806574, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275928, test/num_examples=43793, total_duration=65887.983281, train/accuracy=0.995473, train/loss=0.014144, train/mean_average_precision=0.775728, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293336, validation/num_examples=43793
I0306 04:37:41.373668 139776167794432 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.14043119549751282, loss=0.020146608352661133
I0306 04:38:14.039978 139776159401728 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.1517353653907776, loss=0.01953105814754963
I0306 04:38:47.034672 139776167794432 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.1280118227005005, loss=0.015670092776417732
I0306 04:39:19.963191 139776159401728 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.13784988224506378, loss=0.01578485779464245
I0306 04:39:52.813650 139776167794432 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.12216449528932571, loss=0.01765221357345581
I0306 04:40:25.573382 139776159401728 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.14662288129329681, loss=0.018441863358020782
I0306 04:40:58.113966 139776167794432 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.13337482511997223, loss=0.0158580020070076
I0306 04:41:14.671532 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:42:57.073988 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:43:00.130172 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:43:03.268050 139937033598784 submission_runner.py:411] Time since start: 66236.74s, 	Step: 134551, 	{'train/accuracy': 0.9955356121063232, 'train/loss': 0.013982895761728287, 'train/mean_average_precision': 0.7739661532021433, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29318778642851817, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759842286678688, 'test/num_examples': 43793, 'score': 43716.882601976395, 'total_duration': 66236.73903632164, 'accumulated_submission_time': 43716.882601976395, 'accumulated_eval_time': 22508.24461197853, 'accumulated_logging_time': 7.830138921737671}
I0306 04:43:03.310876 139760711874304 logging_writer.py:48] [134551] accumulated_eval_time=22508.244612, accumulated_logging_time=7.830139, accumulated_submission_time=43716.882602, global_step=134551, preemption_count=0, score=43716.882602, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275984, test/num_examples=43793, total_duration=66236.739036, train/accuracy=0.995536, train/loss=0.013983, train/mean_average_precision=0.773966, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293188, validation/num_examples=43793
I0306 04:43:19.487474 139769339950848 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.15121425688266754, loss=0.017521897330880165
I0306 04:43:51.498312 139760711874304 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.13942845165729523, loss=0.017756590619683266
I0306 04:44:23.560063 139769339950848 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.14558455348014832, loss=0.017966005951166153
I0306 04:44:55.405345 139760711874304 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.14196547865867615, loss=0.016653388738632202
I0306 04:45:27.659283 139769339950848 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.14797981083393097, loss=0.018703017383813858
I0306 04:45:59.987894 139760711874304 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.15489627420902252, loss=0.019161883741617203
I0306 04:46:32.543837 139769339950848 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.1365358531475067, loss=0.015645036473870277
I0306 04:47:03.322608 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:48:50.621454 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:48:53.754220 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:48:56.778299 139937033598784 submission_runner.py:411] Time since start: 66590.25s, 	Step: 135297, 	{'train/accuracy': 0.995487630367279, 'train/loss': 0.014126190915703773, 'train/mean_average_precision': 0.7703792187677057, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932659339598245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27593952280175643, 'test/num_examples': 43793, 'score': 43956.86270856857, 'total_duration': 66590.24928569794, 'accumulated_submission_time': 43956.86270856857, 'accumulated_eval_time': 22621.700261354446, 'accumulated_logging_time': 7.88405442237854}
I0306 04:48:56.819288 139768268125952 logging_writer.py:48] [135297] accumulated_eval_time=22621.700261, accumulated_logging_time=7.884054, accumulated_submission_time=43956.862709, global_step=135297, preemption_count=0, score=43956.862709, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275940, test/num_examples=43793, total_duration=66590.249286, train/accuracy=0.995488, train/loss=0.014126, train/mean_average_precision=0.770379, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293266, validation/num_examples=43793
I0306 04:48:58.152430 139776167794432 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.13865190744400024, loss=0.017072366550564766
I0306 04:49:30.621144 139768268125952 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.15661121904850006, loss=0.02015424333512783
I0306 04:50:03.792125 139776167794432 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.14091669023036957, loss=0.01747547835111618
I0306 04:50:35.822376 139768268125952 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.15410912036895752, loss=0.01853841170668602
I0306 04:51:07.969590 139776167794432 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.13522157073020935, loss=0.017758917063474655
I0306 04:51:40.780776 139768268125952 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.13158905506134033, loss=0.016238396987318993
I0306 04:52:13.138720 139776167794432 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.1472453474998474, loss=0.018964625895023346
I0306 04:52:45.609171 139768268125952 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.14222857356071472, loss=0.01856769062578678
I0306 04:52:56.978001 139937033598784 spec.py:321] Evaluating on the training split.
I0306 04:54:43.180315 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 04:54:46.298005 139937033598784 spec.py:349] Evaluating on the test split.
I0306 04:54:49.346983 139937033598784 submission_runner.py:411] Time since start: 66942.82s, 	Step: 136036, 	{'train/accuracy': 0.995522141456604, 'train/loss': 0.014012542553246021, 'train/mean_average_precision': 0.7761438917983973, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932118139807845, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759236846054778, 'test/num_examples': 43793, 'score': 44196.990394830704, 'total_duration': 66942.81797623634, 'accumulated_submission_time': 44196.990394830704, 'accumulated_eval_time': 22734.069207906723, 'accumulated_logging_time': 7.935742139816284}
I0306 04:54:49.388142 139769339950848 logging_writer.py:48] [136036] accumulated_eval_time=22734.069208, accumulated_logging_time=7.935742, accumulated_submission_time=44196.990395, global_step=136036, preemption_count=0, score=44196.990395, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275924, test/num_examples=43793, total_duration=66942.817976, train/accuracy=0.995522, train/loss=0.014013, train/mean_average_precision=0.776144, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293212, validation/num_examples=43793
I0306 04:55:10.947877 139776159401728 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.1423855721950531, loss=0.017149943858385086
I0306 04:55:43.398358 139769339950848 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.14768922328948975, loss=0.01896856538951397
I0306 04:56:15.908418 139776159401728 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.15628258883953094, loss=0.019077550619840622
I0306 04:56:48.220132 139769339950848 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.13585390150547028, loss=0.017262760549783707
I0306 04:57:20.872141 139776159401728 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.12921912968158722, loss=0.016706863418221474
I0306 04:57:53.606003 139769339950848 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.15721279382705688, loss=0.02132069505751133
I0306 04:58:26.339078 139776159401728 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.1510997712612152, loss=0.019908428192138672
I0306 04:58:49.489817 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:00:35.328586 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:00:38.664225 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:00:41.983749 139937033598784 submission_runner.py:411] Time since start: 67295.45s, 	Step: 136772, 	{'train/accuracy': 0.995475709438324, 'train/loss': 0.014075158163905144, 'train/mean_average_precision': 0.7810047617352318, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931009444456341, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27599075012938773, 'test/num_examples': 43793, 'score': 44437.059807538986, 'total_duration': 67295.4547200203, 'accumulated_submission_time': 44437.059807538986, 'accumulated_eval_time': 22846.563081502914, 'accumulated_logging_time': 7.987916707992554}
I0306 05:00:42.030711 139760711874304 logging_writer.py:48] [136772] accumulated_eval_time=22846.563082, accumulated_logging_time=7.987917, accumulated_submission_time=44437.059808, global_step=136772, preemption_count=0, score=44437.059808, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275991, test/num_examples=43793, total_duration=67295.454720, train/accuracy=0.995476, train/loss=0.014075, train/mean_average_precision=0.781005, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293101, validation/num_examples=43793
I0306 05:00:51.659471 139776167794432 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.14815738797187805, loss=0.019118374213576317
I0306 05:01:24.269244 139760711874304 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.13734079897403717, loss=0.018987661227583885
I0306 05:01:56.702718 139776167794432 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.14572381973266602, loss=0.019171644002199173
I0306 05:02:29.661599 139760711874304 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.15532450377941132, loss=0.01823893003165722
I0306 05:03:02.305668 139776167794432 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.13281883299350739, loss=0.016216738149523735
I0306 05:03:34.792854 139760711874304 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.1712011694908142, loss=0.019111530855298042
I0306 05:04:07.271535 139776167794432 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.12970173358917236, loss=0.017613917589187622
I0306 05:04:39.887538 139760711874304 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.14474532008171082, loss=0.018106868490576744
I0306 05:04:42.211179 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:06:24.646597 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:06:27.761509 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:06:32.810616 139937033598784 submission_runner.py:411] Time since start: 67646.28s, 	Step: 137508, 	{'train/accuracy': 0.9954957365989685, 'train/loss': 0.014020131900906563, 'train/mean_average_precision': 0.7739173571564724, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29320006947169325, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27596270562728, 'test/num_examples': 43793, 'score': 44677.20724821091, 'total_duration': 67646.28160524368, 'accumulated_submission_time': 44677.20724821091, 'accumulated_eval_time': 22957.162472248077, 'accumulated_logging_time': 8.046531915664673}
I0306 05:06:32.854138 139768268125952 logging_writer.py:48] [137508] accumulated_eval_time=22957.162472, accumulated_logging_time=8.046532, accumulated_submission_time=44677.207248, global_step=137508, preemption_count=0, score=44677.207248, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275963, test/num_examples=43793, total_duration=67646.281605, train/accuracy=0.995496, train/loss=0.014020, train/mean_average_precision=0.773917, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293200, validation/num_examples=43793
I0306 05:07:03.025535 139776159401728 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.1395924836397171, loss=0.017741704359650612
I0306 05:07:35.504376 139768268125952 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.14274735748767853, loss=0.020204506814479828
I0306 05:08:08.145091 139776159401728 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.1305791735649109, loss=0.016121916472911835
I0306 05:08:40.875395 139768268125952 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.1437552124261856, loss=0.01923513226211071
I0306 05:09:13.466330 139776159401728 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.15841099619865417, loss=0.01913588121533394
I0306 05:09:45.673201 139768268125952 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.1384725421667099, loss=0.02014574036002159
I0306 05:10:18.068901 139776159401728 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.13144950568675995, loss=0.016817141324281693
I0306 05:10:32.919506 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:12:15.268330 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:12:18.353676 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:12:21.338819 139937033598784 submission_runner.py:411] Time since start: 67994.81s, 	Step: 138247, 	{'train/accuracy': 0.9955067038536072, 'train/loss': 0.014062287285923958, 'train/mean_average_precision': 0.779246398721551, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931393059919693, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27590873967096974, 'test/num_examples': 43793, 'score': 44917.2401702404, 'total_duration': 67994.80980920792, 'accumulated_submission_time': 44917.2401702404, 'accumulated_eval_time': 23065.581743240356, 'accumulated_logging_time': 8.102015018463135}
I0306 05:12:21.380620 139760711874304 logging_writer.py:48] [138247] accumulated_eval_time=23065.581743, accumulated_logging_time=8.102015, accumulated_submission_time=44917.240170, global_step=138247, preemption_count=0, score=44917.240170, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275909, test/num_examples=43793, total_duration=67994.809809, train/accuracy=0.995507, train/loss=0.014062, train/mean_average_precision=0.779246, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293139, validation/num_examples=43793
I0306 05:12:38.761966 139776167794432 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.1655726134777069, loss=0.019363168627023697
I0306 05:13:11.029607 139760711874304 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.15014508366584778, loss=0.017595315352082253
I0306 05:13:43.264702 139776167794432 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.14307676255702972, loss=0.01838086172938347
I0306 05:14:15.564124 139760711874304 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.14576955139636993, loss=0.017617613077163696
I0306 05:14:47.724909 139776167794432 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.16157227754592896, loss=0.01883796416223049
I0306 05:15:19.815432 139760711874304 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.16209428012371063, loss=0.018020307645201683
I0306 05:15:52.143111 139776167794432 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.150974303483963, loss=0.018906239420175552
I0306 05:16:21.344280 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:18:01.621579 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:18:04.678387 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:18:07.727297 139937033598784 submission_runner.py:411] Time since start: 68341.20s, 	Step: 138991, 	{'train/accuracy': 0.9954759478569031, 'train/loss': 0.014120928011834621, 'train/mean_average_precision': 0.7633788395828764, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931961719757504, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759749694299141, 'test/num_examples': 43793, 'score': 45157.172426223755, 'total_duration': 68341.19829106331, 'accumulated_submission_time': 45157.172426223755, 'accumulated_eval_time': 23171.964718818665, 'accumulated_logging_time': 8.154671430587769}
I0306 05:18:07.768842 139769339950848 logging_writer.py:48] [138991] accumulated_eval_time=23171.964719, accumulated_logging_time=8.154671, accumulated_submission_time=45157.172426, global_step=138991, preemption_count=0, score=45157.172426, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275975, test/num_examples=43793, total_duration=68341.198291, train/accuracy=0.995476, train/loss=0.014121, train/mean_average_precision=0.763379, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293196, validation/num_examples=43793
I0306 05:18:11.139605 139776159401728 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.15691514313220978, loss=0.01759033463895321
I0306 05:18:43.530243 139769339950848 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.14025276899337769, loss=0.01728176511824131
I0306 05:19:15.178259 139776159401728 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.15003807842731476, loss=0.01834133453667164
I0306 05:19:47.297090 139769339950848 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.12344178557395935, loss=0.0156340841203928
I0306 05:20:19.544536 139776159401728 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.14533188939094543, loss=0.019911281764507294
I0306 05:20:51.683160 139769339950848 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.1440318524837494, loss=0.018319180235266685
I0306 05:21:24.624193 139776159401728 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.14725252985954285, loss=0.016953429207205772
I0306 05:21:57.900532 139769339950848 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.1333232820034027, loss=0.01589927449822426
I0306 05:22:07.884212 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:23:47.953506 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:23:50.955034 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:23:53.970606 139937033598784 submission_runner.py:411] Time since start: 68687.44s, 	Step: 139731, 	{'train/accuracy': 0.9955406785011292, 'train/loss': 0.014015556313097477, 'train/mean_average_precision': 0.7742222547996289, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29314201152439906, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27593317337070056, 'test/num_examples': 43793, 'score': 45397.25456047058, 'total_duration': 68687.44159507751, 'accumulated_submission_time': 45397.25456047058, 'accumulated_eval_time': 23278.051063776016, 'accumulated_logging_time': 8.208355903625488}
I0306 05:23:54.013144 139760711874304 logging_writer.py:48] [139731] accumulated_eval_time=23278.051064, accumulated_logging_time=8.208356, accumulated_submission_time=45397.254560, global_step=139731, preemption_count=0, score=45397.254560, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275933, test/num_examples=43793, total_duration=68687.441595, train/accuracy=0.995541, train/loss=0.014016, train/mean_average_precision=0.774222, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293142, validation/num_examples=43793
I0306 05:24:16.575017 139768268125952 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.14433155953884125, loss=0.016689760610461235
I0306 05:24:48.347167 139760711874304 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.12351905554533005, loss=0.016856878995895386
I0306 05:25:20.575092 139768268125952 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.15505856275558472, loss=0.020038165152072906
I0306 05:25:52.468165 139760711874304 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.13967278599739075, loss=0.017983298748731613
I0306 05:26:24.649900 139768268125952 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.15273940563201904, loss=0.019625773653388023
I0306 05:26:56.266365 139760711874304 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.12366192042827606, loss=0.016735315322875977
I0306 05:27:28.512292 139768268125952 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.14532624185085297, loss=0.01844140887260437
I0306 05:27:54.315455 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:29:37.620232 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:29:40.746964 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:29:43.746048 139937033598784 submission_runner.py:411] Time since start: 69037.22s, 	Step: 140480, 	{'train/accuracy': 0.9954973459243774, 'train/loss': 0.01400283444672823, 'train/mean_average_precision': 0.786076248561501, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29327087110214634, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759290721408751, 'test/num_examples': 43793, 'score': 45637.525134563446, 'total_duration': 69037.21703863144, 'accumulated_submission_time': 45637.525134563446, 'accumulated_eval_time': 23387.48162317276, 'accumulated_logging_time': 8.26203179359436}
I0306 05:29:43.791346 139769339950848 logging_writer.py:48] [140480] accumulated_eval_time=23387.481623, accumulated_logging_time=8.262032, accumulated_submission_time=45637.525135, global_step=140480, preemption_count=0, score=45637.525135, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275929, test/num_examples=43793, total_duration=69037.217039, train/accuracy=0.995497, train/loss=0.014003, train/mean_average_precision=0.786076, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293271, validation/num_examples=43793
I0306 05:29:50.431218 139776159401728 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.14003559947013855, loss=0.01811802200973034
I0306 05:30:22.379096 139769339950848 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.1367129683494568, loss=0.01914660632610321
I0306 05:30:54.518373 139776159401728 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.12906882166862488, loss=0.01665297895669937
I0306 05:31:26.344190 139769339950848 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.12813594937324524, loss=0.016547691076993942
I0306 05:31:58.498734 139776159401728 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.1521608531475067, loss=0.01747569441795349
I0306 05:32:30.325521 139769339950848 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.1252417415380478, loss=0.017323030158877373
I0306 05:33:02.425835 139776159401728 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.1456642597913742, loss=0.017597319558262825
I0306 05:33:35.141808 139769339950848 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.1404723823070526, loss=0.016545210033655167
I0306 05:33:43.957983 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:35:28.705890 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:35:31.742086 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:35:34.749205 139937033598784 submission_runner.py:411] Time since start: 69388.22s, 	Step: 141228, 	{'train/accuracy': 0.9955053925514221, 'train/loss': 0.014024607837200165, 'train/mean_average_precision': 0.7788733946088782, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29328869168685684, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2760197268325939, 'test/num_examples': 43793, 'score': 45877.65914797783, 'total_duration': 69388.22018957138, 'accumulated_submission_time': 45877.65914797783, 'accumulated_eval_time': 23498.272793293, 'accumulated_logging_time': 8.319572448730469}
I0306 05:35:34.791804 139768268125952 logging_writer.py:48] [141228] accumulated_eval_time=23498.272793, accumulated_logging_time=8.319572, accumulated_submission_time=45877.659148, global_step=141228, preemption_count=0, score=45877.659148, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276020, test/num_examples=43793, total_duration=69388.220190, train/accuracy=0.995505, train/loss=0.014025, train/mean_average_precision=0.778873, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293289, validation/num_examples=43793
I0306 05:35:58.595365 139776167794432 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.12413271516561508, loss=0.016177356243133545
I0306 05:36:31.089868 139768268125952 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.14982397854328156, loss=0.018735777586698532
I0306 05:37:04.105081 139776167794432 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.1598520427942276, loss=0.019034355878829956
I0306 05:37:36.519140 139768268125952 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.1459694355726242, loss=0.01915610022842884
I0306 05:38:08.761602 139776167794432 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.1350642591714859, loss=0.017303548753261566
I0306 05:38:40.994344 139768268125952 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.1529093086719513, loss=0.019570667296648026
I0306 05:39:12.887416 139776167794432 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.14738671481609344, loss=0.017923997715115547
I0306 05:39:34.823233 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:41:12.061967 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:41:15.124892 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:41:18.163076 139937033598784 submission_runner.py:411] Time since start: 69731.63s, 	Step: 141970, 	{'train/accuracy': 0.9954705834388733, 'train/loss': 0.014141528867185116, 'train/mean_average_precision': 0.7747681263770184, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29316372447355404, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760298456241528, 'test/num_examples': 43793, 'score': 46117.658863544464, 'total_duration': 69731.63406729698, 'accumulated_submission_time': 46117.658863544464, 'accumulated_eval_time': 23601.612596273422, 'accumulated_logging_time': 8.372910976409912}
I0306 05:41:18.205122 139769339950848 logging_writer.py:48] [141970] accumulated_eval_time=23601.612596, accumulated_logging_time=8.372911, accumulated_submission_time=46117.658864, global_step=141970, preemption_count=0, score=46117.658864, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276030, test/num_examples=43793, total_duration=69731.634067, train/accuracy=0.995471, train/loss=0.014142, train/mean_average_precision=0.774768, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293164, validation/num_examples=43793
I0306 05:41:28.278868 139776159401728 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.13596655428409576, loss=0.018517404794692993
I0306 05:42:00.357606 139769339950848 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.14143159985542297, loss=0.019119540229439735
I0306 05:42:32.559678 139776159401728 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.1323331892490387, loss=0.019008291885256767
I0306 05:43:04.669881 139769339950848 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.13300733268260956, loss=0.016473159193992615
I0306 05:43:36.759696 139776159401728 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.14357204735279083, loss=0.018474487587809563
I0306 05:44:09.317431 139769339950848 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.1552685797214508, loss=0.01739882491528988
I0306 05:44:42.265244 139776159401728 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.1417175680398941, loss=0.020320825278759003
I0306 05:45:15.308729 139769339950848 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.14580707252025604, loss=0.018566591665148735
I0306 05:45:18.274890 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:47:01.781366 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:47:04.850147 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:47:07.884881 139937033598784 submission_runner.py:411] Time since start: 70081.36s, 	Step: 142710, 	{'train/accuracy': 0.9955161213874817, 'train/loss': 0.01399329025298357, 'train/mean_average_precision': 0.76743690980945, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321529447273414, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759479146602972, 'test/num_examples': 43793, 'score': 46357.696271657944, 'total_duration': 70081.35587143898, 'accumulated_submission_time': 46357.696271657944, 'accumulated_eval_time': 23711.222553491592, 'accumulated_logging_time': 8.42588186264038}
I0306 05:47:07.927607 139768268125952 logging_writer.py:48] [142710] accumulated_eval_time=23711.222553, accumulated_logging_time=8.425882, accumulated_submission_time=46357.696272, global_step=142710, preemption_count=0, score=46357.696272, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275948, test/num_examples=43793, total_duration=70081.355871, train/accuracy=0.995516, train/loss=0.013993, train/mean_average_precision=0.767437, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293215, validation/num_examples=43793
I0306 05:47:37.642798 139776167794432 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.16218917071819305, loss=0.019074516370892525
I0306 05:48:10.338720 139768268125952 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.15121246874332428, loss=0.016480932012200356
I0306 05:48:42.873857 139776167794432 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.13870936632156372, loss=0.01688646525144577
I0306 05:49:15.041524 139768268125952 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.136440709233284, loss=0.01744796521961689
I0306 05:49:47.346963 139776167794432 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.15247821807861328, loss=0.017990635707974434
I0306 05:50:19.982332 139768268125952 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.12445054203271866, loss=0.01584351249039173
I0306 05:50:52.324203 139776167794432 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.14179988205432892, loss=0.0183316208422184
I0306 05:51:08.136066 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:52:47.859547 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:52:50.926499 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:52:53.948531 139937033598784 submission_runner.py:411] Time since start: 70427.42s, 	Step: 143450, 	{'train/accuracy': 0.9954485893249512, 'train/loss': 0.014231325127184391, 'train/mean_average_precision': 0.7793896921346442, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29319621933586115, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759225501940873, 'test/num_examples': 43793, 'score': 46597.8717443943, 'total_duration': 70427.41950631142, 'accumulated_submission_time': 46597.8717443943, 'accumulated_eval_time': 23817.034957647324, 'accumulated_logging_time': 8.481031894683838}
I0306 05:52:53.991483 139760711874304 logging_writer.py:48] [143450] accumulated_eval_time=23817.034958, accumulated_logging_time=8.481032, accumulated_submission_time=46597.871744, global_step=143450, preemption_count=0, score=46597.871744, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275923, test/num_examples=43793, total_duration=70427.419506, train/accuracy=0.995449, train/loss=0.014231, train/mean_average_precision=0.779390, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293196, validation/num_examples=43793
I0306 05:53:10.325792 139776159401728 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.14366872608661652, loss=0.018895329907536507
I0306 05:53:42.482468 139760711874304 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.14191578328609467, loss=0.01688949204981327
I0306 05:54:14.562389 139776159401728 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.1514311581850052, loss=0.018546469509601593
I0306 05:54:46.819360 139760711874304 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.13796791434288025, loss=0.018782850354909897
I0306 05:55:19.263942 139776159401728 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.14353445172309875, loss=0.018049078062176704
I0306 05:55:51.873303 139760711874304 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.13680139183998108, loss=0.017412617802619934
I0306 05:56:24.234867 139776159401728 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.171712726354599, loss=0.01948654092848301
I0306 05:56:54.143300 139937033598784 spec.py:321] Evaluating on the training split.
I0306 05:58:40.421885 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 05:58:43.896210 139937033598784 spec.py:349] Evaluating on the test split.
I0306 05:58:47.225937 139937033598784 submission_runner.py:411] Time since start: 70780.70s, 	Step: 144195, 	{'train/accuracy': 0.9955180287361145, 'train/loss': 0.014017091132700443, 'train/mean_average_precision': 0.7712021251836819, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29324763803252324, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27597236672649045, 'test/num_examples': 43793, 'score': 46837.9917037487, 'total_duration': 70780.69690322876, 'accumulated_submission_time': 46837.9917037487, 'accumulated_eval_time': 23930.11753678322, 'accumulated_logging_time': 8.535125494003296}
I0306 05:58:47.274735 139768268125952 logging_writer.py:48] [144195] accumulated_eval_time=23930.117537, accumulated_logging_time=8.535125, accumulated_submission_time=46837.991704, global_step=144195, preemption_count=0, score=46837.991704, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275972, test/num_examples=43793, total_duration=70780.696903, train/accuracy=0.995518, train/loss=0.014017, train/mean_average_precision=0.771202, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293248, validation/num_examples=43793
I0306 05:58:49.296273 139769339950848 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.13043181598186493, loss=0.017490873113274574
I0306 05:59:21.854181 139768268125952 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.1418965458869934, loss=0.017077917233109474
I0306 05:59:54.096740 139769339950848 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.13476450741291046, loss=0.017090870067477226
I0306 06:00:26.650366 139768268125952 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.14874093234539032, loss=0.01801486127078533
I0306 06:00:59.245883 139769339950848 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.13969679176807404, loss=0.015787694603204727
I0306 06:01:31.765959 139768268125952 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.13652172684669495, loss=0.016034895554184914
I0306 06:02:03.895267 139769339950848 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.12399684637784958, loss=0.015831345692276955
I0306 06:02:35.996257 139768268125952 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.1497395932674408, loss=0.017637858167290688
I0306 06:02:47.359519 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:04:32.738422 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:04:35.949029 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:04:39.062762 139937033598784 submission_runner.py:411] Time since start: 71132.53s, 	Step: 144936, 	{'train/accuracy': 0.9955515265464783, 'train/loss': 0.013878235593438148, 'train/mean_average_precision': 0.7903820845194295, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932307445075147, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27589432844933504, 'test/num_examples': 43793, 'score': 47078.042530059814, 'total_duration': 71132.5337510109, 'accumulated_submission_time': 47078.042530059814, 'accumulated_eval_time': 24041.820734739304, 'accumulated_logging_time': 8.595582008361816}
I0306 06:04:39.106023 139760711874304 logging_writer.py:48] [144936] accumulated_eval_time=24041.820735, accumulated_logging_time=8.595582, accumulated_submission_time=47078.042530, global_step=144936, preemption_count=0, score=47078.042530, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275894, test/num_examples=43793, total_duration=71132.533751, train/accuracy=0.995552, train/loss=0.013878, train/mean_average_precision=0.790382, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293231, validation/num_examples=43793
I0306 06:05:00.054610 139776167794432 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.15890060365200043, loss=0.01694246008992195
I0306 06:05:32.308209 139760711874304 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.14280763268470764, loss=0.017272457480430603
I0306 06:06:04.216455 139776167794432 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.13232283294200897, loss=0.01672337017953396
I0306 06:06:36.412983 139760711874304 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.13511039316654205, loss=0.017226092517375946
I0306 06:07:08.480836 139776167794432 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.1299723982810974, loss=0.01642511412501335
I0306 06:07:40.845389 139760711874304 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.15383726358413696, loss=0.019257834181189537
I0306 06:08:12.817865 139776167794432 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.13849292695522308, loss=0.01757548749446869
I0306 06:08:39.307506 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:10:23.662448 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:10:26.648668 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:10:29.605675 139937033598784 submission_runner.py:411] Time since start: 71483.08s, 	Step: 145685, 	{'train/accuracy': 0.9954928755760193, 'train/loss': 0.014047583565115929, 'train/mean_average_precision': 0.7702413222315503, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29323092145953855, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27591838093499926, 'test/num_examples': 43793, 'score': 47318.212882995605, 'total_duration': 71483.07665705681, 'accumulated_submission_time': 47318.212882995605, 'accumulated_eval_time': 24152.11885714531, 'accumulated_logging_time': 8.649686574935913}
I0306 06:10:29.648230 139768268125952 logging_writer.py:48] [145685] accumulated_eval_time=24152.118857, accumulated_logging_time=8.649687, accumulated_submission_time=47318.212883, global_step=145685, preemption_count=0, score=47318.212883, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275918, test/num_examples=43793, total_duration=71483.076657, train/accuracy=0.995493, train/loss=0.014048, train/mean_average_precision=0.770241, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293231, validation/num_examples=43793
I0306 06:10:34.755920 139769339950848 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.14300836622714996, loss=0.019147295504808426
I0306 06:11:06.593106 139768268125952 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.13214200735092163, loss=0.016236336901783943
I0306 06:11:38.416380 139769339950848 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.14574484527111053, loss=0.014728941023349762
I0306 06:12:10.439552 139768268125952 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.15374022722244263, loss=0.01924092322587967
I0306 06:12:42.645937 139769339950848 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.15114834904670715, loss=0.018685558810830116
I0306 06:13:15.067975 139768268125952 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.13961158692836761, loss=0.01753993332386017
I0306 06:13:46.804220 139769339950848 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.11577180027961731, loss=0.015687815845012665
I0306 06:14:19.411488 139768268125952 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.1360190510749817, loss=0.017150089144706726
I0306 06:14:29.731207 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:16:07.897346 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:16:11.041799 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:16:14.053218 139937033598784 submission_runner.py:411] Time since start: 71827.52s, 	Step: 146433, 	{'train/accuracy': 0.9955329895019531, 'train/loss': 0.014045913703739643, 'train/mean_average_precision': 0.7693459707086195, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932822507404936, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27609296919586607, 'test/num_examples': 43793, 'score': 47558.26383471489, 'total_duration': 71827.52420902252, 'accumulated_submission_time': 47558.26383471489, 'accumulated_eval_time': 24256.440824985504, 'accumulated_logging_time': 8.703256368637085}
I0306 06:16:14.096574 139760711874304 logging_writer.py:48] [146433] accumulated_eval_time=24256.440825, accumulated_logging_time=8.703256, accumulated_submission_time=47558.263835, global_step=146433, preemption_count=0, score=47558.263835, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276093, test/num_examples=43793, total_duration=71827.524209, train/accuracy=0.995533, train/loss=0.014046, train/mean_average_precision=0.769346, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293282, validation/num_examples=43793
I0306 06:16:35.945252 139776159401728 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.1293334662914276, loss=0.015544667840003967
I0306 06:17:07.992233 139760711874304 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.1513308882713318, loss=0.017924189567565918
I0306 06:17:40.013924 139776159401728 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.14704254269599915, loss=0.018266253173351288
I0306 06:18:11.730205 139760711874304 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.1574544757604599, loss=0.018963703885674477
I0306 06:18:43.943051 139776159401728 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.1383337825536728, loss=0.017609400674700737
I0306 06:19:15.961724 139760711874304 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.13231517374515533, loss=0.01835949718952179
I0306 06:19:47.683265 139776159401728 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.13817505538463593, loss=0.017818089574575424
I0306 06:20:14.184979 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:21:51.164218 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:21:54.166619 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:21:57.119310 139937033598784 submission_runner.py:411] Time since start: 72170.59s, 	Step: 147184, 	{'train/accuracy': 0.9954391717910767, 'train/loss': 0.014232967048883438, 'train/mean_average_precision': 0.7748171236902126, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2933502504790597, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759415898627759, 'test/num_examples': 43793, 'score': 47798.320831775665, 'total_duration': 72170.59030127525, 'accumulated_submission_time': 47798.320831775665, 'accumulated_eval_time': 24359.37511229515, 'accumulated_logging_time': 8.758103132247925}
I0306 06:21:57.162115 139768268125952 logging_writer.py:48] [147184] accumulated_eval_time=24359.375112, accumulated_logging_time=8.758103, accumulated_submission_time=47798.320832, global_step=147184, preemption_count=0, score=47798.320832, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275942, test/num_examples=43793, total_duration=72170.590301, train/accuracy=0.995439, train/loss=0.014233, train/mean_average_precision=0.774817, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293350, validation/num_examples=43793
I0306 06:22:02.759838 139776167794432 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.14631447196006775, loss=0.017711684107780457
I0306 06:22:34.865477 139768268125952 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.13267630338668823, loss=0.016946202144026756
I0306 06:23:07.087898 139776167794432 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.1327882707118988, loss=0.016390156000852585
I0306 06:23:39.294086 139768268125952 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.13822413980960846, loss=0.018486693501472473
I0306 06:24:11.387650 139776167794432 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.15601304173469543, loss=0.019680188968777657
I0306 06:24:43.675869 139768268125952 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.13778455555438995, loss=0.017351321876049042
I0306 06:25:15.615444 139776167794432 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.12242816388607025, loss=0.014929363504052162
I0306 06:25:47.311809 139768268125952 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.12468983978033066, loss=0.017252247780561447
I0306 06:25:57.202060 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:27:37.582139 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:27:40.585128 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:27:43.552058 139937033598784 submission_runner.py:411] Time since start: 72517.02s, 	Step: 147932, 	{'train/accuracy': 0.9954963326454163, 'train/loss': 0.01406779419630766, 'train/mean_average_precision': 0.7755266511394573, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931353810233713, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2759515052454083, 'test/num_examples': 43793, 'score': 48038.327733278275, 'total_duration': 72517.02304172516, 'accumulated_submission_time': 48038.327733278275, 'accumulated_eval_time': 24465.72505545616, 'accumulated_logging_time': 8.8131422996521}
I0306 06:27:43.595249 139769339950848 logging_writer.py:48] [147932] accumulated_eval_time=24465.725055, accumulated_logging_time=8.813142, accumulated_submission_time=48038.327733, global_step=147932, preemption_count=0, score=48038.327733, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275952, test/num_examples=43793, total_duration=72517.023042, train/accuracy=0.995496, train/loss=0.014068, train/mean_average_precision=0.775527, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293135, validation/num_examples=43793
I0306 06:28:05.922127 139776159401728 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.1521080583333969, loss=0.018025819212198257
I0306 06:28:38.030887 139769339950848 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.1562071591615677, loss=0.019647926092147827
I0306 06:29:10.072260 139776159401728 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.14709119498729706, loss=0.018073614686727524
I0306 06:29:42.321693 139769339950848 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.138141930103302, loss=0.018909700214862823
I0306 06:30:14.339509 139776159401728 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.13357694447040558, loss=0.01678909733891487
I0306 06:30:46.409272 139769339950848 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.14260633289813995, loss=0.018513942137360573
I0306 06:31:19.062076 139776159401728 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.14680558443069458, loss=0.0175510011613369
I0306 06:31:43.675997 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:33:23.675575 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:33:26.655143 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:33:29.758500 139937033598784 submission_runner.py:411] Time since start: 72863.23s, 	Step: 148677, 	{'train/accuracy': 0.995520830154419, 'train/loss': 0.013939730823040009, 'train/mean_average_precision': 0.7862674566821604, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29315622458132334, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759377518485115, 'test/num_examples': 43793, 'score': 48278.375985860825, 'total_duration': 72863.22947764397, 'accumulated_submission_time': 48278.375985860825, 'accumulated_eval_time': 24571.807502031326, 'accumulated_logging_time': 8.868491649627686}
I0306 06:33:29.804004 139760711874304 logging_writer.py:48] [148677] accumulated_eval_time=24571.807502, accumulated_logging_time=8.868492, accumulated_submission_time=48278.375986, global_step=148677, preemption_count=0, score=48278.375986, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275938, test/num_examples=43793, total_duration=72863.229478, train/accuracy=0.995521, train/loss=0.013940, train/mean_average_precision=0.786267, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293156, validation/num_examples=43793
I0306 06:33:37.693984 139768268125952 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.14935214817523956, loss=0.01752605102956295
I0306 06:34:10.126451 139760711874304 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.16311538219451904, loss=0.01931888237595558
I0306 06:34:42.694386 139768268125952 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.1313617080450058, loss=0.02001030184328556
I0306 06:35:15.473543 139760711874304 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.1611482948064804, loss=0.019749591127038002
I0306 06:35:47.667429 139768268125952 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.1515955775976181, loss=0.021110936999320984
I0306 06:36:19.807621 139760711874304 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.1501539945602417, loss=0.018451601266860962
I0306 06:36:51.766971 139768268125952 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.14045381546020508, loss=0.015488232485949993
I0306 06:37:24.286703 139760711874304 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.12700708210468292, loss=0.017884060740470886
I0306 06:37:29.883745 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:39:15.400186 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:39:18.449679 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:39:21.428586 139937033598784 submission_runner.py:411] Time since start: 73214.90s, 	Step: 149418, 	{'train/accuracy': 0.9955045580863953, 'train/loss': 0.014024760574102402, 'train/mean_average_precision': 0.7792347084590541, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932035091562113, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2758873321968428, 'test/num_examples': 43793, 'score': 48518.42352437973, 'total_duration': 73214.89954638481, 'accumulated_submission_time': 48518.42352437973, 'accumulated_eval_time': 24683.352266073227, 'accumulated_logging_time': 8.925062656402588}
I0306 06:39:21.473331 139769339950848 logging_writer.py:48] [149418] accumulated_eval_time=24683.352266, accumulated_logging_time=8.925063, accumulated_submission_time=48518.423524, global_step=149418, preemption_count=0, score=48518.423524, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275887, test/num_examples=43793, total_duration=73214.899546, train/accuracy=0.995505, train/loss=0.014025, train/mean_average_precision=0.779235, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293204, validation/num_examples=43793
I0306 06:39:47.869645 139776159401728 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.14546829462051392, loss=0.017421932891011238
I0306 06:40:20.770122 139769339950848 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.13904854655265808, loss=0.017067143693566322
I0306 06:40:53.652707 139776159401728 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.12490497529506683, loss=0.014938438311219215
I0306 06:41:25.725498 139769339950848 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.14348731935024261, loss=0.017687086015939713
I0306 06:41:57.982649 139776159401728 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.15379318594932556, loss=0.018113095313310623
I0306 06:42:30.526358 139769339950848 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.13971124589443207, loss=0.016386711969971657
I0306 06:43:02.742247 139776159401728 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.1514378935098648, loss=0.019490458071231842
I0306 06:43:21.674645 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:45:06.617312 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:45:09.680067 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:45:12.673433 139937033598784 submission_runner.py:411] Time since start: 73566.14s, 	Step: 150160, 	{'train/accuracy': 0.9955043196678162, 'train/loss': 0.01406107097864151, 'train/mean_average_precision': 0.7787874434069905, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932349068101686, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759207687265104, 'test/num_examples': 43793, 'score': 48758.59159564972, 'total_duration': 73566.14442420006, 'accumulated_submission_time': 48758.59159564972, 'accumulated_eval_time': 24794.351008176804, 'accumulated_logging_time': 8.982405424118042}
I0306 06:45:12.717579 139768268125952 logging_writer.py:48] [150160] accumulated_eval_time=24794.351008, accumulated_logging_time=8.982405, accumulated_submission_time=48758.591596, global_step=150160, preemption_count=0, score=48758.591596, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275921, test/num_examples=43793, total_duration=73566.144424, train/accuracy=0.995504, train/loss=0.014061, train/mean_average_precision=0.778787, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293235, validation/num_examples=43793
I0306 06:45:25.969752 139776167794432 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.14416968822479248, loss=0.0174669548869133
I0306 06:45:58.761080 139768268125952 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.14333565533161163, loss=0.017479222267866135
I0306 06:46:30.841587 139776167794432 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.14522817730903625, loss=0.019042417407035828
I0306 06:47:03.114314 139768268125952 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.134999081492424, loss=0.018330564722418785
I0306 06:47:35.137948 139776167794432 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.16073080897331238, loss=0.019422253593802452
I0306 06:48:07.272276 139768268125952 logging_writer.py:48] [150700] global_step=150700, grad_norm=0.14114737510681152, loss=0.01628638245165348
I0306 06:48:40.022838 139776167794432 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.13111071288585663, loss=0.014618409797549248
I0306 06:49:12.389454 139768268125952 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.149773970246315, loss=0.017972344532608986
I0306 06:49:12.758711 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:50:51.630842 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:50:54.699505 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:50:57.641021 139937033598784 submission_runner.py:411] Time since start: 73911.11s, 	Step: 150901, 	{'train/accuracy': 0.995482325553894, 'train/loss': 0.014095024205744267, 'train/mean_average_precision': 0.7615456821250393, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29308897602624967, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27590936455435444, 'test/num_examples': 43793, 'score': 48998.60104203224, 'total_duration': 73911.11200237274, 'accumulated_submission_time': 48998.60104203224, 'accumulated_eval_time': 24899.233260393143, 'accumulated_logging_time': 9.03764295578003}
I0306 06:50:57.685517 139760711874304 logging_writer.py:48] [150901] accumulated_eval_time=24899.233260, accumulated_logging_time=9.037643, accumulated_submission_time=48998.601042, global_step=150901, preemption_count=0, score=48998.601042, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275909, test/num_examples=43793, total_duration=73911.112002, train/accuracy=0.995482, train/loss=0.014095, train/mean_average_precision=0.761546, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293089, validation/num_examples=43793
I0306 06:51:30.319584 139776159401728 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.14220702648162842, loss=0.017859263345599174
I0306 06:52:02.883965 139760711874304 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.13397209346294403, loss=0.016370557248592377
I0306 06:52:36.308524 139776159401728 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.14686201512813568, loss=0.020993700250983238
I0306 06:53:09.603871 139760711874304 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.13542626798152924, loss=0.016025612130761147
I0306 06:53:42.840696 139776159401728 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.14295484125614166, loss=0.01910284161567688
I0306 06:54:16.249330 139760711874304 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.14750899374485016, loss=0.017898591235280037
I0306 06:54:49.990419 139776159401728 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.16153399646282196, loss=0.02072908915579319
I0306 06:54:57.745553 139937033598784 spec.py:321] Evaluating on the training split.
I0306 06:56:42.365105 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 06:56:45.361176 139937033598784 spec.py:349] Evaluating on the test split.
I0306 06:56:48.328409 139937033598784 submission_runner.py:411] Time since start: 74261.80s, 	Step: 151624, 	{'train/accuracy': 0.9954485893249512, 'train/loss': 0.01420136634260416, 'train/mean_average_precision': 0.7779760903966846, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29318974091976785, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759801610305899, 'test/num_examples': 43793, 'score': 49238.62616467476, 'total_duration': 74261.79939770699, 'accumulated_submission_time': 49238.62616467476, 'accumulated_eval_time': 25009.81607890129, 'accumulated_logging_time': 9.094947576522827}
I0306 06:56:48.372109 139769339950848 logging_writer.py:48] [151624] accumulated_eval_time=25009.816079, accumulated_logging_time=9.094948, accumulated_submission_time=49238.626165, global_step=151624, preemption_count=0, score=49238.626165, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275980, test/num_examples=43793, total_duration=74261.799398, train/accuracy=0.995449, train/loss=0.014201, train/mean_average_precision=0.777976, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293190, validation/num_examples=43793
I0306 06:57:13.835117 139776167794432 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.14932358264923096, loss=0.015315760858356953
I0306 06:57:46.602515 139769339950848 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.1445477157831192, loss=0.019551996141672134
I0306 06:58:19.755702 139776167794432 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.13132180273532867, loss=0.016651418060064316
I0306 06:58:52.493038 139769339950848 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.14899463951587677, loss=0.018308429047465324
I0306 06:59:24.711924 139776167794432 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.14588014781475067, loss=0.01810738816857338
I0306 06:59:56.721327 139769339950848 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.12771621346473694, loss=0.01677781157195568
I0306 07:00:28.446704 139776167794432 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.13854184746742249, loss=0.01630704291164875
I0306 07:00:48.395042 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:02:27.972970 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:02:30.966856 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:02:33.947900 139937033598784 submission_runner.py:411] Time since start: 74607.42s, 	Step: 152363, 	{'train/accuracy': 0.9955525994300842, 'train/loss': 0.01391151174902916, 'train/mean_average_precision': 0.7837436456993829, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29340633552254425, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759371192052875, 'test/num_examples': 43793, 'score': 49478.61374115944, 'total_duration': 74607.41888213158, 'accumulated_submission_time': 49478.61374115944, 'accumulated_eval_time': 25115.368890285492, 'accumulated_logging_time': 9.151697158813477}
I0306 07:02:33.991780 139768268125952 logging_writer.py:48] [152363] accumulated_eval_time=25115.368890, accumulated_logging_time=9.151697, accumulated_submission_time=49478.613741, global_step=152363, preemption_count=0, score=49478.613741, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275937, test/num_examples=43793, total_duration=74607.418882, train/accuracy=0.995553, train/loss=0.013912, train/mean_average_precision=0.783744, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293406, validation/num_examples=43793
I0306 07:02:46.318096 139776159401728 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.13084597885608673, loss=0.017286764457821846
I0306 07:03:18.334041 139768268125952 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.15879128873348236, loss=0.02008203975856304
I0306 07:03:50.368583 139776159401728 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.15892189741134644, loss=0.017803845927119255
I0306 07:04:22.852141 139768268125952 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.142081618309021, loss=0.018855512142181396
I0306 07:04:55.805243 139776159401728 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.12608537077903748, loss=0.01699935644865036
I0306 07:05:28.577029 139768268125952 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.13577185571193695, loss=0.018155086785554886
I0306 07:06:01.496095 139776159401728 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.13892216980457306, loss=0.01749522052705288
I0306 07:06:33.694411 139768268125952 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.15672491490840912, loss=0.01751319132745266
I0306 07:06:34.019094 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:08:18.467976 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:08:21.621831 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:08:24.688070 139937033598784 submission_runner.py:411] Time since start: 74958.16s, 	Step: 153102, 	{'train/accuracy': 0.9954980611801147, 'train/loss': 0.014045431278645992, 'train/mean_average_precision': 0.7813701511430701, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29330101249426876, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27590422734712006, 'test/num_examples': 43793, 'score': 49718.60764694214, 'total_duration': 74958.15905070305, 'accumulated_submission_time': 49718.60764694214, 'accumulated_eval_time': 25226.037808418274, 'accumulated_logging_time': 9.207793951034546}
I0306 07:08:24.731936 139760711874304 logging_writer.py:48] [153102] accumulated_eval_time=25226.037808, accumulated_logging_time=9.207794, accumulated_submission_time=49718.607647, global_step=153102, preemption_count=0, score=49718.607647, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275904, test/num_examples=43793, total_duration=74958.159051, train/accuracy=0.995498, train/loss=0.014045, train/mean_average_precision=0.781370, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293301, validation/num_examples=43793
I0306 07:08:56.962476 139769339950848 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.1307811141014099, loss=0.01692795567214489
I0306 07:09:29.262507 139760711874304 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.1682903915643692, loss=0.021933700889348984
I0306 07:10:01.157283 139769339950848 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.13795718550682068, loss=0.018824679777026176
I0306 07:10:33.014739 139760711874304 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.13425463438034058, loss=0.019352763891220093
I0306 07:11:05.295367 139769339950848 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.1443324238061905, loss=0.017413577064871788
I0306 07:11:37.433070 139760711874304 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.13623614609241486, loss=0.017449351027607918
I0306 07:12:09.483847 139769339950848 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.12450092285871506, loss=0.015926800668239594
I0306 07:12:24.895234 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:14:02.121050 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:14:05.675967 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:14:09.191883 139937033598784 submission_runner.py:411] Time since start: 75302.66s, 	Step: 153849, 	{'train/accuracy': 0.995534360408783, 'train/loss': 0.013938337564468384, 'train/mean_average_precision': 0.7729496941587721, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932470890447527, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.2758850884604144, 'test/num_examples': 43793, 'score': 49958.73828434944, 'total_duration': 75302.66285181046, 'accumulated_submission_time': 49958.73828434944, 'accumulated_eval_time': 25330.334392786026, 'accumulated_logging_time': 9.263737916946411}
I0306 07:14:09.245003 139768268125952 logging_writer.py:48] [153849] accumulated_eval_time=25330.334393, accumulated_logging_time=9.263738, accumulated_submission_time=49958.738284, global_step=153849, preemption_count=0, score=49958.738284, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275885, test/num_examples=43793, total_duration=75302.662852, train/accuracy=0.995534, train/loss=0.013938, train/mean_average_precision=0.772950, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293247, validation/num_examples=43793
I0306 07:14:26.529818 139776159401728 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.14164197444915771, loss=0.015675989910960197
I0306 07:14:59.541884 139768268125952 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.14632055163383484, loss=0.01760689914226532
I0306 07:15:32.647048 139776159401728 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.13996388018131256, loss=0.01908378303050995
I0306 07:16:04.779640 139768268125952 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.14077377319335938, loss=0.01683419942855835
I0306 07:16:37.107404 139776159401728 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.14759546518325806, loss=0.019589073956012726
I0306 07:17:09.192720 139768268125952 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.14068371057510376, loss=0.0167345330119133
I0306 07:17:41.329751 139776159401728 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.1522153913974762, loss=0.020738499239087105
I0306 07:18:09.466149 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:19:49.283943 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:19:52.432015 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:19:55.475821 139937033598784 submission_runner.py:411] Time since start: 75648.95s, 	Step: 154588, 	{'train/accuracy': 0.9954615831375122, 'train/loss': 0.014223264530301094, 'train/mean_average_precision': 0.7669854386099493, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932654833117584, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27604163631205425, 'test/num_examples': 43793, 'score': 50198.92618584633, 'total_duration': 75648.9467959404, 'accumulated_submission_time': 50198.92618584633, 'accumulated_eval_time': 25436.344005584717, 'accumulated_logging_time': 9.328450679779053}
I0306 07:19:55.520557 139760711874304 logging_writer.py:48] [154588] accumulated_eval_time=25436.344006, accumulated_logging_time=9.328451, accumulated_submission_time=50198.926186, global_step=154588, preemption_count=0, score=50198.926186, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276042, test/num_examples=43793, total_duration=75648.946796, train/accuracy=0.995462, train/loss=0.014223, train/mean_average_precision=0.766985, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293265, validation/num_examples=43793
I0306 07:19:59.691211 139769339950848 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.1345914751291275, loss=0.01707306131720543
I0306 07:20:32.115405 139760711874304 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.12806448340415955, loss=0.016831709071993828
I0306 07:21:04.533126 139769339950848 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.14594244956970215, loss=0.0179472453892231
I0306 07:21:37.189230 139760711874304 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.12851649522781372, loss=0.016512567177414894
I0306 07:22:09.496899 139769339950848 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.13140900433063507, loss=0.016373002901673317
I0306 07:22:41.706594 139760711874304 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.14251717925071716, loss=0.01829400099813938
I0306 07:23:14.231424 139769339950848 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.12978211045265198, loss=0.01677010767161846
I0306 07:23:47.134841 139760711874304 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.13717325031757355, loss=0.018041836097836494
I0306 07:23:55.483484 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:25:36.759538 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:25:40.029326 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:25:43.120726 139937033598784 submission_runner.py:411] Time since start: 75996.59s, 	Step: 155327, 	{'train/accuracy': 0.9954792261123657, 'train/loss': 0.01413098443299532, 'train/mean_average_precision': 0.77602726156619, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932259288464124, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759604086981166, 'test/num_examples': 43793, 'score': 50438.856477975845, 'total_duration': 75996.59171676636, 'accumulated_submission_time': 50438.856477975845, 'accumulated_eval_time': 25543.98120045662, 'accumulated_logging_time': 9.385296106338501}
I0306 07:25:43.165454 139768268125952 logging_writer.py:48] [155327] accumulated_eval_time=25543.981200, accumulated_logging_time=9.385296, accumulated_submission_time=50438.856478, global_step=155327, preemption_count=0, score=50438.856478, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275960, test/num_examples=43793, total_duration=75996.591717, train/accuracy=0.995479, train/loss=0.014131, train/mean_average_precision=0.776027, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293226, validation/num_examples=43793
I0306 07:26:07.319912 139776159401728 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.13941413164138794, loss=0.0172785222530365
I0306 07:26:40.646347 139768268125952 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.13961586356163025, loss=0.01888420805335045
I0306 07:27:13.077682 139776159401728 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.14723241329193115, loss=0.018201112747192383
I0306 07:27:45.544883 139768268125952 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.1354662925004959, loss=0.016726156696677208
I0306 07:28:17.875386 139776159401728 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.15122970938682556, loss=0.019899291917681694
I0306 07:28:50.414788 139768268125952 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.1401064395904541, loss=0.017862388864159584
I0306 07:29:22.833113 139776159401728 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.1508641541004181, loss=0.01802460476756096
I0306 07:29:43.419759 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:31:24.280679 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:31:27.281801 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:31:30.248295 139937033598784 submission_runner.py:411] Time since start: 76343.72s, 	Step: 156065, 	{'train/accuracy': 0.9955069422721863, 'train/loss': 0.013964182697236538, 'train/mean_average_precision': 0.7782063057074599, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2930974827205313, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27594018321930647, 'test/num_examples': 43793, 'score': 50679.07929563522, 'total_duration': 76343.71928310394, 'accumulated_submission_time': 50679.07929563522, 'accumulated_eval_time': 25650.809689760208, 'accumulated_logging_time': 9.441604852676392}
I0306 07:31:30.294178 139760711874304 logging_writer.py:48] [156065] accumulated_eval_time=25650.809690, accumulated_logging_time=9.441605, accumulated_submission_time=50679.079296, global_step=156065, preemption_count=0, score=50679.079296, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275940, test/num_examples=43793, total_duration=76343.719283, train/accuracy=0.995507, train/loss=0.013964, train/mean_average_precision=0.778206, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293097, validation/num_examples=43793
I0306 07:31:42.054398 139769339950848 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.1537429839372635, loss=0.018336977809667587
I0306 07:32:14.730540 139760711874304 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.13765288889408112, loss=0.01858839951455593
I0306 07:32:46.658261 139769339950848 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.1331130713224411, loss=0.01860073208808899
I0306 07:33:19.219152 139760711874304 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.14354422688484192, loss=0.018474405631422997
I0306 07:33:51.239965 139769339950848 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.155756875872612, loss=0.01613428071141243
I0306 07:34:22.916598 139760711874304 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.13744279742240906, loss=0.019118256866931915
I0306 07:34:55.816553 139769339950848 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.11789397895336151, loss=0.016555506736040115
I0306 07:35:28.265578 139760711874304 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.14722928404808044, loss=0.016016334295272827
I0306 07:35:30.511127 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:37:17.171314 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:37:20.637901 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:37:24.068684 139937033598784 submission_runner.py:411] Time since start: 76697.54s, 	Step: 156808, 	{'train/accuracy': 0.995521605014801, 'train/loss': 0.01399571169167757, 'train/mean_average_precision': 0.7851263115303592, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29336368907660243, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27601604528413315, 'test/num_examples': 43793, 'score': 50919.26484918594, 'total_duration': 76697.53965449333, 'accumulated_submission_time': 50919.26484918594, 'accumulated_eval_time': 25764.367182970047, 'accumulated_logging_time': 9.498466730117798}
I0306 07:37:24.124861 139768268125952 logging_writer.py:48] [156808] accumulated_eval_time=25764.367183, accumulated_logging_time=9.498467, accumulated_submission_time=50919.264849, global_step=156808, preemption_count=0, score=50919.264849, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276016, test/num_examples=43793, total_duration=76697.539654, train/accuracy=0.995522, train/loss=0.013996, train/mean_average_precision=0.785126, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293364, validation/num_examples=43793
I0306 07:37:55.022014 139776159401728 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.13327822089195251, loss=0.0176004059612751
I0306 07:38:28.442417 139768268125952 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.13405191898345947, loss=0.017943724989891052
I0306 07:39:01.171349 139776159401728 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.1390301138162613, loss=0.017371170222759247
I0306 07:39:33.887465 139768268125952 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.14558863639831543, loss=0.020147131755948067
I0306 07:40:06.536244 139776159401728 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.15436437726020813, loss=0.017661020159721375
I0306 07:40:39.886624 139768268125952 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.1374952793121338, loss=0.015981746837496758
I0306 07:41:13.046460 139776159401728 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.1335507035255432, loss=0.017060434445738792
I0306 07:41:24.323849 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:43:08.225815 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:43:11.238409 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:43:14.235626 139937033598784 submission_runner.py:411] Time since start: 77047.71s, 	Step: 157535, 	{'train/accuracy': 0.9954743981361389, 'train/loss': 0.014096721075475216, 'train/mean_average_precision': 0.7758255457820338, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29311610345951405, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27589446222977354, 'test/num_examples': 43793, 'score': 51159.42909312248, 'total_duration': 77047.70661783218, 'accumulated_submission_time': 51159.42909312248, 'accumulated_eval_time': 25874.278917074203, 'accumulated_logging_time': 9.567306756973267}
I0306 07:43:14.282516 139760711874304 logging_writer.py:48] [157535] accumulated_eval_time=25874.278917, accumulated_logging_time=9.567307, accumulated_submission_time=51159.429093, global_step=157535, preemption_count=0, score=51159.429093, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275894, test/num_examples=43793, total_duration=77047.706618, train/accuracy=0.995474, train/loss=0.014097, train/mean_average_precision=0.775826, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293116, validation/num_examples=43793
I0306 07:43:35.729743 139776167794432 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.16217465698719025, loss=0.019244443625211716
I0306 07:44:08.392863 139760711874304 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.12891601026058197, loss=0.017687605693936348
I0306 07:44:40.715001 139776167794432 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.15781192481517792, loss=0.020563369616866112
I0306 07:45:12.704035 139760711874304 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.1384495049715042, loss=0.017681322991847992
I0306 07:45:44.435024 139776167794432 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.1381026804447174, loss=0.020145414397120476
I0306 07:46:16.772930 139760711874304 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.13641096651554108, loss=0.018021972849965096
I0306 07:46:48.973317 139776167794432 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.13088825345039368, loss=0.016770610585808754
I0306 07:47:14.287190 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:48:55.388068 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:48:58.429063 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:49:01.405660 139937033598784 submission_runner.py:411] Time since start: 77394.88s, 	Step: 158280, 	{'train/accuracy': 0.9955391883850098, 'train/loss': 0.013982100412249565, 'train/mean_average_precision': 0.7747015856380461, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29316080993308713, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.275884767550971, 'test/num_examples': 43793, 'score': 51399.40223193169, 'total_duration': 77394.87665104866, 'accumulated_submission_time': 51399.40223193169, 'accumulated_eval_time': 25981.39734506607, 'accumulated_logging_time': 9.625344038009644}
I0306 07:49:01.452147 139768268125952 logging_writer.py:48] [158280] accumulated_eval_time=25981.397345, accumulated_logging_time=9.625344, accumulated_submission_time=51399.402232, global_step=158280, preemption_count=0, score=51399.402232, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275885, test/num_examples=43793, total_duration=77394.876651, train/accuracy=0.995539, train/loss=0.013982, train/mean_average_precision=0.774702, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293161, validation/num_examples=43793
I0306 07:49:08.212874 139769339950848 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.1583515852689743, loss=0.018601970747113228
I0306 07:49:40.234467 139768268125952 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.16678592562675476, loss=0.017596108838915825
I0306 07:50:12.713604 139769339950848 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.14125338196754456, loss=0.018233682960271835
I0306 07:50:45.293114 139768268125952 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.15130499005317688, loss=0.017313677817583084
I0306 07:51:17.910642 139769339950848 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.12978537380695343, loss=0.01644129492342472
I0306 07:51:50.641175 139768268125952 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.14787468314170837, loss=0.02011740393936634
I0306 07:52:23.062071 139769339950848 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.14204496145248413, loss=0.017101867124438286
I0306 07:52:55.264335 139768268125952 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.13498105108737946, loss=0.017082253471016884
I0306 07:53:01.734239 139937033598784 spec.py:321] Evaluating on the training split.
I0306 07:54:45.513970 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 07:54:49.004370 139937033598784 spec.py:349] Evaluating on the test split.
I0306 07:54:52.413966 139937033598784 submission_runner.py:411] Time since start: 77745.88s, 	Step: 159021, 	{'train/accuracy': 0.9954434037208557, 'train/loss': 0.014222544617950916, 'train/mean_average_precision': 0.7661622075147424, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931163238204452, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759102206121796, 'test/num_examples': 43793, 'score': 51639.65240359306, 'total_duration': 77745.88492846489, 'accumulated_submission_time': 51639.65240359306, 'accumulated_eval_time': 26092.077012062073, 'accumulated_logging_time': 9.682848691940308}
I0306 07:54:52.465237 139776159401728 logging_writer.py:48] [159021] accumulated_eval_time=26092.077012, accumulated_logging_time=9.682849, accumulated_submission_time=51639.652404, global_step=159021, preemption_count=0, score=51639.652404, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275910, test/num_examples=43793, total_duration=77745.884928, train/accuracy=0.995443, train/loss=0.014223, train/mean_average_precision=0.766162, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293116, validation/num_examples=43793
I0306 07:55:18.578156 139776167794432 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.1407119631767273, loss=0.01555560901761055
I0306 07:55:50.933190 139776159401728 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.15137271583080292, loss=0.019190406426787376
I0306 07:56:23.378654 139776167794432 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.1387059986591339, loss=0.017709219828248024
I0306 07:56:55.903861 139776159401728 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.13977962732315063, loss=0.017401009798049927
I0306 07:57:28.355113 139776167794432 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.12770403921604156, loss=0.017669769003987312
I0306 07:58:00.826850 139776159401728 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.12567852437496185, loss=0.019188443198800087
I0306 07:58:32.973986 139776167794432 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.15031911432743073, loss=0.018610673025250435
I0306 07:58:52.422448 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:00:31.643483 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:00:35.119284 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:00:38.504062 139937033598784 submission_runner.py:411] Time since start: 78091.98s, 	Step: 159762, 	{'train/accuracy': 0.9955324530601501, 'train/loss': 0.014043060131371021, 'train/mean_average_precision': 0.7827541050873437, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29312989704958303, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27597584875064807, 'test/num_examples': 43793, 'score': 51879.577176332474, 'total_duration': 78091.97503495216, 'accumulated_submission_time': 51879.577176332474, 'accumulated_eval_time': 26198.158568143845, 'accumulated_logging_time': 9.7459876537323}
I0306 08:00:38.561151 139760711874304 logging_writer.py:48] [159762] accumulated_eval_time=26198.158568, accumulated_logging_time=9.745988, accumulated_submission_time=51879.577176, global_step=159762, preemption_count=0, score=51879.577176, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275976, test/num_examples=43793, total_duration=78091.975035, train/accuracy=0.995532, train/loss=0.014043, train/mean_average_precision=0.782754, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293130, validation/num_examples=43793
I0306 08:00:51.405888 139768268125952 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.15433380007743835, loss=0.021352581679821014
I0306 08:01:24.030489 139760711874304 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.13911068439483643, loss=0.016831761226058006
I0306 08:01:56.586548 139768268125952 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.15059830248355865, loss=0.018747350201010704
I0306 08:02:28.932134 139760711874304 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.13938568532466888, loss=0.017390085384249687
I0306 08:03:01.835009 139768268125952 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.1528024524450302, loss=0.01838383637368679
I0306 08:03:34.348681 139760711874304 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.14229770004749298, loss=0.018348202109336853
I0306 08:04:06.897522 139768268125952 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.13603931665420532, loss=0.017131315544247627
I0306 08:04:38.597667 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:06:14.820837 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:06:18.363015 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:06:21.822491 139937033598784 submission_runner.py:411] Time since start: 78435.29s, 	Step: 160498, 	{'train/accuracy': 0.995540201663971, 'train/loss': 0.013905257917940617, 'train/mean_average_precision': 0.7825101913603374, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931839937265932, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27594411009660214, 'test/num_examples': 43793, 'score': 52119.58177232742, 'total_duration': 78435.2933254242, 'accumulated_submission_time': 52119.58177232742, 'accumulated_eval_time': 26301.383195638657, 'accumulated_logging_time': 9.815279960632324}
I0306 08:06:21.875119 139719039305472 logging_writer.py:48] [160498] accumulated_eval_time=26301.383196, accumulated_logging_time=9.815280, accumulated_submission_time=52119.581772, global_step=160498, preemption_count=0, score=52119.581772, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275944, test/num_examples=43793, total_duration=78435.293325, train/accuracy=0.995540, train/loss=0.013905, train/mean_average_precision=0.782510, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293184, validation/num_examples=43793
I0306 08:06:23.003545 139776167794432 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.14527219533920288, loss=0.019670823588967323
I0306 08:06:56.229655 139719039305472 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.14076614379882812, loss=0.01813729666173458
I0306 08:07:28.967086 139776167794432 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.1362442821264267, loss=0.01720132865011692
I0306 08:08:01.665680 139719039305472 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.1673879325389862, loss=0.020570697262883186
I0306 08:08:33.932442 139776167794432 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.14411723613739014, loss=0.016970910131931305
I0306 08:09:06.086036 139719039305472 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.13517175614833832, loss=0.017137423157691956
I0306 08:09:38.235646 139776167794432 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.13328106701374054, loss=0.015709154307842255
I0306 08:10:10.813035 139719039305472 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.13900886476039886, loss=0.018648043274879456
I0306 08:10:21.956996 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:12:02.503868 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:12:05.693594 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:12:08.751677 139937033598784 submission_runner.py:411] Time since start: 78782.22s, 	Step: 161235, 	{'train/accuracy': 0.9954307675361633, 'train/loss': 0.014125431887805462, 'train/mean_average_precision': 0.7702380539786756, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29328309182732476, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759489303136601, 'test/num_examples': 43793, 'score': 52359.627802848816, 'total_duration': 78782.22266626358, 'accumulated_submission_time': 52359.627802848816, 'accumulated_eval_time': 26408.17783999443, 'accumulated_logging_time': 9.880241632461548}
I0306 08:12:08.799958 139760711874304 logging_writer.py:48] [161235] accumulated_eval_time=26408.177840, accumulated_logging_time=9.880242, accumulated_submission_time=52359.627803, global_step=161235, preemption_count=0, score=52359.627803, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275949, test/num_examples=43793, total_duration=78782.222666, train/accuracy=0.995431, train/loss=0.014125, train/mean_average_precision=0.770238, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293283, validation/num_examples=43793
I0306 08:12:30.241110 139768268125952 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.1591205596923828, loss=0.020296817645430565
I0306 08:13:02.528945 139760711874304 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.15348389744758606, loss=0.01822786219418049
I0306 08:13:34.916138 139768268125952 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.13249365985393524, loss=0.017480170354247093
I0306 08:14:06.999701 139760711874304 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.15201494097709656, loss=0.01916288025677204
I0306 08:14:39.134466 139768268125952 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.13400529325008392, loss=0.01760823465883732
I0306 08:15:11.388311 139760711874304 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.14610368013381958, loss=0.016812223941087723
I0306 08:15:43.453030 139768268125952 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.15756195783615112, loss=0.016883375123143196
I0306 08:16:08.762686 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:17:49.335339 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:17:52.418609 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:17:55.414983 139937033598784 submission_runner.py:411] Time since start: 79128.89s, 	Step: 161978, 	{'train/accuracy': 0.9955178499221802, 'train/loss': 0.014078348875045776, 'train/mean_average_precision': 0.778865225406159, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29316672312475883, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27596798397832106, 'test/num_examples': 43793, 'score': 52599.55924654007, 'total_duration': 79128.88596534729, 'accumulated_submission_time': 52599.55924654007, 'accumulated_eval_time': 26514.83008337021, 'accumulated_logging_time': 9.939560890197754}
I0306 08:17:55.460785 139719039305472 logging_writer.py:48] [161978] accumulated_eval_time=26514.830083, accumulated_logging_time=9.939561, accumulated_submission_time=52599.559247, global_step=161978, preemption_count=0, score=52599.559247, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275968, test/num_examples=43793, total_duration=79128.885965, train/accuracy=0.995518, train/loss=0.014078, train/mean_average_precision=0.778865, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293167, validation/num_examples=43793
I0306 08:18:02.981409 139769339950848 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.15865732729434967, loss=0.017457278445363045
I0306 08:18:35.248728 139719039305472 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.14381729066371918, loss=0.01859157159924507
I0306 08:19:07.620086 139769339950848 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.15214723348617554, loss=0.01785447634756565
I0306 08:19:39.644260 139719039305472 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.13461297750473022, loss=0.017998265102505684
I0306 08:20:11.976653 139769339950848 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.16268056631088257, loss=0.019470268860459328
I0306 08:20:44.701272 139719039305472 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.133736714720726, loss=0.017618414014577866
I0306 08:21:17.248370 139769339950848 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.129372239112854, loss=0.01766211912035942
I0306 08:21:49.870691 139719039305472 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.13385216891765594, loss=0.017862584441900253
I0306 08:21:55.724414 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:23:34.008351 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:23:37.074309 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:23:40.052928 139937033598784 submission_runner.py:411] Time since start: 79473.52s, 	Step: 162719, 	{'train/accuracy': 0.9954817891120911, 'train/loss': 0.014129121787846088, 'train/mean_average_precision': 0.7618262995083183, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932867883896753, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760785878656386, 'test/num_examples': 43793, 'score': 52839.79007482529, 'total_duration': 79473.52391839027, 'accumulated_submission_time': 52839.79007482529, 'accumulated_eval_time': 26619.158552885056, 'accumulated_logging_time': 9.997602939605713}
I0306 08:23:40.099259 139760711874304 logging_writer.py:48] [162719] accumulated_eval_time=26619.158553, accumulated_logging_time=9.997603, accumulated_submission_time=52839.790075, global_step=162719, preemption_count=0, score=52839.790075, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276079, test/num_examples=43793, total_duration=79473.523918, train/accuracy=0.995482, train/loss=0.014129, train/mean_average_precision=0.761826, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293287, validation/num_examples=43793
I0306 08:24:06.978667 139768268125952 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.15325118601322174, loss=0.01785770058631897
I0306 08:24:39.797656 139760711874304 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.15269730985164642, loss=0.0184012521058321
I0306 08:25:12.357876 139768268125952 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.12910152971744537, loss=0.017649386078119278
I0306 08:25:44.714497 139760711874304 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.13973833620548248, loss=0.01707988977432251
I0306 08:26:17.360228 139768268125952 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.13237722218036652, loss=0.016894930973649025
I0306 08:26:49.947331 139760711874304 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.13495956361293793, loss=0.016111044213175774
I0306 08:27:22.809484 139768268125952 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.13814173638820648, loss=0.017803704366087914
I0306 08:27:40.103934 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:29:21.570890 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:29:24.629708 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:29:27.655618 139937033598784 submission_runner.py:411] Time since start: 79821.13s, 	Step: 163454, 	{'train/accuracy': 0.9954960942268372, 'train/loss': 0.014114203862845898, 'train/mean_average_precision': 0.7793173555613062, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29320467304290826, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27590885356088307, 'test/num_examples': 43793, 'score': 53079.76247572899, 'total_duration': 79821.12660717964, 'accumulated_submission_time': 53079.76247572899, 'accumulated_eval_time': 26726.71019911766, 'accumulated_logging_time': 10.055290699005127}
I0306 08:29:27.702110 139769339950848 logging_writer.py:48] [163454] accumulated_eval_time=26726.710199, accumulated_logging_time=10.055291, accumulated_submission_time=53079.762476, global_step=163454, preemption_count=0, score=53079.762476, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275909, test/num_examples=43793, total_duration=79821.126607, train/accuracy=0.995496, train/loss=0.014114, train/mean_average_precision=0.779317, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293205, validation/num_examples=43793
I0306 08:29:43.269719 139776167794432 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.14160092175006866, loss=0.019090229645371437
I0306 08:30:15.778128 139769339950848 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.1379929631948471, loss=0.0182773619890213
I0306 08:30:47.804370 139776167794432 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.1356058567762375, loss=0.01731680892407894
I0306 08:31:20.109352 139769339950848 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.13546767830848694, loss=0.01845558173954487
I0306 08:31:52.184063 139776167794432 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.14733633399009705, loss=0.02003604918718338
I0306 08:32:24.514696 139769339950848 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.14765223860740662, loss=0.017543770372867584
I0306 08:32:57.101467 139776167794432 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.1352500021457672, loss=0.01785416714847088
I0306 08:33:27.852570 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:35:05.327187 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:35:08.384607 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:35:11.416599 139937033598784 submission_runner.py:411] Time since start: 80164.89s, 	Step: 164196, 	{'train/accuracy': 0.9955257773399353, 'train/loss': 0.013932894915342331, 'train/mean_average_precision': 0.7799171908740734, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2933241683517332, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27609126670408723, 'test/num_examples': 43793, 'score': 53319.88084149361, 'total_duration': 80164.88756608963, 'accumulated_submission_time': 53319.88084149361, 'accumulated_eval_time': 26830.27416396141, 'accumulated_logging_time': 10.113618850708008}
I0306 08:35:11.463791 139719039305472 logging_writer.py:48] [164196] accumulated_eval_time=26830.274164, accumulated_logging_time=10.113619, accumulated_submission_time=53319.880841, global_step=164196, preemption_count=0, score=53319.880841, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276091, test/num_examples=43793, total_duration=80164.887566, train/accuracy=0.995526, train/loss=0.013933, train/mean_average_precision=0.779917, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293324, validation/num_examples=43793
I0306 08:35:13.193890 139760711874304 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.14460721611976624, loss=0.018118133768439293
I0306 08:35:45.568426 139719039305472 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.13446471095085144, loss=0.019124988466501236
I0306 08:36:17.996799 139760711874304 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.14943192899227142, loss=0.017540976405143738
I0306 08:36:50.424360 139719039305472 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.13240472972393036, loss=0.014874896965920925
I0306 08:37:22.256704 139760711874304 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.1547289490699768, loss=0.019418461248278618
I0306 08:37:54.568033 139719039305472 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.1450577974319458, loss=0.016093257814645767
I0306 08:38:26.551264 139760711874304 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.14793232083320618, loss=0.01845303364098072
I0306 08:38:58.729656 139719039305472 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.14451980590820312, loss=0.017599327489733696
I0306 08:39:11.579427 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:40:49.176346 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:40:52.212120 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:40:55.239495 139937033598784 submission_runner.py:411] Time since start: 80508.71s, 	Step: 164940, 	{'train/accuracy': 0.9955264925956726, 'train/loss': 0.013974946923553944, 'train/mean_average_precision': 0.781608569478042, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29307835910275026, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27591604361309846, 'test/num_examples': 43793, 'score': 53559.964792728424, 'total_duration': 80508.71048593521, 'accumulated_submission_time': 53559.964792728424, 'accumulated_eval_time': 26933.934188604355, 'accumulated_logging_time': 10.171966552734375}
I0306 08:40:55.286575 139768268125952 logging_writer.py:48] [164940] accumulated_eval_time=26933.934189, accumulated_logging_time=10.171967, accumulated_submission_time=53559.964793, global_step=164940, preemption_count=0, score=53559.964793, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275916, test/num_examples=43793, total_duration=80508.710486, train/accuracy=0.995526, train/loss=0.013975, train/mean_average_precision=0.781609, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293078, validation/num_examples=43793
I0306 08:41:14.966464 139776167794432 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.16092105209827423, loss=0.019343428313732147
I0306 08:41:46.955487 139768268125952 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.1449882984161377, loss=0.01859036646783352
I0306 08:42:19.445797 139776167794432 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.14679892361164093, loss=0.01824822649359703
I0306 08:42:51.720641 139768268125952 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.14105771481990814, loss=0.017781473696231842
I0306 08:43:24.129857 139776167794432 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.14911288022994995, loss=0.01841629296541214
I0306 08:43:56.270237 139768268125952 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.1479838490486145, loss=0.017461730167269707
I0306 08:44:28.685759 139776167794432 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.1572793871164322, loss=0.019855614751577377
I0306 08:44:55.451786 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:46:37.258023 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:46:40.324910 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:46:43.294331 139937033598784 submission_runner.py:411] Time since start: 80856.77s, 	Step: 165683, 	{'train/accuracy': 0.9954779148101807, 'train/loss': 0.014112996868789196, 'train/mean_average_precision': 0.7717062319651007, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29321514546801086, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27588985490068135, 'test/num_examples': 43793, 'score': 53800.09856343269, 'total_duration': 80856.76532030106, 'accumulated_submission_time': 53800.09856343269, 'accumulated_eval_time': 27041.776702404022, 'accumulated_logging_time': 10.230546236038208}
I0306 08:46:43.341741 139719039305472 logging_writer.py:48] [165683] accumulated_eval_time=27041.776702, accumulated_logging_time=10.230546, accumulated_submission_time=53800.098563, global_step=165683, preemption_count=0, score=53800.098563, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275890, test/num_examples=43793, total_duration=80856.765320, train/accuracy=0.995478, train/loss=0.014113, train/mean_average_precision=0.771706, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293215, validation/num_examples=43793
I0306 08:46:49.278064 139769339950848 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.1506327986717224, loss=0.021764017641544342
I0306 08:47:21.485822 139719039305472 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.15078045427799225, loss=0.018352093175053596
I0306 08:47:53.652845 139769339950848 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.14528493583202362, loss=0.018112141638994217
I0306 08:48:25.741359 139719039305472 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.1413145214319229, loss=0.01909465529024601
I0306 08:48:57.598687 139769339950848 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.1465342789888382, loss=0.01835831068456173
I0306 08:49:29.599511 139719039305472 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.1381876915693283, loss=0.015986988320946693
I0306 08:50:01.694536 139769339950848 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.1360505372285843, loss=0.01761886663734913
I0306 08:50:34.244210 139719039305472 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.13240891695022583, loss=0.017670748755335808
I0306 08:50:43.514241 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:52:24.839693 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:52:28.312263 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:52:31.622609 139937033598784 submission_runner.py:411] Time since start: 81205.09s, 	Step: 166430, 	{'train/accuracy': 0.9955308437347412, 'train/loss': 0.01399469468742609, 'train/mean_average_precision': 0.7687587064128196, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2931156943476471, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.053930800408124924, 'test/mean_average_precision': 0.27590943560336817, 'test/num_examples': 43793, 'score': 54040.2394015789, 'total_duration': 81205.09357523918, 'accumulated_submission_time': 54040.2394015789, 'accumulated_eval_time': 27149.885001897812, 'accumulated_logging_time': 10.289138793945312}
I0306 08:52:31.677214 139768268125952 logging_writer.py:48] [166430] accumulated_eval_time=27149.885002, accumulated_logging_time=10.289139, accumulated_submission_time=54040.239402, global_step=166430, preemption_count=0, score=54040.239402, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275909, test/num_examples=43793, total_duration=81205.093575, train/accuracy=0.995531, train/loss=0.013995, train/mean_average_precision=0.768759, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293116, validation/num_examples=43793
I0306 08:52:55.443474 139776167794432 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.12944693863391876, loss=0.016808828338980675
I0306 08:53:27.801558 139768268125952 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.1362929493188858, loss=0.016703294590115547
I0306 08:53:59.754898 139776167794432 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.1526770442724228, loss=0.01959337666630745
I0306 08:54:31.826420 139768268125952 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.14981889724731445, loss=0.0195663720369339
I0306 08:55:03.816280 139776167794432 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.14590837061405182, loss=0.018280867487192154
I0306 08:55:36.320058 139768268125952 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.1573425531387329, loss=0.02035730518400669
I0306 08:56:08.968663 139776167794432 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.13982050120830536, loss=0.018648769706487656
I0306 08:56:31.746247 139937033598784 spec.py:321] Evaluating on the training split.
I0306 08:58:14.149310 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 08:58:17.204392 139937033598784 spec.py:349] Evaluating on the test split.
I0306 08:58:20.225926 139937033598784 submission_runner.py:411] Time since start: 81553.70s, 	Step: 167172, 	{'train/accuracy': 0.9954442977905273, 'train/loss': 0.0142495296895504, 'train/mean_average_precision': 0.7769547425832453, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2930756406702172, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.276066158956547, 'test/num_examples': 43793, 'score': 54280.27514696121, 'total_duration': 81553.69691586494, 'accumulated_submission_time': 54280.27514696121, 'accumulated_eval_time': 27258.364639759064, 'accumulated_logging_time': 10.35668396949768}
I0306 08:58:20.273030 139760711874304 logging_writer.py:48] [167172] accumulated_eval_time=27258.364640, accumulated_logging_time=10.356684, accumulated_submission_time=54280.275147, global_step=167172, preemption_count=0, score=54280.275147, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276066, test/num_examples=43793, total_duration=81553.696916, train/accuracy=0.995444, train/loss=0.014250, train/mean_average_precision=0.776955, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293076, validation/num_examples=43793
I0306 08:58:29.649380 139769339950848 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.16316348314285278, loss=0.01806686446070671
I0306 08:59:02.219560 139760711874304 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.15295957028865814, loss=0.018080128356814384
I0306 08:59:34.422564 139769339950848 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.13145361840724945, loss=0.015545991249382496
I0306 09:00:06.896754 139760711874304 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.13605454564094543, loss=0.017748815938830376
I0306 09:00:38.879714 139769339950848 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.13775509595870972, loss=0.01685699075460434
I0306 09:01:11.382201 139760711874304 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.1456785351037979, loss=0.019063139334321022
I0306 09:01:43.320315 139769339950848 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.16146543622016907, loss=0.018064230680465698
I0306 09:02:15.479364 139760711874304 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.1409076601266861, loss=0.015837371349334717
I0306 09:02:20.242636 139937033598784 spec.py:321] Evaluating on the training split.
I0306 09:03:57.689560 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 09:04:00.700459 139937033598784 spec.py:349] Evaluating on the test split.
I0306 09:04:03.780425 139937033598784 submission_runner.py:411] Time since start: 81897.25s, 	Step: 167916, 	{'train/accuracy': 0.9955223202705383, 'train/loss': 0.013966403901576996, 'train/mean_average_precision': 0.778954704864464, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29309916199019836, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759194664446954, 'test/num_examples': 43793, 'score': 54520.21311235428, 'total_duration': 81897.25141072273, 'accumulated_submission_time': 54520.21311235428, 'accumulated_eval_time': 27361.902376651764, 'accumulated_logging_time': 10.414924383163452}
I0306 09:04:03.827407 139719039305472 logging_writer.py:48] [167916] accumulated_eval_time=27361.902377, accumulated_logging_time=10.414924, accumulated_submission_time=54520.213112, global_step=167916, preemption_count=0, score=54520.213112, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275919, test/num_examples=43793, total_duration=81897.251411, train/accuracy=0.995522, train/loss=0.013966, train/mean_average_precision=0.778955, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293099, validation/num_examples=43793
I0306 09:04:30.797318 139768268125952 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.13203653693199158, loss=0.01828921213746071
I0306 09:05:03.167629 139719039305472 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.1443052738904953, loss=0.017774337902665138
I0306 09:05:34.820266 139768268125952 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.12797747552394867, loss=0.01636522263288498
I0306 09:06:06.655170 139719039305472 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.12482214719057083, loss=0.01872388646006584
I0306 09:06:38.526412 139768268125952 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.1528598517179489, loss=0.01716170646250248
I0306 09:07:10.768661 139719039305472 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.14301933348178864, loss=0.01727950945496559
I0306 09:07:42.869915 139768268125952 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.13337358832359314, loss=0.01724417321383953
I0306 09:08:03.808108 139937033598784 spec.py:321] Evaluating on the training split.
I0306 09:09:44.798703 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 09:09:47.787359 139937033598784 spec.py:349] Evaluating on the test split.
I0306 09:09:50.750974 139937033598784 submission_runner.py:411] Time since start: 82244.22s, 	Step: 168667, 	{'train/accuracy': 0.9955002069473267, 'train/loss': 0.014002409763634205, 'train/mean_average_precision': 0.781999000653092, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29317627448710937, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2760429089668513, 'test/num_examples': 43793, 'score': 54760.16240620613, 'total_duration': 82244.221940279, 'accumulated_submission_time': 54760.16240620613, 'accumulated_eval_time': 27468.845179080963, 'accumulated_logging_time': 10.472869873046875}
I0306 09:09:50.796553 139769339950848 logging_writer.py:48] [168667] accumulated_eval_time=27468.845179, accumulated_logging_time=10.472870, accumulated_submission_time=54760.162406, global_step=168667, preemption_count=0, score=54760.162406, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.276043, test/num_examples=43793, total_duration=82244.221940, train/accuracy=0.995500, train/loss=0.014002, train/mean_average_precision=0.781999, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293176, validation/num_examples=43793
I0306 09:10:01.796983 139776167794432 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.13361084461212158, loss=0.01681111752986908
I0306 09:10:34.139389 139769339950848 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.13451088964939117, loss=0.016955111175775528
I0306 09:11:07.059560 139776167794432 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.12488805502653122, loss=0.014420640654861927
I0306 09:11:38.954933 139769339950848 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.15384617447853088, loss=0.01741418056190014
I0306 09:12:11.233045 139776167794432 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.13081057369709015, loss=0.01721247099339962
I0306 09:12:43.509209 139769339950848 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.14685897529125214, loss=0.014985806308686733
I0306 09:13:15.954880 139776167794432 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.1435202956199646, loss=0.01873665675520897
I0306 09:13:48.042737 139769339950848 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.1504485309123993, loss=0.018922045826911926
I0306 09:13:50.988765 139937033598784 spec.py:321] Evaluating on the training split.
I0306 09:15:26.859742 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 09:15:29.877051 139937033598784 spec.py:349] Evaluating on the test split.
I0306 09:15:32.850601 139937033598784 submission_runner.py:411] Time since start: 82586.32s, 	Step: 169410, 	{'train/accuracy': 0.9954855442047119, 'train/loss': 0.01405261643230915, 'train/mean_average_precision': 0.7778016832839559, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.29322294892579065, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.2759854657923026, 'test/num_examples': 43793, 'score': 55000.32378101349, 'total_duration': 82586.32157897949, 'accumulated_submission_time': 55000.32378101349, 'accumulated_eval_time': 27570.706958293915, 'accumulated_logging_time': 10.529196977615356}
I0306 09:15:32.897779 139719039305472 logging_writer.py:48] [169410] accumulated_eval_time=27570.706958, accumulated_logging_time=10.529197, accumulated_submission_time=55000.323781, global_step=169410, preemption_count=0, score=55000.323781, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275985, test/num_examples=43793, total_duration=82586.321579, train/accuracy=0.995486, train/loss=0.014053, train/mean_average_precision=0.777802, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293223, validation/num_examples=43793
I0306 09:16:02.253486 139760711874304 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.16504870355129242, loss=0.01881307363510132
I0306 09:16:34.157074 139719039305472 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.1526980847120285, loss=0.017838457599282265
I0306 09:17:06.053856 139760711874304 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.15020088851451874, loss=0.01683085970580578
I0306 09:17:38.083141 139719039305472 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.1518089771270752, loss=0.01852325350046158
I0306 09:18:10.003304 139760711874304 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.1380504071712494, loss=0.01694284938275814
I0306 09:18:41.927922 139719039305472 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.14141003787517548, loss=0.018817422911524773
I0306 09:19:13.636319 139760711874304 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.14067134261131287, loss=0.016911476850509644
I0306 09:19:32.874520 139937033598784 spec.py:321] Evaluating on the training split.
I0306 09:21:11.352726 139937033598784 spec.py:333] Evaluating on the validation split.
I0306 09:21:14.401341 139937033598784 spec.py:349] Evaluating on the test split.
I0306 09:21:17.358696 139937033598784 submission_runner.py:411] Time since start: 82930.83s, 	Step: 170162, 	{'train/accuracy': 0.9955334067344666, 'train/loss': 0.013999897986650467, 'train/mean_average_precision': 0.768694405169295, 'validation/accuracy': 0.9870207905769348, 'validation/loss': 0.05048840120434761, 'validation/mean_average_precision': 0.2932143589528342, 'validation/num_examples': 43793, 'test/accuracy': 0.9861586689949036, 'test/loss': 0.05393080785870552, 'test/mean_average_precision': 0.27592051244043825, 'test/num_examples': 43793, 'score': 55240.26776766777, 'total_duration': 82930.82968711853, 'accumulated_submission_time': 55240.26776766777, 'accumulated_eval_time': 27675.19109106064, 'accumulated_logging_time': 10.588984966278076}
I0306 09:21:17.405828 139769339950848 logging_writer.py:48] [170162] accumulated_eval_time=27675.191091, accumulated_logging_time=10.588985, accumulated_submission_time=55240.267768, global_step=170162, preemption_count=0, score=55240.267768, test/accuracy=0.986159, test/loss=0.053931, test/mean_average_precision=0.275921, test/num_examples=43793, total_duration=82930.829687, train/accuracy=0.995533, train/loss=0.014000, train/mean_average_precision=0.768694, validation/accuracy=0.987021, validation/loss=0.050488, validation/mean_average_precision=0.293214, validation/num_examples=43793
I0306 09:21:30.220598 139776167794432 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.12920044362545013, loss=0.01806071028113365
I0306 09:22:02.478794 139769339950848 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.1488317996263504, loss=0.018439212813973427
I0306 09:22:34.367201 139776167794432 logging_writer.py:48] [170400] global_step=170400, grad_norm=0.1417512446641922, loss=0.019653284922242165
I0306 09:23:06.154748 139769339950848 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.1392584890127182, loss=0.017810383811593056
I0306 09:23:38.117473 139776167794432 logging_writer.py:48] [170600] global_step=170600, grad_norm=0.14193768799304962, loss=0.017826929688453674
I0306 09:24:09.982461 139769339950848 logging_writer.py:48] [170700] global_step=170700, grad_norm=0.1485271155834198, loss=0.018070632591843605
I0306 09:24:28.211504 139776167794432 logging_writer.py:48] [170758] global_step=170758, preemption_count=0, score=55431.012091
I0306 09:24:28.262555 139937033598784 checkpoints.py:490] Saving checkpoint at step: 170758
I0306 09:24:28.390333 139937033598784 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1/checkpoint_170758
I0306 09:24:28.391561 139937033598784 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_1/ogbg_jax/trial_1/checkpoint_170758.
I0306 09:24:28.576341 139937033598784 submission_runner.py:676] Final ogbg score: 55431.01209068298
Dataset ogbg_molpcba downloaded and prepared to /root/data/ogbg_molpcba/0.1.3. Subsequent calls will reuse this data.
