python3 submission_runner.py --framework=jax --workload=ogbg --submission_path=prize_qualification_baselines/self_tuning/jax_nadamw_full_budget.py --data_dir=/data/ogbg --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=prize_qualification_self_tuning/study_2 --overwrite=true --save_checkpoints=false --rng_seed=3769186076 --max_global_steps=240000 --tuning_ruleset=self 2>&1 | tee -a /logs/ogbg_jax_03-05-2024-10-04-38.log
I0305 10:05:00.088268 140576608098112 logger_utils.py:76] Creating experiment directory at /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax.
I0305 10:05:01.125751 140576608098112 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0305 10:05:01.126590 140576608098112 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0305 10:05:01.126732 140576608098112 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0305 10:05:02.157298 140576608098112 submission_runner.py:605] Creating directory at /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1.
I0305 10:05:02.408097 140576608098112 submission_runner.py:206] Initializing dataset.
I0305 10:05:02.732491 140576608098112 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:05:02.741092 140576608098112 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
W0305 10:05:03.014848 140576608098112 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/reader.py:101: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.counter(...)` instead.
I0305 10:05:03.077155 140576608098112 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:05:03.158477 140576608098112 submission_runner.py:213] Initializing model.
I0305 10:05:08.009543 140576608098112 submission_runner.py:255] Initializing optimizer.
I0305 10:05:08.665926 140576608098112 submission_runner.py:262] Initializing metrics bundle.
I0305 10:05:08.666124 140576608098112 submission_runner.py:280] Initializing checkpoint and logger.
I0305 10:05:08.666824 140576608098112 checkpoints.py:915] Found no checkpoint files in /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1 with prefix checkpoint_
I0305 10:05:08.666979 140576608098112 submission_runner.py:300] Saving meta data to /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1/meta_data_0.json.
I0305 10:05:08.667177 140576608098112 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0305 10:05:08.667238 140576608098112 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0305 10:05:08.990562 140576608098112 logger_utils.py:220] Unable to record git information. Continuing without it.
I0305 10:05:09.281837 140576608098112 submission_runner.py:304] Saving flags to /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1/flags_0.json.
I0305 10:05:09.291604 140576608098112 submission_runner.py:314] Starting training loop.
I0305 10:05:28.462533 140415468459776 logging_writer.py:48] [0] global_step=0, grad_norm=1.8176769018173218, loss=0.7969515919685364
I0305 10:05:28.479857 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:05:28.486686 140576608098112 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:05:28.491014 140576608098112 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:05:28.567855 140576608098112 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split train, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:07:28.082364 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:07:28.086527 140576608098112 dataset_info.py:578] Load dataset info from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:07:28.091404 140576608098112 dataset_builder.py:528] Reusing dataset ogbg_molpcba (/data/ogbg/ogbg_molpcba/0.1.3)
I0305 10:07:28.166448 140576608098112 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split validation, from /data/ogbg/ogbg_molpcba/0.1.3
I0305 10:09:04.599280 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:09:05.085671 140576608098112 dataset_info.py:736] Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: ogbg_molpcba/0.1.3
I0305 10:09:06.069537 140576608098112 dataset_info.py:578] Load dataset info from /tmp/tmp_359snegtfds
I0305 10:09:06.072984 140576608098112 dataset_info.py:669] Fields info.[description, release_notes, splits, module_name] from disk and from code do not match. Keeping the one from code.
I0305 10:09:06.073436 140576608098112 dataset_builder.py:593] Generating dataset ogbg_molpcba (/root/data/ogbg_molpcba/0.1.3)
Downloading and preparing dataset 37.70 MiB (download: 37.70 MiB, generated: 822.53 MiB, total: 860.23 MiB) to /root/data/ogbg_molpcba/0.1.3...
Dl Completed...: 0 url [00:00, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[AI0305 10:09:06.483991 140576608098112 download_manager.py:400] Downloading https://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip into /root/data/downloads/snap.stan.edu_ogb_grap_csv_mol_down_pcbapc4I82Cv1THcU-IggPHK8IHZ8qM-BJ3VDk-q_rtqrf4.zip.tmp.5bd527ca5a844f84bfb501730f9c189d...
Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...: 0 MiB [00:00, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]
Dl Size...:   0%|          | 0/37 [00:00<?, ? MiB/s][A

Extraction completed...: 0 file [00:00, ? file/s][A[A
Dl Size...:   3%|â–Ž         | 1/37 [00:03<01:49,  3.05s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   3%|â–Ž         | 1/37 [00:03<01:49,  3.05s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:59,  1.70s/ MiB][ADl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]
Dl Size...:   5%|â–Œ         | 2/37 [00:03<00:59,  1.70s/ MiB][A

Extraction completed...: 0 file [00:03, ? file/s][A[A
Dl Size...:   8%|â–Š         | 3/37 [00:04<00:38,  1.13s/ MiB][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:   8%|â–Š         | 3/37 [00:04<00:38,  1.13s/ MiB][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.34 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  11%|â–ˆ         | 4/37 [00:04<00:24,  1.34 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.72 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  14%|â–ˆâ–Ž        | 5/37 [00:04<00:18,  1.72 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:13,  2.29 MiB/s][ADl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]
Dl Size...:  16%|â–ˆâ–Œ        | 6/37 [00:04<00:13,  2.29 MiB/s][A

Extraction completed...: 0 file [00:04, ? file/s][A[A
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:05<00:10,  2.92 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  19%|â–ˆâ–‰        | 7/37 [00:05<00:10,  2.92 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:08,  3.56 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  22%|â–ˆâ–ˆâ–       | 8/37 [00:05<00:08,  3.56 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:05<00:06,  4.17 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  24%|â–ˆâ–ˆâ–       | 9/37 [00:05<00:06,  4.17 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  27%|â–ˆâ–ˆâ–‹       | 10/37 [00:05<00:06,  4.17 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:05<00:04,  6.12 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:05<00:04,  6.12 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:03,  6.30 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:05<00:03,  6.30 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 13/37 [00:05<00:03,  6.30 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:02,  8.05 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:05<00:02,  8.05 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:05<00:02,  8.05 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  9.44 MiB/s][ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 16/37 [00:05<00:02,  9.44 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]
Dl Size...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 17/37 [00:05<00:02,  9.44 MiB/s][A

Extraction completed...: 0 file [00:05, ? file/s][A[A
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:06<00:01, 10.53 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [00:06<00:01, 10.53 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 19/37 [00:06<00:01, 10.53 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 11.38 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [00:06<00:01, 11.38 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 21/37 [00:06<00:01, 11.38 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 22/37 [00:06<00:01, 11.38 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:06<00:01, 13.73 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [00:06<00:01, 13.73 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 24/37 [00:06<00:00, 13.73 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.76 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [00:06<00:00, 13.76 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 26/37 [00:06<00:00, 13.76 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 27/37 [00:06<00:00, 13.76 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:06<00:00, 15.51 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [00:06<00:00, 15.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 29/37 [00:06<00:00, 15.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [00:06<00:00, 15.51 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/37 [00:06<00:00, 16.77 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 31/37 [00:06<00:00, 16.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [00:06<00:00, 16.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [00:06<00:00, 16.77 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[A
Dl Size...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [00:06<00:00, 17.69 MiB/s][ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [00:06<00:00, 17.69 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]
Dl Size...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 35/37 [00:06<00:00, 17.69 MiB/s][A

Extraction completed...: 0 file [00:06, ? file/s][A[ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [00:07<00:00, 17.69 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[A
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][ADl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...: 0 file [00:07, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/1 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/2 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/3 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/4 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/5 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/6 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/7 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/8 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/9 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/10 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/11 [00:07<?, ? file/s][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   0%|          | 0/12 [00:07<?, ? file/s][A[A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:20,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:   8%|â–Š         | 1/12 [00:07<01:20,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  17%|â–ˆâ–‹        | 2/12 [00:07<01:12,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [00:07<01:05,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 4/12 [00:07<00:58,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [00:07<00:50,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [00:07<00:43,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [00:07<00:36,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [00:07<00:29,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [00:07<00:21,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 10/12 [00:07<00:14,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [00:07<00:07,  7.28s/ file][A[ADl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.14s/ url]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00, 18.35 MiB/s][A

Extraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  7.28s/ file][A[AExtraction completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:07<00:00,  1.65 file/s]
Dl Size...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:07<00:00,  5.08 MiB/s]
Dl Completed...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.28s/ url]
Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]
Generating train examples...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Generating train examples...:   0%|          | 77/350343 [00:00<07:35, 768.26 examples/s][A
Generating train examples...:   0%|          | 163/350343 [00:00<07:59, 730.07 examples/s][A
Generating train examples...:   0%|          | 390/350343 [00:00<04:12, 1385.83 examples/s][A
Generating train examples...:   0%|          | 620/350343 [00:00<03:22, 1729.71 examples/s][A
Generating train examples...:   0%|          | 854/350343 [00:00<02:59, 1941.96 examples/s][A
Generating train examples...:   0%|          | 1088/350343 [00:00<02:48, 2072.96 examples/s][A
Generating train examples...:   0%|          | 1317/350343 [00:00<02:43, 2141.19 examples/s][A
Generating train examples...:   0%|          | 1556/350343 [00:00<02:37, 2218.42 examples/s][A
Generating train examples...:   1%|          | 1786/350343 [00:00<02:35, 2243.11 examples/s][A
Generating train examples...:   1%|          | 2016/350343 [00:01<02:34, 2260.55 examples/s][A
Generating train examples...:   1%|          | 2243/350343 [00:01<02:37, 2209.51 examples/s][A
Generating train examples...:   1%|          | 2470/350343 [00:01<02:36, 2226.61 examples/s][A
Generating train examples...:   1%|          | 2702/350343 [00:01<02:34, 2254.07 examples/s][A
Generating train examples...:   1%|          | 2937/350343 [00:01<02:32, 2281.64 examples/s][A
Generating train examples...:   1%|          | 3169/350343 [00:01<02:31, 2291.70 examples/s][A
Generating train examples...:   1%|          | 3406/350343 [00:01<02:29, 2313.03 examples/s][A
Generating train examples...:   1%|          | 3648/350343 [00:01<02:27, 2343.30 examples/s][A
Generating train examples...:   1%|          | 3883/350343 [00:01<02:28, 2334.30 examples/s][A
Generating train examples...:   1%|          | 4117/350343 [00:01<02:29, 2311.94 examples/s][A
Generating train examples...:   1%|          | 4349/350343 [00:02<02:30, 2302.82 examples/s][A
Generating train examples...:   1%|â–         | 4580/350343 [00:02<02:31, 2282.80 examples/s][A
Generating train examples...:   1%|â–         | 4810/350343 [00:02<02:31, 2286.58 examples/s][A
Generating train examples...:   1%|â–         | 5043/350343 [00:02<02:30, 2297.43 examples/s][A
Generating train examples...:   2%|â–         | 5274/350343 [00:02<02:30, 2298.83 examples/s][A
Generating train examples...:   2%|â–         | 5513/350343 [00:02<02:28, 2324.74 examples/s][A
Generating train examples...:   2%|â–         | 5746/350343 [00:02<02:28, 2323.22 examples/s][A
Generating train examples...:   2%|â–         | 5979/350343 [00:02<02:28, 2312.23 examples/s][A
Generating train examples...:   2%|â–         | 6211/350343 [00:02<02:34, 2222.25 examples/s][A
Generating train examples...:   2%|â–         | 6444/350343 [00:02<02:32, 2252.04 examples/s][A
Generating train examples...:   2%|â–         | 6671/350343 [00:03<02:32, 2256.29 examples/s][A
Generating train examples...:   2%|â–         | 6898/350343 [00:03<02:32, 2249.79 examples/s][A
Generating train examples...:   2%|â–         | 7128/350343 [00:03<02:31, 2262.24 examples/s][A
Generating train examples...:   2%|â–         | 7355/350343 [00:03<02:31, 2262.40 examples/s][A
Generating train examples...:   2%|â–         | 7590/350343 [00:03<02:29, 2286.93 examples/s][A
Generating train examples...:   2%|â–         | 7826/350343 [00:03<02:28, 2306.94 examples/s][A
Generating train examples...:   2%|â–         | 8057/350343 [00:03<02:28, 2299.19 examples/s][A
Generating train examples...:   2%|â–         | 8287/350343 [00:03<02:28, 2298.20 examples/s][A
Generating train examples...:   2%|â–         | 8524/350343 [00:03<02:27, 2318.03 examples/s][A
Generating train examples...:   2%|â–         | 8758/350343 [00:03<02:26, 2323.88 examples/s][A
Generating train examples...:   3%|â–Ž         | 8994/350343 [00:04<02:26, 2332.31 examples/s][A
Generating train examples...:   3%|â–Ž         | 9228/350343 [00:04<02:27, 2315.49 examples/s][A
Generating train examples...:   3%|â–Ž         | 9461/350343 [00:04<02:26, 2319.54 examples/s][A
Generating train examples...:   3%|â–Ž         | 9693/350343 [00:04<02:27, 2306.74 examples/s][A
Generating train examples...:   3%|â–Ž         | 9924/350343 [00:04<02:28, 2295.95 examples/s][A
Generating train examples...:   3%|â–Ž         | 10154/350343 [00:04<02:28, 2290.32 examples/s][A
Generating train examples...:   3%|â–Ž         | 10390/350343 [00:04<02:27, 2309.13 examples/s][A
Generating train examples...:   3%|â–Ž         | 10626/350343 [00:04<02:26, 2322.42 examples/s][A
Generating train examples...:   3%|â–Ž         | 10861/350343 [00:04<02:25, 2329.60 examples/s][A
Generating train examples...:   3%|â–Ž         | 11096/350343 [00:04<02:25, 2334.71 examples/s][A
Generating train examples...:   3%|â–Ž         | 11331/350343 [00:05<02:25, 2336.88 examples/s][A
Generating train examples...:   3%|â–Ž         | 11565/350343 [00:05<02:26, 2310.60 examples/s][A
Generating train examples...:   3%|â–Ž         | 11797/350343 [00:05<02:26, 2311.84 examples/s][A
Generating train examples...:   3%|â–Ž         | 12029/350343 [00:05<02:33, 2201.25 examples/s][A
Generating train examples...:   3%|â–Ž         | 12262/350343 [00:05<02:31, 2236.99 examples/s][A
Generating train examples...:   4%|â–Ž         | 12497/350343 [00:05<02:28, 2269.27 examples/s][A
Generating train examples...:   4%|â–Ž         | 12732/350343 [00:05<02:27, 2292.60 examples/s][A
Generating train examples...:   4%|â–Ž         | 12966/350343 [00:05<02:26, 2306.17 examples/s][A
Generating train examples...:   4%|â–         | 13198/350343 [00:05<02:26, 2304.91 examples/s][A
Generating train examples...:   4%|â–         | 13429/350343 [00:05<02:26, 2298.15 examples/s][A
Generating train examples...:   4%|â–         | 13660/350343 [00:06<02:26, 2300.11 examples/s][A
Generating train examples...:   4%|â–         | 13891/350343 [00:06<02:27, 2288.73 examples/s][A
Generating train examples...:   4%|â–         | 14120/350343 [00:06<02:27, 2280.48 examples/s][A
Generating train examples...:   4%|â–         | 14351/350343 [00:06<02:26, 2287.03 examples/s][A
Generating train examples...:   4%|â–         | 14581/350343 [00:06<02:26, 2289.61 examples/s][A
Generating train examples...:   4%|â–         | 14812/350343 [00:06<02:26, 2293.38 examples/s][A
Generating train examples...:   4%|â–         | 15048/350343 [00:06<02:24, 2312.87 examples/s][A
Generating train examples...:   4%|â–         | 15287/350343 [00:06<02:23, 2333.48 examples/s][A
Generating train examples...:   4%|â–         | 15524/350343 [00:06<02:22, 2344.13 examples/s][A
Generating train examples...:   4%|â–         | 15759/350343 [00:07<02:22, 2340.31 examples/s][A
Generating train examples...:   5%|â–         | 15994/350343 [00:07<02:23, 2326.84 examples/s][A
Generating train examples...:   5%|â–         | 16227/350343 [00:07<02:24, 2305.68 examples/s][A
Generating train examples...:   5%|â–         | 16459/350343 [00:07<02:24, 2308.44 examples/s][A
Generating train examples...:   5%|â–         | 16690/350343 [00:07<02:25, 2298.16 examples/s][A
Generating train examples...:   5%|â–         | 16920/350343 [00:07<02:25, 2296.82 examples/s][A
Generating train examples...:   5%|â–         | 17153/350343 [00:07<02:24, 2305.97 examples/s][A
Generating train examples...:   5%|â–         | 17386/350343 [00:07<02:24, 2311.09 examples/s][A
Generating train examples...:   5%|â–Œ         | 17618/350343 [00:07<02:29, 2227.36 examples/s][A
Generating train examples...:   5%|â–Œ         | 17854/350343 [00:07<02:26, 2263.76 examples/s][A
Generating train examples...:   5%|â–Œ         | 18085/350343 [00:08<02:25, 2276.48 examples/s][A
Generating train examples...:   5%|â–Œ         | 18314/350343 [00:08<02:26, 2270.52 examples/s][A
Generating train examples...:   5%|â–Œ         | 18542/350343 [00:08<02:32, 2174.79 examples/s][A
Generating train examples...:   5%|â–Œ         | 18761/350343 [00:08<02:37, 2098.83 examples/s][A
Generating train examples...:   5%|â–Œ         | 18987/350343 [00:08<02:34, 2143.35 examples/s][A
Generating train examples...:   5%|â–Œ         | 19223/350343 [00:08<02:30, 2204.33 examples/s][A
Generating train examples...:   6%|â–Œ         | 19452/350343 [00:08<02:28, 2228.48 examples/s][A
Generating train examples...:   6%|â–Œ         | 19681/350343 [00:08<02:27, 2245.04 examples/s][A
Generating train examples...:   6%|â–Œ         | 19915/350343 [00:08<02:25, 2270.55 examples/s][A
Generating train examples...:   6%|â–Œ         | 20151/350343 [00:08<02:23, 2293.87 examples/s][A
Generating train examples...:   6%|â–Œ         | 20389/350343 [00:09<02:22, 2316.52 examples/s][A
Generating train examples...:   6%|â–Œ         | 20621/350343 [00:09<02:23, 2299.09 examples/s][A
Generating train examples...:   6%|â–Œ         | 20854/350343 [00:09<02:22, 2306.36 examples/s][A
Generating train examples...:   6%|â–Œ         | 21093/350343 [00:09<02:21, 2330.26 examples/s][A
Generating train examples...:   6%|â–Œ         | 21327/350343 [00:09<02:21, 2330.22 examples/s][A
Generating train examples...:   6%|â–Œ         | 21563/350343 [00:09<02:20, 2338.80 examples/s][A
Generating train examples...:   6%|â–Œ         | 21797/350343 [00:09<02:21, 2329.90 examples/s][A
Generating train examples...:   6%|â–‹         | 22031/350343 [00:09<02:20, 2331.48 examples/s][A
Generating train examples...:   6%|â–‹         | 22265/350343 [00:09<02:21, 2324.76 examples/s][A
Generating train examples...:   6%|â–‹         | 22500/350343 [00:09<02:20, 2331.77 examples/s][A
Generating train examples...:   6%|â–‹         | 22734/350343 [00:10<02:22, 2305.39 examples/s][A
Generating train examples...:   7%|â–‹         | 22965/350343 [00:10<02:22, 2293.89 examples/s][A
Generating train examples...:   7%|â–‹         | 23195/350343 [00:10<02:22, 2288.88 examples/s][A
Generating train examples...:   7%|â–‹         | 23425/350343 [00:10<02:22, 2290.74 examples/s][A
Generating train examples...:   7%|â–‹         | 23656/350343 [00:10<02:22, 2294.74 examples/s][A
Generating train examples...:   7%|â–‹         | 23886/350343 [00:10<02:27, 2215.85 examples/s][A
Generating train examples...:   7%|â–‹         | 24116/350343 [00:10<02:25, 2239.36 examples/s][A
Generating train examples...:   7%|â–‹         | 24348/350343 [00:10<02:24, 2261.24 examples/s][A
Generating train examples...:   7%|â–‹         | 24583/350343 [00:10<02:22, 2285.36 examples/s][A
Generating train examples...:   7%|â–‹         | 24820/350343 [00:10<02:20, 2309.46 examples/s][A
Generating train examples...:   7%|â–‹         | 25052/350343 [00:11<02:20, 2307.84 examples/s][A
Generating train examples...:   7%|â–‹         | 25283/350343 [00:11<02:22, 2273.62 examples/s][A
Generating train examples...:   7%|â–‹         | 25511/350343 [00:11<02:23, 2269.29 examples/s][A
Generating train examples...:   7%|â–‹         | 25744/350343 [00:11<02:21, 2286.02 examples/s][A
Generating train examples...:   7%|â–‹         | 25975/350343 [00:11<02:21, 2291.75 examples/s][A
Generating train examples...:   7%|â–‹         | 26211/350343 [00:11<02:20, 2310.00 examples/s][A
Generating train examples...:   8%|â–Š         | 26447/350343 [00:11<02:19, 2324.84 examples/s][A
Generating train examples...:   8%|â–Š         | 26682/350343 [00:11<02:18, 2331.24 examples/s][A
Generating train examples...:   8%|â–Š         | 26916/350343 [00:11<02:19, 2325.43 examples/s][A
Generating train examples...:   8%|â–Š         | 27155/350343 [00:11<02:17, 2343.06 examples/s][A
Generating train examples...:   8%|â–Š         | 27390/350343 [00:12<02:19, 2313.56 examples/s][A
Generating train examples...:   8%|â–Š         | 27622/350343 [00:12<02:19, 2305.76 examples/s][A
Generating train examples...:   8%|â–Š         | 27858/350343 [00:12<02:18, 2320.87 examples/s][A
Generating train examples...:   8%|â–Š         | 28097/350343 [00:12<02:17, 2339.17 examples/s][A
Generating train examples...:   8%|â–Š         | 28333/350343 [00:12<02:17, 2343.81 examples/s][A
Generating train examples...:   8%|â–Š         | 28568/350343 [00:12<02:19, 2312.92 examples/s][A
Generating train examples...:   8%|â–Š         | 28806/350343 [00:12<02:23, 2241.91 examples/s][A
Generating train examples...:   8%|â–Š         | 29045/350343 [00:12<02:20, 2284.56 examples/s][A
Generating train examples...:   8%|â–Š         | 29280/350343 [00:12<02:19, 2301.11 examples/s][A
Generating train examples...:   8%|â–Š         | 29511/350343 [00:13<02:20, 2284.51 examples/s][A
Generating train examples...:   8%|â–Š         | 29742/350343 [00:13<02:19, 2290.21 examples/s][A
Generating train examples...:   9%|â–Š         | 29981/350343 [00:13<02:18, 2318.02 examples/s][A
Generating train examples...:   9%|â–Š         | 30213/350343 [00:13<02:18, 2309.07 examples/s][A
Generating train examples...:   9%|â–Š         | 30447/350343 [00:13<02:18, 2317.72 examples/s][A
Generating train examples...:   9%|â–‰         | 30680/350343 [00:13<02:17, 2319.26 examples/s][A
Generating train examples...:   9%|â–‰         | 30912/350343 [00:13<02:17, 2317.13 examples/s][A
Generating train examples...:   9%|â–‰         | 31145/350343 [00:13<02:17, 2319.60 examples/s][A
Generating train examples...:   9%|â–‰         | 31381/350343 [00:13<02:16, 2329.35 examples/s][A
Generating train examples...:   9%|â–‰         | 31614/350343 [00:13<02:17, 2316.15 examples/s][A
Generating train examples...:   9%|â–‰         | 31846/350343 [00:14<02:17, 2308.68 examples/s][A
Generating train examples...:   9%|â–‰         | 32077/350343 [00:14<02:19, 2274.99 examples/s][A
Generating train examples...:   9%|â–‰         | 32307/350343 [00:14<02:19, 2282.30 examples/s][A
Generating train examples...:   9%|â–‰         | 32538/350343 [00:14<02:18, 2290.06 examples/s][A
Generating train examples...:   9%|â–‰         | 32768/350343 [00:14<02:19, 2279.87 examples/s][A
Generating train examples...:   9%|â–‰         | 33009/350343 [00:14<02:16, 2317.18 examples/s][A
Generating train examples...:   9%|â–‰         | 33244/350343 [00:14<02:16, 2325.50 examples/s][A
Generating train examples...:  10%|â–‰         | 33477/350343 [00:14<02:16, 2326.25 examples/s][A
Generating train examples...:  10%|â–‰         | 33710/350343 [00:14<02:16, 2315.14 examples/s][A
Generating train examples...:  10%|â–‰         | 33942/350343 [00:14<02:16, 2312.52 examples/s][A
Generating train examples...:  10%|â–‰         | 34174/350343 [00:15<02:23, 2204.74 examples/s][A
Generating train examples...:  10%|â–‰         | 34399/350343 [00:15<02:22, 2216.65 examples/s][A
Generating train examples...:  10%|â–‰         | 34631/350343 [00:15<02:20, 2244.62 examples/s][A
Generating train examples...:  10%|â–‰         | 34866/350343 [00:15<02:18, 2274.28 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35096/350343 [00:15<02:18, 2280.11 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35325/350343 [00:15<02:18, 2281.01 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35564/350343 [00:15<02:16, 2311.44 examples/s][A
Generating train examples...:  10%|â–ˆ         | 35799/350343 [00:15<02:15, 2322.08 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36032/350343 [00:15<02:15, 2312.51 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36267/350343 [00:15<02:15, 2323.18 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36501/350343 [00:16<02:14, 2325.88 examples/s][A
Generating train examples...:  10%|â–ˆ         | 36734/350343 [00:16<02:17, 2279.97 examples/s][A
Generating train examples...:  11%|â–ˆ         | 36967/350343 [00:16<02:16, 2294.38 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37197/350343 [00:16<02:16, 2292.59 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37429/350343 [00:16<02:16, 2299.75 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37662/350343 [00:16<02:15, 2307.33 examples/s][A
Generating train examples...:  11%|â–ˆ         | 37895/350343 [00:16<02:15, 2312.06 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38129/350343 [00:16<02:14, 2319.69 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38362/350343 [00:16<02:14, 2319.31 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38594/350343 [00:16<02:14, 2313.82 examples/s][A
Generating train examples...:  11%|â–ˆ         | 38826/350343 [00:17<02:14, 2309.09 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39057/350343 [00:17<02:14, 2307.02 examples/s][A
Generating train examples...:  11%|â–ˆ         | 39291/350343 [00:17<02:14, 2314.86 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39527/350343 [00:17<02:13, 2327.80 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39760/350343 [00:17<02:13, 2326.17 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 39994/350343 [00:17<02:18, 2247.72 examples/s][A
Generating train examples...:  11%|â–ˆâ–        | 40220/350343 [00:17<02:18, 2232.59 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40451/350343 [00:17<02:17, 2252.88 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40678/350343 [00:17<02:17, 2257.41 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 40911/350343 [00:17<02:15, 2278.48 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41142/350343 [00:18<02:15, 2286.08 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41371/350343 [00:18<02:15, 2274.53 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41610/350343 [00:18<02:13, 2307.45 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 41845/350343 [00:18<02:13, 2318.22 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42077/350343 [00:18<02:13, 2312.38 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42313/350343 [00:18<02:12, 2323.70 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42546/350343 [00:18<02:12, 2324.72 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 42779/350343 [00:18<02:12, 2316.06 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43011/350343 [00:18<02:12, 2314.57 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43243/350343 [00:18<02:12, 2311.93 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43476/350343 [00:19<02:12, 2315.69 examples/s][A
Generating train examples...:  12%|â–ˆâ–        | 43708/350343 [00:19<02:12, 2306.64 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 43939/350343 [00:19<02:13, 2293.40 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44176/350343 [00:19<02:12, 2316.02 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44408/350343 [00:19<02:12, 2308.11 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44639/350343 [00:19<02:13, 2297.39 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 44869/350343 [00:19<02:13, 2296.36 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45104/350343 [00:19<02:12, 2310.30 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45337/350343 [00:19<02:11, 2315.47 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45573/350343 [00:20<02:10, 2327.58 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 45806/350343 [00:20<02:16, 2225.56 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46031/350343 [00:20<02:16, 2231.68 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46262/350343 [00:20<02:14, 2252.73 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46498/350343 [00:20<02:13, 2283.71 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46731/350343 [00:20<02:12, 2295.57 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 46961/350343 [00:20<02:12, 2292.44 examples/s][A
Generating train examples...:  13%|â–ˆâ–Ž        | 47197/350343 [00:20<02:11, 2310.56 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47429/350343 [00:20<02:11, 2307.20 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47666/350343 [00:20<02:10, 2323.73 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 47899/350343 [00:21<02:10, 2316.50 examples/s][A
Generating train examples...:  14%|â–ˆâ–Ž        | 48131/350343 [00:21<02:12, 2287.20 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48360/350343 [00:21<02:12, 2285.18 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48603/350343 [00:21<02:09, 2326.85 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 48840/350343 [00:21<02:08, 2339.66 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49075/350343 [00:21<02:09, 2333.12 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49315/350343 [00:21<02:07, 2352.54 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49551/350343 [00:21<02:08, 2345.51 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 49786/350343 [00:21<02:08, 2344.21 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50021/350343 [00:21<02:08, 2333.03 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50255/350343 [00:22<02:09, 2315.38 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50487/350343 [00:22<02:12, 2266.47 examples/s][A
Generating train examples...:  14%|â–ˆâ–        | 50717/350343 [00:22<02:11, 2274.54 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 50945/350343 [00:22<02:11, 2275.71 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51173/350343 [00:22<02:16, 2191.17 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51403/350343 [00:22<02:14, 2220.32 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51636/350343 [00:22<02:12, 2250.45 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 51873/350343 [00:22<02:10, 2285.25 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52102/350343 [00:22<02:10, 2286.55 examples/s][A
Generating train examples...:  15%|â–ˆâ–        | 52336/350343 [00:22<02:09, 2301.35 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52567/350343 [00:23<02:09, 2299.29 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 52798/350343 [00:23<02:10, 2288.05 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53027/350343 [00:23<02:10, 2283.50 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53261/350343 [00:23<02:09, 2298.22 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53491/350343 [00:23<02:09, 2286.19 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53720/350343 [00:23<02:09, 2287.13 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 53951/350343 [00:23<02:09, 2292.65 examples/s][A
Generating train examples...:  15%|â–ˆâ–Œ        | 54187/350343 [00:23<02:08, 2312.25 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54421/350343 [00:23<02:07, 2318.85 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54658/350343 [00:23<02:06, 2332.95 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 54892/350343 [00:24<02:06, 2328.53 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55125/350343 [00:24<02:08, 2304.36 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55356/350343 [00:24<02:08, 2299.97 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55588/350343 [00:24<02:07, 2305.54 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 55822/350343 [00:24<02:07, 2313.54 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56055/350343 [00:24<02:06, 2317.76 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56289/350343 [00:24<02:06, 2324.08 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56523/350343 [00:24<02:06, 2327.51 examples/s][A
Generating train examples...:  16%|â–ˆâ–Œ        | 56757/350343 [00:24<02:05, 2331.06 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 56993/350343 [00:24<02:10, 2247.70 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57223/350343 [00:25<02:09, 2260.56 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57450/350343 [00:25<02:09, 2259.72 examples/s][A
Generating train examples...:  16%|â–ˆâ–‹        | 57683/350343 [00:25<02:08, 2278.74 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 57912/350343 [00:25<02:08, 2281.49 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58141/350343 [00:25<02:08, 2280.33 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58375/350343 [00:25<02:07, 2296.59 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58607/350343 [00:25<02:06, 2302.59 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 58841/350343 [00:25<02:06, 2312.99 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59073/350343 [00:25<02:06, 2311.61 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59305/350343 [00:25<02:06, 2307.86 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59536/350343 [00:26<02:06, 2303.51 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59767/350343 [00:26<02:07, 2280.07 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 59998/350343 [00:26<02:06, 2287.94 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60228/350343 [00:26<02:06, 2291.42 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60459/350343 [00:26<02:06, 2294.39 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60692/350343 [00:26<02:05, 2304.21 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 60924/350343 [00:26<02:05, 2306.50 examples/s][A
Generating train examples...:  17%|â–ˆâ–‹        | 61159/350343 [00:26<02:04, 2319.26 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61393/350343 [00:26<02:04, 2323.02 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61627/350343 [00:26<02:04, 2327.23 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 61860/350343 [00:27<02:03, 2326.99 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62093/350343 [00:27<02:04, 2311.76 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62325/350343 [00:27<02:05, 2293.80 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62563/350343 [00:27<02:04, 2318.16 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 62795/350343 [00:27<02:04, 2317.24 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63027/350343 [00:27<02:06, 2278.63 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63256/350343 [00:27<02:15, 2122.31 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63482/350343 [00:27<02:12, 2158.77 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63708/350343 [00:27<02:11, 2187.77 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 63939/350343 [00:28<02:08, 2222.26 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64171/350343 [00:28<02:07, 2250.55 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64397/350343 [00:28<02:07, 2247.57 examples/s][A
Generating train examples...:  18%|â–ˆâ–Š        | 64627/350343 [00:28<02:06, 2263.07 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 64859/350343 [00:28<02:05, 2278.36 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65090/350343 [00:28<02:04, 2286.18 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65321/350343 [00:28<02:04, 2293.06 examples/s][A
Generating train examples...:  19%|â–ˆâ–Š        | 65551/350343 [00:28<02:04, 2288.10 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 65782/350343 [00:28<02:04, 2292.86 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66012/350343 [00:28<02:04, 2287.56 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66245/350343 [00:29<02:03, 2297.31 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66475/350343 [00:29<02:03, 2291.49 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66705/350343 [00:29<02:03, 2287.73 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 66940/350343 [00:29<02:02, 2304.88 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67172/350343 [00:29<02:02, 2307.30 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67407/350343 [00:29<02:02, 2317.91 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67642/350343 [00:29<02:01, 2325.91 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 67875/350343 [00:29<02:01, 2323.86 examples/s][A
Generating train examples...:  19%|â–ˆâ–‰        | 68110/350343 [00:29<02:01, 2329.33 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68347/350343 [00:29<02:00, 2339.50 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68581/350343 [00:30<02:01, 2325.65 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 68814/350343 [00:30<02:01, 2309.37 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69045/350343 [00:30<02:02, 2288.59 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69274/350343 [00:30<02:02, 2288.35 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69503/350343 [00:30<02:03, 2277.42 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69735/350343 [00:30<02:02, 2288.12 examples/s][A
Generating train examples...:  20%|â–ˆâ–‰        | 69967/350343 [00:30<02:06, 2213.60 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70197/350343 [00:30<02:05, 2237.80 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70422/350343 [00:30<02:11, 2130.53 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70637/350343 [00:31<02:20, 1993.29 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 70839/350343 [00:31<02:28, 1885.56 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71030/350343 [00:31<02:34, 1813.39 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71213/350343 [00:31<02:37, 1774.46 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71392/350343 [00:31<02:37, 1769.25 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71583/350343 [00:31<02:34, 1806.43 examples/s][A
Generating train examples...:  20%|â–ˆâ–ˆ        | 71795/350343 [00:31<02:26, 1895.08 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72020/350343 [00:31<02:19, 1997.58 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72253/350343 [00:31<02:12, 2092.76 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72483/350343 [00:31<02:09, 2152.25 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72717/350343 [00:32<02:05, 2206.85 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 72943/350343 [00:32<02:04, 2221.35 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73179/350343 [00:32<02:02, 2260.92 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73407/350343 [00:32<02:02, 2265.57 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73636/350343 [00:32<02:01, 2270.52 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 73867/350343 [00:32<02:01, 2281.13 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74099/350343 [00:32<02:00, 2290.47 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆ        | 74334/350343 [00:32<01:59, 2306.85 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74565/350343 [00:32<01:59, 2299.92 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 74796/350343 [00:32<02:00, 2294.83 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 75026/350343 [00:33<02:00, 2292.37 examples/s][A
Generating train examples...:  21%|â–ˆâ–ˆâ–       | 75263/350343 [00:33<01:58, 2314.15 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75495/350343 [00:33<01:59, 2308.40 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75727/350343 [00:33<01:58, 2310.06 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 75959/350343 [00:33<01:59, 2292.51 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76192/350343 [00:33<01:59, 2302.87 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76423/350343 [00:33<01:59, 2298.01 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76653/350343 [00:33<01:59, 2296.58 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 76889/350343 [00:33<01:58, 2313.18 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77125/350343 [00:33<01:57, 2326.70 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77358/350343 [00:34<02:02, 2229.12 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77589/350343 [00:34<02:01, 2251.01 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 77823/350343 [00:34<01:59, 2275.47 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78052/350343 [00:34<02:00, 2264.89 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78288/350343 [00:34<01:58, 2290.39 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78518/350343 [00:34<01:58, 2289.76 examples/s][A
Generating train examples...:  22%|â–ˆâ–ˆâ–       | 78748/350343 [00:34<01:58, 2285.50 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 78979/350343 [00:34<01:58, 2291.56 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79212/350343 [00:34<01:57, 2301.73 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79448/350343 [00:34<01:56, 2317.10 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79681/350343 [00:35<01:56, 2319.98 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 79917/350343 [00:35<01:55, 2331.37 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80155/350343 [00:35<01:55, 2343.29 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80390/350343 [00:35<01:55, 2327.31 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80623/350343 [00:35<01:57, 2303.90 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 80854/350343 [00:35<01:56, 2304.42 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81086/350343 [00:35<01:56, 2308.26 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81317/350343 [00:35<01:57, 2297.91 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81547/350343 [00:35<01:57, 2295.25 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 81777/350343 [00:35<01:57, 2292.06 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82007/350343 [00:36<01:57, 2291.22 examples/s][A
Generating train examples...:  23%|â–ˆâ–ˆâ–Ž       | 82237/350343 [00:36<01:56, 2293.58 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82468/350343 [00:36<01:56, 2297.88 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82698/350343 [00:36<01:56, 2292.93 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 82928/350343 [00:36<01:56, 2285.84 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–Ž       | 83158/350343 [00:36<01:56, 2289.43 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83387/350343 [00:36<02:02, 2172.32 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83611/350343 [00:36<02:01, 2189.72 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 83839/350343 [00:36<02:00, 2214.45 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84067/350343 [00:37<01:59, 2231.29 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84295/350343 [00:37<01:58, 2245.63 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84525/350343 [00:37<01:57, 2260.33 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84755/350343 [00:37<01:56, 2270.82 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 84983/350343 [00:37<01:57, 2266.07 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85212/350343 [00:37<01:56, 2271.26 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85440/350343 [00:37<01:56, 2269.75 examples/s][A
Generating train examples...:  24%|â–ˆâ–ˆâ–       | 85673/350343 [00:37<01:55, 2285.47 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 85910/350343 [00:37<01:54, 2309.32 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86142/350343 [00:37<01:54, 2311.85 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86374/350343 [00:38<01:54, 2306.98 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86606/350343 [00:38<01:54, 2310.01 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 86838/350343 [00:38<01:54, 2299.61 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87069/350343 [00:38<01:54, 2301.15 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87300/350343 [00:38<01:55, 2287.22 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–       | 87534/350343 [00:38<01:54, 2302.86 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 87769/350343 [00:38<01:53, 2314.37 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88001/350343 [00:38<01:53, 2305.54 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88232/350343 [00:38<01:53, 2299.32 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88462/350343 [00:38<01:53, 2299.10 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88695/350343 [00:39<01:53, 2306.03 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 88926/350343 [00:39<01:53, 2298.14 examples/s][A
Generating train examples...:  25%|â–ˆâ–ˆâ–Œ       | 89158/350343 [00:39<01:53, 2303.03 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89389/350343 [00:39<01:53, 2302.40 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89621/350343 [00:39<01:53, 2305.41 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 89855/350343 [00:39<01:52, 2314.72 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90087/350343 [00:39<01:52, 2304.29 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90319/350343 [00:39<01:52, 2307.93 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90550/350343 [00:39<01:56, 2221.54 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 90779/350343 [00:39<01:55, 2239.12 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91015/350343 [00:40<01:54, 2274.30 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91244/350343 [00:40<01:53, 2277.78 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91473/350343 [00:40<01:53, 2276.94 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91705/350343 [00:40<01:53, 2287.83 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–Œ       | 91934/350343 [00:40<01:54, 2266.21 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92172/350343 [00:40<01:52, 2297.59 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92402/350343 [00:40<01:52, 2296.06 examples/s][A
Generating train examples...:  26%|â–ˆâ–ˆâ–‹       | 92642/350343 [00:40<01:50, 2325.40 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 92875/350343 [00:40<01:50, 2321.22 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93109/350343 [00:40<01:50, 2325.09 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93342/350343 [00:41<01:50, 2321.26 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93575/350343 [00:41<01:50, 2322.94 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 93808/350343 [00:41<01:50, 2324.11 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94041/350343 [00:41<01:50, 2317.00 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94273/350343 [00:41<01:50, 2308.60 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94512/350343 [00:41<01:49, 2331.68 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94746/350343 [00:41<01:49, 2328.80 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 94984/350343 [00:41<01:49, 2341.99 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95219/350343 [00:41<01:49, 2337.07 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95453/350343 [00:41<01:49, 2333.11 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95687/350343 [00:42<01:49, 2323.05 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 95920/350343 [00:42<01:53, 2243.80 examples/s][A
Generating train examples...:  27%|â–ˆâ–ˆâ–‹       | 96149/350343 [00:42<01:52, 2256.31 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96380/350343 [00:42<01:51, 2271.80 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96608/350343 [00:42<01:52, 2264.98 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 96839/350343 [00:42<01:51, 2278.25 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97068/350343 [00:42<01:51, 2279.26 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97297/350343 [00:42<01:51, 2279.68 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97531/350343 [00:42<01:50, 2296.44 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97766/350343 [00:42<01:49, 2312.09 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 97998/350343 [00:43<01:49, 2301.55 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98239/350343 [00:43<01:48, 2333.10 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98476/350343 [00:43<01:47, 2343.16 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98711/350343 [00:43<01:47, 2335.11 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 98945/350343 [00:43<01:48, 2317.64 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99180/350343 [00:43<01:47, 2327.21 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99413/350343 [00:43<01:47, 2326.08 examples/s][A
Generating train examples...:  28%|â–ˆâ–ˆâ–Š       | 99646/350343 [00:43<01:48, 2316.71 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 99879/350343 [00:43<01:47, 2320.58 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100116/350343 [00:43<01:47, 2333.13 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100350/350343 [00:44<01:47, 2326.48 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–Š       | 100583/350343 [00:44<01:47, 2321.92 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 100816/350343 [00:44<01:47, 2322.60 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101049/350343 [00:44<01:47, 2318.68 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101281/350343 [00:44<01:47, 2316.26 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101513/350343 [00:44<01:51, 2230.12 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101744/350343 [00:44<01:50, 2252.15 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 101978/350343 [00:44<01:49, 2277.55 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102215/350343 [00:44<01:47, 2302.46 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102446/350343 [00:45<01:47, 2304.19 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102677/350343 [00:45<01:47, 2300.72 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 102908/350343 [00:45<01:47, 2298.81 examples/s][A
Generating train examples...:  29%|â–ˆâ–ˆâ–‰       | 103138/350343 [00:45<01:48, 2284.89 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103367/350343 [00:45<01:48, 2273.16 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103595/350343 [00:45<01:49, 2262.58 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 103828/350343 [00:45<01:48, 2281.41 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104057/350343 [00:45<01:47, 2281.25 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104291/350343 [00:45<01:47, 2296.24 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104524/350343 [00:45<01:46, 2303.22 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104760/350343 [00:46<01:45, 2317.82 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–‰       | 104993/350343 [00:46<01:45, 2318.82 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105229/350343 [00:46<01:45, 2329.11 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105462/350343 [00:46<01:45, 2329.16 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105695/350343 [00:46<01:45, 2310.36 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 105927/350343 [00:46<01:45, 2310.82 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106161/350343 [00:46<01:45, 2318.64 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106393/350343 [00:46<01:48, 2249.15 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106619/350343 [00:46<01:49, 2220.72 examples/s][A
Generating train examples...:  30%|â–ˆâ–ˆâ–ˆ       | 106842/350343 [00:46<01:54, 2119.23 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107070/350343 [00:47<01:52, 2162.83 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107303/350343 [00:47<01:49, 2210.26 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107530/350343 [00:47<01:49, 2227.45 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107754/350343 [00:47<01:48, 2227.02 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 107978/350343 [00:47<01:49, 2217.04 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108207/350343 [00:47<01:48, 2238.29 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108436/350343 [00:47<01:47, 2252.01 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108669/350343 [00:47<01:46, 2274.64 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 108899/350343 [00:47<01:49, 2202.35 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109131/350343 [00:47<01:47, 2235.05 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆ       | 109362/350343 [00:48<01:46, 2254.28 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109588/350343 [00:48<01:46, 2251.03 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 109814/350343 [00:48<01:46, 2253.21 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110040/350343 [00:48<01:46, 2246.15 examples/s][A
Generating train examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 110266/350343 [00:48<01:46, 2249.24 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110501/350343 [00:48<01:45, 2277.83 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110730/350343 [00:48<01:45, 2278.74 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 110961/350343 [00:48<01:44, 2287.39 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111195/350343 [00:48<01:43, 2301.66 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111433/350343 [00:48<01:42, 2325.02 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111672/350343 [00:49<01:41, 2343.02 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 111907/350343 [00:49<01:42, 2335.86 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112141/350343 [00:49<01:43, 2300.16 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112374/350343 [00:49<01:43, 2308.86 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112605/350343 [00:49<01:44, 2284.08 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 112839/350343 [00:49<01:43, 2300.52 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113070/350343 [00:49<01:43, 2295.04 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113302/350343 [00:49<01:42, 2302.39 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113537/350343 [00:49<01:42, 2314.41 examples/s][A
Generating train examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 113769/350343 [00:49<01:42, 2306.26 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114000/350343 [00:50<01:42, 2306.08 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114237/350343 [00:50<01:41, 2323.84 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114473/350343 [00:50<01:41, 2333.14 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114710/350343 [00:50<01:40, 2343.65 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 114945/350343 [00:50<01:45, 2224.79 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115175/350343 [00:50<01:44, 2246.08 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115401/350343 [00:50<01:44, 2248.08 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115634/350343 [00:50<01:43, 2270.67 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115868/350343 [00:50<01:42, 2289.08 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116103/350343 [00:50<01:41, 2307.00 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116334/350343 [00:51<01:41, 2307.02 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116567/350343 [00:51<01:41, 2313.66 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 116801/350343 [00:51<01:40, 2321.13 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117036/350343 [00:51<01:40, 2328.64 examples/s][A
Generating train examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 117269/350343 [00:51<01:41, 2296.28 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117499/350343 [00:51<01:41, 2290.91 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117729/350343 [00:51<01:41, 2292.38 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 117959/350343 [00:51<01:41, 2293.81 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–Ž      | 118194/350343 [00:51<01:40, 2308.02 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118425/350343 [00:52<01:42, 2271.40 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118653/350343 [00:52<01:43, 2239.41 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 118888/350343 [00:52<01:41, 2271.76 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119118/350343 [00:52<01:41, 2279.97 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119347/350343 [00:52<01:41, 2280.23 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119576/350343 [00:52<01:42, 2255.34 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 119808/350343 [00:52<01:41, 2273.40 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120039/350343 [00:52<01:40, 2282.71 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120271/350343 [00:52<01:40, 2291.63 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120502/350343 [00:52<01:40, 2296.58 examples/s][A
Generating train examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 120732/350343 [00:53<01:40, 2295.48 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 120962/350343 [00:53<01:40, 2292.06 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121198/350343 [00:53<01:39, 2311.33 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121430/350343 [00:53<01:43, 2211.75 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121662/350343 [00:53<01:41, 2242.67 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 121889/350343 [00:53<01:41, 2249.85 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122123/350343 [00:53<01:40, 2274.79 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122352/350343 [00:53<01:40, 2276.91 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 122585/350343 [00:53<01:39, 2290.43 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 122815/350343 [00:53<01:39, 2278.55 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123044/350343 [00:54<01:39, 2275.25 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123272/350343 [00:54<01:39, 2272.41 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123506/350343 [00:54<01:38, 2291.85 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123739/350343 [00:54<01:38, 2302.68 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 123973/350343 [00:54<01:37, 2312.72 examples/s][A
Generating train examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 124205/350343 [00:54<01:38, 2291.73 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124441/350343 [00:54<01:37, 2310.98 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124673/350343 [00:54<01:37, 2304.76 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 124907/350343 [00:54<01:37, 2314.52 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125139/350343 [00:54<01:37, 2312.29 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125372/350343 [00:55<01:37, 2317.10 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125604/350343 [00:55<01:37, 2309.82 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 125836/350343 [00:55<01:37, 2303.04 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126071/350343 [00:55<01:36, 2316.10 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126303/350343 [00:55<01:37, 2306.25 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126534/350343 [00:55<01:37, 2289.36 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 126769/350343 [00:55<01:36, 2306.33 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127007/350343 [00:55<01:35, 2327.91 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127240/350343 [00:55<01:39, 2248.02 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127469/350343 [00:55<01:38, 2259.35 examples/s][A
Generating train examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 127698/350343 [00:56<01:38, 2267.68 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 127932/350343 [00:56<01:37, 2289.01 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128165/350343 [00:56<01:36, 2299.93 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128397/350343 [00:56<01:36, 2304.82 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128628/350343 [00:56<01:36, 2297.01 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128858/350343 [00:56<01:36, 2289.80 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129090/350343 [00:56<01:36, 2298.18 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129325/350343 [00:56<01:35, 2311.96 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129558/350343 [00:56<01:35, 2315.43 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 129790/350343 [00:56<01:35, 2314.14 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130022/350343 [00:57<01:35, 2310.04 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130254/350343 [00:57<01:35, 2308.38 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130485/350343 [00:57<01:35, 2298.16 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130716/350343 [00:57<01:35, 2300.62 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 130947/350343 [00:57<01:35, 2296.49 examples/s][A
Generating train examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 131177/350343 [00:57<01:35, 2283.40 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131410/350343 [00:57<01:35, 2295.18 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131643/350343 [00:57<01:34, 2305.01 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 131877/350343 [00:57<01:34, 2314.87 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132109/350343 [00:57<01:35, 2276.78 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132337/350343 [00:58<01:37, 2245.85 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132567/350343 [00:58<01:36, 2260.18 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 132794/350343 [00:58<01:36, 2262.47 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133021/350343 [00:58<01:36, 2260.63 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133251/350343 [00:58<01:35, 2270.69 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133479/350343 [00:58<01:35, 2270.06 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133707/350343 [00:58<01:35, 2262.70 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 133940/350343 [00:58<01:34, 2280.24 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134169/350343 [00:58<01:35, 2271.34 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134398/350343 [00:58<01:34, 2274.81 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134626/350343 [00:59<01:34, 2272.35 examples/s][A
Generating train examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 134854/350343 [00:59<01:34, 2272.90 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135082/350343 [00:59<01:38, 2184.71 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135313/350343 [00:59<01:36, 2219.35 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 135538/350343 [00:59<01:36, 2226.04 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 135762/350343 [00:59<01:36, 2220.97 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 135985/350343 [00:59<01:39, 2157.77 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136202/350343 [00:59<01:42, 2092.51 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136418/350343 [00:59<01:41, 2106.31 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136649/350343 [01:00<01:38, 2164.19 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 136881/350343 [01:00<01:36, 2208.03 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137105/350343 [01:00<01:36, 2217.13 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137331/350343 [01:00<01:35, 2227.87 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137560/350343 [01:00<01:34, 2245.29 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 137793/350343 [01:00<01:33, 2269.36 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138023/350343 [01:00<01:33, 2278.10 examples/s][A
Generating train examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 138256/350343 [01:00<01:32, 2291.19 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138490/350343 [01:00<01:31, 2304.06 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138730/350343 [01:00<01:30, 2331.39 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 138964/350343 [01:01<01:31, 2319.16 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139196/350343 [01:01<01:31, 2308.64 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139427/350343 [01:01<01:31, 2305.49 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139662/350343 [01:01<01:30, 2316.66 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 139894/350343 [01:01<01:31, 2303.61 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 140129/350343 [01:01<01:30, 2317.09 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140361/350343 [01:01<01:31, 2302.62 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140593/350343 [01:01<01:30, 2305.23 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 140828/350343 [01:01<01:30, 2317.76 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141063/350343 [01:01<01:30, 2324.75 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141298/350343 [01:02<01:29, 2331.01 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141532/350343 [01:02<01:30, 2311.46 examples/s][A
Generating train examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141764/350343 [01:02<01:30, 2304.55 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 141995/350343 [01:02<01:30, 2302.56 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142226/350343 [01:02<01:30, 2299.18 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142456/350343 [01:02<01:30, 2288.97 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142685/350343 [01:02<01:34, 2193.85 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 142921/350343 [01:02<01:32, 2240.30 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143153/350343 [01:02<01:31, 2262.23 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143382/350343 [01:02<01:31, 2269.58 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143614/350343 [01:03<01:30, 2283.36 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143843/350343 [01:03<01:30, 2270.25 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144077/350343 [01:03<01:30, 2288.93 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 144310/350343 [01:03<01:29, 2300.11 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144541/350343 [01:03<01:29, 2291.30 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 144771/350343 [01:03<01:29, 2287.95 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145000/350343 [01:03<01:30, 2275.60 examples/s][A
Generating train examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145234/350343 [01:03<01:29, 2294.44 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145475/350343 [01:03<01:28, 2326.73 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145708/350343 [01:03<01:28, 2321.08 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 145942/350343 [01:04<01:27, 2324.98 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146175/350343 [01:04<01:28, 2317.51 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146409/350343 [01:04<01:27, 2322.38 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146643/350343 [01:04<01:27, 2324.73 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 146876/350343 [01:04<01:28, 2311.09 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147108/350343 [01:04<01:28, 2305.73 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147339/350343 [01:04<01:28, 2299.90 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147572/350343 [01:04<01:27, 2308.29 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147809/350343 [01:04<01:27, 2324.45 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148042/350343 [01:04<01:27, 2318.85 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148274/350343 [01:05<01:27, 2317.09 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148506/350343 [01:05<01:30, 2231.64 examples/s][A
Generating train examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148735/350343 [01:05<01:29, 2247.74 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 148971/350343 [01:05<01:28, 2279.06 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149202/350343 [01:05<01:27, 2285.98 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149432/350343 [01:05<01:27, 2289.66 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149663/350343 [01:05<01:27, 2294.42 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 149893/350343 [01:05<01:27, 2289.85 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150125/350343 [01:05<01:27, 2296.51 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150357/350343 [01:06<01:26, 2302.23 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150588/350343 [01:06<01:26, 2297.01 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 150819/350343 [01:06<01:26, 2299.86 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151054/350343 [01:06<01:26, 2313.90 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151287/350343 [01:06<01:25, 2318.31 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151519/350343 [01:06<01:25, 2317.47 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151751/350343 [01:06<01:26, 2308.94 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 151982/350343 [01:06<01:26, 2285.87 examples/s][A
Generating train examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152213/350343 [01:06<01:26, 2291.22 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152446/350343 [01:06<01:26, 2300.82 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152677/350343 [01:07<01:25, 2299.27 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 152907/350343 [01:07<01:25, 2297.54 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 153139/350343 [01:07<01:25, 2302.23 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153370/350343 [01:07<01:25, 2298.91 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153604/350343 [01:07<01:25, 2310.08 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 153836/350343 [01:07<01:25, 2311.25 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154068/350343 [01:07<01:25, 2298.07 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154300/350343 [01:07<01:25, 2304.43 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154531/350343 [01:07<01:25, 2301.68 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154763/350343 [01:07<01:24, 2304.88 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 154994/350343 [01:08<01:28, 2196.50 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155226/350343 [01:08<01:27, 2229.52 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155452/350343 [01:08<01:27, 2237.37 examples/s][A
Generating train examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155682/350343 [01:08<01:26, 2253.97 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 155909/350343 [01:08<01:26, 2256.44 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156137/350343 [01:08<01:25, 2262.83 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156364/350343 [01:08<01:25, 2260.67 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156594/350343 [01:08<01:25, 2270.04 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 156827/350343 [01:08<01:24, 2285.47 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157059/350343 [01:08<01:24, 2293.66 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157289/350343 [01:09<01:24, 2283.49 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 157518/350343 [01:09<01:24, 2271.13 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157746/350343 [01:09<01:24, 2270.64 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 157974/350343 [01:09<01:24, 2267.26 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158207/350343 [01:09<01:24, 2284.64 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158438/350343 [01:09<01:23, 2291.95 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158668/350343 [01:09<01:23, 2293.90 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158898/350343 [01:09<01:23, 2295.39 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159128/350343 [01:09<01:23, 2294.93 examples/s][A
Generating train examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159358/350343 [01:09<01:23, 2290.25 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159592/350343 [01:10<01:22, 2302.66 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 159823/350343 [01:10<01:22, 2302.97 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160054/350343 [01:10<01:22, 2304.61 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160288/350343 [01:10<01:22, 2312.74 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160520/350343 [01:10<01:22, 2304.82 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160751/350343 [01:10<01:22, 2293.16 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 160981/350343 [01:10<01:23, 2274.70 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161213/350343 [01:10<01:22, 2287.22 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161445/350343 [01:10<01:22, 2296.54 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161676/350343 [01:10<01:22, 2298.74 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 161906/350343 [01:11<01:22, 2291.65 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162136/350343 [01:11<01:22, 2285.72 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162368/350343 [01:11<01:21, 2293.21 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162598/350343 [01:11<01:25, 2197.84 examples/s][A
Generating train examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 162831/350343 [01:11<01:23, 2234.67 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163064/350343 [01:11<01:22, 2260.94 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163291/350343 [01:11<01:22, 2259.48 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163518/350343 [01:11<01:22, 2259.80 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163749/350343 [01:11<01:22, 2273.01 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 163977/350343 [01:11<01:22, 2270.87 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164205/350343 [01:12<01:22, 2269.08 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164433/350343 [01:12<01:21, 2272.02 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164666/350343 [01:12<01:21, 2287.97 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 164899/350343 [01:12<01:20, 2299.74 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165130/350343 [01:12<01:20, 2289.93 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165360/350343 [01:12<01:20, 2286.02 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165589/350343 [01:12<01:21, 2277.33 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 165818/350343 [01:12<01:20, 2279.96 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166052/350343 [01:12<01:20, 2296.64 examples/s][A
Generating train examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 166282/350343 [01:12<01:20, 2293.97 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166514/350343 [01:13<01:19, 2301.40 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166751/350343 [01:13<01:19, 2320.71 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 166985/350343 [01:13<01:18, 2324.87 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167218/350343 [01:13<01:19, 2310.41 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167450/350343 [01:13<01:19, 2311.25 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167686/350343 [01:13<01:18, 2324.55 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 167919/350343 [01:13<01:19, 2299.78 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168150/350343 [01:13<01:19, 2279.41 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168384/350343 [01:13<01:19, 2295.05 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168614/350343 [01:13<01:19, 2295.67 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 168852/350343 [01:14<01:18, 2318.27 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169084/350343 [01:14<01:21, 2217.52 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169315/350343 [01:14<01:20, 2243.56 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169548/350343 [01:14<01:19, 2268.11 examples/s][A
Generating train examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 169780/350343 [01:14<01:19, 2283.15 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170018/350343 [01:14<01:18, 2310.74 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170250/350343 [01:14<01:18, 2296.56 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170480/350343 [01:14<01:18, 2294.86 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 170716/350343 [01:14<01:17, 2312.31 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 170948/350343 [01:15<01:17, 2313.27 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171185/350343 [01:15<01:16, 2329.22 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171419/350343 [01:15<01:16, 2330.25 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171653/350343 [01:15<01:16, 2322.67 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 171886/350343 [01:15<01:17, 2317.41 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172118/350343 [01:15<01:17, 2303.07 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172349/350343 [01:15<01:17, 2293.74 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172579/350343 [01:15<01:18, 2259.85 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 172806/350343 [01:15<01:18, 2259.49 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173039/350343 [01:15<01:17, 2278.68 examples/s][A
Generating train examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173269/350343 [01:16<01:17, 2284.28 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173499/350343 [01:16<01:17, 2288.30 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173728/350343 [01:16<01:17, 2282.54 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173960/350343 [01:16<01:16, 2291.67 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174190/350343 [01:16<01:17, 2283.26 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174419/350343 [01:16<01:17, 2280.44 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174656/350343 [01:16<01:16, 2304.84 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 174887/350343 [01:16<01:17, 2268.45 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 175115/350343 [01:16<01:17, 2269.55 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175344/350343 [01:16<01:19, 2195.37 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175573/350343 [01:17<01:18, 2221.20 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 175806/350343 [01:17<01:17, 2250.95 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176032/350343 [01:17<01:17, 2245.03 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176264/350343 [01:17<01:16, 2266.33 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176491/350343 [01:17<01:16, 2258.85 examples/s][A
Generating train examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176724/350343 [01:17<01:16, 2277.88 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 176958/350343 [01:17<01:15, 2295.39 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177188/350343 [01:17<01:15, 2293.60 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177421/350343 [01:17<01:15, 2302.27 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177652/350343 [01:17<01:15, 2295.74 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 177884/350343 [01:18<01:14, 2302.16 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178115/350343 [01:18<01:14, 2300.29 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178348/350343 [01:18<01:14, 2309.03 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178582/350343 [01:18<01:14, 2317.03 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 178814/350343 [01:18<01:14, 2303.15 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179050/350343 [01:18<01:13, 2318.18 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179284/350343 [01:18<01:13, 2323.18 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 179517/350343 [01:18<01:14, 2289.58 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179751/350343 [01:18<01:14, 2304.26 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 179982/350343 [01:18<01:14, 2274.47 examples/s][A
Generating train examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180210/350343 [01:19<01:14, 2269.04 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180437/350343 [01:19<01:14, 2268.53 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180672/350343 [01:19<01:14, 2291.21 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 180904/350343 [01:19<01:13, 2299.16 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181135/350343 [01:19<01:13, 2301.19 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181368/350343 [01:19<01:13, 2309.31 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181603/350343 [01:19<01:12, 2320.28 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181836/350343 [01:19<01:15, 2217.63 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182071/350343 [01:19<01:14, 2255.66 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182303/350343 [01:19<01:13, 2273.94 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182532/350343 [01:20<01:13, 2276.47 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182761/350343 [01:20<01:13, 2272.08 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182991/350343 [01:20<01:13, 2279.51 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183223/350343 [01:20<01:12, 2291.17 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183454/350343 [01:20<01:12, 2295.08 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183686/350343 [01:20<01:12, 2302.44 examples/s][A
Generating train examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183917/350343 [01:20<01:12, 2303.94 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184148/350343 [01:20<01:12, 2291.93 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184382/350343 [01:20<01:11, 2306.09 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184613/350343 [01:20<01:12, 2288.89 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 184842/350343 [01:21<01:12, 2285.22 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185073/350343 [01:21<01:12, 2292.05 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185303/350343 [01:21<01:12, 2284.18 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185536/350343 [01:21<01:11, 2295.95 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185766/350343 [01:21<01:12, 2277.45 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 185995/350343 [01:21<01:12, 2280.76 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186233/350343 [01:21<01:11, 2308.76 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186464/350343 [01:21<01:11, 2293.33 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186700/350343 [01:21<01:10, 2312.62 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 186932/350343 [01:21<01:10, 2307.56 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187167/350343 [01:22<01:10, 2318.47 examples/s][A
Generating train examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187400/350343 [01:22<01:10, 2320.00 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187633/350343 [01:22<01:10, 2312.16 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 187865/350343 [01:22<01:10, 2308.63 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 188096/350343 [01:22<01:13, 2203.63 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188329/350343 [01:22<01:12, 2239.62 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188557/350343 [01:22<01:11, 2250.01 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188793/350343 [01:22<01:10, 2280.39 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189026/350343 [01:22<01:10, 2293.79 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189256/350343 [01:23<01:10, 2288.35 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189489/350343 [01:23<01:09, 2299.51 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189723/350343 [01:23<01:09, 2310.87 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 189957/350343 [01:23<01:09, 2318.72 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190192/350343 [01:23<01:08, 2325.43 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190425/350343 [01:23<01:08, 2319.14 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190665/350343 [01:23<01:08, 2341.41 examples/s][A
Generating train examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 190900/350343 [01:23<01:08, 2325.25 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191133/350343 [01:23<01:08, 2309.13 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191364/350343 [01:23<01:08, 2306.15 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191595/350343 [01:24<01:09, 2294.74 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 191825/350343 [01:24<01:09, 2289.79 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192054/350343 [01:24<01:09, 2279.08 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192284/350343 [01:24<01:09, 2283.40 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 192513/350343 [01:24<01:09, 2277.69 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192741/350343 [01:24<01:09, 2275.43 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 192969/350343 [01:24<01:09, 2276.52 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193197/350343 [01:24<01:09, 2268.90 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193432/350343 [01:24<01:08, 2291.63 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193662/350343 [01:24<01:08, 2292.60 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 193892/350343 [01:25<01:08, 2279.31 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194120/350343 [01:25<01:08, 2277.37 examples/s][A
Generating train examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194351/350343 [01:25<01:08, 2286.23 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194580/350343 [01:25<01:11, 2189.99 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 194810/350343 [01:25<01:10, 2221.11 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195043/350343 [01:25<01:08, 2251.93 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195279/350343 [01:25<01:07, 2282.36 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195508/350343 [01:25<01:07, 2277.12 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195742/350343 [01:25<01:07, 2293.98 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 195975/350343 [01:25<01:07, 2303.53 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196206/350343 [01:26<01:07, 2291.58 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196436/350343 [01:26<01:07, 2284.74 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196665/350343 [01:26<01:07, 2282.29 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 196895/350343 [01:26<01:07, 2284.98 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197124/350343 [01:26<01:07, 2285.45 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197358/350343 [01:26<01:06, 2300.94 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197591/350343 [01:26<01:06, 2307.65 examples/s][A
Generating train examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 197822/350343 [01:26<01:06, 2288.11 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198060/350343 [01:26<01:05, 2314.15 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198294/350343 [01:26<01:05, 2320.01 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198527/350343 [01:27<01:06, 2287.48 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198759/350343 [01:27<01:06, 2295.92 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 198993/350343 [01:27<01:05, 2307.51 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199225/350343 [01:27<01:05, 2308.78 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199456/350343 [01:27<01:05, 2298.46 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199686/350343 [01:27<01:05, 2284.78 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 199917/350343 [01:27<01:05, 2292.11 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200147/350343 [01:27<01:05, 2278.77 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200380/350343 [01:27<01:05, 2293.54 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200617/350343 [01:27<01:04, 2315.81 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 200849/350343 [01:28<01:07, 2227.00 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201073/350343 [01:28<01:07, 2226.23 examples/s][A
Generating train examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 201302/350343 [01:28<01:06, 2243.16 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201527/350343 [01:28<01:06, 2245.11 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201752/350343 [01:28<01:06, 2241.28 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 201977/350343 [01:28<01:07, 2199.95 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202198/350343 [01:28<01:08, 2172.61 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202416/350343 [01:28<01:08, 2172.81 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202644/350343 [01:28<01:07, 2202.86 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 202865/350343 [01:29<01:08, 2150.40 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203083/350343 [01:29<01:08, 2156.12 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203316/350343 [01:29<01:06, 2206.88 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203539/350343 [01:29<01:06, 2212.80 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203771/350343 [01:29<01:05, 2242.66 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204003/350343 [01:29<01:04, 2264.65 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204236/350343 [01:29<01:03, 2284.01 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204472/350343 [01:29<01:03, 2306.53 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204703/350343 [01:29<01:03, 2296.71 examples/s][A
Generating train examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 204933/350343 [01:29<01:03, 2295.86 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205166/350343 [01:30<01:02, 2304.71 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205398/350343 [01:30<01:02, 2309.00 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 205633/350343 [01:30<01:02, 2319.39 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 205865/350343 [01:30<01:02, 2307.91 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206096/350343 [01:30<01:02, 2295.62 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206326/350343 [01:30<01:02, 2288.84 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206560/350343 [01:30<01:02, 2303.40 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 206791/350343 [01:30<01:02, 2296.57 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207021/350343 [01:30<01:02, 2284.00 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207258/350343 [01:30<01:01, 2308.23 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207489/350343 [01:31<01:02, 2302.15 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207720/350343 [01:31<01:01, 2300.49 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 207952/350343 [01:31<01:01, 2304.18 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208188/350343 [01:31<01:01, 2318.62 examples/s][A
Generating train examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208423/350343 [01:31<01:01, 2325.48 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208656/350343 [01:31<01:01, 2320.54 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 208889/350343 [01:31<01:13, 1918.05 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209093/350343 [01:31<01:16, 1837.47 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209285/350343 [01:31<01:22, 1705.95 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209463/350343 [01:32<01:22, 1705.56 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209641/350343 [01:32<01:21, 1723.85 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 209839/350343 [01:32<01:18, 1793.09 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 210060/350343 [01:32<01:13, 1908.23 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210290/350343 [01:32<01:09, 2019.37 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210521/350343 [01:32<01:06, 2102.61 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210750/350343 [01:32<01:04, 2155.79 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 210981/350343 [01:32<01:03, 2199.40 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211214/350343 [01:32<01:02, 2237.68 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211446/350343 [01:32<01:01, 2261.91 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211676/350343 [01:33<01:01, 2271.42 examples/s][A
Generating train examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 211909/350343 [01:33<01:00, 2287.50 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212144/350343 [01:33<00:59, 2303.60 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212379/350343 [01:33<00:59, 2317.39 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212611/350343 [01:33<00:59, 2317.82 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 212843/350343 [01:33<00:59, 2317.80 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213075/350343 [01:33<00:59, 2315.58 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213307/350343 [01:33<00:59, 2316.14 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213539/350343 [01:33<00:59, 2314.24 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 213771/350343 [01:33<00:59, 2310.11 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214003/350343 [01:34<00:59, 2277.34 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214232/350343 [01:34<00:59, 2280.17 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 214461/350343 [01:34<00:59, 2276.86 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214689/350343 [01:34<00:59, 2277.02 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 214918/350343 [01:34<00:59, 2278.58 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215147/350343 [01:34<00:59, 2279.42 examples/s][A
Generating train examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215375/350343 [01:34<00:59, 2278.00 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215603/350343 [01:34<00:59, 2276.16 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215831/350343 [01:34<01:00, 2206.14 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216061/350343 [01:34<01:00, 2233.43 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216285/350343 [01:35<01:00, 2214.22 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216515/350343 [01:35<00:59, 2237.92 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216747/350343 [01:35<00:59, 2258.53 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216975/350343 [01:35<00:58, 2262.95 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217202/350343 [01:35<00:58, 2257.88 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217439/350343 [01:35<00:58, 2290.87 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217674/350343 [01:35<00:57, 2307.83 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217908/350343 [01:35<00:57, 2316.81 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218141/350343 [01:35<00:57, 2318.38 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218373/350343 [01:35<00:57, 2307.30 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218604/350343 [01:36<00:58, 2268.99 examples/s][A
Generating train examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218832/350343 [01:36<00:58, 2265.02 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219060/350343 [01:36<00:57, 2269.18 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219291/350343 [01:36<00:57, 2280.19 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219524/350343 [01:36<00:57, 2293.30 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219761/350343 [01:36<00:56, 2315.80 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 219994/350343 [01:36<00:56, 2318.42 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220228/350343 [01:36<00:56, 2322.46 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220461/350343 [01:36<00:55, 2320.78 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220696/350343 [01:36<00:55, 2327.81 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 220929/350343 [01:37<00:56, 2302.69 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221162/350343 [01:37<00:55, 2310.12 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221398/350343 [01:37<00:55, 2323.57 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221631/350343 [01:37<00:55, 2317.68 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 221863/350343 [01:37<00:55, 2318.25 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222095/350343 [01:37<00:57, 2230.89 examples/s][A
Generating train examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222327/350343 [01:37<00:56, 2256.09 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222556/350343 [01:37<00:56, 2263.30 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 222784/350343 [01:37<00:56, 2268.17 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223017/350343 [01:38<00:55, 2285.74 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 223246/350343 [01:38<00:55, 2278.80 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223475/350343 [01:38<00:55, 2273.34 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223708/350343 [01:38<00:55, 2289.20 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 223940/350343 [01:38<00:55, 2297.19 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224170/350343 [01:38<00:55, 2288.56 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224400/350343 [01:38<00:54, 2290.22 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224631/350343 [01:38<00:54, 2294.33 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 224861/350343 [01:38<00:54, 2293.23 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225096/350343 [01:38<00:54, 2309.37 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225327/350343 [01:39<00:54, 2303.64 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225558/350343 [01:39<00:54, 2278.59 examples/s][A
Generating train examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 225786/350343 [01:39<00:54, 2276.74 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226014/350343 [01:39<00:54, 2269.41 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226242/350343 [01:39<00:54, 2271.88 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226470/350343 [01:39<00:54, 2266.68 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226700/350343 [01:39<00:54, 2274.21 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 226929/350343 [01:39<00:54, 2277.05 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227158/350343 [01:39<00:54, 2279.98 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227387/350343 [01:39<00:53, 2282.79 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 227619/350343 [01:40<00:53, 2291.19 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 227849/350343 [01:40<00:53, 2287.66 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228080/350343 [01:40<00:53, 2291.92 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228313/350343 [01:40<00:53, 2301.72 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228547/350343 [01:40<00:52, 2310.80 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 228780/350343 [01:40<00:52, 2316.00 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229013/350343 [01:40<00:52, 2318.92 examples/s][A
Generating train examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229245/350343 [01:40<00:52, 2303.31 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229476/350343 [01:40<00:54, 2212.60 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229710/350343 [01:40<00:53, 2247.76 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 229945/350343 [01:41<00:52, 2277.36 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230174/350343 [01:41<00:53, 2266.36 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230401/350343 [01:41<00:53, 2262.64 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230633/350343 [01:41<00:52, 2278.60 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 230862/350343 [01:41<00:52, 2280.09 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231095/350343 [01:41<00:52, 2292.79 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231327/350343 [01:41<00:51, 2299.41 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231558/350343 [01:41<00:51, 2293.02 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 231790/350343 [01:41<00:51, 2299.13 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 232025/350343 [01:41<00:51, 2313.26 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232257/350343 [01:42<00:51, 2314.33 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232489/350343 [01:42<00:50, 2313.48 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232721/350343 [01:42<00:51, 2290.32 examples/s][A
Generating train examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 232956/350343 [01:42<00:50, 2307.34 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233187/350343 [01:42<00:50, 2300.94 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233418/350343 [01:42<00:50, 2302.13 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233649/350343 [01:42<00:50, 2300.44 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 233885/350343 [01:42<00:50, 2315.20 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234119/350343 [01:42<00:50, 2320.01 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234352/350343 [01:42<00:50, 2318.59 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234585/350343 [01:43<00:49, 2319.17 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234817/350343 [01:43<00:50, 2310.43 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235049/350343 [01:43<00:49, 2312.13 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235281/350343 [01:43<00:49, 2301.79 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235512/350343 [01:43<00:49, 2299.06 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235742/350343 [01:43<00:51, 2226.03 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 235979/350343 [01:43<00:50, 2266.85 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236209/350343 [01:43<00:50, 2274.24 examples/s][A
Generating train examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 236438/350343 [01:43<00:49, 2278.21 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236668/350343 [01:43<00:49, 2282.62 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 236897/350343 [01:44<00:49, 2279.49 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237126/350343 [01:44<00:50, 2258.10 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237361/350343 [01:44<00:49, 2283.77 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237595/350343 [01:44<00:49, 2299.88 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 237828/350343 [01:44<00:48, 2308.68 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238060/350343 [01:44<00:48, 2311.63 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238292/350343 [01:44<00:48, 2310.46 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238524/350343 [01:44<00:48, 2306.88 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238755/350343 [01:44<00:48, 2306.67 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 238988/350343 [01:44<00:48, 2312.59 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239220/350343 [01:45<00:48, 2283.77 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239452/350343 [01:45<00:48, 2293.64 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239685/350343 [01:45<00:48, 2304.16 examples/s][A
Generating train examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 239916/350343 [01:45<00:48, 2294.32 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240147/350343 [01:45<00:47, 2298.04 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240381/350343 [01:45<00:47, 2309.82 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240613/350343 [01:45<00:47, 2300.91 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 240844/350343 [01:45<00:47, 2298.17 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241074/350343 [01:45<00:47, 2296.09 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241308/350343 [01:45<00:47, 2307.10 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241539/350343 [01:46<00:47, 2286.25 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 241773/350343 [01:46<00:47, 2300.99 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242004/350343 [01:46<00:48, 2219.56 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242234/350343 [01:46<00:48, 2242.12 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242469/350343 [01:46<00:47, 2271.88 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242698/350343 [01:46<00:47, 2274.72 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 242926/350343 [01:46<00:47, 2275.42 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243157/350343 [01:46<00:46, 2283.37 examples/s][A
Generating train examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243390/350343 [01:46<00:46, 2295.47 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243625/350343 [01:47<00:46, 2310.89 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 243857/350343 [01:47<00:46, 2274.94 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244085/350343 [01:47<00:46, 2274.28 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244315/350343 [01:47<00:46, 2279.77 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244546/350343 [01:47<00:46, 2288.20 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 244776/350343 [01:47<00:46, 2290.93 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 245006/350343 [01:47<00:46, 2289.17 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 245239/350343 [01:47<00:45, 2300.88 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245470/350343 [01:47<00:45, 2284.86 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245703/350343 [01:47<00:45, 2298.15 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 245933/350343 [01:48<00:45, 2296.67 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246163/350343 [01:48<00:45, 2282.62 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246400/350343 [01:48<00:45, 2306.55 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246633/350343 [01:48<00:44, 2312.33 examples/s][A
Generating train examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 246865/350343 [01:48<00:44, 2311.92 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247097/350343 [01:48<00:44, 2312.14 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247329/350343 [01:48<00:44, 2313.04 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247564/350343 [01:48<00:44, 2323.50 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 247797/350343 [01:48<00:44, 2320.10 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248030/350343 [01:48<00:44, 2316.01 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248264/350343 [01:49<00:45, 2233.12 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248488/350343 [01:49<00:45, 2222.38 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248717/350343 [01:49<00:45, 2241.28 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 248944/350343 [01:49<00:45, 2249.23 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249176/350343 [01:49<00:44, 2270.01 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249410/350343 [01:49<00:44, 2289.45 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249640/350343 [01:49<00:43, 2290.72 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249871/350343 [01:49<00:43, 2293.90 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250102/350343 [01:49<00:43, 2296.71 examples/s][A
Generating train examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250335/350343 [01:49<00:43, 2305.81 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250566/350343 [01:50<00:43, 2298.10 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250796/350343 [01:50<00:43, 2270.61 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251028/350343 [01:50<00:43, 2283.38 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251258/350343 [01:50<00:43, 2287.90 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251487/350343 [01:50<00:43, 2288.46 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251721/350343 [01:50<00:42, 2303.52 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251952/350343 [01:50<00:42, 2301.40 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252186/350343 [01:50<00:42, 2311.76 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252419/350343 [01:50<00:42, 2316.78 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252651/350343 [01:50<00:42, 2305.43 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 252882/350343 [01:51<00:42, 2293.46 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253112/350343 [01:51<00:42, 2271.16 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253340/350343 [01:51<00:42, 2273.56 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253568/350343 [01:51<00:42, 2274.12 examples/s][A
Generating train examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 253803/350343 [01:51<00:42, 2296.17 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254035/350343 [01:51<00:41, 2302.17 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254266/350343 [01:51<00:41, 2294.69 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254496/350343 [01:51<00:41, 2293.97 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254730/350343 [01:51<00:41, 2305.60 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 254965/350343 [01:51<00:41, 2316.27 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255197/350343 [01:52<00:42, 2229.64 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255426/350343 [01:52<00:42, 2245.09 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255658/350343 [01:52<00:41, 2265.69 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 255887/350343 [01:52<00:41, 2272.46 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256122/350343 [01:52<00:41, 2294.80 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256352/350343 [01:52<00:41, 2287.19 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256586/350343 [01:52<00:40, 2302.21 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 256817/350343 [01:52<00:40, 2304.42 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257052/350343 [01:52<00:40, 2316.46 examples/s][A
Generating train examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257284/350343 [01:52<00:40, 2308.51 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257515/350343 [01:53<00:40, 2306.44 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257746/350343 [01:53<00:40, 2287.97 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 257977/350343 [01:53<00:40, 2291.94 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 258207/350343 [01:53<00:40, 2287.64 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258445/350343 [01:53<00:39, 2313.46 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258677/350343 [01:53<00:39, 2314.71 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 258910/350343 [01:53<00:39, 2317.03 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259142/350343 [01:53<00:39, 2313.58 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259375/350343 [01:53<00:39, 2317.11 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259607/350343 [01:53<00:39, 2316.88 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 259839/350343 [01:54<00:39, 2311.30 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260071/350343 [01:54<00:39, 2295.47 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260301/350343 [01:54<00:39, 2279.02 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260530/350343 [01:54<00:39, 2280.26 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260762/350343 [01:54<00:39, 2290.79 examples/s][A
Generating train examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 260992/350343 [01:54<00:39, 2283.31 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261227/350343 [01:54<00:38, 2302.80 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261458/350343 [01:54<00:40, 2211.25 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261690/350343 [01:54<00:39, 2242.15 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 261920/350343 [01:55<00:39, 2258.62 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262153/350343 [01:55<00:38, 2278.69 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262382/350343 [01:55<00:39, 2255.08 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 262608/350343 [01:55<00:39, 2247.54 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 262834/350343 [01:55<00:38, 2248.98 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263060/350343 [01:55<00:38, 2251.36 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263289/350343 [01:55<00:38, 2261.98 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263523/350343 [01:55<00:38, 2283.67 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263752/350343 [01:55<00:37, 2285.31 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 263985/350343 [01:55<00:37, 2298.24 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264215/350343 [01:56<00:37, 2295.36 examples/s][A
Generating train examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264445/350343 [01:56<00:37, 2277.08 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264673/350343 [01:56<00:38, 2251.92 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264904/350343 [01:56<00:37, 2268.24 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265134/350343 [01:56<00:37, 2275.52 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265365/350343 [01:56<00:37, 2285.20 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265600/350343 [01:56<00:36, 2304.06 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 265834/350343 [01:56<00:36, 2314.64 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266066/350343 [01:56<00:36, 2305.03 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266297/350343 [01:56<00:36, 2301.66 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266534/350343 [01:57<00:36, 2320.97 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266767/350343 [01:57<00:36, 2315.57 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 266999/350343 [01:57<00:36, 2304.60 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267230/350343 [01:57<00:36, 2303.38 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267461/350343 [01:57<00:36, 2294.43 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267691/350343 [01:57<00:36, 2293.24 examples/s][A
Generating train examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 267922/350343 [01:57<00:35, 2297.72 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268156/350343 [01:57<00:37, 2220.28 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268383/350343 [01:57<00:36, 2234.23 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268615/350343 [01:57<00:36, 2257.35 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 268850/350343 [01:58<00:35, 2284.29 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269084/350343 [01:58<00:35, 2298.64 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269315/350343 [01:58<00:35, 2292.04 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269552/350343 [01:58<00:34, 2314.03 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 269785/350343 [01:58<00:34, 2318.32 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270018/350343 [01:58<00:34, 2321.38 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270251/350343 [01:58<00:34, 2307.97 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270482/350343 [01:58<00:34, 2298.82 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270712/350343 [01:58<00:34, 2285.87 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 270941/350343 [01:58<00:35, 2220.27 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271164/350343 [01:59<00:35, 2207.61 examples/s][A
Generating train examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 271394/350343 [01:59<00:35, 2232.77 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271621/350343 [01:59<00:35, 2242.28 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 271851/350343 [01:59<00:34, 2258.92 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272079/350343 [01:59<00:34, 2265.00 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272313/350343 [01:59<00:34, 2285.66 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272546/350343 [01:59<00:33, 2298.13 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 272776/350343 [01:59<00:34, 2256.00 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273002/350343 [01:59<00:35, 2158.51 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273219/350343 [01:59<00:35, 2159.09 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273440/350343 [02:00<00:35, 2172.66 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273670/350343 [02:00<00:34, 2209.88 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 273895/350343 [02:00<00:34, 2220.02 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274124/350343 [02:00<00:34, 2240.05 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274352/350343 [02:00<00:33, 2251.85 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274583/350343 [02:00<00:33, 2267.11 examples/s][A
Generating train examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 274813/350343 [02:00<00:33, 2275.88 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275049/350343 [02:00<00:32, 2300.69 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275280/350343 [02:00<00:32, 2302.99 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275511/350343 [02:00<00:32, 2301.11 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 275742/350343 [02:01<00:32, 2285.86 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 275974/350343 [02:01<00:32, 2294.56 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276204/350343 [02:01<00:32, 2283.67 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276435/350343 [02:01<00:33, 2203.39 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276667/350343 [02:01<00:32, 2235.94 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 276899/350343 [02:01<00:32, 2258.44 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277128/350343 [02:01<00:32, 2267.40 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277365/350343 [02:01<00:31, 2297.74 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277602/350343 [02:01<00:31, 2317.18 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 277834/350343 [02:02<00:31, 2294.13 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278069/350343 [02:02<00:31, 2308.96 examples/s][A
Generating train examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278301/350343 [02:02<00:31, 2307.06 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278534/350343 [02:02<00:31, 2313.13 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278766/350343 [02:02<00:31, 2303.19 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 278997/350343 [02:02<00:31, 2289.05 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279231/350343 [02:02<00:30, 2301.66 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279462/350343 [02:02<00:30, 2294.48 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279692/350343 [02:02<00:30, 2295.24 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279929/350343 [02:02<00:30, 2316.66 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 280162/350343 [02:03<00:30, 2319.63 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280397/350343 [02:03<00:30, 2328.48 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280630/350343 [02:03<00:30, 2307.89 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 280861/350343 [02:03<00:30, 2296.01 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281091/350343 [02:03<00:30, 2289.67 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281320/350343 [02:03<00:30, 2269.45 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281550/350343 [02:03<00:30, 2276.76 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 281778/350343 [02:03<00:30, 2275.70 examples/s][A
Generating train examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282010/350343 [02:03<00:29, 2288.43 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282246/350343 [02:03<00:29, 2309.55 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282477/350343 [02:04<00:30, 2216.41 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282708/350343 [02:04<00:30, 2241.61 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 282936/350343 [02:04<00:29, 2251.50 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283163/350343 [02:04<00:29, 2255.99 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283399/350343 [02:04<00:29, 2284.07 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283628/350343 [02:04<00:29, 2279.91 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 283857/350343 [02:04<00:29, 2281.75 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284089/350343 [02:04<00:28, 2292.45 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284323/350343 [02:04<00:28, 2303.84 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 284554/350343 [02:04<00:28, 2290.46 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284786/350343 [02:05<00:28, 2297.74 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285016/350343 [02:05<00:28, 2289.21 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285245/350343 [02:05<00:28, 2272.49 examples/s][A
Generating train examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285473/350343 [02:05<00:28, 2274.22 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285701/350343 [02:05<00:28, 2274.56 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285931/350343 [02:05<00:28, 2281.07 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286166/350343 [02:05<00:27, 2300.66 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286397/350343 [02:05<00:27, 2297.93 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286637/350343 [02:05<00:27, 2327.40 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 286870/350343 [02:05<00:27, 2322.49 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287103/350343 [02:06<00:27, 2309.88 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287335/350343 [02:06<00:27, 2297.53 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287565/350343 [02:06<00:27, 2291.02 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 287796/350343 [02:06<00:27, 2295.22 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288033/350343 [02:06<00:26, 2316.33 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288270/350343 [02:06<00:26, 2329.97 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288505/350343 [02:06<00:26, 2335.92 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288739/350343 [02:06<00:27, 2242.74 examples/s][A
Generating train examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 288974/350343 [02:06<00:27, 2272.70 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289207/350343 [02:06<00:26, 2289.02 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289440/350343 [02:07<00:26, 2298.60 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289671/350343 [02:07<00:26, 2293.18 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 289901/350343 [02:07<00:26, 2269.52 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290130/350343 [02:07<00:26, 2273.55 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290358/350343 [02:07<00:26, 2268.83 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290588/350343 [02:07<00:26, 2277.47 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 290816/350343 [02:07<00:26, 2277.47 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291046/350343 [02:07<00:25, 2282.29 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291284/350343 [02:07<00:25, 2309.63 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291521/350343 [02:07<00:25, 2326.40 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291754/350343 [02:08<00:25, 2318.66 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 291986/350343 [02:08<00:25, 2315.20 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292218/350343 [02:08<00:25, 2292.92 examples/s][A
Generating train examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292451/350343 [02:08<00:25, 2303.54 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292684/350343 [02:08<00:24, 2309.34 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 292917/350343 [02:08<00:24, 2315.17 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293152/350343 [02:08<00:24, 2323.85 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 293385/350343 [02:08<00:24, 2323.27 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293618/350343 [02:08<00:24, 2314.69 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 293857/350343 [02:08<00:24, 2334.32 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294091/350343 [02:09<00:24, 2324.05 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294324/350343 [02:09<00:25, 2237.80 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294552/350343 [02:09<00:24, 2248.44 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 294784/350343 [02:09<00:24, 2266.35 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295014/350343 [02:09<00:24, 2276.07 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295245/350343 [02:09<00:24, 2284.64 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295482/350343 [02:09<00:23, 2308.71 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295714/350343 [02:09<00:23, 2293.66 examples/s][A
Generating train examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295944/350343 [02:09<00:23, 2278.90 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296172/350343 [02:10<00:24, 2186.13 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296392/350343 [02:10<00:25, 2151.36 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296617/350343 [02:10<00:24, 2178.37 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 296836/350343 [02:10<00:24, 2171.33 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297068/350343 [02:10<00:24, 2213.82 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297294/350343 [02:10<00:23, 2225.82 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297528/350343 [02:10<00:23, 2257.04 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 297754/350343 [02:10<00:23, 2240.19 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 297979/350343 [02:10<00:23, 2239.66 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298211/350343 [02:10<00:23, 2261.57 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298446/350343 [02:11<00:22, 2287.71 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298676/350343 [02:11<00:22, 2290.69 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 298906/350343 [02:11<00:22, 2289.76 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299136/350343 [02:11<00:22, 2267.05 examples/s][A
Generating train examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299369/350343 [02:11<00:22, 2285.37 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299598/350343 [02:11<00:22, 2284.91 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 299829/350343 [02:11<00:22, 2292.12 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300061/350343 [02:11<00:21, 2298.66 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300292/350343 [02:11<00:21, 2300.86 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300528/350343 [02:11<00:21, 2317.98 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300763/350343 [02:12<00:21, 2325.71 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 300996/350343 [02:12<00:21, 2303.32 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301227/350343 [02:12<00:21, 2296.35 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301457/350343 [02:12<00:21, 2287.53 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301686/350343 [02:12<00:21, 2285.71 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 301916/350343 [02:12<00:21, 2288.63 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 302145/350343 [02:12<00:21, 2282.70 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302374/350343 [02:12<00:21, 2192.82 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302611/350343 [02:12<00:21, 2243.77 examples/s][A
Generating train examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 302846/350343 [02:12<00:20, 2274.33 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303078/350343 [02:13<00:20, 2286.10 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303313/350343 [02:13<00:20, 2303.96 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303546/350343 [02:13<00:20, 2310.75 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 303778/350343 [02:13<00:20, 2286.75 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304007/350343 [02:13<00:20, 2284.96 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304238/350343 [02:13<00:20, 2292.31 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304468/350343 [02:13<00:20, 2293.10 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304700/350343 [02:13<00:19, 2299.57 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 304936/350343 [02:13<00:19, 2317.53 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305168/350343 [02:13<00:19, 2315.35 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305400/350343 [02:14<00:19, 2307.04 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305631/350343 [02:14<00:19, 2307.35 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 305864/350343 [02:14<00:19, 2312.03 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306096/350343 [02:14<00:19, 2296.52 examples/s][A
Generating train examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 306333/350343 [02:14<00:19, 2315.14 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306565/350343 [02:14<00:18, 2308.36 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 306796/350343 [02:14<00:18, 2306.76 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307027/350343 [02:14<00:18, 2295.86 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307257/350343 [02:14<00:18, 2291.36 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307487/350343 [02:14<00:18, 2287.48 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307721/350343 [02:15<00:18, 2302.29 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 307956/350343 [02:15<00:18, 2314.92 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308188/350343 [02:15<00:18, 2220.23 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308411/350343 [02:15<00:18, 2219.96 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308640/350343 [02:15<00:18, 2240.36 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 308870/350343 [02:15<00:18, 2257.51 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309097/350343 [02:15<00:18, 2252.97 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309332/350343 [02:15<00:17, 2280.25 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309565/350343 [02:15<00:17, 2292.83 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 309797/350343 [02:15<00:17, 2300.33 examples/s][A
Generating train examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310028/350343 [02:16<00:17, 2294.05 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310258/350343 [02:16<00:17, 2287.77 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310487/350343 [02:16<00:17, 2273.68 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310715/350343 [02:16<00:17, 2271.83 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 310945/350343 [02:16<00:17, 2278.51 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311180/350343 [02:16<00:17, 2298.26 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311413/350343 [02:16<00:16, 2305.24 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311647/350343 [02:16<00:16, 2315.47 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 311880/350343 [02:16<00:16, 2319.01 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312112/350343 [02:17<00:16, 2318.05 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312347/350343 [02:17<00:16, 2325.44 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312580/350343 [02:17<00:16, 2325.94 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 312813/350343 [02:17<00:16, 2308.18 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313044/350343 [02:17<00:16, 2287.25 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313273/350343 [02:17<00:16, 2282.73 examples/s][A
Generating train examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313502/350343 [02:17<00:16, 2283.10 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313736/350343 [02:17<00:15, 2298.71 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 313969/350343 [02:17<00:15, 2306.96 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314204/350343 [02:17<00:15, 2319.70 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314439/350343 [02:18<00:15, 2327.65 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314672/350343 [02:18<00:15, 2231.55 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 314899/350343 [02:18<00:15, 2240.43 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 315127/350343 [02:18<00:15, 2250.50 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315353/350343 [02:18<00:15, 2250.92 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315580/350343 [02:18<00:15, 2254.61 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 315812/350343 [02:18<00:15, 2272.26 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316044/350343 [02:18<00:15, 2283.72 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316273/350343 [02:18<00:14, 2283.05 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316506/350343 [02:18<00:14, 2295.63 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316736/350343 [02:19<00:14, 2296.27 examples/s][A
Generating train examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 316969/350343 [02:19<00:14, 2304.00 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317200/350343 [02:19<00:14, 2303.55 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317434/350343 [02:19<00:14, 2312.60 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317666/350343 [02:19<00:14, 2300.53 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 317897/350343 [02:19<00:14, 2293.84 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318128/350343 [02:19<00:14, 2297.05 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318358/350343 [02:19<00:13, 2297.76 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318588/350343 [02:19<00:13, 2297.52 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 318823/350343 [02:19<00:13, 2312.04 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319055/350343 [02:20<00:13, 2311.66 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319287/350343 [02:20<00:13, 2297.15 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 319517/350343 [02:20<00:13, 2292.20 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319754/350343 [02:20<00:13, 2313.21 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319986/350343 [02:20<00:13, 2295.58 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320221/350343 [02:20<00:13, 2311.68 examples/s][A
Generating train examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320453/350343 [02:20<00:13, 2298.62 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320683/350343 [02:20<00:13, 2279.29 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 320913/350343 [02:20<00:12, 2284.90 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321142/350343 [02:20<00:12, 2283.69 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321371/350343 [02:21<00:13, 2203.07 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321604/350343 [02:21<00:12, 2238.23 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 321835/350343 [02:21<00:12, 2258.38 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322067/350343 [02:21<00:12, 2274.56 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322297/350343 [02:21<00:12, 2279.93 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322529/350343 [02:21<00:12, 2289.47 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322762/350343 [02:21<00:11, 2299.34 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 322993/350343 [02:21<00:11, 2283.44 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323224/350343 [02:21<00:11, 2289.63 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323458/350343 [02:21<00:11, 2303.31 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323689/350343 [02:22<00:11, 2302.04 examples/s][A
Generating train examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 323920/350343 [02:22<00:11, 2286.14 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324153/350343 [02:22<00:11, 2297.61 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324383/350343 [02:22<00:11, 2295.53 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324613/350343 [02:22<00:11, 2288.40 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 324846/350343 [02:22<00:11, 2298.49 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325076/350343 [02:22<00:11, 2292.50 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325310/350343 [02:22<00:10, 2304.61 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325544/350343 [02:22<00:10, 2314.22 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 325776/350343 [02:22<00:10, 2303.69 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326009/350343 [02:23<00:10, 2309.51 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326240/350343 [02:23<00:10, 2299.12 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326470/350343 [02:23<00:10, 2296.90 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326700/350343 [02:23<00:10, 2289.93 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326930/350343 [02:23<00:10, 2270.11 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327158/350343 [02:23<00:10, 2265.55 examples/s][A
Generating train examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327390/350343 [02:23<00:10, 2279.78 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327622/350343 [02:23<00:09, 2289.60 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 327860/350343 [02:23<00:10, 2228.68 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328093/350343 [02:23<00:09, 2257.69 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 328326/350343 [02:24<00:09, 2278.02 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328556/350343 [02:24<00:09, 2282.87 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 328792/350343 [02:24<00:09, 2303.71 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329023/350343 [02:24<00:09, 2288.76 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329254/350343 [02:24<00:09, 2294.41 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329485/350343 [02:24<00:09, 2297.77 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329719/350343 [02:24<00:08, 2310.00 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 329952/350343 [02:24<00:08, 2315.63 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330184/350343 [02:24<00:08, 2306.35 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330415/350343 [02:25<00:08, 2306.62 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330646/350343 [02:25<00:08, 2297.80 examples/s][A
Generating train examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 330880/350343 [02:25<00:08, 2309.10 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331111/350343 [02:25<00:08, 2306.99 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331342/350343 [02:25<00:08, 2292.68 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331574/350343 [02:25<00:08, 2300.15 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 331805/350343 [02:25<00:08, 2300.55 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332036/350343 [02:25<00:08, 2288.29 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332271/350343 [02:25<00:07, 2304.25 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332503/350343 [02:25<00:07, 2308.24 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 332739/350343 [02:26<00:07, 2321.78 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 332972/350343 [02:26<00:07, 2318.30 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333204/350343 [02:26<00:07, 2306.84 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333438/350343 [02:26<00:07, 2316.12 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333670/350343 [02:26<00:07, 2315.16 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 333902/350343 [02:26<00:07, 2229.68 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334131/350343 [02:26<00:07, 2246.20 examples/s][A
Generating train examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334362/350343 [02:26<00:07, 2264.53 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334592/350343 [02:26<00:06, 2274.57 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 334820/350343 [02:26<00:06, 2269.26 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335054/350343 [02:27<00:06, 2288.07 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335287/350343 [02:27<00:06, 2297.90 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335517/350343 [02:27<00:06, 2291.87 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335753/350343 [02:27<00:06, 2311.32 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 335985/350343 [02:27<00:06, 2289.69 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336218/350343 [02:27<00:06, 2300.28 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336449/350343 [02:27<00:06, 2292.68 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336680/350343 [02:27<00:05, 2297.73 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 336912/350343 [02:27<00:05, 2304.32 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 337143/350343 [02:27<00:05, 2302.41 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337379/350343 [02:28<00:05, 2318.23 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337611/350343 [02:28<00:05, 2312.13 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 337843/350343 [02:28<00:05, 2313.59 examples/s][A
Generating train examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338077/350343 [02:28<00:05, 2318.46 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338309/350343 [02:28<00:05, 2316.56 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338544/350343 [02:28<00:05, 2324.80 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 338777/350343 [02:28<00:04, 2323.53 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339010/350343 [02:28<00:04, 2322.33 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339243/350343 [02:28<00:04, 2323.35 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339476/350343 [02:28<00:04, 2324.57 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339709/350343 [02:29<00:04, 2313.83 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 339941/350343 [02:29<00:04, 2207.71 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340169/350343 [02:29<00:04, 2228.16 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340406/350343 [02:29<00:04, 2269.54 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340634/350343 [02:29<00:04, 2255.82 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 340861/350343 [02:29<00:04, 2210.54 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341083/350343 [02:29<00:04, 2168.09 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341307/350343 [02:29<00:04, 2187.65 examples/s][A
Generating train examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341535/350343 [02:29<00:03, 2212.36 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341762/350343 [02:29<00:03, 2229.32 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 341993/350343 [02:30<00:03, 2251.22 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342219/350343 [02:30<00:03, 2249.62 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342449/350343 [02:30<00:03, 2263.61 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342685/350343 [02:30<00:03, 2292.17 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 342915/350343 [02:30<00:03, 2280.64 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343152/350343 [02:30<00:03, 2305.53 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343383/350343 [02:30<00:03, 2297.20 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343619/350343 [02:30<00:02, 2314.03 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 343851/350343 [02:30<00:02, 2300.85 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344082/350343 [02:30<00:02, 2280.32 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344311/350343 [02:31<00:02, 2274.68 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344539/350343 [02:31<00:02, 2243.45 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344764/350343 [02:31<00:02, 1898.78 examples/s][A
Generating train examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 344963/350343 [02:31<00:02, 1821.51 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345152/350343 [02:31<00:03, 1690.27 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345327/350343 [02:31<00:03, 1664.14 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345528/350343 [02:31<00:02, 1705.46 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 345752/350343 [02:31<00:02, 1798.10 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 345978/350343 [02:32<00:02, 1923.19 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346213/350343 [02:32<00:02, 2041.62 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346446/350343 [02:32<00:01, 2121.20 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346679/350343 [02:32<00:01, 2180.23 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 346911/350343 [02:32<00:01, 2219.78 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347146/350343 [02:32<00:01, 2256.46 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347379/350343 [02:32<00:01, 2276.82 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347608/350343 [02:32<00:01, 2268.68 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 347841/350343 [02:32<00:01, 2284.65 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348071/350343 [02:32<00:00, 2289.19 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348306/350343 [02:33<00:00, 2306.55 examples/s][A
Generating train examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348537/350343 [02:33<00:00, 2294.93 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348767/350343 [02:33<00:00, 2278.44 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 348998/350343 [02:33<00:00, 2286.23 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349227/350343 [02:33<00:00, 2285.60 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349462/350343 [02:33<00:00, 2303.78 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349697/350343 [02:33<00:00, 2316.05 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 349929/350343 [02:33<00:00, 2308.51 examples/s][A
Generating train examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 350162/350343 [02:33<00:00, 2313.79 examples/s][A
                                                                                              [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:   0%|          | 0/350343 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:   0%|          | 1/350343 [00:00<40:46:51,  2.39 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:   4%|â–         | 14204/350343 [00:00<00:09, 36110.05 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:   8%|â–Š         | 29087/350343 [00:00<00:04, 66138.99 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  13%|â–ˆâ–Ž        | 44145/350343 [00:00<00:03, 89415.37 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  17%|â–ˆâ–‹        | 59226/350343 [00:00<00:02, 106758.70 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  21%|â–ˆâ–ˆ        | 74415/350343 [00:00<00:02, 119731.52 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  25%|â–ˆâ–ˆâ–Œ       | 88571/350343 [00:01<00:02, 122775.95 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  29%|â–ˆâ–ˆâ–‰       | 102335/350343 [00:01<00:02, 123716.83 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 115723/350343 [00:01<00:01, 124427.61 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 128868/350343 [00:01<00:01, 125114.85 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 143066/350343 [00:01<00:01, 129975.02 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 158117/350343 [00:01<00:01, 135960.97 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 173278/350343 [00:01<00:01, 140561.58 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 188509/350343 [00:01<00:01, 144034.14 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 203559/350343 [00:01<00:01, 145950.65 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218878/350343 [00:01<00:00, 148104.15 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 234139/350343 [00:02<00:00, 149444.26 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 249140/350343 [00:02<00:00, 145581.04 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 264372/350343 [00:02<00:00, 147554.42 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 279717/350343 [00:02<00:00, 149292.26 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 295162/350343 [00:02<00:00, 150818.07 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 310538/350343 [00:02<00:00, 151687.31 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 326012/350343 [00:02<00:00, 152594.31 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 341409/350343 [00:02<00:00, 153003.02 examples/s][A
                                                                                                                                                            [AI0305 10:12:07.152438 140576608098112 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-train.tfrecord*. Number of examples: 350343 (shards: [43793, 43793, 43793, 43793, 43792, 43793, 43793, 43793])
Generating splits...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [02:36<05:13, 156.86s/ splits]
Generating validation examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating validation examples...:   0%|          | 108/43793 [00:00<00:48, 899.83 examples/s][A
Generating validation examples...:   1%|          | 330/43793 [00:00<00:27, 1597.36 examples/s][A
Generating validation examples...:   1%|â–         | 553/43793 [00:00<00:23, 1871.25 examples/s][A
Generating validation examples...:   2%|â–         | 775/43793 [00:00<00:21, 2002.32 examples/s][A
Generating validation examples...:   2%|â–         | 1001/43793 [00:00<00:20, 2093.15 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1222/43793 [00:00<00:19, 2129.00 examples/s][A
Generating validation examples...:   3%|â–Ž         | 1447/43793 [00:00<00:19, 2166.89 examples/s][A
Generating validation examples...:   4%|â–         | 1667/43793 [00:00<00:19, 2176.84 examples/s][A
Generating validation examples...:   4%|â–         | 1888/43793 [00:00<00:19, 2186.09 examples/s][A
Generating validation examples...:   5%|â–         | 2115/43793 [00:01<00:18, 2210.47 examples/s][A
Generating validation examples...:   5%|â–Œ         | 2337/43793 [00:01<00:18, 2206.41 examples/s][A
Generating validation examples...:   6%|â–Œ         | 2562/43793 [00:01<00:18, 2218.08 examples/s][A
Generating validation examples...:   6%|â–‹         | 2784/43793 [00:01<00:18, 2217.63 examples/s][A
Generating validation examples...:   7%|â–‹         | 3012/43793 [00:01<00:18, 2235.48 examples/s][A
Generating validation examples...:   7%|â–‹         | 3237/43793 [00:01<00:18, 2238.60 examples/s][A
Generating validation examples...:   8%|â–Š         | 3461/43793 [00:01<00:18, 2213.55 examples/s][A
Generating validation examples...:   8%|â–Š         | 3683/43793 [00:01<00:18, 2209.60 examples/s][A
Generating validation examples...:   9%|â–‰         | 3913/43793 [00:01<00:17, 2235.90 examples/s][A
Generating validation examples...:   9%|â–‰         | 4137/43793 [00:01<00:17, 2223.86 examples/s][A
Generating validation examples...:  10%|â–‰         | 4360/43793 [00:02<00:17, 2222.96 examples/s][A
Generating validation examples...:  10%|â–ˆ         | 4583/43793 [00:02<00:17, 2223.06 examples/s][A
Generating validation examples...:  11%|â–ˆ         | 4806/43793 [00:02<00:17, 2221.75 examples/s][A
Generating validation examples...:  11%|â–ˆâ–        | 5029/43793 [00:02<00:17, 2221.33 examples/s][A
Generating validation examples...:  12%|â–ˆâ–        | 5256/43793 [00:02<00:17, 2235.18 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5480/43793 [00:02<00:17, 2231.40 examples/s][A
Generating validation examples...:  13%|â–ˆâ–Ž        | 5704/43793 [00:02<00:17, 2225.66 examples/s][A
Generating validation examples...:  14%|â–ˆâ–Ž        | 5927/43793 [00:02<00:17, 2208.33 examples/s][A
Generating validation examples...:  14%|â–ˆâ–        | 6148/43793 [00:02<00:17, 2199.79 examples/s][A
Generating validation examples...:  15%|â–ˆâ–        | 6369/43793 [00:02<00:17, 2180.17 examples/s][A
Generating validation examples...:  15%|â–ˆâ–Œ        | 6590/43793 [00:03<00:17, 2187.38 examples/s][A
Generating validation examples...:  16%|â–ˆâ–Œ        | 6815/43793 [00:03<00:16, 2204.73 examples/s][A
Generating validation examples...:  16%|â–ˆâ–Œ        | 7037/43793 [00:03<00:16, 2208.82 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7258/43793 [00:03<00:16, 2206.21 examples/s][A
Generating validation examples...:  17%|â–ˆâ–‹        | 7486/43793 [00:03<00:16, 2225.56 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7709/43793 [00:03<00:16, 2225.03 examples/s][A
Generating validation examples...:  18%|â–ˆâ–Š        | 7932/43793 [00:03<00:16, 2220.82 examples/s][A
Generating validation examples...:  19%|â–ˆâ–Š        | 8157/43793 [00:03<00:15, 2228.71 examples/s][A
Generating validation examples...:  19%|â–ˆâ–‰        | 8380/43793 [00:03<00:15, 2226.64 examples/s][A
Generating validation examples...:  20%|â–ˆâ–‰        | 8603/43793 [00:03<00:15, 2221.93 examples/s][A
Generating validation examples...:  20%|â–ˆâ–ˆ        | 8826/43793 [00:04<00:15, 2217.42 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9048/43793 [00:04<00:15, 2196.15 examples/s][A
Generating validation examples...:  21%|â–ˆâ–ˆ        | 9268/43793 [00:04<00:15, 2188.18 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9491/43793 [00:04<00:15, 2198.49 examples/s][A
Generating validation examples...:  22%|â–ˆâ–ˆâ–       | 9711/43793 [00:04<00:15, 2193.65 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 9934/43793 [00:04<00:15, 2202.62 examples/s][A
Generating validation examples...:  23%|â–ˆâ–ˆâ–Ž       | 10155/43793 [00:04<00:15, 2197.60 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–Ž       | 10382/43793 [00:04<00:15, 2218.84 examples/s][A
Generating validation examples...:  24%|â–ˆâ–ˆâ–       | 10608/43793 [00:04<00:14, 2229.06 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–       | 10831/43793 [00:04<00:14, 2212.55 examples/s][A
Generating validation examples...:  25%|â–ˆâ–ˆâ–Œ       | 11053/43793 [00:05<00:14, 2207.15 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–Œ       | 11274/43793 [00:05<00:14, 2205.50 examples/s][A
Generating validation examples...:  26%|â–ˆâ–ˆâ–‹       | 11496/43793 [00:05<00:14, 2208.50 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11717/43793 [00:05<00:14, 2197.16 examples/s][A
Generating validation examples...:  27%|â–ˆâ–ˆâ–‹       | 11938/43793 [00:05<00:14, 2200.39 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12160/43793 [00:05<00:14, 2203.85 examples/s][A
Generating validation examples...:  28%|â–ˆâ–ˆâ–Š       | 12381/43793 [00:05<00:14, 2202.44 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–‰       | 12602/43793 [00:05<00:14, 2194.92 examples/s][A
Generating validation examples...:  29%|â–ˆâ–ˆâ–‰       | 12823/43793 [00:05<00:14, 2198.02 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–‰       | 13043/43793 [00:05<00:14, 2187.43 examples/s][A
Generating validation examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13265/43793 [00:06<00:13, 2196.04 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13485/43793 [00:06<00:13, 2193.13 examples/s][A
Generating validation examples...:  31%|â–ˆâ–ˆâ–ˆâ–      | 13706/43793 [00:06<00:13, 2196.71 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13928/43793 [00:06<00:13, 2202.37 examples/s][A
Generating validation examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14151/43793 [00:06<00:13, 2209.82 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14374/43793 [00:06<00:13, 2215.29 examples/s][A
Generating validation examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14596/43793 [00:06<00:13, 2208.19 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14823/43793 [00:06<00:13, 2225.04 examples/s][A
Generating validation examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 15048/43793 [00:06<00:12, 2231.43 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15272/43793 [00:06<00:12, 2227.29 examples/s][A
Generating validation examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15495/43793 [00:07<00:12, 2213.23 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15720/43793 [00:07<00:12, 2222.00 examples/s][A
Generating validation examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 15944/43793 [00:07<00:12, 2226.30 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16169/43793 [00:07<00:12, 2233.34 examples/s][A
Generating validation examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16393/43793 [00:07<00:12, 2228.82 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16616/43793 [00:07<00:12, 2223.59 examples/s][A
Generating validation examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16839/43793 [00:07<00:12, 2214.50 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17065/43793 [00:07<00:12, 2224.05 examples/s][A
Generating validation examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17288/43793 [00:07<00:11, 2215.56 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17510/43793 [00:07<00:11, 2195.03 examples/s][A
Generating validation examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17730/43793 [00:08<00:11, 2179.52 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17948/43793 [00:08<00:11, 2175.78 examples/s][A
Generating validation examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18168/43793 [00:08<00:11, 2182.12 examples/s][A
Generating validation examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18392/43793 [00:08<00:11, 2199.09 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18613/43793 [00:08<00:11, 2201.43 examples/s][A
Generating validation examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18834/43793 [00:08<00:11, 2195.93 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19058/43793 [00:08<00:11, 2206.74 examples/s][A
Generating validation examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19279/43793 [00:08<00:11, 2204.40 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19500/43793 [00:08<00:11, 2195.22 examples/s][A
Generating validation examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19720/43793 [00:08<00:11, 2188.13 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19939/43793 [00:09<00:10, 2176.15 examples/s][A
Generating validation examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20158/43793 [00:09<00:10, 2179.48 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20376/43793 [00:09<00:10, 2169.56 examples/s][A
Generating validation examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20593/43793 [00:09<00:10, 2160.22 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20813/43793 [00:09<00:10, 2171.88 examples/s][A
Generating validation examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21034/43793 [00:09<00:10, 2181.37 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21255/43793 [00:09<00:10, 2188.90 examples/s][A
Generating validation examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21482/43793 [00:09<00:10, 2211.41 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21705/43793 [00:09<00:09, 2214.84 examples/s][A
Generating validation examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21927/43793 [00:09<00:09, 2212.01 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22149/43793 [00:10<00:09, 2214.05 examples/s][A
Generating validation examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22371/43793 [00:10<00:09, 2205.13 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22592/43793 [00:10<00:09, 2203.34 examples/s][A
Generating validation examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22813/43793 [00:10<00:09, 2185.69 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23035/43793 [00:10<00:09, 2193.83 examples/s][A
Generating validation examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23255/43793 [00:10<00:09, 2193.33 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23475/43793 [00:10<00:09, 2171.44 examples/s][A
Generating validation examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23693/43793 [00:10<00:09, 2165.61 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23919/43793 [00:10<00:09, 2193.38 examples/s][A
Generating validation examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24140/43793 [00:11<00:08, 2198.29 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24360/43793 [00:11<00:08, 2189.88 examples/s][A
Generating validation examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24580/43793 [00:11<00:08, 2191.84 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24800/43793 [00:11<00:08, 2189.14 examples/s][A
Generating validation examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25019/43793 [00:11<00:08, 2183.03 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25246/43793 [00:11<00:08, 2206.49 examples/s][A
Generating validation examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25467/43793 [00:11<00:08, 2196.33 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25689/43793 [00:11<00:08, 2203.17 examples/s][A
Generating validation examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25910/43793 [00:11<00:08, 2199.79 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26130/43793 [00:11<00:08, 2195.34 examples/s][A
Generating validation examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26350/43793 [00:12<00:07, 2186.88 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26572/43793 [00:12<00:07, 2194.82 examples/s][A
Generating validation examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26793/43793 [00:12<00:07, 2197.18 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27014/43793 [00:12<00:07, 2199.52 examples/s][A
Generating validation examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27239/43793 [00:12<00:07, 2213.56 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27461/43793 [00:12<00:07, 2213.10 examples/s][A
Generating validation examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27684/43793 [00:12<00:07, 2216.63 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27909/43793 [00:12<00:07, 2225.57 examples/s][A
Generating validation examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28132/43793 [00:12<00:07, 2193.77 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28352/43793 [00:12<00:07, 2080.61 examples/s][A
Generating validation examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28562/43793 [00:13<00:07, 2005.85 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28784/43793 [00:13<00:07, 2039.15 examples/s][A
Generating validation examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 29006/43793 [00:13<00:07, 2075.31 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29229/43793 [00:13<00:06, 2118.12 examples/s][A
Generating validation examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29450/43793 [00:13<00:06, 2134.77 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29672/43793 [00:13<00:06, 2147.25 examples/s][A
Generating validation examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29895/43793 [00:13<00:06, 2168.18 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30119/43793 [00:13<00:06, 2187.79 examples/s][A
Generating validation examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30345/43793 [00:13<00:06, 2207.32 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30566/43793 [00:13<00:06, 2183.01 examples/s][A
Generating validation examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30790/43793 [00:14<00:05, 2199.47 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31016/43793 [00:14<00:05, 2217.31 examples/s][A
Generating validation examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31238/43793 [00:14<00:05, 2210.16 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31460/43793 [00:14<00:05, 2210.43 examples/s][A
Generating validation examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31682/43793 [00:14<00:05, 2201.04 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31911/43793 [00:14<00:05, 2224.95 examples/s][A
Generating validation examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32134/43793 [00:14<00:05, 2212.45 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32356/43793 [00:14<00:05, 2208.43 examples/s][A
Generating validation examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32581/43793 [00:14<00:05, 2218.58 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32804/43793 [00:14<00:04, 2221.46 examples/s][A
Generating validation examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33027/43793 [00:15<00:04, 2197.95 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33247/43793 [00:15<00:04, 2191.63 examples/s][A
Generating validation examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33467/43793 [00:15<00:04, 2190.59 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33687/43793 [00:15<00:04, 2160.20 examples/s][A
Generating validation examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33910/43793 [00:15<00:04, 2178.99 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34133/43793 [00:15<00:04, 2192.68 examples/s][A
Generating validation examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34355/43793 [00:15<00:04, 2200.42 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34582/43793 [00:15<00:04, 2220.79 examples/s][A
Generating validation examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34806/43793 [00:15<00:04, 2224.49 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 35030/43793 [00:15<00:03, 2218.44 examples/s][A
Generating validation examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35252/43793 [00:16<00:03, 2199.94 examples/s][A
Generating validation examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35473/43793 [00:16<00:03, 2188.91 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35693/43793 [00:16<00:03, 2191.94 examples/s][A
Generating validation examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35921/43793 [00:16<00:03, 2217.17 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36146/43793 [00:16<00:03, 2225.11 examples/s][A
Generating validation examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36370/43793 [00:16<00:03, 2228.67 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36593/43793 [00:16<00:03, 2227.68 examples/s][A
Generating validation examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36816/43793 [00:16<00:03, 2227.53 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37039/43793 [00:16<00:03, 2223.53 examples/s][A
Generating validation examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37265/43793 [00:17<00:02, 2234.03 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37492/43793 [00:17<00:02, 2242.52 examples/s][A
Generating validation examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37719/43793 [00:17<00:02, 2237.42 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37945/43793 [00:17<00:02, 2243.40 examples/s][A
Generating validation examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38170/43793 [00:17<00:02, 2241.87 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38395/43793 [00:17<00:02, 2241.98 examples/s][A
Generating validation examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38620/43793 [00:17<00:02, 2232.05 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38844/43793 [00:17<00:02, 2232.33 examples/s][A
Generating validation examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39068/43793 [00:17<00:02, 2227.22 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39291/43793 [00:17<00:02, 2221.20 examples/s][A
Generating validation examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39514/43793 [00:18<00:01, 2223.02 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39741/43793 [00:18<00:01, 2236.81 examples/s][A
Generating validation examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 39965/43793 [00:18<00:01, 2233.80 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40191/43793 [00:18<00:01, 2241.59 examples/s][A
Generating validation examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40416/43793 [00:18<00:01, 2233.83 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40640/43793 [00:18<00:01, 2234.44 examples/s][A
Generating validation examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40864/43793 [00:18<00:01, 2222.43 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41090/43793 [00:18<00:01, 2233.08 examples/s][A
Generating validation examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41314/43793 [00:18<00:01, 2225.47 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41537/43793 [00:18<00:01, 2219.16 examples/s][A
Generating validation examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41759/43793 [00:19<00:00, 2214.19 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41984/43793 [00:19<00:00, 2223.83 examples/s][A
Generating validation examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42208/43793 [00:19<00:00, 2226.96 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42431/43793 [00:19<00:00, 2208.37 examples/s][A
Generating validation examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42652/43793 [00:19<00:00, 2180.20 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42871/43793 [00:19<00:00, 2147.77 examples/s][A
Generating validation examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43091/43793 [00:19<00:00, 2160.91 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43314/43793 [00:19<00:00, 2179.17 examples/s][A
Generating validation examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43534/43793 [00:19<00:00, 2183.02 examples/s][A
Generating validation examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43757/43793 [00:19<00:00, 2195.85 examples/s][A
                                                                                                 [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-validation.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-validation.tfrecord*...:  23%|â–ˆâ–ˆâ–Ž       | 10141/43793 [00:00<00:00, 101393.17 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-validation.tfrecord*...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25418/43793 [00:00<00:00, 131607.49 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-validation.tfrecord*...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40851/43793 [00:00<00:00, 141975.10 examples/s][A
                                                                                                                                                               [AI0305 10:12:27.515795 140576608098112 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-validation.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:57<01:16, 76.52s/ splits] 
Generating test examples...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Generating test examples...:   0%|          | 162/43793 [00:00<00:27, 1613.46 examples/s][A
Generating test examples...:   1%|          | 373/43793 [00:00<00:22, 1901.36 examples/s][A
Generating test examples...:   1%|â–         | 597/43793 [00:00<00:21, 2054.54 examples/s][A
Generating test examples...:   2%|â–         | 817/43793 [00:00<00:20, 2110.42 examples/s][A
Generating test examples...:   2%|â–         | 1042/43793 [00:00<00:19, 2159.56 examples/s][A
Generating test examples...:   3%|â–Ž         | 1265/43793 [00:00<00:19, 2182.19 examples/s][A
Generating test examples...:   3%|â–Ž         | 1488/43793 [00:00<00:19, 2195.46 examples/s][A
Generating test examples...:   4%|â–         | 1716/43793 [00:00<00:18, 2221.45 examples/s][A
Generating test examples...:   4%|â–         | 1945/43793 [00:00<00:18, 2240.84 examples/s][A
Generating test examples...:   5%|â–         | 2176/43793 [00:01<00:18, 2262.11 examples/s][A
Generating test examples...:   5%|â–Œ         | 2403/43793 [00:01<00:18, 2262.94 examples/s][A
Generating test examples...:   6%|â–Œ         | 2630/43793 [00:01<00:18, 2225.92 examples/s][A
Generating test examples...:   7%|â–‹         | 2855/43793 [00:01<00:18, 2232.95 examples/s][A
Generating test examples...:   7%|â–‹         | 3085/43793 [00:01<00:18, 2252.74 examples/s][A
Generating test examples...:   8%|â–Š         | 3313/43793 [00:01<00:17, 2260.42 examples/s][A
Generating test examples...:   8%|â–Š         | 3540/43793 [00:01<00:17, 2256.33 examples/s][A
Generating test examples...:   9%|â–Š         | 3767/43793 [00:01<00:17, 2259.61 examples/s][A
Generating test examples...:   9%|â–‰         | 3993/43793 [00:01<00:17, 2237.76 examples/s][A
Generating test examples...:  10%|â–‰         | 4218/43793 [00:01<00:17, 2239.26 examples/s][A
Generating test examples...:  10%|â–ˆ         | 4442/43793 [00:02<00:17, 2221.77 examples/s][A
Generating test examples...:  11%|â–ˆ         | 4665/43793 [00:02<00:17, 2214.85 examples/s][A
Generating test examples...:  11%|â–ˆ         | 4891/43793 [00:02<00:17, 2226.70 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5116/43793 [00:02<00:17, 2231.51 examples/s][A
Generating test examples...:  12%|â–ˆâ–        | 5340/43793 [00:02<00:17, 2220.85 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5563/43793 [00:02<00:17, 2213.04 examples/s][A
Generating test examples...:  13%|â–ˆâ–Ž        | 5788/43793 [00:02<00:17, 2221.21 examples/s][A
Generating test examples...:  14%|â–ˆâ–Ž        | 6011/43793 [00:02<00:17, 2145.58 examples/s][A
Generating test examples...:  14%|â–ˆâ–        | 6233/43793 [00:02<00:17, 2166.60 examples/s][A
Generating test examples...:  15%|â–ˆâ–        | 6457/43793 [00:02<00:17, 2187.86 examples/s][A
Generating test examples...:  15%|â–ˆâ–Œ        | 6678/43793 [00:03<00:16, 2192.89 examples/s][A
Generating test examples...:  16%|â–ˆâ–Œ        | 6901/43793 [00:03<00:16, 2202.26 examples/s][A
Generating test examples...:  16%|â–ˆâ–‹        | 7126/43793 [00:03<00:16, 2215.48 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7348/43793 [00:03<00:16, 2214.31 examples/s][A
Generating test examples...:  17%|â–ˆâ–‹        | 7574/43793 [00:03<00:16, 2227.27 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 7799/43793 [00:03<00:16, 2233.71 examples/s][A
Generating test examples...:  18%|â–ˆâ–Š        | 8025/43793 [00:03<00:15, 2239.55 examples/s][A
Generating test examples...:  19%|â–ˆâ–‰        | 8250/43793 [00:03<00:15, 2240.87 examples/s][A
Generating test examples...:  19%|â–ˆâ–‰        | 8475/43793 [00:03<00:15, 2232.09 examples/s][A
Generating test examples...:  20%|â–ˆâ–‰        | 8700/43793 [00:03<00:15, 2236.87 examples/s][A
Generating test examples...:  20%|â–ˆâ–ˆ        | 8925/43793 [00:04<00:15, 2238.85 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆ        | 9149/43793 [00:04<00:15, 2235.10 examples/s][A
Generating test examples...:  21%|â–ˆâ–ˆâ–       | 9376/43793 [00:04<00:15, 2243.33 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9604/43793 [00:04<00:15, 2253.56 examples/s][A
Generating test examples...:  22%|â–ˆâ–ˆâ–       | 9832/43793 [00:04<00:15, 2259.22 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 10059/43793 [00:04<00:14, 2261.30 examples/s][A
Generating test examples...:  23%|â–ˆâ–ˆâ–Ž       | 10286/43793 [00:04<00:14, 2254.12 examples/s][A
Generating test examples...:  24%|â–ˆâ–ˆâ–       | 10513/43793 [00:04<00:14, 2255.69 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–       | 10739/43793 [00:04<00:14, 2247.05 examples/s][A
Generating test examples...:  25%|â–ˆâ–ˆâ–Œ       | 10964/43793 [00:04<00:14, 2233.34 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11188/43793 [00:05<00:14, 2198.28 examples/s][A
Generating test examples...:  26%|â–ˆâ–ˆâ–Œ       | 11418/43793 [00:05<00:15, 2139.83 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11642/43793 [00:05<00:14, 2167.63 examples/s][A
Generating test examples...:  27%|â–ˆâ–ˆâ–‹       | 11869/43793 [00:05<00:14, 2196.49 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12095/43793 [00:05<00:14, 2213.84 examples/s][A
Generating test examples...:  28%|â–ˆâ–ˆâ–Š       | 12321/43793 [00:05<00:14, 2226.82 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–Š       | 12545/43793 [00:05<00:14, 2229.71 examples/s][A
Generating test examples...:  29%|â–ˆâ–ˆâ–‰       | 12771/43793 [00:05<00:13, 2237.07 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–‰       | 12997/43793 [00:05<00:13, 2241.91 examples/s][A
Generating test examples...:  30%|â–ˆâ–ˆâ–ˆ       | 13222/43793 [00:05<00:13, 2240.16 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13447/43793 [00:06<00:13, 2240.64 examples/s][A
Generating test examples...:  31%|â–ˆâ–ˆâ–ˆ       | 13672/43793 [00:06<00:13, 2241.07 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 13900/43793 [00:06<00:13, 2250.96 examples/s][A
Generating test examples...:  32%|â–ˆâ–ˆâ–ˆâ–      | 14126/43793 [00:06<00:13, 2235.78 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14352/43793 [00:06<00:13, 2242.48 examples/s][A
Generating test examples...:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 14580/43793 [00:06<00:12, 2251.68 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 14806/43793 [00:06<00:13, 2209.14 examples/s][A
Generating test examples...:  34%|â–ˆâ–ˆâ–ˆâ–      | 15029/43793 [00:06<00:12, 2214.25 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–      | 15251/43793 [00:06<00:13, 2184.76 examples/s][A
Generating test examples...:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 15475/43793 [00:06<00:12, 2199.06 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 15696/43793 [00:07<00:12, 2198.67 examples/s][A
Generating test examples...:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 15918/43793 [00:07<00:12, 2204.86 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16147/43793 [00:07<00:12, 2229.45 examples/s][A
Generating test examples...:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 16371/43793 [00:07<00:12, 2211.55 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16593/43793 [00:07<00:12, 2211.77 examples/s][A
Generating test examples...:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 16815/43793 [00:07<00:12, 2212.97 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17037/43793 [00:07<00:12, 2205.38 examples/s][A
Generating test examples...:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 17259/43793 [00:07<00:12, 2208.35 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 17480/43793 [00:07<00:11, 2206.07 examples/s][A
Generating test examples...:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17701/43793 [00:07<00:11, 2187.37 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 17921/43793 [00:08<00:11, 2189.59 examples/s][A
Generating test examples...:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18140/43793 [00:08<00:11, 2187.20 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18361/43793 [00:08<00:11, 2192.55 examples/s][A
Generating test examples...:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18581/43793 [00:08<00:11, 2190.47 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 18804/43793 [00:08<00:11, 2201.98 examples/s][A
Generating test examples...:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19027/43793 [00:08<00:11, 2208.34 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19250/43793 [00:08<00:11, 2213.69 examples/s][A
Generating test examples...:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19474/43793 [00:08<00:10, 2219.92 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 19696/43793 [00:08<00:10, 2213.15 examples/s][A
Generating test examples...:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 19918/43793 [00:08<00:10, 2206.02 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20140/43793 [00:09<00:10, 2209.07 examples/s][A
Generating test examples...:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20361/43793 [00:09<00:10, 2191.63 examples/s][A
Generating test examples...:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 20582/43793 [00:09<00:10, 2195.62 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 20803/43793 [00:09<00:10, 2198.93 examples/s][A
Generating test examples...:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21025/43793 [00:09<00:10, 2204.26 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21248/43793 [00:09<00:10, 2210.06 examples/s][A
Generating test examples...:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21478/43793 [00:09<00:09, 2235.68 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 21705/43793 [00:09<00:09, 2245.20 examples/s][A
Generating test examples...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21933/43793 [00:09<00:09, 2255.41 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22160/43793 [00:10<00:09, 2259.50 examples/s][A
Generating test examples...:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22389/43793 [00:10<00:09, 2267.77 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22616/43793 [00:10<00:09, 2256.04 examples/s][A
Generating test examples...:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22842/43793 [00:10<00:09, 2234.02 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23066/43793 [00:10<00:09, 2233.37 examples/s][A
Generating test examples...:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23292/43793 [00:10<00:09, 2240.92 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 23517/43793 [00:10<00:09, 2230.93 examples/s][A
Generating test examples...:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23743/43793 [00:10<00:08, 2238.99 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23971/43793 [00:10<00:08, 2248.88 examples/s][A
Generating test examples...:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24196/43793 [00:10<00:08, 2235.52 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 24420/43793 [00:11<00:08, 2229.33 examples/s][A
Generating test examples...:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24648/43793 [00:11<00:08, 2242.35 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 24876/43793 [00:11<00:08, 2252.78 examples/s][A
Generating test examples...:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25102/43793 [00:11<00:08, 2251.27 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25330/43793 [00:11<00:08, 2257.39 examples/s][A
Generating test examples...:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 25556/43793 [00:11<00:08, 2246.33 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 25781/43793 [00:11<00:08, 2238.12 examples/s][A
Generating test examples...:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26009/43793 [00:11<00:07, 2248.71 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26234/43793 [00:11<00:07, 2248.97 examples/s][A
Generating test examples...:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26459/43793 [00:11<00:07, 2237.97 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 26683/43793 [00:12<00:07, 2234.08 examples/s][A
Generating test examples...:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26910/43793 [00:12<00:07, 2243.88 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27135/43793 [00:12<00:07, 2245.50 examples/s][A
Generating test examples...:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27365/43793 [00:12<00:07, 2261.43 examples/s][A
Generating test examples...:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27592/43793 [00:12<00:07, 2247.07 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 27817/43793 [00:12<00:07, 2247.79 examples/s][A
Generating test examples...:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28044/43793 [00:12<00:06, 2251.74 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28270/43793 [00:12<00:06, 2246.71 examples/s][A
Generating test examples...:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28495/43793 [00:12<00:06, 2240.73 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28720/43793 [00:12<00:06, 2220.59 examples/s][A
Generating test examples...:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 28943/43793 [00:13<00:06, 2213.40 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29165/43793 [00:13<00:06, 2194.95 examples/s][A
Generating test examples...:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 29385/43793 [00:13<00:06, 2192.96 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29605/43793 [00:13<00:06, 2179.40 examples/s][A
Generating test examples...:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 29823/43793 [00:13<00:06, 2177.76 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 30041/43793 [00:13<00:06, 2167.35 examples/s][A
Generating test examples...:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30260/43793 [00:13<00:06, 2172.36 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 30489/43793 [00:13<00:06, 2205.85 examples/s][A
Generating test examples...:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30712/43793 [00:13<00:05, 2212.78 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 30934/43793 [00:13<00:05, 2210.89 examples/s][A
Generating test examples...:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31156/43793 [00:14<00:05, 2196.62 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31376/43793 [00:14<00:05, 2192.38 examples/s][A
Generating test examples...:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 31597/43793 [00:14<00:05, 2195.20 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 31817/43793 [00:14<00:05, 2189.32 examples/s][A
Generating test examples...:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32042/43793 [00:14<00:05, 2205.57 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32265/43793 [00:14<00:05, 2212.72 examples/s][A
Generating test examples...:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32487/43793 [00:14<00:05, 2196.59 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 32707/43793 [00:14<00:05, 2187.80 examples/s][A
Generating test examples...:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 32929/43793 [00:14<00:04, 2196.93 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33149/43793 [00:14<00:04, 2186.26 examples/s][A
Generating test examples...:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33368/43793 [00:15<00:04, 2180.83 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33587/43793 [00:15<00:04, 2179.47 examples/s][A
Generating test examples...:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 33814/43793 [00:15<00:04, 2204.21 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34040/43793 [00:15<00:04, 2220.72 examples/s][A
Generating test examples...:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 34263/43793 [00:15<00:04, 2201.73 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34488/43793 [00:15<00:04, 2214.99 examples/s][A
Generating test examples...:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34714/43793 [00:15<00:04, 2226.66 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 34940/43793 [00:15<00:03, 2236.50 examples/s][A
Generating test examples...:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35169/43793 [00:15<00:03, 2251.04 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35395/43793 [00:15<00:03, 2247.76 examples/s][A
Generating test examples...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35620/43793 [00:16<00:03, 2149.16 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 35844/43793 [00:16<00:03, 2175.07 examples/s][A
Generating test examples...:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36072/43793 [00:16<00:03, 2205.07 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36295/43793 [00:16<00:03, 2209.95 examples/s][A
Generating test examples...:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 36518/43793 [00:16<00:03, 2214.62 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36745/43793 [00:16<00:03, 2228.51 examples/s][A
Generating test examples...:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36969/43793 [00:16<00:03, 2225.33 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37193/43793 [00:16<00:02, 2228.21 examples/s][A
Generating test examples...:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37423/43793 [00:16<00:02, 2248.89 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 37648/43793 [00:16<00:02, 2230.80 examples/s][A
Generating test examples...:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 37872/43793 [00:17<00:02, 2225.29 examples/s][A
Generating test examples...:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38097/43793 [00:17<00:02, 2231.84 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38324/43793 [00:17<00:02, 2241.08 examples/s][A
Generating test examples...:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38549/43793 [00:17<00:02, 2233.26 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 38773/43793 [00:17<00:02, 2231.09 examples/s][A
Generating test examples...:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 38997/43793 [00:17<00:02, 2213.78 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39224/43793 [00:17<00:02, 2230.21 examples/s][A
Generating test examples...:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39452/43793 [00:17<00:01, 2243.49 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39683/43793 [00:17<00:01, 2261.91 examples/s][A
Generating test examples...:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 39910/43793 [00:17<00:01, 2252.37 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40136/43793 [00:18<00:01, 2244.93 examples/s][A
Generating test examples...:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 40363/43793 [00:18<00:01, 2252.23 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40589/43793 [00:18<00:01, 2235.26 examples/s][A
Generating test examples...:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 40813/43793 [00:18<00:01, 2229.58 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 41040/43793 [00:18<00:01, 2239.89 examples/s][A
Generating test examples...:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41265/43793 [00:18<00:01, 2230.76 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 41489/43793 [00:18<00:01, 2196.50 examples/s][A
Generating test examples...:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41712/43793 [00:18<00:00, 2205.29 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 41938/43793 [00:18<00:00, 2220.36 examples/s][A
Generating test examples...:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42161/43793 [00:19<00:00, 2212.79 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42391/43793 [00:19<00:00, 2238.43 examples/s][A
Generating test examples...:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 42619/43793 [00:19<00:00, 2249.56 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 42845/43793 [00:19<00:00, 2240.03 examples/s][A
Generating test examples...:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43074/43793 [00:19<00:00, 2253.34 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43300/43793 [00:19<00:00, 2241.18 examples/s][A
Generating test examples...:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43525/43793 [00:19<00:00, 2219.85 examples/s][A
Generating test examples...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 43753/43793 [00:19<00:00, 2237.29 examples/s][A
                                                                                           [A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-test.tfrecord*...:   0%|          | 0/43793 [00:00<?, ? examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-test.tfrecord*...:  19%|â–ˆâ–‰        | 8379/43793 [00:00<00:00, 83778.49 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-test.tfrecord*...:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21910/43793 [00:00<00:00, 114085.66 examples/s][A
Shuffling /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-test.tfrecord*...:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 35525/43793 [00:00<00:00, 124156.75 examples/s][A
                                                                                                                                                         [AI0305 10:12:47.640275 140576608098112 writer.py:301] Done writing /root/data/ogbg_molpcba/0.1.3.incompleteBGINX9/ogbg_molpcba-test.tfrecord*. Number of examples: 43793 (shards: [43793])
Generating splits...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:17<00:00, 50.77s/ splits]                                                                        I0305 10:12:47.728940 140576608098112 logging_logger.py:49] Constructing tf.data.Dataset ogbg_molpcba for split test, from /root/data/ogbg_molpcba/0.1.3
I0305 10:14:25.340452 140576608098112 submission_runner.py:411] Time since start: 556.05s, 	Step: 1, 	{'train/accuracy': 0.41592565178871155, 'train/loss': 0.7988808155059814, 'train/mean_average_precision': 0.021598215528716165, 'validation/accuracy': 0.41298335790634155, 'validation/loss': 0.7986935973167419, 'validation/mean_average_precision': 0.02520961705024715, 'validation/num_examples': 43793, 'test/accuracy': 0.41331201791763306, 'test/loss': 0.7984466552734375, 'test/mean_average_precision': 0.02606502795169988, 'test/num_examples': 43793, 'score': 19.18818688392639, 'total_duration': 556.0487859249115, 'accumulated_submission_time': 19.18818688392639, 'accumulated_eval_time': 536.8605499267578, 'accumulated_logging_time': 0}
I0305 10:14:25.358665 140409007232768 logging_writer.py:48] [1] accumulated_eval_time=536.860550, accumulated_logging_time=0, accumulated_submission_time=19.188187, global_step=1, preemption_count=0, score=19.188187, test/accuracy=0.413312, test/loss=0.798447, test/mean_average_precision=0.026065, test/num_examples=43793, total_duration=556.048786, train/accuracy=0.415926, train/loss=0.798881, train/mean_average_precision=0.021598, validation/accuracy=0.412983, validation/loss=0.798694, validation/mean_average_precision=0.025210, validation/num_examples=43793
I0305 10:14:57.711541 140409291962112 logging_writer.py:48] [100] global_step=100, grad_norm=0.3168574869632721, loss=0.2923794686794281
I0305 10:15:30.267814 140409007232768 logging_writer.py:48] [200] global_step=200, grad_norm=0.10401216149330139, loss=0.1152581200003624
I0305 10:16:02.400421 140409291962112 logging_writer.py:48] [300] global_step=300, grad_norm=0.03403453901410103, loss=0.07123976945877075
I0305 10:16:34.532197 140409007232768 logging_writer.py:48] [400] global_step=400, grad_norm=0.020361101254820824, loss=0.06212102621793747
I0305 10:17:06.845525 140409291962112 logging_writer.py:48] [500] global_step=500, grad_norm=0.037369001656770706, loss=0.05511300265789032
I0305 10:17:38.922729 140409007232768 logging_writer.py:48] [600] global_step=600, grad_norm=0.08303835988044739, loss=0.05112222209572792
I0305 10:18:11.117672 140409291962112 logging_writer.py:48] [700] global_step=700, grad_norm=0.022644823417067528, loss=0.051207032054662704
I0305 10:18:25.393747 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:20:21.714467 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:20:24.708487 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:20:27.755889 140576608098112 submission_runner.py:411] Time since start: 918.46s, 	Step: 744, 	{'train/accuracy': 0.9869332909584045, 'train/loss': 0.05064266547560692, 'train/mean_average_precision': 0.05642479312067797, 'validation/accuracy': 0.9842758178710938, 'validation/loss': 0.06036971136927605, 'validation/mean_average_precision': 0.05551938292269297, 'validation/num_examples': 43793, 'test/accuracy': 0.9832848310470581, 'test/loss': 0.06363740563392639, 'test/mean_average_precision': 0.0572574487730643, 'test/num_examples': 43793, 'score': 259.19083619117737, 'total_duration': 918.4641320705414, 'accumulated_submission_time': 259.19083619117737, 'accumulated_eval_time': 659.2226083278656, 'accumulated_logging_time': 0.030831575393676758}
I0305 10:20:27.771731 140408968165120 logging_writer.py:48] [744] accumulated_eval_time=659.222608, accumulated_logging_time=0.030832, accumulated_submission_time=259.190836, global_step=744, preemption_count=0, score=259.190836, test/accuracy=0.983285, test/loss=0.063637, test/mean_average_precision=0.057257, test/num_examples=43793, total_duration=918.464132, train/accuracy=0.986933, train/loss=0.050643, train/mean_average_precision=0.056425, validation/accuracy=0.984276, validation/loss=0.060370, validation/mean_average_precision=0.055519, validation/num_examples=43793
I0305 10:20:46.755989 140409007232768 logging_writer.py:48] [800] global_step=800, grad_norm=0.01399243250489235, loss=0.047376785427331924
I0305 10:21:19.269967 140408968165120 logging_writer.py:48] [900] global_step=900, grad_norm=0.024515338242053986, loss=0.0537433922290802
I0305 10:21:51.649827 140409007232768 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.029711734503507614, loss=0.05074154585599899
I0305 10:22:23.896633 140408968165120 logging_writer.py:48] [1100] global_step=1100, grad_norm=0.018164707347750664, loss=0.048954978585243225
I0305 10:22:56.815543 140409007232768 logging_writer.py:48] [1200] global_step=1200, grad_norm=0.025821255519986153, loss=0.047876980155706406
I0305 10:23:29.753820 140408968165120 logging_writer.py:48] [1300] global_step=1300, grad_norm=0.04317326471209526, loss=0.05063452571630478
I0305 10:24:02.073863 140409007232768 logging_writer.py:48] [1400] global_step=1400, grad_norm=0.02897041104733944, loss=0.0478874035179615
I0305 10:24:27.839756 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:26:25.193766 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:26:28.244720 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:26:31.190536 140576608098112 submission_runner.py:411] Time since start: 1281.90s, 	Step: 1481, 	{'train/accuracy': 0.9874383211135864, 'train/loss': 0.04634902626276016, 'train/mean_average_precision': 0.1024011862839557, 'validation/accuracy': 0.9846505522727966, 'validation/loss': 0.05596816912293434, 'validation/mean_average_precision': 0.10306452145023913, 'validation/num_examples': 43793, 'test/accuracy': 0.9836386442184448, 'test/loss': 0.05921081081032753, 'test/mean_average_precision': 0.10045570281263416, 'test/num_examples': 43793, 'score': 499.22732520103455, 'total_duration': 1281.8988349437714, 'accumulated_submission_time': 499.22732520103455, 'accumulated_eval_time': 782.5733218193054, 'accumulated_logging_time': 0.05858325958251953}
I0305 10:26:31.206264 140409015625472 logging_writer.py:48] [1481] accumulated_eval_time=782.573322, accumulated_logging_time=0.058583, accumulated_submission_time=499.227325, global_step=1481, preemption_count=0, score=499.227325, test/accuracy=0.983639, test/loss=0.059211, test/mean_average_precision=0.100456, test/num_examples=43793, total_duration=1281.898835, train/accuracy=0.987438, train/loss=0.046349, train/mean_average_precision=0.102401, validation/accuracy=0.984651, validation/loss=0.055968, validation/mean_average_precision=0.103065, validation/num_examples=43793
I0305 10:26:37.742338 140409291962112 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.018785696476697922, loss=0.04853490740060806
I0305 10:27:10.330329 140409015625472 logging_writer.py:48] [1600] global_step=1600, grad_norm=0.02449173852801323, loss=0.045746952295303345
I0305 10:27:42.833949 140409291962112 logging_writer.py:48] [1700] global_step=1700, grad_norm=0.0373837985098362, loss=0.04447953775525093
I0305 10:28:15.685643 140409015625472 logging_writer.py:48] [1800] global_step=1800, grad_norm=0.016535695642232895, loss=0.044894829392433167
I0305 10:28:48.184511 140409291962112 logging_writer.py:48] [1900] global_step=1900, grad_norm=0.015485272742807865, loss=0.047062020748853683
I0305 10:29:20.623178 140409015625472 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.018703071400523186, loss=0.047695111483335495
I0305 10:29:53.312906 140409291962112 logging_writer.py:48] [2100] global_step=2100, grad_norm=0.011753741651773453, loss=0.03985079377889633
I0305 10:30:25.543701 140409015625472 logging_writer.py:48] [2200] global_step=2200, grad_norm=0.010969803668558598, loss=0.038609448820352554
I0305 10:30:31.385121 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:32:28.027111 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:32:31.053296 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:32:34.060726 140576608098112 submission_runner.py:411] Time since start: 1644.77s, 	Step: 2219, 	{'train/accuracy': 0.9876528978347778, 'train/loss': 0.04442020133137703, 'train/mean_average_precision': 0.14165772638642565, 'validation/accuracy': 0.984782874584198, 'validation/loss': 0.054146599024534225, 'validation/mean_average_precision': 0.13727679196997405, 'validation/num_examples': 43793, 'test/accuracy': 0.9837620854377747, 'test/loss': 0.057377394288778305, 'test/mean_average_precision': 0.13402395443794682, 'test/num_examples': 43793, 'score': 739.374596118927, 'total_duration': 1644.7690522670746, 'accumulated_submission_time': 739.374596118927, 'accumulated_eval_time': 905.2488882541656, 'accumulated_logging_time': 0.08561348915100098}
I0305 10:32:34.075783 140409007232768 logging_writer.py:48] [2219] accumulated_eval_time=905.248888, accumulated_logging_time=0.085613, accumulated_submission_time=739.374596, global_step=2219, preemption_count=0, score=739.374596, test/accuracy=0.983762, test/loss=0.057377, test/mean_average_precision=0.134024, test/num_examples=43793, total_duration=1644.769052, train/accuracy=0.987653, train/loss=0.044420, train/mean_average_precision=0.141658, validation/accuracy=0.984783, validation/loss=0.054147, validation/mean_average_precision=0.137277, validation/num_examples=43793
I0305 10:33:00.631194 140409024018176 logging_writer.py:48] [2300] global_step=2300, grad_norm=0.019303249195218086, loss=0.04200927913188934
I0305 10:33:32.986174 140409007232768 logging_writer.py:48] [2400] global_step=2400, grad_norm=0.011556304059922695, loss=0.040515799075365067
I0305 10:34:05.172227 140409024018176 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.012202272191643715, loss=0.042695388197898865
I0305 10:34:36.864331 140409007232768 logging_writer.py:48] [2600] global_step=2600, grad_norm=0.018665248528122902, loss=0.04210348054766655
I0305 10:35:08.461993 140409024018176 logging_writer.py:48] [2700] global_step=2700, grad_norm=0.018503105267882347, loss=0.045497551560401917
I0305 10:35:40.161076 140409007232768 logging_writer.py:48] [2800] global_step=2800, grad_norm=0.019552594050765038, loss=0.04496784135699272
I0305 10:36:11.815587 140409024018176 logging_writer.py:48] [2900] global_step=2900, grad_norm=0.01136059034615755, loss=0.044459689408540726
I0305 10:36:34.328147 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:38:32.692720 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:38:35.743904 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:38:38.673547 140576608098112 submission_runner.py:411] Time since start: 2009.38s, 	Step: 2972, 	{'train/accuracy': 0.988052487373352, 'train/loss': 0.04234116151928902, 'train/mean_average_precision': 0.16346337124655141, 'validation/accuracy': 0.9852432012557983, 'validation/loss': 0.051890794187784195, 'validation/mean_average_precision': 0.15265183395364867, 'validation/num_examples': 43793, 'test/accuracy': 0.9842371940612793, 'test/loss': 0.05496326833963394, 'test/mean_average_precision': 0.15104269674361767, 'test/num_examples': 43793, 'score': 979.5938329696655, 'total_duration': 2009.3818798065186, 'accumulated_submission_time': 979.5938329696655, 'accumulated_eval_time': 1029.594246149063, 'accumulated_logging_time': 0.11414551734924316}
I0305 10:38:38.688313 140408968165120 logging_writer.py:48] [2972] accumulated_eval_time=1029.594246, accumulated_logging_time=0.114146, accumulated_submission_time=979.593833, global_step=2972, preemption_count=0, score=979.593833, test/accuracy=0.984237, test/loss=0.054963, test/mean_average_precision=0.151043, test/num_examples=43793, total_duration=2009.381880, train/accuracy=0.988052, train/loss=0.042341, train/mean_average_precision=0.163463, validation/accuracy=0.985243, validation/loss=0.051891, validation/mean_average_precision=0.152652, validation/num_examples=43793
I0305 10:38:47.836275 140409291962112 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.01611384004354477, loss=0.04246388375759125
I0305 10:39:19.775780 140408968165120 logging_writer.py:48] [3100] global_step=3100, grad_norm=0.010604904033243656, loss=0.041930779814720154
I0305 10:39:51.471581 140409291962112 logging_writer.py:48] [3200] global_step=3200, grad_norm=0.009723874740302563, loss=0.04379313439130783
I0305 10:40:22.876509 140408968165120 logging_writer.py:48] [3300] global_step=3300, grad_norm=0.011847021989524364, loss=0.04265066236257553
I0305 10:40:54.581553 140409291962112 logging_writer.py:48] [3400] global_step=3400, grad_norm=0.012679039500653744, loss=0.04406096786260605
I0305 10:41:26.238180 140408968165120 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.011518175713717937, loss=0.03868749737739563
I0305 10:41:57.727942 140409291962112 logging_writer.py:48] [3600] global_step=3600, grad_norm=0.011132274754345417, loss=0.038447365164756775
I0305 10:42:29.414703 140408968165120 logging_writer.py:48] [3700] global_step=3700, grad_norm=0.01003553718328476, loss=0.040817223489284515
I0305 10:42:38.811007 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:44:39.720494 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:44:42.689846 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:44:45.681090 140576608098112 submission_runner.py:411] Time since start: 2376.39s, 	Step: 3731, 	{'train/accuracy': 0.9881816506385803, 'train/loss': 0.0408075787127018, 'train/mean_average_precision': 0.1910989147009367, 'validation/accuracy': 0.9852667450904846, 'validation/loss': 0.05024158954620361, 'validation/mean_average_precision': 0.1666665480608038, 'validation/num_examples': 43793, 'test/accuracy': 0.9843096137046814, 'test/loss': 0.05325167626142502, 'test/mean_average_precision': 0.16435608165422938, 'test/num_examples': 43793, 'score': 1219.6854343414307, 'total_duration': 2376.389413833618, 'accumulated_submission_time': 1219.6854343414307, 'accumulated_eval_time': 1156.464278936386, 'accumulated_logging_time': 0.14005827903747559}
I0305 10:44:45.696457 140409007232768 logging_writer.py:48] [3731] accumulated_eval_time=1156.464279, accumulated_logging_time=0.140058, accumulated_submission_time=1219.685434, global_step=3731, preemption_count=0, score=1219.685434, test/accuracy=0.984310, test/loss=0.053252, test/mean_average_precision=0.164356, test/num_examples=43793, total_duration=2376.389414, train/accuracy=0.988182, train/loss=0.040808, train/mean_average_precision=0.191099, validation/accuracy=0.985267, validation/loss=0.050242, validation/mean_average_precision=0.166667, validation/num_examples=43793
I0305 10:45:08.108989 140409024018176 logging_writer.py:48] [3800] global_step=3800, grad_norm=0.012210219167172909, loss=0.04310892894864082
I0305 10:45:39.819203 140409007232768 logging_writer.py:48] [3900] global_step=3900, grad_norm=0.022705161944031715, loss=0.04519740864634514
I0305 10:46:11.542440 140409024018176 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.014875881373882294, loss=0.03958050534129143
I0305 10:46:43.570866 140409007232768 logging_writer.py:48] [4100] global_step=4100, grad_norm=0.012138566933572292, loss=0.04014899954199791
I0305 10:47:15.018306 140409024018176 logging_writer.py:48] [4200] global_step=4200, grad_norm=0.014418310485780239, loss=0.04675816744565964
I0305 10:47:46.434301 140409007232768 logging_writer.py:48] [4300] global_step=4300, grad_norm=0.015723370015621185, loss=0.037158168852329254
I0305 10:48:17.905844 140409024018176 logging_writer.py:48] [4400] global_step=4400, grad_norm=0.018405171111226082, loss=0.0409163236618042
I0305 10:48:45.838967 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:50:45.466526 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:50:48.482948 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:50:51.434115 140576608098112 submission_runner.py:411] Time since start: 2742.14s, 	Step: 4491, 	{'train/accuracy': 0.9886085391044617, 'train/loss': 0.038811203092336655, 'train/mean_average_precision': 0.2143681979482724, 'validation/accuracy': 0.985697865486145, 'validation/loss': 0.04873007908463478, 'validation/mean_average_precision': 0.18579148097794465, 'validation/num_examples': 43793, 'test/accuracy': 0.9847784042358398, 'test/loss': 0.051520898938179016, 'test/mean_average_precision': 0.18928726213732827, 'test/num_examples': 43793, 'score': 1459.7971086502075, 'total_duration': 2742.1424379348755, 'accumulated_submission_time': 1459.7971086502075, 'accumulated_eval_time': 1282.059383392334, 'accumulated_logging_time': 0.16614770889282227}
I0305 10:50:51.450327 140409325532928 logging_writer.py:48] [4491] accumulated_eval_time=1282.059383, accumulated_logging_time=0.166148, accumulated_submission_time=1459.797109, global_step=4491, preemption_count=0, score=1459.797109, test/accuracy=0.984778, test/loss=0.051521, test/mean_average_precision=0.189287, test/num_examples=43793, total_duration=2742.142438, train/accuracy=0.988609, train/loss=0.038811, train/mean_average_precision=0.214368, validation/accuracy=0.985698, validation/loss=0.048730, validation/mean_average_precision=0.185791, validation/num_examples=43793
I0305 10:50:54.585397 140514577852160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.008281799033284187, loss=0.035406533628702164
I0305 10:51:26.146836 140409325532928 logging_writer.py:48] [4600] global_step=4600, grad_norm=0.014743689447641373, loss=0.04196110740303993
I0305 10:51:58.023877 140514577852160 logging_writer.py:48] [4700] global_step=4700, grad_norm=0.012739566154778004, loss=0.03716655820608139
I0305 10:52:29.570308 140409325532928 logging_writer.py:48] [4800] global_step=4800, grad_norm=0.011105230078101158, loss=0.04026885703206062
I0305 10:53:01.011551 140514577852160 logging_writer.py:48] [4900] global_step=4900, grad_norm=0.02047010324895382, loss=0.03568148985505104
I0305 10:53:32.391679 140409325532928 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.013109330087900162, loss=0.03701921924948692
I0305 10:54:04.029532 140514577852160 logging_writer.py:48] [5100] global_step=5100, grad_norm=0.012750775553286076, loss=0.03750687465071678
I0305 10:54:35.688491 140409325532928 logging_writer.py:48] [5200] global_step=5200, grad_norm=0.018333282321691513, loss=0.03942663595080376
I0305 10:54:51.666120 140576608098112 spec.py:321] Evaluating on the training split.
I0305 10:56:55.133458 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 10:56:58.641021 140576608098112 spec.py:349] Evaluating on the test split.
I0305 10:57:02.016671 140576608098112 submission_runner.py:411] Time since start: 3112.72s, 	Step: 5251, 	{'train/accuracy': 0.9887627959251404, 'train/loss': 0.03807639703154564, 'train/mean_average_precision': 0.24717780397379124, 'validation/accuracy': 0.9855229258537292, 'validation/loss': 0.04804953187704086, 'validation/mean_average_precision': 0.19638457633918352, 'validation/num_examples': 43793, 'test/accuracy': 0.9847156405448914, 'test/loss': 0.050624020397663116, 'test/mean_average_precision': 0.20099490600409428, 'test/num_examples': 43793, 'score': 1699.9812216758728, 'total_duration': 3112.7249789237976, 'accumulated_submission_time': 1699.9812216758728, 'accumulated_eval_time': 1412.4098732471466, 'accumulated_logging_time': 0.1936049461364746}
I0305 10:57:02.038709 140415782377216 logging_writer.py:48] [5251] accumulated_eval_time=1412.409873, accumulated_logging_time=0.193605, accumulated_submission_time=1699.981222, global_step=5251, preemption_count=0, score=1699.981222, test/accuracy=0.984716, test/loss=0.050624, test/mean_average_precision=0.200995, test/num_examples=43793, total_duration=3112.724979, train/accuracy=0.988763, train/loss=0.038076, train/mean_average_precision=0.247178, validation/accuracy=0.985523, validation/loss=0.048050, validation/mean_average_precision=0.196385, validation/num_examples=43793
I0305 10:57:18.209370 140415790769920 logging_writer.py:48] [5300] global_step=5300, grad_norm=0.014067859388887882, loss=0.03661409020423889
I0305 10:57:49.634839 140415782377216 logging_writer.py:48] [5400] global_step=5400, grad_norm=0.009936525486409664, loss=0.036215413361787796
I0305 10:58:21.770870 140415790769920 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.011091537773609161, loss=0.04224493354558945
I0305 10:58:53.548452 140415782377216 logging_writer.py:48] [5600] global_step=5600, grad_norm=0.014835397712886333, loss=0.03955318406224251
I0305 10:59:25.384462 140415790769920 logging_writer.py:48] [5700] global_step=5700, grad_norm=0.014413313008844852, loss=0.03922561928629875
I0305 10:59:56.853468 140415782377216 logging_writer.py:48] [5800] global_step=5800, grad_norm=0.009702097624540329, loss=0.03961841017007828
I0305 11:00:28.337287 140415790769920 logging_writer.py:48] [5900] global_step=5900, grad_norm=0.01362769678235054, loss=0.04201853275299072
I0305 11:01:00.095609 140415782377216 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.010958779603242874, loss=0.03476112335920334
I0305 11:01:02.133184 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:03:03.463299 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:03:06.559375 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:03:09.493168 140576608098112 submission_runner.py:411] Time since start: 3480.20s, 	Step: 6007, 	{'train/accuracy': 0.9892771244049072, 'train/loss': 0.03653166815638542, 'train/mean_average_precision': 0.2702459337040197, 'validation/accuracy': 0.9860550761222839, 'validation/loss': 0.046674393117427826, 'validation/mean_average_precision': 0.22053794758307177, 'validation/num_examples': 43793, 'test/accuracy': 0.9851406216621399, 'test/loss': 0.04936806112527847, 'test/mean_average_precision': 0.21638141318882773, 'test/num_examples': 43793, 'score': 1940.042201757431, 'total_duration': 3480.2014927864075, 'accumulated_submission_time': 1940.042201757431, 'accumulated_eval_time': 1539.7698109149933, 'accumulated_logging_time': 0.22908258438110352}
I0305 11:03:09.509157 140409325532928 logging_writer.py:48] [6007] accumulated_eval_time=1539.769811, accumulated_logging_time=0.229083, accumulated_submission_time=1940.042202, global_step=6007, preemption_count=0, score=1940.042202, test/accuracy=0.985141, test/loss=0.049368, test/mean_average_precision=0.216381, test/num_examples=43793, total_duration=3480.201493, train/accuracy=0.989277, train/loss=0.036532, train/mean_average_precision=0.270246, validation/accuracy=0.986055, validation/loss=0.046674, validation/mean_average_precision=0.220538, validation/num_examples=43793
I0305 11:03:39.519500 140514586244864 logging_writer.py:48] [6100] global_step=6100, grad_norm=0.013831773772835732, loss=0.03621227294206619
I0305 11:04:11.436463 140409325532928 logging_writer.py:48] [6200] global_step=6200, grad_norm=0.012907646596431732, loss=0.03683846443891525
I0305 11:04:43.326238 140514586244864 logging_writer.py:48] [6300] global_step=6300, grad_norm=0.017017444595694542, loss=0.03931703045964241
I0305 11:05:15.656794 140409325532928 logging_writer.py:48] [6400] global_step=6400, grad_norm=0.016929518431425095, loss=0.04010206088423729
I0305 11:05:48.344084 140514586244864 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.014949970878660679, loss=0.037511684000492096
I0305 11:06:20.637589 140409325532928 logging_writer.py:48] [6600] global_step=6600, grad_norm=0.017933325842022896, loss=0.03936130553483963
I0305 11:06:52.471173 140514586244864 logging_writer.py:48] [6700] global_step=6700, grad_norm=0.013554265722632408, loss=0.03296085074543953
I0305 11:07:09.740744 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:09:13.328909 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:09:16.354708 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:09:19.351933 140576608098112 submission_runner.py:411] Time since start: 3850.06s, 	Step: 6754, 	{'train/accuracy': 0.9893851280212402, 'train/loss': 0.03585827350616455, 'train/mean_average_precision': 0.2830660872122692, 'validation/accuracy': 0.9861866235733032, 'validation/loss': 0.04666838049888611, 'validation/mean_average_precision': 0.21843254133428266, 'validation/num_examples': 43793, 'test/accuracy': 0.985238790512085, 'test/loss': 0.04943390190601349, 'test/mean_average_precision': 0.2141966573978142, 'test/num_examples': 43793, 'score': 2180.241662979126, 'total_duration': 3850.060249567032, 'accumulated_submission_time': 2180.241662979126, 'accumulated_eval_time': 1669.380942583084, 'accumulated_logging_time': 0.25618600845336914}
I0305 11:09:19.367952 140415782377216 logging_writer.py:48] [6754] accumulated_eval_time=1669.380943, accumulated_logging_time=0.256186, accumulated_submission_time=2180.241663, global_step=6754, preemption_count=0, score=2180.241663, test/accuracy=0.985239, test/loss=0.049434, test/mean_average_precision=0.214197, test/num_examples=43793, total_duration=3850.060250, train/accuracy=0.989385, train/loss=0.035858, train/mean_average_precision=0.283066, validation/accuracy=0.986187, validation/loss=0.046668, validation/mean_average_precision=0.218433, validation/num_examples=43793
I0305 11:09:34.567721 140514577852160 logging_writer.py:48] [6800] global_step=6800, grad_norm=0.014867359772324562, loss=0.03765863552689552
I0305 11:10:06.997107 140415782377216 logging_writer.py:48] [6900] global_step=6900, grad_norm=0.016852781176567078, loss=0.037515003234148026
I0305 11:10:39.151904 140514577852160 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.017276393249630928, loss=0.03955468907952309
I0305 11:11:11.243039 140415782377216 logging_writer.py:48] [7100] global_step=7100, grad_norm=0.01267259381711483, loss=0.03983427584171295
I0305 11:11:43.335653 140514577852160 logging_writer.py:48] [7200] global_step=7200, grad_norm=0.015313121490180492, loss=0.036409515887498856
I0305 11:12:15.181984 140415782377216 logging_writer.py:48] [7300] global_step=7300, grad_norm=0.02619240991771221, loss=0.03761405870318413
I0305 11:12:47.112569 140514577852160 logging_writer.py:48] [7400] global_step=7400, grad_norm=0.014376548118889332, loss=0.04086313769221306
I0305 11:13:19.108031 140415782377216 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.018792610615491867, loss=0.038109686225652695
I0305 11:13:19.429085 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:15:16.423492 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:15:19.504108 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:15:22.556395 140576608098112 submission_runner.py:411] Time since start: 4213.26s, 	Step: 7502, 	{'train/accuracy': 0.9895324110984802, 'train/loss': 0.035286109894514084, 'train/mean_average_precision': 0.3089379075079039, 'validation/accuracy': 0.9861618280410767, 'validation/loss': 0.045740630477666855, 'validation/mean_average_precision': 0.2240028748716719, 'validation/num_examples': 43793, 'test/accuracy': 0.9853314161300659, 'test/loss': 0.04817408695816994, 'test/mean_average_precision': 0.22259951185935545, 'test/num_examples': 43793, 'score': 2420.270122051239, 'total_duration': 4213.264726400375, 'accumulated_submission_time': 2420.270122051239, 'accumulated_eval_time': 1792.50821018219, 'accumulated_logging_time': 0.2849104404449463}
I0305 11:15:22.573400 140415790769920 logging_writer.py:48] [7502] accumulated_eval_time=1792.508210, accumulated_logging_time=0.284910, accumulated_submission_time=2420.270122, global_step=7502, preemption_count=0, score=2420.270122, test/accuracy=0.985331, test/loss=0.048174, test/mean_average_precision=0.222600, test/num_examples=43793, total_duration=4213.264726, train/accuracy=0.989532, train/loss=0.035286, train/mean_average_precision=0.308938, validation/accuracy=0.986162, validation/loss=0.045741, validation/mean_average_precision=0.224003, validation/num_examples=43793
I0305 11:15:53.871425 140514586244864 logging_writer.py:48] [7600] global_step=7600, grad_norm=0.01373959518969059, loss=0.04007493704557419
I0305 11:16:25.646213 140415790769920 logging_writer.py:48] [7700] global_step=7700, grad_norm=0.01335336733609438, loss=0.03755282238125801
I0305 11:16:57.451020 140514586244864 logging_writer.py:48] [7800] global_step=7800, grad_norm=0.017298927530646324, loss=0.04022233188152313
I0305 11:17:29.584544 140415790769920 logging_writer.py:48] [7900] global_step=7900, grad_norm=0.015531247481703758, loss=0.03385305404663086
I0305 11:18:01.180610 140514586244864 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.014315586537122726, loss=0.03268981724977493
I0305 11:18:33.486321 140415790769920 logging_writer.py:48] [8100] global_step=8100, grad_norm=0.024891354143619537, loss=0.04010079428553581
I0305 11:19:06.715392 140514586244864 logging_writer.py:48] [8200] global_step=8200, grad_norm=0.028276817873120308, loss=0.03451121598482132
I0305 11:19:22.842778 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:21:24.293711 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:21:27.339430 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:21:30.312416 140576608098112 submission_runner.py:411] Time since start: 4581.02s, 	Step: 8252, 	{'train/accuracy': 0.9896116852760315, 'train/loss': 0.03484518453478813, 'train/mean_average_precision': 0.2959481314296294, 'validation/accuracy': 0.9864756464958191, 'validation/loss': 0.04530481621623039, 'validation/mean_average_precision': 0.23308276045208356, 'validation/num_examples': 43793, 'test/accuracy': 0.9855239391326904, 'test/loss': 0.04810308292508125, 'test/mean_average_precision': 0.22935709963216575, 'test/num_examples': 43793, 'score': 2660.5069646835327, 'total_duration': 4581.020742177963, 'accumulated_submission_time': 2660.5069646835327, 'accumulated_eval_time': 1919.977798461914, 'accumulated_logging_time': 0.31324124336242676}
I0305 11:21:30.328694 140409325532928 logging_writer.py:48] [8252] accumulated_eval_time=1919.977798, accumulated_logging_time=0.313241, accumulated_submission_time=2660.506965, global_step=8252, preemption_count=0, score=2660.506965, test/accuracy=0.985524, test/loss=0.048103, test/mean_average_precision=0.229357, test/num_examples=43793, total_duration=4581.020742, train/accuracy=0.989612, train/loss=0.034845, train/mean_average_precision=0.295948, validation/accuracy=0.986476, validation/loss=0.045305, validation/mean_average_precision=0.233083, validation/num_examples=43793
I0305 11:21:45.848972 140415782377216 logging_writer.py:48] [8300] global_step=8300, grad_norm=0.019652709364891052, loss=0.03356414660811424
I0305 11:22:17.675632 140409325532928 logging_writer.py:48] [8400] global_step=8400, grad_norm=0.015834731981158257, loss=0.03719675913453102
I0305 11:22:49.675470 140415782377216 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.02305592969059944, loss=0.0345422625541687
I0305 11:23:21.264128 140409325532928 logging_writer.py:48] [8600] global_step=8600, grad_norm=0.029696444049477577, loss=0.03678815811872482
I0305 11:23:53.142843 140415782377216 logging_writer.py:48] [8700] global_step=8700, grad_norm=0.01674533821642399, loss=0.03259333595633507
I0305 11:24:24.987609 140409325532928 logging_writer.py:48] [8800] global_step=8800, grad_norm=0.017136909067630768, loss=0.03406341373920441
I0305 11:24:56.493639 140415782377216 logging_writer.py:48] [8900] global_step=8900, grad_norm=0.017627915367484093, loss=0.03522175922989845
I0305 11:25:29.663084 140409325532928 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.02040727436542511, loss=0.03790874034166336
I0305 11:25:30.394221 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:27:34.691801 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:27:37.773679 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:27:40.742114 140576608098112 submission_runner.py:411] Time since start: 4951.45s, 	Step: 9003, 	{'train/accuracy': 0.9896876811981201, 'train/loss': 0.03449136018753052, 'train/mean_average_precision': 0.3023390847822264, 'validation/accuracy': 0.9864464402198792, 'validation/loss': 0.045672327280044556, 'validation/mean_average_precision': 0.2355386945639433, 'validation/num_examples': 43793, 'test/accuracy': 0.9855251908302307, 'test/loss': 0.048323314636945724, 'test/mean_average_precision': 0.24166668390014487, 'test/num_examples': 43793, 'score': 2900.5411689281464, 'total_duration': 4951.450447320938, 'accumulated_submission_time': 2900.5411689281464, 'accumulated_eval_time': 2050.3256623744965, 'accumulated_logging_time': 0.3406836986541748}
I0305 11:27:40.758821 140415790769920 logging_writer.py:48] [9003] accumulated_eval_time=2050.325662, accumulated_logging_time=0.340684, accumulated_submission_time=2900.541169, global_step=9003, preemption_count=0, score=2900.541169, test/accuracy=0.985525, test/loss=0.048323, test/mean_average_precision=0.241667, test/num_examples=43793, total_duration=4951.450447, train/accuracy=0.989688, train/loss=0.034491, train/mean_average_precision=0.302339, validation/accuracy=0.986446, validation/loss=0.045672, validation/mean_average_precision=0.235539, validation/num_examples=43793
I0305 11:28:14.040473 140514586244864 logging_writer.py:48] [9100] global_step=9100, grad_norm=0.020687559619545937, loss=0.0387064665555954
I0305 11:28:45.804516 140415790769920 logging_writer.py:48] [9200] global_step=9200, grad_norm=0.01593802496790886, loss=0.032560188323259354
I0305 11:29:18.536669 140514586244864 logging_writer.py:48] [9300] global_step=9300, grad_norm=0.02583964727818966, loss=0.035187121480703354
I0305 11:29:52.028441 140415790769920 logging_writer.py:48] [9400] global_step=9400, grad_norm=0.018991362303495407, loss=0.03559945151209831
I0305 11:30:24.756325 140514586244864 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.015270545147359371, loss=0.03389802947640419
I0305 11:30:57.064633 140415790769920 logging_writer.py:48] [9600] global_step=9600, grad_norm=0.024726809933781624, loss=0.030358677729964256
I0305 11:31:29.128226 140514586244864 logging_writer.py:48] [9700] global_step=9700, grad_norm=0.021257445216178894, loss=0.03491465374827385
I0305 11:31:40.809937 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:33:41.074215 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:33:44.202389 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:33:47.164249 140576608098112 submission_runner.py:411] Time since start: 5317.87s, 	Step: 9738, 	{'train/accuracy': 0.9900109171867371, 'train/loss': 0.033156365156173706, 'train/mean_average_precision': 0.33917339732084917, 'validation/accuracy': 0.9865714311599731, 'validation/loss': 0.04498814046382904, 'validation/mean_average_precision': 0.24180298509194895, 'validation/num_examples': 43793, 'test/accuracy': 0.9857316017150879, 'test/loss': 0.047628626227378845, 'test/mean_average_precision': 0.2443877526841293, 'test/num_examples': 43793, 'score': 3140.5600502490997, 'total_duration': 5317.872577667236, 'accumulated_submission_time': 3140.5600502490997, 'accumulated_eval_time': 2176.679932117462, 'accumulated_logging_time': 0.3683798313140869}
I0305 11:33:47.181061 140409325532928 logging_writer.py:48] [9738] accumulated_eval_time=2176.679932, accumulated_logging_time=0.368380, accumulated_submission_time=3140.560050, global_step=9738, preemption_count=0, score=3140.560050, test/accuracy=0.985732, test/loss=0.047629, test/mean_average_precision=0.244388, test/num_examples=43793, total_duration=5317.872578, train/accuracy=0.990011, train/loss=0.033156, train/mean_average_precision=0.339173, validation/accuracy=0.986571, validation/loss=0.044988, validation/mean_average_precision=0.241803, validation/num_examples=43793
I0305 11:34:07.635936 140415782377216 logging_writer.py:48] [9800] global_step=9800, grad_norm=0.019524838775396347, loss=0.03294609114527702
I0305 11:34:40.346934 140409325532928 logging_writer.py:48] [9900] global_step=9900, grad_norm=0.01997731253504753, loss=0.029488500207662582
I0305 11:35:12.972050 140415782377216 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.019072972238063812, loss=0.03247839957475662
I0305 11:35:45.021009 140409325532928 logging_writer.py:48] [10100] global_step=10100, grad_norm=0.022847777232527733, loss=0.04138489440083504
I0305 11:36:17.283582 140415782377216 logging_writer.py:48] [10200] global_step=10200, grad_norm=0.02280368097126484, loss=0.0372580923140049
I0305 11:36:49.243335 140409325532928 logging_writer.py:48] [10300] global_step=10300, grad_norm=0.02265327423810959, loss=0.03609200194478035
I0305 11:37:21.568500 140415782377216 logging_writer.py:48] [10400] global_step=10400, grad_norm=0.023600205779075623, loss=0.03364771977066994
I0305 11:37:47.253303 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:39:51.701149 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:39:54.800182 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:39:57.804713 140576608098112 submission_runner.py:411] Time since start: 5688.51s, 	Step: 10481, 	{'train/accuracy': 0.9901197552680969, 'train/loss': 0.03296390920877457, 'train/mean_average_precision': 0.3427990933394376, 'validation/accuracy': 0.9865767359733582, 'validation/loss': 0.04451901838183403, 'validation/mean_average_precision': 0.2523031039881308, 'validation/num_examples': 43793, 'test/accuracy': 0.9857054352760315, 'test/loss': 0.04737091064453125, 'test/mean_average_precision': 0.24807122514977753, 'test/num_examples': 43793, 'score': 3380.5997710227966, 'total_duration': 5688.5130405426025, 'accumulated_submission_time': 3380.5997710227966, 'accumulated_eval_time': 2307.231298685074, 'accumulated_logging_time': 0.39751648902893066}
I0305 11:39:57.822418 140415790769920 logging_writer.py:48] [10481] accumulated_eval_time=2307.231299, accumulated_logging_time=0.397516, accumulated_submission_time=3380.599771, global_step=10481, preemption_count=0, score=3380.599771, test/accuracy=0.985705, test/loss=0.047371, test/mean_average_precision=0.248071, test/num_examples=43793, total_duration=5688.513041, train/accuracy=0.990120, train/loss=0.032964, train/mean_average_precision=0.342799, validation/accuracy=0.986577, validation/loss=0.044519, validation/mean_average_precision=0.252303, validation/num_examples=43793
I0305 11:40:04.602512 140514577852160 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.023924676701426506, loss=0.03502284735441208
I0305 11:40:36.606716 140415790769920 logging_writer.py:48] [10600] global_step=10600, grad_norm=0.026141677051782608, loss=0.037900250405073166
I0305 11:41:08.641048 140514577852160 logging_writer.py:48] [10700] global_step=10700, grad_norm=0.023289667442440987, loss=0.03491953760385513
I0305 11:41:40.839895 140415790769920 logging_writer.py:48] [10800] global_step=10800, grad_norm=0.028320718556642532, loss=0.03397362306714058
I0305 11:42:12.773787 140514577852160 logging_writer.py:48] [10900] global_step=10900, grad_norm=0.020257167518138885, loss=0.03257067874073982
I0305 11:42:44.775774 140415790769920 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.024104854092001915, loss=0.03234683349728584
I0305 11:43:17.025625 140514577852160 logging_writer.py:48] [11100] global_step=11100, grad_norm=0.025206491351127625, loss=0.03652665764093399
I0305 11:43:49.232753 140415790769920 logging_writer.py:48] [11200] global_step=11200, grad_norm=0.020701196044683456, loss=0.03405970707535744
I0305 11:43:57.851589 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:46:01.251389 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:46:04.746457 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:46:08.055576 140576608098112 submission_runner.py:411] Time since start: 6058.76s, 	Step: 11228, 	{'train/accuracy': 0.9903004765510559, 'train/loss': 0.032292574644088745, 'train/mean_average_precision': 0.3670847887656891, 'validation/accuracy': 0.9866266250610352, 'validation/loss': 0.04465373605489731, 'validation/mean_average_precision': 0.256547329570424, 'validation/num_examples': 43793, 'test/accuracy': 0.9857454895973206, 'test/loss': 0.0474274680018425, 'test/mean_average_precision': 0.24291461831760397, 'test/num_examples': 43793, 'score': 3620.5970821380615, 'total_duration': 6058.763890743256, 'accumulated_submission_time': 3620.5970821380615, 'accumulated_eval_time': 2437.435226202011, 'accumulated_logging_time': 0.4268772602081299}
I0305 11:46:08.074875 140415782377216 logging_writer.py:48] [11228] accumulated_eval_time=2437.435226, accumulated_logging_time=0.426877, accumulated_submission_time=3620.597082, global_step=11228, preemption_count=0, score=3620.597082, test/accuracy=0.985745, test/loss=0.047427, test/mean_average_precision=0.242915, test/num_examples=43793, total_duration=6058.763891, train/accuracy=0.990300, train/loss=0.032293, train/mean_average_precision=0.367085, validation/accuracy=0.986627, validation/loss=0.044654, validation/mean_average_precision=0.256547, validation/num_examples=43793
I0305 11:46:32.695868 140514586244864 logging_writer.py:48] [11300] global_step=11300, grad_norm=0.022960972040891647, loss=0.03476664796471596
I0305 11:47:05.143203 140415782377216 logging_writer.py:48] [11400] global_step=11400, grad_norm=0.024858707562088966, loss=0.03199990093708038
I0305 11:47:37.559994 140514586244864 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.025958256796002388, loss=0.03295302391052246
I0305 11:48:10.098182 140415782377216 logging_writer.py:48] [11600] global_step=11600, grad_norm=0.024222884327173233, loss=0.029755128547549248
I0305 11:48:43.059181 140514586244864 logging_writer.py:48] [11700] global_step=11700, grad_norm=0.029143134132027626, loss=0.035057976841926575
I0305 11:49:15.931278 140415782377216 logging_writer.py:48] [11800] global_step=11800, grad_norm=0.023344561457633972, loss=0.036948494613170624
I0305 11:49:48.032135 140514586244864 logging_writer.py:48] [11900] global_step=11900, grad_norm=0.024155713617801666, loss=0.03238591551780701
I0305 11:50:08.205468 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:52:16.857368 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:52:21.067658 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:52:24.320200 140576608098112 submission_runner.py:411] Time since start: 6435.03s, 	Step: 11964, 	{'train/accuracy': 0.990463137626648, 'train/loss': 0.03144628927111626, 'train/mean_average_precision': 0.39242784829205135, 'validation/accuracy': 0.9866583347320557, 'validation/loss': 0.04461675509810448, 'validation/mean_average_precision': 0.25977705759605496, 'validation/num_examples': 43793, 'test/accuracy': 0.9857821464538574, 'test/loss': 0.0473833791911602, 'test/mean_average_precision': 0.2490345214654525, 'test/num_examples': 43793, 'score': 3860.6927313804626, 'total_duration': 6435.028518915176, 'accumulated_submission_time': 3860.6927313804626, 'accumulated_eval_time': 2573.5499074459076, 'accumulated_logging_time': 0.45883870124816895}
I0305 11:52:24.340438 140408968165120 logging_writer.py:48] [11964] accumulated_eval_time=2573.549907, accumulated_logging_time=0.458839, accumulated_submission_time=3860.692731, global_step=11964, preemption_count=0, score=3860.692731, test/accuracy=0.985782, test/loss=0.047383, test/mean_average_precision=0.249035, test/num_examples=43793, total_duration=6435.028519, train/accuracy=0.990463, train/loss=0.031446, train/mean_average_precision=0.392428, validation/accuracy=0.986658, validation/loss=0.044617, validation/mean_average_precision=0.259777, validation/num_examples=43793
I0305 11:52:36.339163 140415790769920 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.02818826027214527, loss=0.034065667539834976
I0305 11:53:08.716336 140408968165120 logging_writer.py:48] [12100] global_step=12100, grad_norm=0.029530292376875877, loss=0.03539951890707016
I0305 11:53:41.028260 140415790769920 logging_writer.py:48] [12200] global_step=12200, grad_norm=0.028059236705303192, loss=0.03338075801730156
I0305 11:54:13.645833 140408968165120 logging_writer.py:48] [12300] global_step=12300, grad_norm=0.026381729170680046, loss=0.030423322692513466
I0305 11:54:45.967922 140415790769920 logging_writer.py:48] [12400] global_step=12400, grad_norm=0.020731687545776367, loss=0.03161688894033432
I0305 11:55:18.338836 140408968165120 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.03384843096137047, loss=0.0340527705848217
I0305 11:55:50.184287 140415790769920 logging_writer.py:48] [12600] global_step=12600, grad_norm=0.026957547292113304, loss=0.034945692867040634
I0305 11:56:22.580542 140408968165120 logging_writer.py:48] [12700] global_step=12700, grad_norm=0.02724381536245346, loss=0.033796001225709915
I0305 11:56:24.503402 140576608098112 spec.py:321] Evaluating on the training split.
I0305 11:58:28.575382 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 11:58:31.596253 140576608098112 spec.py:349] Evaluating on the test split.
I0305 11:58:34.608150 140576608098112 submission_runner.py:411] Time since start: 6805.32s, 	Step: 12707, 	{'train/accuracy': 0.9905829429626465, 'train/loss': 0.030994145199656487, 'train/mean_average_precision': 0.38798652387881905, 'validation/accuracy': 0.9866992831230164, 'validation/loss': 0.04457744210958481, 'validation/mean_average_precision': 0.26299673542100505, 'validation/num_examples': 43793, 'test/accuracy': 0.9858554005622864, 'test/loss': 0.047482188791036606, 'test/mean_average_precision': 0.2588720984792856, 'test/num_examples': 43793, 'score': 4100.820775270462, 'total_duration': 6805.316474914551, 'accumulated_submission_time': 4100.820775270462, 'accumulated_eval_time': 2703.6546020507812, 'accumulated_logging_time': 0.4913444519042969}
I0305 11:58:34.626496 140415782377216 logging_writer.py:48] [12707] accumulated_eval_time=2703.654602, accumulated_logging_time=0.491344, accumulated_submission_time=4100.820775, global_step=12707, preemption_count=0, score=4100.820775, test/accuracy=0.985855, test/loss=0.047482, test/mean_average_precision=0.258872, test/num_examples=43793, total_duration=6805.316475, train/accuracy=0.990583, train/loss=0.030994, train/mean_average_precision=0.387987, validation/accuracy=0.986699, validation/loss=0.044577, validation/mean_average_precision=0.262997, validation/num_examples=43793
I0305 11:59:05.450658 140514586244864 logging_writer.py:48] [12800] global_step=12800, grad_norm=0.02912207692861557, loss=0.03473480045795441
I0305 11:59:37.496609 140415782377216 logging_writer.py:48] [12900] global_step=12900, grad_norm=0.02750661037862301, loss=0.03418482095003128
I0305 12:00:09.425121 140514586244864 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.025699781253933907, loss=0.030459308996796608
I0305 12:00:41.296939 140415782377216 logging_writer.py:48] [13100] global_step=13100, grad_norm=0.023714600130915642, loss=0.0310653243213892
I0305 12:01:13.649338 140514586244864 logging_writer.py:48] [13200] global_step=13200, grad_norm=0.030174802988767624, loss=0.034326810389757156
I0305 12:01:45.914738 140415782377216 logging_writer.py:48] [13300] global_step=13300, grad_norm=0.03029705211520195, loss=0.034337036311626434
I0305 12:02:18.463470 140514586244864 logging_writer.py:48] [13400] global_step=13400, grad_norm=0.025604620575904846, loss=0.029890554025769234
I0305 12:02:34.744102 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:04:39.739909 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:04:42.799252 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:04:45.798011 140576608098112 submission_runner.py:411] Time since start: 7176.51s, 	Step: 13452, 	{'train/accuracy': 0.9906689524650574, 'train/loss': 0.03070739284157753, 'train/mean_average_precision': 0.3913719672774121, 'validation/accuracy': 0.9868081212043762, 'validation/loss': 0.044262323528528214, 'validation/mean_average_precision': 0.26061023076219003, 'validation/num_examples': 43793, 'test/accuracy': 0.9859215617179871, 'test/loss': 0.04704206436872482, 'test/mean_average_precision': 0.25743542197444186, 'test/num_examples': 43793, 'score': 4340.906958580017, 'total_duration': 7176.506344079971, 'accumulated_submission_time': 4340.906958580017, 'accumulated_eval_time': 2834.7084777355194, 'accumulated_logging_time': 0.521092414855957}
I0305 12:04:45.815794 140408968165120 logging_writer.py:48] [13452] accumulated_eval_time=2834.708478, accumulated_logging_time=0.521092, accumulated_submission_time=4340.906959, global_step=13452, preemption_count=0, score=4340.906959, test/accuracy=0.985922, test/loss=0.047042, test/mean_average_precision=0.257435, test/num_examples=43793, total_duration=7176.506344, train/accuracy=0.990669, train/loss=0.030707, train/mean_average_precision=0.391372, validation/accuracy=0.986808, validation/loss=0.044262, validation/mean_average_precision=0.260610, validation/num_examples=43793
I0305 12:05:01.599022 140409291962112 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.02440738119184971, loss=0.03595919907093048
I0305 12:05:33.428821 140408968165120 logging_writer.py:48] [13600] global_step=13600, grad_norm=0.028844496235251427, loss=0.030375702306628227
I0305 12:06:06.282620 140409291962112 logging_writer.py:48] [13700] global_step=13700, grad_norm=0.02714839018881321, loss=0.02992931567132473
I0305 12:06:38.246190 140408968165120 logging_writer.py:48] [13800] global_step=13800, grad_norm=0.023077279329299927, loss=0.027971068397164345
I0305 12:07:10.664308 140409291962112 logging_writer.py:48] [13900] global_step=13900, grad_norm=0.045609667897224426, loss=0.03580055385828018
I0305 12:07:42.663010 140408968165120 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.04007450118660927, loss=0.0335618257522583
I0305 12:08:14.652974 140409291962112 logging_writer.py:48] [14100] global_step=14100, grad_norm=0.03752300888299942, loss=0.033179689198732376
I0305 12:08:46.099451 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:10:46.789029 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:10:49.816712 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:10:52.805668 140576608098112 submission_runner.py:411] Time since start: 7543.51s, 	Step: 14200, 	{'train/accuracy': 0.9906706213951111, 'train/loss': 0.030850909650325775, 'train/mean_average_precision': 0.3912459024790902, 'validation/accuracy': 0.9867467880249023, 'validation/loss': 0.04410111531615257, 'validation/mean_average_precision': 0.2619010691977937, 'validation/num_examples': 43793, 'test/accuracy': 0.985874354839325, 'test/loss': 0.04691276326775551, 'test/mean_average_precision': 0.2512387055784661, 'test/num_examples': 43793, 'score': 4581.159161567688, 'total_duration': 7543.514000415802, 'accumulated_submission_time': 4581.159161567688, 'accumulated_eval_time': 2961.4146535396576, 'accumulated_logging_time': 0.5505294799804688}
I0305 12:10:52.823848 140415790769920 logging_writer.py:48] [14200] accumulated_eval_time=2961.414654, accumulated_logging_time=0.550529, accumulated_submission_time=4581.159162, global_step=14200, preemption_count=0, score=4581.159162, test/accuracy=0.985874, test/loss=0.046913, test/mean_average_precision=0.251239, test/num_examples=43793, total_duration=7543.514000, train/accuracy=0.990671, train/loss=0.030851, train/mean_average_precision=0.391246, validation/accuracy=0.986747, validation/loss=0.044101, validation/mean_average_precision=0.261901, validation/num_examples=43793
I0305 12:10:53.187454 140514586244864 logging_writer.py:48] [14200] global_step=14200, grad_norm=0.03575867787003517, loss=0.02899700216948986
I0305 12:11:25.779739 140415790769920 logging_writer.py:48] [14300] global_step=14300, grad_norm=0.02752663567662239, loss=0.034761179238557816
I0305 12:11:57.730718 140514586244864 logging_writer.py:48] [14400] global_step=14400, grad_norm=0.04043193161487579, loss=0.03593200072646141
I0305 12:12:30.043999 140415790769920 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.03811104595661163, loss=0.031345490366220474
I0305 12:13:02.517860 140514586244864 logging_writer.py:48] [14600] global_step=14600, grad_norm=0.02533956617116928, loss=0.030273571610450745
I0305 12:13:34.693800 140415790769920 logging_writer.py:48] [14700] global_step=14700, grad_norm=0.04530024155974388, loss=0.03333157300949097
I0305 12:14:06.911300 140514586244864 logging_writer.py:48] [14800] global_step=14800, grad_norm=0.03810926526784897, loss=0.03159630671143532
I0305 12:14:39.078153 140415790769920 logging_writer.py:48] [14900] global_step=14900, grad_norm=0.03147124871611595, loss=0.031624410301446915
I0305 12:14:52.893187 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:16:57.922498 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:17:01.373118 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:17:04.453058 140576608098112 submission_runner.py:411] Time since start: 7915.16s, 	Step: 14944, 	{'train/accuracy': 0.9905703663825989, 'train/loss': 0.031081784516572952, 'train/mean_average_precision': 0.38299997404297703, 'validation/accuracy': 0.9868085384368896, 'validation/loss': 0.044543128460645676, 'validation/mean_average_precision': 0.2633013852647012, 'validation/num_examples': 43793, 'test/accuracy': 0.9859371185302734, 'test/loss': 0.04730701074004173, 'test/mean_average_precision': 0.25924203435652926, 'test/num_examples': 43793, 'score': 4821.198109149933, 'total_duration': 7915.161383390427, 'accumulated_submission_time': 4821.198109149933, 'accumulated_eval_time': 3092.9744811058044, 'accumulated_logging_time': 0.5795705318450928}
I0305 12:17:04.470832 140408968165120 logging_writer.py:48] [14944] accumulated_eval_time=3092.974481, accumulated_logging_time=0.579571, accumulated_submission_time=4821.198109, global_step=14944, preemption_count=0, score=4821.198109, test/accuracy=0.985937, test/loss=0.047307, test/mean_average_precision=0.259242, test/num_examples=43793, total_duration=7915.161383, train/accuracy=0.990570, train/loss=0.031082, train/mean_average_precision=0.383000, validation/accuracy=0.986809, validation/loss=0.044543, validation/mean_average_precision=0.263301, validation/num_examples=43793
I0305 12:17:22.610280 140415782377216 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.02523409202694893, loss=0.029617715626955032
I0305 12:17:54.501553 140408968165120 logging_writer.py:48] [15100] global_step=15100, grad_norm=0.048977479338645935, loss=0.0322435237467289
I0305 12:18:26.747772 140415782377216 logging_writer.py:48] [15200] global_step=15200, grad_norm=0.027312153950333595, loss=0.027449315413832664
I0305 12:18:59.035030 140408968165120 logging_writer.py:48] [15300] global_step=15300, grad_norm=0.035874269902706146, loss=0.035767488181591034
I0305 12:19:31.150094 140415782377216 logging_writer.py:48] [15400] global_step=15400, grad_norm=0.04182958975434303, loss=0.039079699665308
I0305 12:20:03.115780 140408968165120 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0334823802113533, loss=0.03425339236855507
I0305 12:20:35.290181 140415782377216 logging_writer.py:48] [15600] global_step=15600, grad_norm=0.04391854628920555, loss=0.03273862227797508
I0305 12:21:04.735345 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:23:08.619360 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:23:11.699058 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:23:14.755146 140576608098112 submission_runner.py:411] Time since start: 8285.46s, 	Step: 15692, 	{'train/accuracy': 0.9907108545303345, 'train/loss': 0.030502811074256897, 'train/mean_average_precision': 0.41012207133367495, 'validation/accuracy': 0.9868288040161133, 'validation/loss': 0.043967798352241516, 'validation/mean_average_precision': 0.27110705593329865, 'validation/num_examples': 43793, 'test/accuracy': 0.9859729409217834, 'test/loss': 0.04673616960644722, 'test/mean_average_precision': 0.26242948446967257, 'test/num_examples': 43793, 'score': 5061.430529117584, 'total_duration': 8285.463474035263, 'accumulated_submission_time': 5061.430529117584, 'accumulated_eval_time': 3222.9942405223846, 'accumulated_logging_time': 0.6097488403320312}
I0305 12:23:14.773897 140415790769920 logging_writer.py:48] [15692] accumulated_eval_time=3222.994241, accumulated_logging_time=0.609749, accumulated_submission_time=5061.430529, global_step=15692, preemption_count=0, score=5061.430529, test/accuracy=0.985973, test/loss=0.046736, test/mean_average_precision=0.262429, test/num_examples=43793, total_duration=8285.463474, train/accuracy=0.990711, train/loss=0.030503, train/mean_average_precision=0.410122, validation/accuracy=0.986829, validation/loss=0.043968, validation/mean_average_precision=0.271107, validation/num_examples=43793
I0305 12:23:17.710035 140514586244864 logging_writer.py:48] [15700] global_step=15700, grad_norm=0.040753401815891266, loss=0.03615333139896393
I0305 12:23:50.121620 140415790769920 logging_writer.py:48] [15800] global_step=15800, grad_norm=0.03618035838007927, loss=0.03272329270839691
I0305 12:24:22.642102 140514586244864 logging_writer.py:48] [15900] global_step=15900, grad_norm=0.033987946808338165, loss=0.03282333165407181
I0305 12:24:55.137351 140415790769920 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.03169536963105202, loss=0.031230460852384567
I0305 12:25:27.730652 140514586244864 logging_writer.py:48] [16100] global_step=16100, grad_norm=0.03454677760601044, loss=0.03179100155830383
I0305 12:25:59.908539 140415790769920 logging_writer.py:48] [16200] global_step=16200, grad_norm=0.04711218550801277, loss=0.03161933273077011
I0305 12:26:32.069262 140514586244864 logging_writer.py:48] [16300] global_step=16300, grad_norm=0.033032432198524475, loss=0.028423262760043144
I0305 12:27:04.263173 140415790769920 logging_writer.py:48] [16400] global_step=16400, grad_norm=0.034885648638010025, loss=0.03231698274612427
I0305 12:27:14.927242 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:29:18.318341 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:29:21.325889 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:29:24.299020 140576608098112 submission_runner.py:411] Time since start: 8655.01s, 	Step: 16434, 	{'train/accuracy': 0.9906653761863708, 'train/loss': 0.030565960332751274, 'train/mean_average_precision': 0.39613450793746174, 'validation/accuracy': 0.9868064522743225, 'validation/loss': 0.04404061660170555, 'validation/mean_average_precision': 0.27083248818621664, 'validation/num_examples': 43793, 'test/accuracy': 0.9860348105430603, 'test/loss': 0.0468795970082283, 'test/mean_average_precision': 0.26683018272939363, 'test/num_examples': 43793, 'score': 5301.552769899368, 'total_duration': 8655.007350206375, 'accumulated_submission_time': 5301.552769899368, 'accumulated_eval_time': 3352.3659710884094, 'accumulated_logging_time': 0.6399135589599609}
I0305 12:29:24.317154 140408968165120 logging_writer.py:48] [16434] accumulated_eval_time=3352.365971, accumulated_logging_time=0.639914, accumulated_submission_time=5301.552770, global_step=16434, preemption_count=0, score=5301.552770, test/accuracy=0.986035, test/loss=0.046880, test/mean_average_precision=0.266830, test/num_examples=43793, total_duration=8655.007350, train/accuracy=0.990665, train/loss=0.030566, train/mean_average_precision=0.396135, validation/accuracy=0.986806, validation/loss=0.044041, validation/mean_average_precision=0.270832, validation/num_examples=43793
I0305 12:29:45.904013 140415782377216 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.046775367110967636, loss=0.03455544635653496
I0305 12:30:18.693340 140408968165120 logging_writer.py:48] [16600] global_step=16600, grad_norm=0.03200806677341461, loss=0.030005142092704773
I0305 12:30:50.790431 140415782377216 logging_writer.py:48] [16700] global_step=16700, grad_norm=0.05935993418097496, loss=0.03404281660914421
I0305 12:31:23.097248 140408968165120 logging_writer.py:48] [16800] global_step=16800, grad_norm=0.04383612424135208, loss=0.03316181153059006
I0305 12:31:54.960693 140415782377216 logging_writer.py:48] [16900] global_step=16900, grad_norm=0.037312667816877365, loss=0.02990344539284706
I0305 12:32:27.318217 140408968165120 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.040749743580818176, loss=0.034838274121284485
I0305 12:33:00.045083 140415782377216 logging_writer.py:48] [17100] global_step=17100, grad_norm=0.03789675980806351, loss=0.033509280532598495
I0305 12:33:24.327481 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:35:28.051610 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:35:31.080256 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:35:34.027267 140576608098112 submission_runner.py:411] Time since start: 9024.74s, 	Step: 17176, 	{'train/accuracy': 0.9908429384231567, 'train/loss': 0.03007511980831623, 'train/mean_average_precision': 0.4131038732595066, 'validation/accuracy': 0.9868007898330688, 'validation/loss': 0.044173236936330795, 'validation/mean_average_precision': 0.26319069911213616, 'validation/num_examples': 43793, 'test/accuracy': 0.9859800934791565, 'test/loss': 0.04678065702319145, 'test/mean_average_precision': 0.2602629097743209, 'test/num_examples': 43793, 'score': 5541.53219461441, 'total_duration': 9024.735594034195, 'accumulated_submission_time': 5541.53219461441, 'accumulated_eval_time': 3482.0657093524933, 'accumulated_logging_time': 0.6692507266998291}
I0305 12:35:34.046153 140415790769920 logging_writer.py:48] [17176] accumulated_eval_time=3482.065709, accumulated_logging_time=0.669251, accumulated_submission_time=5541.532195, global_step=17176, preemption_count=0, score=5541.532195, test/accuracy=0.985980, test/loss=0.046781, test/mean_average_precision=0.260263, test/num_examples=43793, total_duration=9024.735594, train/accuracy=0.990843, train/loss=0.030075, train/mean_average_precision=0.413104, validation/accuracy=0.986801, validation/loss=0.044173, validation/mean_average_precision=0.263191, validation/num_examples=43793
I0305 12:35:41.927531 140514586244864 logging_writer.py:48] [17200] global_step=17200, grad_norm=0.03894709050655365, loss=0.03029865026473999
I0305 12:36:13.782036 140415790769920 logging_writer.py:48] [17300] global_step=17300, grad_norm=0.0408233143389225, loss=0.03228401020169258
I0305 12:36:45.559881 140514586244864 logging_writer.py:48] [17400] global_step=17400, grad_norm=0.03980389982461929, loss=0.03346089646220207
I0305 12:37:17.357275 140415790769920 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.03805677965283394, loss=0.03302503377199173
I0305 12:37:48.802918 140514586244864 logging_writer.py:48] [17600] global_step=17600, grad_norm=0.035571709275245667, loss=0.0333871953189373
I0305 12:38:20.616482 140415790769920 logging_writer.py:48] [17700] global_step=17700, grad_norm=0.03584490716457367, loss=0.03132278844714165
I0305 12:38:51.972474 140514586244864 logging_writer.py:48] [17800] global_step=17800, grad_norm=0.038398925215005875, loss=0.03012317605316639
I0305 12:39:23.928041 140415790769920 logging_writer.py:48] [17900] global_step=17900, grad_norm=0.039511922746896744, loss=0.032838691025972366
I0305 12:39:34.139244 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:41:34.891805 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:41:37.923914 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:41:40.918166 140576608098112 submission_runner.py:411] Time since start: 9391.63s, 	Step: 17933, 	{'train/accuracy': 0.9910299777984619, 'train/loss': 0.02942151017487049, 'train/mean_average_precision': 0.4207735413460807, 'validation/accuracy': 0.9868117570877075, 'validation/loss': 0.04416516050696373, 'validation/mean_average_precision': 0.26553810445476234, 'validation/num_examples': 43793, 'test/accuracy': 0.9859012961387634, 'test/loss': 0.046769119799137115, 'test/mean_average_precision': 0.26259653165543306, 'test/num_examples': 43793, 'score': 5781.594381570816, 'total_duration': 9391.62649512291, 'accumulated_submission_time': 5781.594381570816, 'accumulated_eval_time': 3608.844582557678, 'accumulated_logging_time': 0.699115514755249}
I0305 12:41:40.936818 140408968165120 logging_writer.py:48] [17933] accumulated_eval_time=3608.844583, accumulated_logging_time=0.699116, accumulated_submission_time=5781.594382, global_step=17933, preemption_count=0, score=5781.594382, test/accuracy=0.985901, test/loss=0.046769, test/mean_average_precision=0.262597, test/num_examples=43793, total_duration=9391.626495, train/accuracy=0.991030, train/loss=0.029422, train/mean_average_precision=0.420774, validation/accuracy=0.986812, validation/loss=0.044165, validation/mean_average_precision=0.265538, validation/num_examples=43793
I0305 12:42:03.159606 140409291962112 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.049304693937301636, loss=0.030149931088089943
I0305 12:42:35.417786 140408968165120 logging_writer.py:48] [18100] global_step=18100, grad_norm=0.0451616495847702, loss=0.030357683077454567
I0305 12:43:08.068504 140409291962112 logging_writer.py:48] [18200] global_step=18200, grad_norm=0.0495176836848259, loss=0.03593229874968529
I0305 12:43:39.948562 140408968165120 logging_writer.py:48] [18300] global_step=18300, grad_norm=0.043797388672828674, loss=0.03225405886769295
I0305 12:44:12.220039 140409291962112 logging_writer.py:48] [18400] global_step=18400, grad_norm=0.03769828379154205, loss=0.03214487060904503
I0305 12:44:44.311213 140408968165120 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.05225622281432152, loss=0.031275536864995956
I0305 12:45:16.410316 140409291962112 logging_writer.py:48] [18600] global_step=18600, grad_norm=0.041978802531957626, loss=0.031226621940732002
I0305 12:45:41.068127 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:47:44.713283 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:47:47.751597 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:47:50.722992 140576608098112 submission_runner.py:411] Time since start: 9761.43s, 	Step: 18678, 	{'train/accuracy': 0.9913448691368103, 'train/loss': 0.028279868885874748, 'train/mean_average_precision': 0.45535882510862913, 'validation/accuracy': 0.9869843125343323, 'validation/loss': 0.04361100494861603, 'validation/mean_average_precision': 0.2739963602729493, 'validation/num_examples': 43793, 'test/accuracy': 0.9860984086990356, 'test/loss': 0.04637132212519646, 'test/mean_average_precision': 0.2668474315698864, 'test/num_examples': 43793, 'score': 6021.693260192871, 'total_duration': 9761.431322336197, 'accumulated_submission_time': 6021.693260192871, 'accumulated_eval_time': 3738.4994037151337, 'accumulated_logging_time': 0.730036735534668}
I0305 12:47:50.742634 140415782377216 logging_writer.py:48] [18678] accumulated_eval_time=3738.499404, accumulated_logging_time=0.730037, accumulated_submission_time=6021.693260, global_step=18678, preemption_count=0, score=6021.693260, test/accuracy=0.986098, test/loss=0.046371, test/mean_average_precision=0.266847, test/num_examples=43793, total_duration=9761.431322, train/accuracy=0.991345, train/loss=0.028280, train/mean_average_precision=0.455359, validation/accuracy=0.986984, validation/loss=0.043611, validation/mean_average_precision=0.273996, validation/num_examples=43793
I0305 12:47:58.179139 140514586244864 logging_writer.py:48] [18700] global_step=18700, grad_norm=0.05029990151524544, loss=0.02964569255709648
I0305 12:48:30.118025 140415782377216 logging_writer.py:48] [18800] global_step=18800, grad_norm=0.047739822417497635, loss=0.03202853351831436
I0305 12:49:01.817361 140514586244864 logging_writer.py:48] [18900] global_step=18900, grad_norm=0.04670656844973564, loss=0.03430812805891037
I0305 12:49:33.862403 140415782377216 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.037562478333711624, loss=0.03260187432169914
I0305 12:50:05.977653 140514586244864 logging_writer.py:48] [19100] global_step=19100, grad_norm=0.048903342336416245, loss=0.03396202623844147
I0305 12:50:39.026774 140415782377216 logging_writer.py:48] [19200] global_step=19200, grad_norm=0.04287378489971161, loss=0.03331756219267845
I0305 12:51:11.804558 140514586244864 logging_writer.py:48] [19300] global_step=19300, grad_norm=0.056835535913705826, loss=0.03506461903452873
I0305 12:51:44.261235 140415782377216 logging_writer.py:48] [19400] global_step=19400, grad_norm=0.03795190528035164, loss=0.028144516050815582
I0305 12:51:50.762693 140576608098112 spec.py:321] Evaluating on the training split.
I0305 12:53:54.476147 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 12:53:57.493595 140576608098112 spec.py:349] Evaluating on the test split.
I0305 12:54:00.470072 140576608098112 submission_runner.py:411] Time since start: 10131.18s, 	Step: 19421, 	{'train/accuracy': 0.991253674030304, 'train/loss': 0.028546135872602463, 'train/mean_average_precision': 0.4504025211918963, 'validation/accuracy': 0.9868957996368408, 'validation/loss': 0.0441204234957695, 'validation/mean_average_precision': 0.27294751989731597, 'validation/num_examples': 43793, 'test/accuracy': 0.9860415458679199, 'test/loss': 0.04682275280356407, 'test/mean_average_precision': 0.2666428469243027, 'test/num_examples': 43793, 'score': 6261.680698633194, 'total_duration': 10131.178402423859, 'accumulated_submission_time': 6261.680698633194, 'accumulated_eval_time': 3868.2067382335663, 'accumulated_logging_time': 0.7608804702758789}
I0305 12:54:00.489012 140409291962112 logging_writer.py:48] [19421] accumulated_eval_time=3868.206738, accumulated_logging_time=0.760880, accumulated_submission_time=6261.680699, global_step=19421, preemption_count=0, score=6261.680699, test/accuracy=0.986042, test/loss=0.046823, test/mean_average_precision=0.266643, test/num_examples=43793, total_duration=10131.178402, train/accuracy=0.991254, train/loss=0.028546, train/mean_average_precision=0.450403, validation/accuracy=0.986896, validation/loss=0.044120, validation/mean_average_precision=0.272948, validation/num_examples=43793
I0305 12:54:26.182688 140415790769920 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.05080621689558029, loss=0.03220124915242195
I0305 12:54:58.247635 140409291962112 logging_writer.py:48] [19600] global_step=19600, grad_norm=0.04404834657907486, loss=0.032321155071258545
I0305 12:55:30.743874 140415790769920 logging_writer.py:48] [19700] global_step=19700, grad_norm=0.040591735392808914, loss=0.03355935961008072
I0305 12:56:03.024643 140409291962112 logging_writer.py:48] [19800] global_step=19800, grad_norm=0.06151067465543747, loss=0.029903903603553772
I0305 12:56:35.392902 140415790769920 logging_writer.py:48] [19900] global_step=19900, grad_norm=0.04169246181845665, loss=0.03305824473500252
I0305 12:57:08.062067 140409291962112 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.04327194765210152, loss=0.030642010271549225
I0305 12:57:40.286031 140415790769920 logging_writer.py:48] [20100] global_step=20100, grad_norm=0.04276508837938309, loss=0.029229991137981415
I0305 12:58:00.653676 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:00:04.382904 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:00:07.409634 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:00:10.356237 140576608098112 submission_runner.py:411] Time since start: 10501.06s, 	Step: 20164, 	{'train/accuracy': 0.9912835359573364, 'train/loss': 0.028436098247766495, 'train/mean_average_precision': 0.4628734931838974, 'validation/accuracy': 0.9869039058685303, 'validation/loss': 0.043829821050167084, 'validation/mean_average_precision': 0.27545483081097744, 'validation/num_examples': 43793, 'test/accuracy': 0.9860731363296509, 'test/loss': 0.04672902077436447, 'test/mean_average_precision': 0.26670280737139773, 'test/num_examples': 43793, 'score': 6501.814088821411, 'total_duration': 10501.064566135406, 'accumulated_submission_time': 6501.814088821411, 'accumulated_eval_time': 3997.909258365631, 'accumulated_logging_time': 0.7910916805267334}
I0305 13:00:10.375116 140408968165120 logging_writer.py:48] [20164] accumulated_eval_time=3997.909258, accumulated_logging_time=0.791092, accumulated_submission_time=6501.814089, global_step=20164, preemption_count=0, score=6501.814089, test/accuracy=0.986073, test/loss=0.046729, test/mean_average_precision=0.266703, test/num_examples=43793, total_duration=10501.064566, train/accuracy=0.991284, train/loss=0.028436, train/mean_average_precision=0.462873, validation/accuracy=0.986904, validation/loss=0.043830, validation/mean_average_precision=0.275455, validation/num_examples=43793
I0305 13:00:22.317176 140514586244864 logging_writer.py:48] [20200] global_step=20200, grad_norm=0.04166361689567566, loss=0.030681202188134193
I0305 13:00:54.898165 140408968165120 logging_writer.py:48] [20300] global_step=20300, grad_norm=0.052672941237688065, loss=0.031390007585287094
I0305 13:01:27.357526 140514586244864 logging_writer.py:48] [20400] global_step=20400, grad_norm=0.04755023121833801, loss=0.02818620577454567
I0305 13:01:59.587380 140408968165120 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.054109424352645874, loss=0.029588298872113228
I0305 13:02:31.907822 140514586244864 logging_writer.py:48] [20600] global_step=20600, grad_norm=0.09327197819948196, loss=0.03722685948014259
I0305 13:03:03.905026 140408968165120 logging_writer.py:48] [20700] global_step=20700, grad_norm=0.04693792387843132, loss=0.029031088575720787
I0305 13:03:35.836056 140514586244864 logging_writer.py:48] [20800] global_step=20800, grad_norm=0.041231948882341385, loss=0.030859878286719322
I0305 13:04:07.947929 140408968165120 logging_writer.py:48] [20900] global_step=20900, grad_norm=0.06015896797180176, loss=0.032906822860240936
I0305 13:04:10.522386 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:06:13.401150 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:06:16.482057 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:06:19.617170 140576608098112 submission_runner.py:411] Time since start: 10870.33s, 	Step: 20909, 	{'train/accuracy': 0.9910857677459717, 'train/loss': 0.029048796743154526, 'train/mean_average_precision': 0.4316780481261128, 'validation/accuracy': 0.9869911670684814, 'validation/loss': 0.04379790276288986, 'validation/mean_average_precision': 0.2797274694723569, 'validation/num_examples': 43793, 'test/accuracy': 0.986042857170105, 'test/loss': 0.04683835431933403, 'test/mean_average_precision': 0.26444721068009136, 'test/num_examples': 43793, 'score': 6741.928815841675, 'total_duration': 10870.3255007267, 'accumulated_submission_time': 6741.928815841675, 'accumulated_eval_time': 4127.003994941711, 'accumulated_logging_time': 0.8224525451660156}
I0305 13:06:19.636865 140415782377216 logging_writer.py:48] [20909] accumulated_eval_time=4127.003995, accumulated_logging_time=0.822453, accumulated_submission_time=6741.928816, global_step=20909, preemption_count=0, score=6741.928816, test/accuracy=0.986043, test/loss=0.046838, test/mean_average_precision=0.264447, test/num_examples=43793, total_duration=10870.325501, train/accuracy=0.991086, train/loss=0.029049, train/mean_average_precision=0.431678, validation/accuracy=0.986991, validation/loss=0.043798, validation/mean_average_precision=0.279727, validation/num_examples=43793
I0305 13:06:49.062899 140415790769920 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.03519492223858833, loss=0.029218077659606934
I0305 13:07:21.012967 140415782377216 logging_writer.py:48] [21100] global_step=21100, grad_norm=0.04957932233810425, loss=0.029235107824206352
I0305 13:07:53.066282 140415790769920 logging_writer.py:48] [21200] global_step=21200, grad_norm=0.0456521213054657, loss=0.03275779262185097
I0305 13:08:25.181466 140415782377216 logging_writer.py:48] [21300] global_step=21300, grad_norm=0.051758501678705215, loss=0.030544154345989227
I0305 13:08:56.864768 140415790769920 logging_writer.py:48] [21400] global_step=21400, grad_norm=0.0457640178501606, loss=0.030701791867613792
I0305 13:09:28.691957 140415782377216 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.04450693354010582, loss=0.02877115085721016
I0305 13:10:00.677759 140415790769920 logging_writer.py:48] [21600] global_step=21600, grad_norm=0.04603204503655434, loss=0.031862929463386536
I0305 13:10:19.681737 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:12:32.745321 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:12:35.761516 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:12:38.761069 140576608098112 submission_runner.py:411] Time since start: 11249.47s, 	Step: 21660, 	{'train/accuracy': 0.9911643266677856, 'train/loss': 0.028825219720602036, 'train/mean_average_precision': 0.4205208436652018, 'validation/accuracy': 0.9870212078094482, 'validation/loss': 0.043848007917404175, 'validation/mean_average_precision': 0.27585816405176655, 'validation/num_examples': 43793, 'test/accuracy': 0.986139714717865, 'test/loss': 0.04665219783782959, 'test/mean_average_precision': 0.27279926825916667, 'test/num_examples': 43793, 'score': 6981.9416534900665, 'total_duration': 11249.46938920021, 'accumulated_submission_time': 6981.9416534900665, 'accumulated_eval_time': 4266.083292245865, 'accumulated_logging_time': 0.8539795875549316}
I0305 13:12:38.780638 140409291962112 logging_writer.py:48] [21660] accumulated_eval_time=4266.083292, accumulated_logging_time=0.853980, accumulated_submission_time=6981.941653, global_step=21660, preemption_count=0, score=6981.941653, test/accuracy=0.986140, test/loss=0.046652, test/mean_average_precision=0.272799, test/num_examples=43793, total_duration=11249.469389, train/accuracy=0.991164, train/loss=0.028825, train/mean_average_precision=0.420521, validation/accuracy=0.987021, validation/loss=0.043848, validation/mean_average_precision=0.275858, validation/num_examples=43793
I0305 13:12:52.067349 140514586244864 logging_writer.py:48] [21700] global_step=21700, grad_norm=0.05366627126932144, loss=0.029842428863048553
I0305 13:13:24.445518 140409291962112 logging_writer.py:48] [21800] global_step=21800, grad_norm=0.05594111979007721, loss=0.031117413192987442
I0305 13:13:56.186417 140514586244864 logging_writer.py:48] [21900] global_step=21900, grad_norm=0.04755937680602074, loss=0.030072402209043503
I0305 13:14:29.173609 140409291962112 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.04900505766272545, loss=0.028472166508436203
I0305 13:15:02.230355 140514586244864 logging_writer.py:48] [22100] global_step=22100, grad_norm=0.04571906849741936, loss=0.02938191406428814
I0305 13:15:34.275241 140409291962112 logging_writer.py:48] [22200] global_step=22200, grad_norm=0.047622788697481155, loss=0.03079027682542801
I0305 13:16:06.646246 140514586244864 logging_writer.py:48] [22300] global_step=22300, grad_norm=0.04632917791604996, loss=0.02899220772087574
I0305 13:16:38.881771 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:18:43.824244 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:18:46.877895 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:18:49.860489 140576608098112 submission_runner.py:411] Time since start: 11620.57s, 	Step: 22400, 	{'train/accuracy': 0.9911457896232605, 'train/loss': 0.029067164286971092, 'train/mean_average_precision': 0.44417653520183065, 'validation/accuracy': 0.9868693947792053, 'validation/loss': 0.04398307576775551, 'validation/mean_average_precision': 0.2746977620566058, 'validation/num_examples': 43793, 'test/accuracy': 0.9860116839408875, 'test/loss': 0.04665590077638626, 'test/mean_average_precision': 0.2678142261886165, 'test/num_examples': 43793, 'score': 7222.0097670555115, 'total_duration': 11620.568714141846, 'accumulated_submission_time': 7222.0097670555115, 'accumulated_eval_time': 4397.061862707138, 'accumulated_logging_time': 0.8849740028381348}
I0305 13:18:49.879835 140408968165120 logging_writer.py:48] [22400] accumulated_eval_time=4397.061863, accumulated_logging_time=0.884974, accumulated_submission_time=7222.009767, global_step=22400, preemption_count=0, score=7222.009767, test/accuracy=0.986012, test/loss=0.046656, test/mean_average_precision=0.267814, test/num_examples=43793, total_duration=11620.568714, train/accuracy=0.991146, train/loss=0.029067, train/mean_average_precision=0.444177, validation/accuracy=0.986869, validation/loss=0.043983, validation/mean_average_precision=0.274698, validation/num_examples=43793
I0305 13:18:50.268304 140415782377216 logging_writer.py:48] [22400] global_step=22400, grad_norm=0.05460579693317413, loss=0.033876389265060425
I0305 13:19:22.416004 140408968165120 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.045695748180150986, loss=0.03204989805817604
I0305 13:19:54.713536 140415782377216 logging_writer.py:48] [22600] global_step=22600, grad_norm=0.04604479670524597, loss=0.030652185901999474
I0305 13:20:26.967990 140408968165120 logging_writer.py:48] [22700] global_step=22700, grad_norm=0.044526319950819016, loss=0.027679987251758575
I0305 13:20:58.932008 140415782377216 logging_writer.py:48] [22800] global_step=22800, grad_norm=0.04731272906064987, loss=0.032458364963531494
I0305 13:21:31.165894 140408968165120 logging_writer.py:48] [22900] global_step=22900, grad_norm=0.04359850287437439, loss=0.02775150164961815
I0305 13:22:03.582668 140415782377216 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.04320816695690155, loss=0.02994273230433464
I0305 13:22:35.485338 140408968165120 logging_writer.py:48] [23100] global_step=23100, grad_norm=0.05017168074846268, loss=0.029968149960041046
I0305 13:22:50.122295 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:24:57.375005 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:25:00.350565 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:25:03.546042 140576608098112 submission_runner.py:411] Time since start: 11994.25s, 	Step: 23147, 	{'train/accuracy': 0.9912724494934082, 'train/loss': 0.028482697904109955, 'train/mean_average_precision': 0.4513491212236842, 'validation/accuracy': 0.987023651599884, 'validation/loss': 0.04372390732169151, 'validation/mean_average_precision': 0.2754543168780456, 'validation/num_examples': 43793, 'test/accuracy': 0.9860929846763611, 'test/loss': 0.046706389635801315, 'test/mean_average_precision': 0.2686252559516493, 'test/num_examples': 43793, 'score': 7462.221218585968, 'total_duration': 11994.254369735718, 'accumulated_submission_time': 7462.221218585968, 'accumulated_eval_time': 4530.485562801361, 'accumulated_logging_time': 0.9154069423675537}
I0305 13:25:03.565737 140409291962112 logging_writer.py:48] [23147] accumulated_eval_time=4530.485563, accumulated_logging_time=0.915407, accumulated_submission_time=7462.221219, global_step=23147, preemption_count=0, score=7462.221219, test/accuracy=0.986093, test/loss=0.046706, test/mean_average_precision=0.268625, test/num_examples=43793, total_duration=11994.254370, train/accuracy=0.991272, train/loss=0.028483, train/mean_average_precision=0.451349, validation/accuracy=0.987024, validation/loss=0.043724, validation/mean_average_precision=0.275454, validation/num_examples=43793
I0305 13:25:20.933526 140514586244864 logging_writer.py:48] [23200] global_step=23200, grad_norm=0.04903402924537659, loss=0.026875542476773262
I0305 13:25:52.908967 140409291962112 logging_writer.py:48] [23300] global_step=23300, grad_norm=0.038237154483795166, loss=0.030307618901133537
I0305 13:26:25.131166 140514586244864 logging_writer.py:48] [23400] global_step=23400, grad_norm=0.0511847585439682, loss=0.030255476012825966
I0305 13:26:57.506752 140409291962112 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.04735477268695831, loss=0.03091367520391941
I0305 13:27:29.398864 140514586244864 logging_writer.py:48] [23600] global_step=23600, grad_norm=0.05234581232070923, loss=0.03178154304623604
I0305 13:28:01.702854 140409291962112 logging_writer.py:48] [23700] global_step=23700, grad_norm=0.04792129248380661, loss=0.02970368228852749
I0305 13:28:34.150215 140514586244864 logging_writer.py:48] [23800] global_step=23800, grad_norm=0.047498106956481934, loss=0.03284348174929619
I0305 13:29:03.642907 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:31:09.730861 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:31:12.770915 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:31:15.719457 140576608098112 submission_runner.py:411] Time since start: 12366.43s, 	Step: 23892, 	{'train/accuracy': 0.9913005828857422, 'train/loss': 0.02820000797510147, 'train/mean_average_precision': 0.4627503649671614, 'validation/accuracy': 0.9869924187660217, 'validation/loss': 0.04423755407333374, 'validation/mean_average_precision': 0.27784457725222994, 'validation/num_examples': 43793, 'test/accuracy': 0.9861068725585938, 'test/loss': 0.047160159796476364, 'test/mean_average_precision': 0.26138790659697775, 'test/num_examples': 43793, 'score': 7702.2666256427765, 'total_duration': 12366.427788496017, 'accumulated_submission_time': 7702.2666256427765, 'accumulated_eval_time': 4662.562074661255, 'accumulated_logging_time': 0.9465370178222656}
I0305 13:31:15.738942 140408968165120 logging_writer.py:48] [23892] accumulated_eval_time=4662.562075, accumulated_logging_time=0.946537, accumulated_submission_time=7702.266626, global_step=23892, preemption_count=0, score=7702.266626, test/accuracy=0.986107, test/loss=0.047160, test/mean_average_precision=0.261388, test/num_examples=43793, total_duration=12366.427788, train/accuracy=0.991301, train/loss=0.028200, train/mean_average_precision=0.462750, validation/accuracy=0.986992, validation/loss=0.044238, validation/mean_average_precision=0.277845, validation/num_examples=43793
I0305 13:31:18.707662 140415790769920 logging_writer.py:48] [23900] global_step=23900, grad_norm=0.06975780427455902, loss=0.028244702145457268
I0305 13:31:51.458312 140408968165120 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.05332459881901741, loss=0.028122415766119957
I0305 13:32:24.043807 140415790769920 logging_writer.py:48] [24100] global_step=24100, grad_norm=0.05317065864801407, loss=0.0289034154266119
I0305 13:32:56.521661 140408968165120 logging_writer.py:48] [24200] global_step=24200, grad_norm=0.049515191465616226, loss=0.030023830011487007
I0305 13:33:29.067106 140415790769920 logging_writer.py:48] [24300] global_step=24300, grad_norm=0.047279633581638336, loss=0.02773909457027912
I0305 13:34:00.750820 140408968165120 logging_writer.py:48] [24400] global_step=24400, grad_norm=0.050004828721284866, loss=0.03049055114388466
I0305 13:34:33.179349 140415790769920 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.04719731584191322, loss=0.02873620204627514
I0305 13:35:05.800316 140408968165120 logging_writer.py:48] [24600] global_step=24600, grad_norm=0.044602785259485245, loss=0.034464895725250244
I0305 13:35:15.919665 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:37:18.864605 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:37:21.912271 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:37:24.897486 140576608098112 submission_runner.py:411] Time since start: 12735.61s, 	Step: 24632, 	{'train/accuracy': 0.991439700126648, 'train/loss': 0.027851887047290802, 'train/mean_average_precision': 0.4635890547062042, 'validation/accuracy': 0.9869059324264526, 'validation/loss': 0.043697815388441086, 'validation/mean_average_precision': 0.2797329279295364, 'validation/num_examples': 43793, 'test/accuracy': 0.9860866665840149, 'test/loss': 0.046415239572525024, 'test/mean_average_precision': 0.26979861602310795, 'test/num_examples': 43793, 'score': 7942.415654420853, 'total_duration': 12735.605735778809, 'accumulated_submission_time': 7942.415654420853, 'accumulated_eval_time': 4791.539767742157, 'accumulated_logging_time': 0.9781191349029541}
I0305 13:37:24.917182 140409291962112 logging_writer.py:48] [24632] accumulated_eval_time=4791.539768, accumulated_logging_time=0.978119, accumulated_submission_time=7942.415654, global_step=24632, preemption_count=0, score=7942.415654, test/accuracy=0.986087, test/loss=0.046415, test/mean_average_precision=0.269799, test/num_examples=43793, total_duration=12735.605736, train/accuracy=0.991440, train/loss=0.027852, train/mean_average_precision=0.463589, validation/accuracy=0.986906, validation/loss=0.043698, validation/mean_average_precision=0.279733, validation/num_examples=43793
I0305 13:37:47.215769 140415782377216 logging_writer.py:48] [24700] global_step=24700, grad_norm=0.055775437504053116, loss=0.033353328704833984
I0305 13:38:19.632895 140409291962112 logging_writer.py:48] [24800] global_step=24800, grad_norm=0.05207021161913872, loss=0.032296717166900635
I0305 13:38:51.704011 140415782377216 logging_writer.py:48] [24900] global_step=24900, grad_norm=0.05396026372909546, loss=0.033939238637685776
I0305 13:39:24.007955 140409291962112 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.05415637791156769, loss=0.030326804146170616
I0305 13:39:56.005162 140415782377216 logging_writer.py:48] [25100] global_step=25100, grad_norm=0.07919168472290039, loss=0.031008951365947723
I0305 13:40:28.494213 140409291962112 logging_writer.py:48] [25200] global_step=25200, grad_norm=0.04577357694506645, loss=0.03013557568192482
I0305 13:41:01.181544 140415782377216 logging_writer.py:48] [25300] global_step=25300, grad_norm=0.05990264564752579, loss=0.03084735944867134
I0305 13:41:25.088354 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:43:28.105413 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:43:31.115228 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:43:34.090097 140576608098112 submission_runner.py:411] Time since start: 13104.80s, 	Step: 25374, 	{'train/accuracy': 0.9915762543678284, 'train/loss': 0.027389703318476677, 'train/mean_average_precision': 0.47491075209151423, 'validation/accuracy': 0.9869404435157776, 'validation/loss': 0.04406937211751938, 'validation/mean_average_precision': 0.2789975196120251, 'validation/num_examples': 43793, 'test/accuracy': 0.9860929846763611, 'test/loss': 0.046653784811496735, 'test/mean_average_precision': 0.2697822094920301, 'test/num_examples': 43793, 'score': 8182.555495977402, 'total_duration': 13104.798302650452, 'accumulated_submission_time': 8182.555495977402, 'accumulated_eval_time': 4920.541341781616, 'accumulated_logging_time': 1.0089633464813232}
I0305 13:43:34.109599 140408968165120 logging_writer.py:48] [25374] accumulated_eval_time=4920.541342, accumulated_logging_time=1.008963, accumulated_submission_time=8182.555496, global_step=25374, preemption_count=0, score=8182.555496, test/accuracy=0.986093, test/loss=0.046654, test/mean_average_precision=0.269782, test/num_examples=43793, total_duration=13104.798303, train/accuracy=0.991576, train/loss=0.027390, train/mean_average_precision=0.474911, validation/accuracy=0.986940, validation/loss=0.044069, validation/mean_average_precision=0.278998, validation/num_examples=43793
I0305 13:43:43.030351 140514586244864 logging_writer.py:48] [25400] global_step=25400, grad_norm=0.05155261233448982, loss=0.029031142592430115
I0305 13:44:14.983877 140408968165120 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.04723930358886719, loss=0.029820166528224945
I0305 13:44:46.792902 140514586244864 logging_writer.py:48] [25600] global_step=25600, grad_norm=0.046704746782779694, loss=0.028456782922148705
I0305 13:45:18.947111 140408968165120 logging_writer.py:48] [25700] global_step=25700, grad_norm=0.05533983185887337, loss=0.028028851374983788
I0305 13:45:50.753661 140514586244864 logging_writer.py:48] [25800] global_step=25800, grad_norm=0.056611135601997375, loss=0.032676588743925095
I0305 13:46:22.759847 140408968165120 logging_writer.py:48] [25900] global_step=25900, grad_norm=0.04765701666474342, loss=0.031858496367931366
I0305 13:46:54.639407 140514586244864 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.05058275908231735, loss=0.02800317481160164
I0305 13:47:26.650290 140408968165120 logging_writer.py:48] [26100] global_step=26100, grad_norm=0.05712214857339859, loss=0.03326532244682312
I0305 13:47:34.403114 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:49:35.462858 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:49:38.492081 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:49:41.492090 140576608098112 submission_runner.py:411] Time since start: 13472.20s, 	Step: 26125, 	{'train/accuracy': 0.9917351603507996, 'train/loss': 0.026702111586928368, 'train/mean_average_precision': 0.4959739328984074, 'validation/accuracy': 0.986988365650177, 'validation/loss': 0.044012270867824554, 'validation/mean_average_precision': 0.2815963343967564, 'validation/num_examples': 43793, 'test/accuracy': 0.9861578345298767, 'test/loss': 0.046824488788843155, 'test/mean_average_precision': 0.27552153010455704, 'test/num_examples': 43793, 'score': 8422.817781925201, 'total_duration': 13472.200419664383, 'accumulated_submission_time': 8422.817781925201, 'accumulated_eval_time': 5047.630271434784, 'accumulated_logging_time': 1.0395777225494385}
I0305 13:49:41.512107 140409291962112 logging_writer.py:48] [26125] accumulated_eval_time=5047.630271, accumulated_logging_time=1.039578, accumulated_submission_time=8422.817782, global_step=26125, preemption_count=0, score=8422.817782, test/accuracy=0.986158, test/loss=0.046824, test/mean_average_precision=0.275522, test/num_examples=43793, total_duration=13472.200420, train/accuracy=0.991735, train/loss=0.026702, train/mean_average_precision=0.495974, validation/accuracy=0.986988, validation/loss=0.044012, validation/mean_average_precision=0.281596, validation/num_examples=43793
I0305 13:50:06.200913 140415782377216 logging_writer.py:48] [26200] global_step=26200, grad_norm=0.052674390375614166, loss=0.028528986498713493
I0305 13:50:38.034580 140409291962112 logging_writer.py:48] [26300] global_step=26300, grad_norm=0.04866395890712738, loss=0.03141423687338829
I0305 13:51:10.185583 140415782377216 logging_writer.py:48] [26400] global_step=26400, grad_norm=0.0497981496155262, loss=0.03009932115674019
I0305 13:51:42.224507 140409291962112 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.07714839279651642, loss=0.03510545566678047
I0305 13:52:14.695865 140415782377216 logging_writer.py:48] [26600] global_step=26600, grad_norm=0.054125890135765076, loss=0.028831690549850464
I0305 13:52:47.036957 140409291962112 logging_writer.py:48] [26700] global_step=26700, grad_norm=0.06968984007835388, loss=0.02916501834988594
I0305 13:53:19.402068 140415782377216 logging_writer.py:48] [26800] global_step=26800, grad_norm=0.05031561851501465, loss=0.028631331399083138
I0305 13:53:41.670134 140576608098112 spec.py:321] Evaluating on the training split.
I0305 13:55:48.523275 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 13:55:51.952244 140576608098112 spec.py:349] Evaluating on the test split.
I0305 13:55:55.396631 140576608098112 submission_runner.py:411] Time since start: 13846.10s, 	Step: 26870, 	{'train/accuracy': 0.9916290044784546, 'train/loss': 0.027274146676063538, 'train/mean_average_precision': 0.4856420312623262, 'validation/accuracy': 0.9868893027305603, 'validation/loss': 0.04377366602420807, 'validation/mean_average_precision': 0.27734565213710677, 'validation/num_examples': 43793, 'test/accuracy': 0.9860352277755737, 'test/loss': 0.04689223691821098, 'test/mean_average_precision': 0.2654404572829221, 'test/num_examples': 43793, 'score': 8662.943137168884, 'total_duration': 13846.104840278625, 'accumulated_submission_time': 8662.943137168884, 'accumulated_eval_time': 5181.35660982132, 'accumulated_logging_time': 1.0719294548034668}
I0305 13:55:55.419700 140408968165120 logging_writer.py:48] [26870] accumulated_eval_time=5181.356610, accumulated_logging_time=1.071929, accumulated_submission_time=8662.943137, global_step=26870, preemption_count=0, score=8662.943137, test/accuracy=0.986035, test/loss=0.046892, test/mean_average_precision=0.265440, test/num_examples=43793, total_duration=13846.104840, train/accuracy=0.991629, train/loss=0.027274, train/mean_average_precision=0.485642, validation/accuracy=0.986889, validation/loss=0.043774, validation/mean_average_precision=0.277346, validation/num_examples=43793
I0305 13:56:05.943231 140415790769920 logging_writer.py:48] [26900] global_step=26900, grad_norm=0.056285060942173004, loss=0.027504263445734978
I0305 13:56:38.070677 140408968165120 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.04941591992974281, loss=0.027956951409578323
I0305 13:57:10.010756 140415790769920 logging_writer.py:48] [27100] global_step=27100, grad_norm=0.05185387656092644, loss=0.02724498324096203
I0305 13:57:41.872505 140408968165120 logging_writer.py:48] [27200] global_step=27200, grad_norm=0.05868930742144585, loss=0.03105512447655201
I0305 13:58:14.001367 140415790769920 logging_writer.py:48] [27300] global_step=27300, grad_norm=0.05254654958844185, loss=0.030377931892871857
I0305 13:58:45.772583 140408968165120 logging_writer.py:48] [27400] global_step=27400, grad_norm=0.059979695826768875, loss=0.03444661945104599
I0305 13:59:18.290018 140415790769920 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.06269112229347229, loss=0.030048593878746033
I0305 13:59:50.059464 140408968165120 logging_writer.py:48] [27600] global_step=27600, grad_norm=0.0462539866566658, loss=0.026266029104590416
I0305 13:59:55.545113 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:02:00.701988 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:02:03.950648 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:02:06.922063 140576608098112 submission_runner.py:411] Time since start: 14217.63s, 	Step: 27618, 	{'train/accuracy': 0.9916507601737976, 'train/loss': 0.02708692103624344, 'train/mean_average_precision': 0.4838185757181377, 'validation/accuracy': 0.9870354533195496, 'validation/loss': 0.04406972602009773, 'validation/mean_average_precision': 0.2777947569709336, 'validation/num_examples': 43793, 'test/accuracy': 0.9862993359565735, 'test/loss': 0.04691658914089203, 'test/mean_average_precision': 0.2668597689788753, 'test/num_examples': 43793, 'score': 8903.035054683685, 'total_duration': 14217.63026714325, 'accumulated_submission_time': 8903.035054683685, 'accumulated_eval_time': 5312.733390331268, 'accumulated_logging_time': 1.108165979385376}
I0305 14:02:06.942446 140407882708736 logging_writer.py:48] [27618] accumulated_eval_time=5312.733390, accumulated_logging_time=1.108166, accumulated_submission_time=8903.035055, global_step=27618, preemption_count=0, score=8903.035055, test/accuracy=0.986299, test/loss=0.046917, test/mean_average_precision=0.266860, test/num_examples=43793, total_duration=14217.630267, train/accuracy=0.991651, train/loss=0.027087, train/mean_average_precision=0.483819, validation/accuracy=0.987035, validation/loss=0.044070, validation/mean_average_precision=0.277795, validation/num_examples=43793
I0305 14:02:33.459608 140415782377216 logging_writer.py:48] [27700] global_step=27700, grad_norm=0.055670637637376785, loss=0.029338236898183823
I0305 14:03:05.667811 140407882708736 logging_writer.py:48] [27800] global_step=27800, grad_norm=0.09418676793575287, loss=0.03240583464503288
I0305 14:03:37.600594 140415782377216 logging_writer.py:48] [27900] global_step=27900, grad_norm=0.04756090044975281, loss=0.028267739340662956
I0305 14:04:09.517325 140407882708736 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.06313946098089218, loss=0.031007077544927597
I0305 14:04:41.673156 140415782377216 logging_writer.py:48] [28100] global_step=28100, grad_norm=0.05684075132012367, loss=0.029615165665745735
I0305 14:05:13.667656 140407882708736 logging_writer.py:48] [28200] global_step=28200, grad_norm=0.05659066513180733, loss=0.028215376660227776
I0305 14:05:45.943597 140415782377216 logging_writer.py:48] [28300] global_step=28300, grad_norm=0.0553123913705349, loss=0.028942160308361053
I0305 14:06:07.116625 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:08:07.534531 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:08:10.513935 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:08:13.505543 140576608098112 submission_runner.py:411] Time since start: 14584.21s, 	Step: 28366, 	{'train/accuracy': 0.9913768172264099, 'train/loss': 0.02799423597753048, 'train/mean_average_precision': 0.46102037043348004, 'validation/accuracy': 0.9870212078094482, 'validation/loss': 0.0440024808049202, 'validation/mean_average_precision': 0.28214401135528394, 'validation/num_examples': 43793, 'test/accuracy': 0.9861291646957397, 'test/loss': 0.04694217070937157, 'test/mean_average_precision': 0.26535029599512283, 'test/num_examples': 43793, 'score': 9143.177698135376, 'total_duration': 14584.213775396347, 'accumulated_submission_time': 9143.177698135376, 'accumulated_eval_time': 5439.122165679932, 'accumulated_logging_time': 1.1402628421783447}
I0305 14:08:13.525630 140409291962112 logging_writer.py:48] [28366] accumulated_eval_time=5439.122166, accumulated_logging_time=1.140263, accumulated_submission_time=9143.177698, global_step=28366, preemption_count=0, score=9143.177698, test/accuracy=0.986129, test/loss=0.046942, test/mean_average_precision=0.265350, test/num_examples=43793, total_duration=14584.213775, train/accuracy=0.991377, train/loss=0.027994, train/mean_average_precision=0.461020, validation/accuracy=0.987021, validation/loss=0.044002, validation/mean_average_precision=0.282144, validation/num_examples=43793
I0305 14:08:24.983209 140415790769920 logging_writer.py:48] [28400] global_step=28400, grad_norm=0.06111457943916321, loss=0.034270305186510086
I0305 14:08:56.744100 140409291962112 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.052806876599788666, loss=0.03272750601172447
I0305 14:09:28.941128 140415790769920 logging_writer.py:48] [28600] global_step=28600, grad_norm=0.0735846608877182, loss=0.02902565523982048
I0305 14:10:01.165367 140409291962112 logging_writer.py:48] [28700] global_step=28700, grad_norm=0.047955576330423355, loss=0.028569495305418968
I0305 14:10:33.215840 140415790769920 logging_writer.py:48] [28800] global_step=28800, grad_norm=0.05612073838710785, loss=0.028986794874072075
I0305 14:11:05.255667 140409291962112 logging_writer.py:48] [28900] global_step=28900, grad_norm=0.052071355283260345, loss=0.030539855360984802
I0305 14:11:37.057944 140415790769920 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.052019692957401276, loss=0.02795291878283024
I0305 14:12:09.525228 140409291962112 logging_writer.py:48] [29100] global_step=29100, grad_norm=0.04974038526415825, loss=0.031967319548130035
I0305 14:12:13.545292 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:14:18.165482 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:14:21.187425 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:14:24.150573 140576608098112 submission_runner.py:411] Time since start: 14954.86s, 	Step: 29113, 	{'train/accuracy': 0.9914528131484985, 'train/loss': 0.027532100677490234, 'train/mean_average_precision': 0.4668566314086093, 'validation/accuracy': 0.9869303107261658, 'validation/loss': 0.04389664903283119, 'validation/mean_average_precision': 0.2802756131313504, 'validation/num_examples': 43793, 'test/accuracy': 0.9860820174217224, 'test/loss': 0.04669174924492836, 'test/mean_average_precision': 0.26628018617277094, 'test/num_examples': 43793, 'score': 9383.163092136383, 'total_duration': 14954.858783721924, 'accumulated_submission_time': 9383.163092136383, 'accumulated_eval_time': 5569.727295160294, 'accumulated_logging_time': 1.1746866703033447}
I0305 14:14:24.171445 140408968165120 logging_writer.py:48] [29113] accumulated_eval_time=5569.727295, accumulated_logging_time=1.174687, accumulated_submission_time=9383.163092, global_step=29113, preemption_count=0, score=9383.163092, test/accuracy=0.986082, test/loss=0.046692, test/mean_average_precision=0.266280, test/num_examples=43793, total_duration=14954.858784, train/accuracy=0.991453, train/loss=0.027532, train/mean_average_precision=0.466857, validation/accuracy=0.986930, validation/loss=0.043897, validation/mean_average_precision=0.280276, validation/num_examples=43793
I0305 14:14:52.704151 140415782377216 logging_writer.py:48] [29200] global_step=29200, grad_norm=0.05768202617764473, loss=0.03175513818860054
I0305 14:15:24.855371 140408968165120 logging_writer.py:48] [29300] global_step=29300, grad_norm=0.05682658031582832, loss=0.030491573736071587
I0305 14:15:56.834547 140415782377216 logging_writer.py:48] [29400] global_step=29400, grad_norm=0.05081932619214058, loss=0.029821278527379036
I0305 14:16:29.481739 140408968165120 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.05870767682790756, loss=0.02922762930393219
I0305 14:17:01.667683 140415782377216 logging_writer.py:48] [29600] global_step=29600, grad_norm=0.04827573522925377, loss=0.02334524691104889
I0305 14:17:33.530893 140408968165120 logging_writer.py:48] [29700] global_step=29700, grad_norm=0.0493822880089283, loss=0.02947121486067772
I0305 14:18:05.653865 140415782377216 logging_writer.py:48] [29800] global_step=29800, grad_norm=0.05069392919540405, loss=0.028121409937739372
I0305 14:18:24.263988 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:20:22.950328 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:20:25.955663 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:20:28.946771 140576608098112 submission_runner.py:411] Time since start: 15319.66s, 	Step: 29860, 	{'train/accuracy': 0.9916426539421082, 'train/loss': 0.026922324672341347, 'train/mean_average_precision': 0.4755547289723321, 'validation/accuracy': 0.9869992733001709, 'validation/loss': 0.04400300979614258, 'validation/mean_average_precision': 0.28887987837583345, 'validation/num_examples': 43793, 'test/accuracy': 0.9861574172973633, 'test/loss': 0.04688797518610954, 'test/mean_average_precision': 0.27571454673421936, 'test/num_examples': 43793, 'score': 9623.224762439728, 'total_duration': 15319.655102968216, 'accumulated_submission_time': 9623.224762439728, 'accumulated_eval_time': 5694.410034656525, 'accumulated_logging_time': 1.206719160079956}
I0305 14:20:28.967642 140407882708736 logging_writer.py:48] [29860] accumulated_eval_time=5694.410035, accumulated_logging_time=1.206719, accumulated_submission_time=9623.224762, global_step=29860, preemption_count=0, score=9623.224762, test/accuracy=0.986157, test/loss=0.046888, test/mean_average_precision=0.275715, test/num_examples=43793, total_duration=15319.655103, train/accuracy=0.991643, train/loss=0.026922, train/mean_average_precision=0.475555, validation/accuracy=0.986999, validation/loss=0.044003, validation/mean_average_precision=0.288880, validation/num_examples=43793
I0305 14:20:42.132171 140415790769920 logging_writer.py:48] [29900] global_step=29900, grad_norm=0.049963343888521194, loss=0.025494815781712532
I0305 14:21:14.467352 140407882708736 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.060175422579050064, loss=0.03206782042980194
I0305 14:21:46.410375 140415790769920 logging_writer.py:48] [30100] global_step=30100, grad_norm=0.05323774367570877, loss=0.026744922623038292
I0305 14:22:18.784363 140407882708736 logging_writer.py:48] [30200] global_step=30200, grad_norm=0.0609310120344162, loss=0.031123580411076546
I0305 14:22:50.970615 140415790769920 logging_writer.py:48] [30300] global_step=30300, grad_norm=0.07783954590559006, loss=0.03169950097799301
I0305 14:23:23.524632 140407882708736 logging_writer.py:48] [30400] global_step=30400, grad_norm=0.051900193095207214, loss=0.025355881080031395
I0305 14:23:55.733318 140415790769920 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.051938094198703766, loss=0.027225201949477196
I0305 14:24:27.997503 140407882708736 logging_writer.py:48] [30600] global_step=30600, grad_norm=0.05852408707141876, loss=0.0279217716306448
I0305 14:24:28.972077 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:26:32.040142 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:26:35.052899 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:26:38.033215 140576608098112 submission_runner.py:411] Time since start: 15688.74s, 	Step: 30604, 	{'train/accuracy': 0.9915139079093933, 'train/loss': 0.02736796997487545, 'train/mean_average_precision': 0.49025850926333997, 'validation/accuracy': 0.987023651599884, 'validation/loss': 0.0440075658261776, 'validation/mean_average_precision': 0.27885622403675214, 'validation/num_examples': 43793, 'test/accuracy': 0.9861317276954651, 'test/loss': 0.04688337445259094, 'test/mean_average_precision': 0.26732608614613884, 'test/num_examples': 43793, 'score': 9863.198410272598, 'total_duration': 15688.741545438766, 'accumulated_submission_time': 9863.198410272598, 'accumulated_eval_time': 5823.4711356163025, 'accumulated_logging_time': 1.2388403415679932}
I0305 14:26:38.054371 140408968165120 logging_writer.py:48] [30604] accumulated_eval_time=5823.471136, accumulated_logging_time=1.238840, accumulated_submission_time=9863.198410, global_step=30604, preemption_count=0, score=9863.198410, test/accuracy=0.986132, test/loss=0.046883, test/mean_average_precision=0.267326, test/num_examples=43793, total_duration=15688.741545, train/accuracy=0.991514, train/loss=0.027368, train/mean_average_precision=0.490259, validation/accuracy=0.987024, validation/loss=0.044008, validation/mean_average_precision=0.278856, validation/num_examples=43793
I0305 14:27:09.530450 140415782377216 logging_writer.py:48] [30700] global_step=30700, grad_norm=0.053085409104824066, loss=0.02779809944331646
I0305 14:27:42.068248 140408968165120 logging_writer.py:48] [30800] global_step=30800, grad_norm=0.06388877332210541, loss=0.029726780951023102
I0305 14:28:14.156480 140415782377216 logging_writer.py:48] [30900] global_step=30900, grad_norm=0.061501387506723404, loss=0.03113693743944168
I0305 14:28:45.904644 140408968165120 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.06235695257782936, loss=0.02574840374290943
I0305 14:29:18.224472 140415782377216 logging_writer.py:48] [31100] global_step=31100, grad_norm=0.047986093908548355, loss=0.029161488637328148
I0305 14:29:49.935300 140408968165120 logging_writer.py:48] [31200] global_step=31200, grad_norm=0.06137009710073471, loss=0.029685230925679207
I0305 14:30:21.904125 140415782377216 logging_writer.py:48] [31300] global_step=31300, grad_norm=0.0589105486869812, loss=0.02948603220283985
I0305 14:30:38.167714 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:32:37.882542 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:32:40.915776 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:32:43.889971 140576608098112 submission_runner.py:411] Time since start: 16054.60s, 	Step: 31352, 	{'train/accuracy': 0.9917362928390503, 'train/loss': 0.026557233184576035, 'train/mean_average_precision': 0.5076482448586728, 'validation/accuracy': 0.987015962600708, 'validation/loss': 0.04452977702021599, 'validation/mean_average_precision': 0.28086747862193595, 'validation/num_examples': 43793, 'test/accuracy': 0.9862100481987, 'test/loss': 0.047140032052993774, 'test/mean_average_precision': 0.2710934963817139, 'test/num_examples': 43793, 'score': 10103.280684947968, 'total_duration': 16054.598301887512, 'accumulated_submission_time': 10103.280684947968, 'accumulated_eval_time': 5949.193348169327, 'accumulated_logging_time': 1.2714645862579346}
I0305 14:32:43.921096 140407882708736 logging_writer.py:48] [31352] accumulated_eval_time=5949.193348, accumulated_logging_time=1.271465, accumulated_submission_time=10103.280685, global_step=31352, preemption_count=0, score=10103.280685, test/accuracy=0.986210, test/loss=0.047140, test/mean_average_precision=0.271093, test/num_examples=43793, total_duration=16054.598302, train/accuracy=0.991736, train/loss=0.026557, train/mean_average_precision=0.507648, validation/accuracy=0.987016, validation/loss=0.044530, validation/mean_average_precision=0.280867, validation/num_examples=43793
I0305 14:32:59.413662 140409291962112 logging_writer.py:48] [31400] global_step=31400, grad_norm=0.06586096435785294, loss=0.030721064656972885
I0305 14:33:31.437097 140407882708736 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.054553091526031494, loss=0.026686066761612892
I0305 14:34:03.567868 140409291962112 logging_writer.py:48] [31600] global_step=31600, grad_norm=0.051664404571056366, loss=0.030536901205778122
I0305 14:34:35.685495 140407882708736 logging_writer.py:48] [31700] global_step=31700, grad_norm=0.07251936942338943, loss=0.030205896124243736
I0305 14:35:08.198123 140409291962112 logging_writer.py:48] [31800] global_step=31800, grad_norm=0.05916230380535126, loss=0.030598938465118408
I0305 14:35:40.406602 140407882708736 logging_writer.py:48] [31900] global_step=31900, grad_norm=0.0637165904045105, loss=0.02598624862730503
I0305 14:36:12.448450 140409291962112 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.06616545468568802, loss=0.034162379801273346
I0305 14:36:44.208508 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:38:45.013583 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:38:48.042248 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:38:51.036920 140576608098112 submission_runner.py:411] Time since start: 16421.75s, 	Step: 32100, 	{'train/accuracy': 0.9919912815093994, 'train/loss': 0.025699807330965996, 'train/mean_average_precision': 0.516720698352363, 'validation/accuracy': 0.9870447516441345, 'validation/loss': 0.04405251517891884, 'validation/mean_average_precision': 0.2836175444213666, 'validation/num_examples': 43793, 'test/accuracy': 0.9861077070236206, 'test/loss': 0.04694873467087746, 'test/mean_average_precision': 0.27231138943050026, 'test/num_examples': 43793, 'score': 10343.537009716034, 'total_duration': 16421.74523949623, 'accumulated_submission_time': 10343.537009716034, 'accumulated_eval_time': 6076.021708488464, 'accumulated_logging_time': 1.3138582706451416}
I0305 14:38:51.059300 140408968165120 logging_writer.py:48] [32100] accumulated_eval_time=6076.021708, accumulated_logging_time=1.313858, accumulated_submission_time=10343.537010, global_step=32100, preemption_count=0, score=10343.537010, test/accuracy=0.986108, test/loss=0.046949, test/mean_average_precision=0.272311, test/num_examples=43793, total_duration=16421.745239, train/accuracy=0.991991, train/loss=0.025700, train/mean_average_precision=0.516721, validation/accuracy=0.987045, validation/loss=0.044053, validation/mean_average_precision=0.283618, validation/num_examples=43793
I0305 14:38:51.406607 140415782377216 logging_writer.py:48] [32100] global_step=32100, grad_norm=0.06874027848243713, loss=0.02938094548881054
I0305 14:39:23.996928 140408968165120 logging_writer.py:48] [32200] global_step=32200, grad_norm=0.06650915741920471, loss=0.029091795906424522
I0305 14:39:56.199079 140415782377216 logging_writer.py:48] [32300] global_step=32300, grad_norm=0.06683004647493362, loss=0.027537427842617035
I0305 14:40:28.409542 140408968165120 logging_writer.py:48] [32400] global_step=32400, grad_norm=0.05288510024547577, loss=0.029255831614136696
I0305 14:41:00.288253 140415782377216 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.055980294942855835, loss=0.02979331836104393
I0305 14:41:32.604764 140408968165120 logging_writer.py:48] [32600] global_step=32600, grad_norm=0.07686371356248856, loss=0.029972368851304054
I0305 14:42:04.814222 140415782377216 logging_writer.py:48] [32700] global_step=32700, grad_norm=0.05665341019630432, loss=0.0277597364038229
I0305 14:42:36.839397 140408968165120 logging_writer.py:48] [32800] global_step=32800, grad_norm=0.06757953763008118, loss=0.030744116753339767
I0305 14:42:51.162138 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:44:52.272051 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:44:55.322625 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:44:58.352077 140576608098112 submission_runner.py:411] Time since start: 16789.06s, 	Step: 32846, 	{'train/accuracy': 0.9919595122337341, 'train/loss': 0.025757083669304848, 'train/mean_average_precision': 0.5138833451896644, 'validation/accuracy': 0.9868990182876587, 'validation/loss': 0.04389920085668564, 'validation/mean_average_precision': 0.2828000828133626, 'validation/num_examples': 43793, 'test/accuracy': 0.985975444316864, 'test/loss': 0.0468091182410717, 'test/mean_average_precision': 0.26972755064950915, 'test/num_examples': 43793, 'score': 10583.608781576157, 'total_duration': 16789.060408830643, 'accumulated_submission_time': 10583.608781576157, 'accumulated_eval_time': 6203.211605548859, 'accumulated_logging_time': 1.3478012084960938}
I0305 14:44:58.373739 140407882708736 logging_writer.py:48] [32846] accumulated_eval_time=6203.211606, accumulated_logging_time=1.347801, accumulated_submission_time=10583.608782, global_step=32846, preemption_count=0, score=10583.608782, test/accuracy=0.985975, test/loss=0.046809, test/mean_average_precision=0.269728, test/num_examples=43793, total_duration=16789.060409, train/accuracy=0.991960, train/loss=0.025757, train/mean_average_precision=0.513883, validation/accuracy=0.986899, validation/loss=0.043899, validation/mean_average_precision=0.282800, validation/num_examples=43793
I0305 14:45:16.424763 140415790769920 logging_writer.py:48] [32900] global_step=32900, grad_norm=0.06060957908630371, loss=0.025451352819800377
I0305 14:45:48.584184 140407882708736 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.07435762137174606, loss=0.03241388499736786
I0305 14:46:21.261498 140415790769920 logging_writer.py:48] [33100] global_step=33100, grad_norm=0.06673037260770798, loss=0.029371753334999084
I0305 14:46:53.943958 140407882708736 logging_writer.py:48] [33200] global_step=33200, grad_norm=0.053547535091638565, loss=0.025647897273302078
I0305 14:47:26.311738 140415790769920 logging_writer.py:48] [33300] global_step=33300, grad_norm=0.062216032296419144, loss=0.02911573462188244
I0305 14:47:58.468234 140407882708736 logging_writer.py:48] [33400] global_step=33400, grad_norm=0.06147700920701027, loss=0.02651890181005001
I0305 14:48:30.733029 140415790769920 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.06179296225309372, loss=0.026424098759889603
I0305 14:48:58.648371 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:50:59.540830 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:51:02.803998 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:51:05.823535 140576608098112 submission_runner.py:411] Time since start: 17156.53s, 	Step: 33587, 	{'train/accuracy': 0.9921208620071411, 'train/loss': 0.0254428181797266, 'train/mean_average_precision': 0.5243534403747736, 'validation/accuracy': 0.9870321750640869, 'validation/loss': 0.04390634596347809, 'validation/mean_average_precision': 0.28328172468329593, 'validation/num_examples': 43793, 'test/accuracy': 0.9861405491828918, 'test/loss': 0.04691572114825249, 'test/mean_average_precision': 0.2739759901800147, 'test/num_examples': 43793, 'score': 10823.852623224258, 'total_duration': 17156.531865119934, 'accumulated_submission_time': 10823.852623224258, 'accumulated_eval_time': 6330.386728048325, 'accumulated_logging_time': 1.3803482055664062}
I0305 14:51:05.845458 140409291962112 logging_writer.py:48] [33587] accumulated_eval_time=6330.386728, accumulated_logging_time=1.380348, accumulated_submission_time=10823.852623, global_step=33587, preemption_count=0, score=10823.852623, test/accuracy=0.986141, test/loss=0.046916, test/mean_average_precision=0.273976, test/num_examples=43793, total_duration=17156.531865, train/accuracy=0.992121, train/loss=0.025443, train/mean_average_precision=0.524353, validation/accuracy=0.987032, validation/loss=0.043906, validation/mean_average_precision=0.283282, validation/num_examples=43793
I0305 14:51:10.318844 140415782377216 logging_writer.py:48] [33600] global_step=33600, grad_norm=0.0686761736869812, loss=0.028735928237438202
I0305 14:51:42.638209 140409291962112 logging_writer.py:48] [33700] global_step=33700, grad_norm=0.05898191034793854, loss=0.030985798686742783
I0305 14:52:14.947717 140415782377216 logging_writer.py:48] [33800] global_step=33800, grad_norm=0.05474506691098213, loss=0.02779657579958439
I0305 14:52:47.077514 140409291962112 logging_writer.py:48] [33900] global_step=33900, grad_norm=0.06667168438434601, loss=0.03233041614294052
I0305 14:53:19.600424 140415782377216 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.06462552398443222, loss=0.0284454133361578
I0305 14:53:51.758839 140409291962112 logging_writer.py:48] [34100] global_step=34100, grad_norm=0.06465747207403183, loss=0.03036586008965969
I0305 14:54:24.055504 140415782377216 logging_writer.py:48] [34200] global_step=34200, grad_norm=0.06084802746772766, loss=0.02840116247534752
I0305 14:54:56.254708 140409291962112 logging_writer.py:48] [34300] global_step=34300, grad_norm=0.06836263835430145, loss=0.03078606352210045
I0305 14:55:06.022777 140576608098112 spec.py:321] Evaluating on the training split.
I0305 14:57:11.134604 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 14:57:14.172069 140576608098112 spec.py:349] Evaluating on the test split.
I0305 14:57:17.171782 140576608098112 submission_runner.py:411] Time since start: 17527.88s, 	Step: 34331, 	{'train/accuracy': 0.9919935464859009, 'train/loss': 0.025831956416368484, 'train/mean_average_precision': 0.5051137293483496, 'validation/accuracy': 0.9869655966758728, 'validation/loss': 0.04414217919111252, 'validation/mean_average_precision': 0.2822380868318115, 'validation/num_examples': 43793, 'test/accuracy': 0.9860954880714417, 'test/loss': 0.04709390923380852, 'test/mean_average_precision': 0.26883364775840923, 'test/num_examples': 43793, 'score': 11063.997322559357, 'total_duration': 17527.88009405136, 'accumulated_submission_time': 11063.997322559357, 'accumulated_eval_time': 6461.535669565201, 'accumulated_logging_time': 1.4151837825775146}
I0305 14:57:17.193789 140407882708736 logging_writer.py:48] [34331] accumulated_eval_time=6461.535670, accumulated_logging_time=1.415184, accumulated_submission_time=11063.997323, global_step=34331, preemption_count=0, score=11063.997323, test/accuracy=0.986095, test/loss=0.047094, test/mean_average_precision=0.268834, test/num_examples=43793, total_duration=17527.880094, train/accuracy=0.991994, train/loss=0.025832, train/mean_average_precision=0.505114, validation/accuracy=0.986966, validation/loss=0.044142, validation/mean_average_precision=0.282238, validation/num_examples=43793
I0305 14:57:39.469433 140415790769920 logging_writer.py:48] [34400] global_step=34400, grad_norm=0.05784278362989426, loss=0.027749784290790558
I0305 14:58:11.474732 140407882708736 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.061373934149742126, loss=0.027796288952231407
I0305 14:58:43.516603 140415790769920 logging_writer.py:48] [34600] global_step=34600, grad_norm=0.06786201894283295, loss=0.03071950562298298
I0305 14:59:15.950444 140407882708736 logging_writer.py:48] [34700] global_step=34700, grad_norm=0.06646184623241425, loss=0.028614314272999763
I0305 14:59:48.250678 140415790769920 logging_writer.py:48] [34800] global_step=34800, grad_norm=0.06364782899618149, loss=0.028192711994051933
I0305 15:00:20.799972 140407882708736 logging_writer.py:48] [34900] global_step=34900, grad_norm=0.04687470197677612, loss=0.02467689849436283
I0305 15:00:52.711610 140415790769920 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.060668230056762695, loss=0.02798314206302166
I0305 15:01:17.250547 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:03:16.332068 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:03:19.385550 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:03:22.356295 140576608098112 submission_runner.py:411] Time since start: 17893.06s, 	Step: 35077, 	{'train/accuracy': 0.9918292164802551, 'train/loss': 0.0262144785374403, 'train/mean_average_precision': 0.5070033636209796, 'validation/accuracy': 0.9870935082435608, 'validation/loss': 0.04442230612039566, 'validation/mean_average_precision': 0.2835313514000049, 'validation/num_examples': 43793, 'test/accuracy': 0.9861544370651245, 'test/loss': 0.0474284365773201, 'test/mean_average_precision': 0.2655076803593601, 'test/num_examples': 43793, 'score': 11304.021178007126, 'total_duration': 17893.06462287903, 'accumulated_submission_time': 11304.021178007126, 'accumulated_eval_time': 6586.641371488571, 'accumulated_logging_time': 1.4497976303100586}
I0305 15:03:22.379035 140408968165120 logging_writer.py:48] [35077] accumulated_eval_time=6586.641371, accumulated_logging_time=1.449798, accumulated_submission_time=11304.021178, global_step=35077, preemption_count=0, score=11304.021178, test/accuracy=0.986154, test/loss=0.047428, test/mean_average_precision=0.265508, test/num_examples=43793, total_duration=17893.064623, train/accuracy=0.991829, train/loss=0.026214, train/mean_average_precision=0.507003, validation/accuracy=0.987094, validation/loss=0.044422, validation/mean_average_precision=0.283531, validation/num_examples=43793
I0305 15:03:30.066520 140409291962112 logging_writer.py:48] [35100] global_step=35100, grad_norm=0.05807990953326225, loss=0.027622109279036522
I0305 15:04:02.202009 140408968165120 logging_writer.py:48] [35200] global_step=35200, grad_norm=0.05370011925697327, loss=0.025336770340800285
I0305 15:04:34.559573 140409291962112 logging_writer.py:48] [35300] global_step=35300, grad_norm=0.0676964819431305, loss=0.029557764530181885
I0305 15:05:07.266257 140408968165120 logging_writer.py:48] [35400] global_step=35400, grad_norm=0.061152759939432144, loss=0.03150615841150284
I0305 15:05:39.503120 140409291962112 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.06338674575090408, loss=0.024980098009109497
I0305 15:06:11.903640 140408968165120 logging_writer.py:48] [35600] global_step=35600, grad_norm=0.06875065714120865, loss=0.024036558344960213
I0305 15:06:44.224488 140409291962112 logging_writer.py:48] [35700] global_step=35700, grad_norm=0.0629805400967598, loss=0.030354149639606476
I0305 15:07:16.748257 140408968165120 logging_writer.py:48] [35800] global_step=35800, grad_norm=0.06661120057106018, loss=0.028607161715626717
I0305 15:07:22.553084 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:09:23.469666 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:09:26.466932 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:09:29.467998 140576608098112 submission_runner.py:411] Time since start: 18260.18s, 	Step: 35819, 	{'train/accuracy': 0.9920508861541748, 'train/loss': 0.0255311019718647, 'train/mean_average_precision': 0.5223396912909026, 'validation/accuracy': 0.9869850873947144, 'validation/loss': 0.04429525509476662, 'validation/mean_average_precision': 0.28101788230582464, 'validation/num_examples': 43793, 'test/accuracy': 0.986088752746582, 'test/loss': 0.04731065034866333, 'test/mean_average_precision': 0.26605864515437305, 'test/num_examples': 43793, 'score': 11544.161909103394, 'total_duration': 18260.17632985115, 'accumulated_submission_time': 11544.161909103394, 'accumulated_eval_time': 6713.556245326996, 'accumulated_logging_time': 1.4856345653533936}
I0305 15:09:29.490526 140415782377216 logging_writer.py:48] [35819] accumulated_eval_time=6713.556245, accumulated_logging_time=1.485635, accumulated_submission_time=11544.161909, global_step=35819, preemption_count=0, score=11544.161909, test/accuracy=0.986089, test/loss=0.047311, test/mean_average_precision=0.266059, test/num_examples=43793, total_duration=18260.176330, train/accuracy=0.992051, train/loss=0.025531, train/mean_average_precision=0.522340, validation/accuracy=0.986985, validation/loss=0.044295, validation/mean_average_precision=0.281018, validation/num_examples=43793
I0305 15:09:55.657524 140415790769920 logging_writer.py:48] [35900] global_step=35900, grad_norm=0.06920605152845383, loss=0.031339410692453384
I0305 15:10:28.057677 140415782377216 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.07770276069641113, loss=0.030419036746025085
I0305 15:11:00.264230 140415790769920 logging_writer.py:48] [36100] global_step=36100, grad_norm=0.06794022768735886, loss=0.02877594716846943
I0305 15:11:32.460435 140415782377216 logging_writer.py:48] [36200] global_step=36200, grad_norm=0.06715700775384903, loss=0.029713325202465057
I0305 15:12:04.755985 140415790769920 logging_writer.py:48] [36300] global_step=36300, grad_norm=0.07510219514369965, loss=0.026552116498351097
I0305 15:12:36.699985 140415782377216 logging_writer.py:48] [36400] global_step=36400, grad_norm=0.07052278518676758, loss=0.026652514934539795
I0305 15:13:09.101676 140415790769920 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.05669329687952995, loss=0.024023625999689102
I0305 15:13:29.627458 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:15:34.766046 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:15:37.802913 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:15:40.799946 140576608098112 submission_runner.py:411] Time since start: 18631.51s, 	Step: 36565, 	{'train/accuracy': 0.9919530153274536, 'train/loss': 0.025791175663471222, 'train/mean_average_precision': 0.5123207397683507, 'validation/accuracy': 0.9869518280029297, 'validation/loss': 0.04422156885266304, 'validation/mean_average_precision': 0.28593843381487943, 'validation/num_examples': 43793, 'test/accuracy': 0.9860820174217224, 'test/loss': 0.04707071930170059, 'test/mean_average_precision': 0.266341244878793, 'test/num_examples': 43793, 'score': 11784.26586842537, 'total_duration': 18631.508261442184, 'accumulated_submission_time': 11784.26586842537, 'accumulated_eval_time': 6844.728674411774, 'accumulated_logging_time': 1.5204980373382568}
I0305 15:15:40.822888 140407882708736 logging_writer.py:48] [36565] accumulated_eval_time=6844.728674, accumulated_logging_time=1.520498, accumulated_submission_time=11784.265868, global_step=36565, preemption_count=0, score=11784.265868, test/accuracy=0.986082, test/loss=0.047071, test/mean_average_precision=0.266341, test/num_examples=43793, total_duration=18631.508261, train/accuracy=0.991953, train/loss=0.025791, train/mean_average_precision=0.512321, validation/accuracy=0.986952, validation/loss=0.044222, validation/mean_average_precision=0.285938, validation/num_examples=43793
I0305 15:15:53.626104 140409291962112 logging_writer.py:48] [36600] global_step=36600, grad_norm=0.06321320682764053, loss=0.027681313455104828
I0305 15:16:26.616685 140407882708736 logging_writer.py:48] [36700] global_step=36700, grad_norm=0.06332850456237793, loss=0.02368301711976528
I0305 15:16:59.192194 140409291962112 logging_writer.py:48] [36800] global_step=36800, grad_norm=0.058965131640434265, loss=0.027767697349190712
I0305 15:17:31.920618 140407882708736 logging_writer.py:48] [36900] global_step=36900, grad_norm=0.06350888311862946, loss=0.03000892512500286
I0305 15:18:04.461594 140409291962112 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.06561771780252457, loss=0.02555513195693493
I0305 15:18:36.961407 140407882708736 logging_writer.py:48] [37100] global_step=37100, grad_norm=0.06632329523563385, loss=0.026123059913516045
I0305 15:19:09.573240 140409291962112 logging_writer.py:48] [37200] global_step=37200, grad_norm=0.06697183847427368, loss=0.027914660051465034
I0305 15:19:40.955768 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:21:44.701556 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:21:47.785270 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:21:50.794033 140576608098112 submission_runner.py:411] Time since start: 19001.50s, 	Step: 37298, 	{'train/accuracy': 0.9920364618301392, 'train/loss': 0.025380199775099754, 'train/mean_average_precision': 0.5294398886495806, 'validation/accuracy': 0.9870569705963135, 'validation/loss': 0.04454030469059944, 'validation/mean_average_precision': 0.28385249856358236, 'validation/num_examples': 43793, 'test/accuracy': 0.9861974120140076, 'test/loss': 0.04754181578755379, 'test/mean_average_precision': 0.26783503402507586, 'test/num_examples': 43793, 'score': 12024.3671708107, 'total_duration': 19001.502363681793, 'accumulated_submission_time': 12024.3671708107, 'accumulated_eval_time': 6974.566897153854, 'accumulated_logging_time': 1.554847240447998}
I0305 15:21:50.816709 140408968165120 logging_writer.py:48] [37298] accumulated_eval_time=6974.566897, accumulated_logging_time=1.554847, accumulated_submission_time=12024.367171, global_step=37298, preemption_count=0, score=12024.367171, test/accuracy=0.986197, test/loss=0.047542, test/mean_average_precision=0.267835, test/num_examples=43793, total_duration=19001.502364, train/accuracy=0.992036, train/loss=0.025380, train/mean_average_precision=0.529440, validation/accuracy=0.987057, validation/loss=0.044540, validation/mean_average_precision=0.283852, validation/num_examples=43793
I0305 15:21:51.922124 140415782377216 logging_writer.py:48] [37300] global_step=37300, grad_norm=0.06366144865751266, loss=0.02618253231048584
I0305 15:22:24.570493 140408968165120 logging_writer.py:48] [37400] global_step=37400, grad_norm=0.06394638121128082, loss=0.032157052308321
I0305 15:22:56.471064 140415782377216 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.07872093468904495, loss=0.02790648303925991
I0305 15:23:28.805153 140408968165120 logging_writer.py:48] [37600] global_step=37600, grad_norm=0.08002504706382751, loss=0.0288558192551136
I0305 15:24:01.331363 140415782377216 logging_writer.py:48] [37700] global_step=37700, grad_norm=0.0613158717751503, loss=0.02516431175172329
I0305 15:24:33.832188 140408968165120 logging_writer.py:48] [37800] global_step=37800, grad_norm=0.06710808724164963, loss=0.02807583473622799
I0305 15:25:07.904956 140415782377216 logging_writer.py:48] [37900] global_step=37900, grad_norm=0.05386282876133919, loss=0.02305697277188301
I0305 15:25:41.384251 140408968165120 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.07846337556838989, loss=0.026484576985239983
I0305 15:25:50.803827 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:27:53.246318 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:27:56.290439 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:27:59.337318 140576608098112 submission_runner.py:411] Time since start: 19370.05s, 	Step: 38029, 	{'train/accuracy': 0.9923050403594971, 'train/loss': 0.02453015372157097, 'train/mean_average_precision': 0.5316224932311173, 'validation/accuracy': 0.9869834780693054, 'validation/loss': 0.044165726751089096, 'validation/mean_average_precision': 0.2815878288103547, 'validation/num_examples': 43793, 'test/accuracy': 0.9860537648200989, 'test/loss': 0.04686699062585831, 'test/mean_average_precision': 0.2705610278003447, 'test/num_examples': 43793, 'score': 12264.32112288475, 'total_duration': 19370.045646190643, 'accumulated_submission_time': 12264.32112288475, 'accumulated_eval_time': 7103.1003510952, 'accumulated_logging_time': 1.5899608135223389}
I0305 15:27:59.361445 140409291962112 logging_writer.py:48] [38029] accumulated_eval_time=7103.100351, accumulated_logging_time=1.589961, accumulated_submission_time=12264.321123, global_step=38029, preemption_count=0, score=12264.321123, test/accuracy=0.986054, test/loss=0.046867, test/mean_average_precision=0.270561, test/num_examples=43793, total_duration=19370.045646, train/accuracy=0.992305, train/loss=0.024530, train/mean_average_precision=0.531622, validation/accuracy=0.986983, validation/loss=0.044166, validation/mean_average_precision=0.281588, validation/num_examples=43793
I0305 15:28:22.529932 140415790769920 logging_writer.py:48] [38100] global_step=38100, grad_norm=0.07817018032073975, loss=0.02964595891535282
I0305 15:28:54.624253 140409291962112 logging_writer.py:48] [38200] global_step=38200, grad_norm=0.07695908844470978, loss=0.026600679382681847
I0305 15:29:26.831638 140415790769920 logging_writer.py:48] [38300] global_step=38300, grad_norm=0.07300348579883575, loss=0.025853289291262627
I0305 15:29:58.805846 140409291962112 logging_writer.py:48] [38400] global_step=38400, grad_norm=0.07398998737335205, loss=0.029160352423787117
I0305 15:30:30.849330 140415790769920 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.07634599506855011, loss=0.027646759524941444
I0305 15:31:03.214413 140409291962112 logging_writer.py:48] [38600] global_step=38600, grad_norm=0.06485041230916977, loss=0.024232881143689156
I0305 15:31:35.224454 140415790769920 logging_writer.py:48] [38700] global_step=38700, grad_norm=0.06224270537495613, loss=0.025537880137562752
I0305 15:31:59.406448 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:33:58.366529 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:34:01.410374 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:34:04.535424 140576608098112 submission_runner.py:411] Time since start: 19735.24s, 	Step: 38775, 	{'train/accuracy': 0.9925279021263123, 'train/loss': 0.023959780111908913, 'train/mean_average_precision': 0.5613465445746542, 'validation/accuracy': 0.9869436621665955, 'validation/loss': 0.04463827237486839, 'validation/mean_average_precision': 0.27956828896728675, 'validation/num_examples': 43793, 'test/accuracy': 0.9861186742782593, 'test/loss': 0.04747012257575989, 'test/mean_average_precision': 0.27001978257050463, 'test/num_examples': 43793, 'score': 12504.334551811218, 'total_duration': 19735.243750810623, 'accumulated_submission_time': 12504.334551811218, 'accumulated_eval_time': 7228.229280233383, 'accumulated_logging_time': 1.6254029273986816}
I0305 15:34:04.558962 140407882708736 logging_writer.py:48] [38775] accumulated_eval_time=7228.229280, accumulated_logging_time=1.625403, accumulated_submission_time=12504.334552, global_step=38775, preemption_count=0, score=12504.334552, test/accuracy=0.986119, test/loss=0.047470, test/mean_average_precision=0.270020, test/num_examples=43793, total_duration=19735.243751, train/accuracy=0.992528, train/loss=0.023960, train/mean_average_precision=0.561347, validation/accuracy=0.986944, validation/loss=0.044638, validation/mean_average_precision=0.279568, validation/num_examples=43793
I0305 15:34:12.857731 140415782377216 logging_writer.py:48] [38800] global_step=38800, grad_norm=0.06913265585899353, loss=0.02745431661605835
I0305 15:34:44.750842 140407882708736 logging_writer.py:48] [38900] global_step=38900, grad_norm=0.06761163473129272, loss=0.025389287620782852
I0305 15:35:16.966940 140415782377216 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.07286062836647034, loss=0.027707941830158234
I0305 15:35:48.923935 140407882708736 logging_writer.py:48] [39100] global_step=39100, grad_norm=0.08320915699005127, loss=0.02612733095884323
I0305 15:36:21.540423 140415782377216 logging_writer.py:48] [39200] global_step=39200, grad_norm=0.08578292280435562, loss=0.024245871230959892
I0305 15:36:53.192950 140407882708736 logging_writer.py:48] [39300] global_step=39300, grad_norm=0.0711483433842659, loss=0.02708706445991993
I0305 15:37:26.193033 140415782377216 logging_writer.py:48] [39400] global_step=39400, grad_norm=0.06547792255878448, loss=0.026951666921377182
I0305 15:37:59.233220 140407882708736 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.08146757632493973, loss=0.027845773845911026
I0305 15:38:04.823859 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:40:01.652706 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:40:04.697499 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:40:07.678874 140576608098112 submission_runner.py:411] Time since start: 20098.39s, 	Step: 39518, 	{'train/accuracy': 0.992580771446228, 'train/loss': 0.023770621046423912, 'train/mean_average_precision': 0.5658957285509353, 'validation/accuracy': 0.986880362033844, 'validation/loss': 0.04439288750290871, 'validation/mean_average_precision': 0.28546028767357673, 'validation/num_examples': 43793, 'test/accuracy': 0.986042857170105, 'test/loss': 0.04738441854715347, 'test/mean_average_precision': 0.2716201668149071, 'test/num_examples': 43793, 'score': 12744.566393852234, 'total_duration': 20098.38708305359, 'accumulated_submission_time': 12744.566393852234, 'accumulated_eval_time': 7351.084131479263, 'accumulated_logging_time': 1.660273790359497}
I0305 15:40:07.701646 140408968165120 logging_writer.py:48] [39518] accumulated_eval_time=7351.084131, accumulated_logging_time=1.660274, accumulated_submission_time=12744.566394, global_step=39518, preemption_count=0, score=12744.566394, test/accuracy=0.986043, test/loss=0.047384, test/mean_average_precision=0.271620, test/num_examples=43793, total_duration=20098.387083, train/accuracy=0.992581, train/loss=0.023771, train/mean_average_precision=0.565896, validation/accuracy=0.986880, validation/loss=0.044393, validation/mean_average_precision=0.285460, validation/num_examples=43793
I0305 15:40:34.462759 140415790769920 logging_writer.py:48] [39600] global_step=39600, grad_norm=0.07502373307943344, loss=0.028158951550722122
I0305 15:41:06.912146 140408968165120 logging_writer.py:48] [39700] global_step=39700, grad_norm=0.05649000033736229, loss=0.024884255602955818
I0305 15:41:39.140060 140415790769920 logging_writer.py:48] [39800] global_step=39800, grad_norm=0.0636412650346756, loss=0.025686077773571014
I0305 15:42:11.230282 140408968165120 logging_writer.py:48] [39900] global_step=39900, grad_norm=0.07097409665584564, loss=0.028078919276595116
I0305 15:42:43.198216 140415790769920 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.07144255936145782, loss=0.028414389118552208
I0305 15:43:15.456721 140408968165120 logging_writer.py:48] [40100] global_step=40100, grad_norm=0.0608467273414135, loss=0.026658905670046806
I0305 15:43:47.543691 140415790769920 logging_writer.py:48] [40200] global_step=40200, grad_norm=0.06488003581762314, loss=0.024371281266212463
I0305 15:44:07.801253 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:46:05.747438 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:46:09.874991 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:46:12.864504 140576608098112 submission_runner.py:411] Time since start: 20463.57s, 	Step: 40264, 	{'train/accuracy': 0.9925384521484375, 'train/loss': 0.023822620511054993, 'train/mean_average_precision': 0.5491199180507131, 'validation/accuracy': 0.9870293140411377, 'validation/loss': 0.044591594487428665, 'validation/mean_average_precision': 0.2869749485756071, 'validation/num_examples': 43793, 'test/accuracy': 0.9861733913421631, 'test/loss': 0.047402843832969666, 'test/mean_average_precision': 0.2700235207289608, 'test/num_examples': 43793, 'score': 12984.634531497955, 'total_duration': 20463.572825193405, 'accumulated_submission_time': 12984.634531497955, 'accumulated_eval_time': 7476.147330522537, 'accumulated_logging_time': 1.694267749786377}
I0305 15:46:12.889082 140407882708736 logging_writer.py:48] [40264] accumulated_eval_time=7476.147331, accumulated_logging_time=1.694268, accumulated_submission_time=12984.634531, global_step=40264, preemption_count=0, score=12984.634531, test/accuracy=0.986173, test/loss=0.047403, test/mean_average_precision=0.270024, test/num_examples=43793, total_duration=20463.572825, train/accuracy=0.992538, train/loss=0.023823, train/mean_average_precision=0.549120, validation/accuracy=0.987029, validation/loss=0.044592, validation/mean_average_precision=0.286975, validation/num_examples=43793
I0305 15:46:24.802398 140415782377216 logging_writer.py:48] [40300] global_step=40300, grad_norm=0.07224258780479431, loss=0.02718653529882431
I0305 15:46:57.265349 140407882708736 logging_writer.py:48] [40400] global_step=40400, grad_norm=0.06599569320678711, loss=0.028165943920612335
I0305 15:47:29.506553 140415782377216 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.06882556527853012, loss=0.02533670887351036
I0305 15:48:01.204107 140407882708736 logging_writer.py:48] [40600] global_step=40600, grad_norm=0.061231937259435654, loss=0.02569030225276947
I0305 15:48:33.165537 140415782377216 logging_writer.py:48] [40700] global_step=40700, grad_norm=0.06490251421928406, loss=0.02771631069481373
I0305 15:49:05.534558 140407882708736 logging_writer.py:48] [40800] global_step=40800, grad_norm=0.06666272133588791, loss=0.02894621342420578
I0305 15:49:37.942819 140415782377216 logging_writer.py:48] [40900] global_step=40900, grad_norm=0.07491026818752289, loss=0.027917996048927307
I0305 15:50:10.107719 140407882708736 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.06701922416687012, loss=0.026397882029414177
I0305 15:50:13.040938 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:52:12.800883 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:52:15.898639 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:52:18.979387 140576608098112 submission_runner.py:411] Time since start: 20829.69s, 	Step: 41010, 	{'train/accuracy': 0.992478609085083, 'train/loss': 0.024154307320713997, 'train/mean_average_precision': 0.5465455027988128, 'validation/accuracy': 0.9870561361312866, 'validation/loss': 0.0445823147892952, 'validation/mean_average_precision': 0.2870653774717982, 'validation/num_examples': 43793, 'test/accuracy': 0.9862361550331116, 'test/loss': 0.047508541494607925, 'test/mean_average_precision': 0.27372036319070975, 'test/num_examples': 43793, 'score': 13224.755261421204, 'total_duration': 20829.687710762024, 'accumulated_submission_time': 13224.755261421204, 'accumulated_eval_time': 7602.085725069046, 'accumulated_logging_time': 1.7297906875610352}
I0305 15:52:19.005336 140408968165120 logging_writer.py:48] [41010] accumulated_eval_time=7602.085725, accumulated_logging_time=1.729791, accumulated_submission_time=13224.755261, global_step=41010, preemption_count=0, score=13224.755261, test/accuracy=0.986236, test/loss=0.047509, test/mean_average_precision=0.273720, test/num_examples=43793, total_duration=20829.687711, train/accuracy=0.992479, train/loss=0.024154, train/mean_average_precision=0.546546, validation/accuracy=0.987056, validation/loss=0.044582, validation/mean_average_precision=0.287065, validation/num_examples=43793
I0305 15:52:48.142772 140409291962112 logging_writer.py:48] [41100] global_step=41100, grad_norm=0.07035058736801147, loss=0.025496676564216614
I0305 15:53:20.400089 140408968165120 logging_writer.py:48] [41200] global_step=41200, grad_norm=0.06615538150072098, loss=0.02615184895694256
I0305 15:53:52.639598 140409291962112 logging_writer.py:48] [41300] global_step=41300, grad_norm=0.07358213514089584, loss=0.02901930920779705
I0305 15:54:25.292250 140408968165120 logging_writer.py:48] [41400] global_step=41400, grad_norm=0.07923895865678787, loss=0.029628008604049683
I0305 15:54:57.428373 140409291962112 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.07932068407535553, loss=0.028043469414114952
I0305 15:55:29.748462 140408968165120 logging_writer.py:48] [41600] global_step=41600, grad_norm=0.07119914144277573, loss=0.026457346975803375
I0305 15:56:01.828774 140409291962112 logging_writer.py:48] [41700] global_step=41700, grad_norm=0.07916244864463806, loss=0.028623657301068306
I0305 15:56:19.026646 140576608098112 spec.py:321] Evaluating on the training split.
I0305 15:58:19.857117 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 15:58:22.896240 140576608098112 spec.py:349] Evaluating on the test split.
I0305 15:58:25.908158 140576608098112 submission_runner.py:411] Time since start: 21196.62s, 	Step: 41754, 	{'train/accuracy': 0.9922721982002258, 'train/loss': 0.024641722440719604, 'train/mean_average_precision': 0.5371658929798946, 'validation/accuracy': 0.9870626330375671, 'validation/loss': 0.044679392129182816, 'validation/mean_average_precision': 0.28815093233111927, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.04777389392256737, 'test/mean_average_precision': 0.2742440255073568, 'test/num_examples': 43793, 'score': 13464.745002746582, 'total_duration': 21196.616488933563, 'accumulated_submission_time': 13464.745002746582, 'accumulated_eval_time': 7728.96719622612, 'accumulated_logging_time': 1.7673423290252686}
I0305 15:58:25.932095 140407882708736 logging_writer.py:48] [41754] accumulated_eval_time=7728.967196, accumulated_logging_time=1.767342, accumulated_submission_time=13464.745003, global_step=41754, preemption_count=0, score=13464.745003, test/accuracy=0.986235, test/loss=0.047774, test/mean_average_precision=0.274244, test/num_examples=43793, total_duration=21196.616489, train/accuracy=0.992272, train/loss=0.024642, train/mean_average_precision=0.537166, validation/accuracy=0.987063, validation/loss=0.044679, validation/mean_average_precision=0.288151, validation/num_examples=43793
I0305 15:58:40.934690 140415782377216 logging_writer.py:48] [41800] global_step=41800, grad_norm=0.06738396733999252, loss=0.02696692757308483
I0305 15:59:13.531760 140407882708736 logging_writer.py:48] [41900] global_step=41900, grad_norm=0.0728548914194107, loss=0.027451002970337868
I0305 15:59:46.901601 140415782377216 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.08183742314577103, loss=0.02741154283285141
I0305 16:00:19.853414 140407882708736 logging_writer.py:48] [42100] global_step=42100, grad_norm=0.07544128596782684, loss=0.028311967849731445
I0305 16:00:51.993196 140415782377216 logging_writer.py:48] [42200] global_step=42200, grad_norm=0.07299704849720001, loss=0.02756434492766857
I0305 16:01:24.052849 140407882708736 logging_writer.py:48] [42300] global_step=42300, grad_norm=0.08200138062238693, loss=0.029697325080633163
I0305 16:01:56.009557 140415782377216 logging_writer.py:48] [42400] global_step=42400, grad_norm=0.09122787415981293, loss=0.026316549628973007
I0305 16:02:26.131927 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:04:23.369668 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:04:26.472903 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:04:29.441295 140576608098112 submission_runner.py:411] Time since start: 21560.15s, 	Step: 42494, 	{'train/accuracy': 0.9924113154411316, 'train/loss': 0.024191364645957947, 'train/mean_average_precision': 0.5530880035861376, 'validation/accuracy': 0.9869915843009949, 'validation/loss': 0.04451877996325493, 'validation/mean_average_precision': 0.28453785227120654, 'validation/num_examples': 43793, 'test/accuracy': 0.9861738085746765, 'test/loss': 0.04734720662236214, 'test/mean_average_precision': 0.2703982284928044, 'test/num_examples': 43793, 'score': 13704.911063671112, 'total_duration': 21560.149626255035, 'accumulated_submission_time': 13704.911063671112, 'accumulated_eval_time': 7852.276547431946, 'accumulated_logging_time': 1.8038551807403564}
I0305 16:04:29.464602 140408968165120 logging_writer.py:48] [42494] accumulated_eval_time=7852.276547, accumulated_logging_time=1.803855, accumulated_submission_time=13704.911064, global_step=42494, preemption_count=0, score=13704.911064, test/accuracy=0.986174, test/loss=0.047347, test/mean_average_precision=0.270398, test/num_examples=43793, total_duration=21560.149626, train/accuracy=0.992411, train/loss=0.024191, train/mean_average_precision=0.553088, validation/accuracy=0.986992, validation/loss=0.044519, validation/mean_average_precision=0.284538, validation/num_examples=43793
I0305 16:04:31.775021 140415790769920 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.07049433887004852, loss=0.025113115087151527
I0305 16:05:04.200385 140408968165120 logging_writer.py:48] [42600] global_step=42600, grad_norm=0.0813932791352272, loss=0.025586053729057312
I0305 16:05:36.268246 140415790769920 logging_writer.py:48] [42700] global_step=42700, grad_norm=0.07411032915115356, loss=0.02629903517663479
I0305 16:06:08.615231 140408968165120 logging_writer.py:48] [42800] global_step=42800, grad_norm=0.06211442872881889, loss=0.02393011376261711
I0305 16:06:41.201121 140415790769920 logging_writer.py:48] [42900] global_step=42900, grad_norm=0.08648320287466049, loss=0.029571305960416794
I0305 16:07:14.254219 140408968165120 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.077911376953125, loss=0.023030314594507217
I0305 16:07:46.245976 140415790769920 logging_writer.py:48] [43100] global_step=43100, grad_norm=0.06610049307346344, loss=0.02323448285460472
I0305 16:08:18.583759 140408968165120 logging_writer.py:48] [43200] global_step=43200, grad_norm=0.08197271078824997, loss=0.028781693428754807
I0305 16:08:29.487143 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:10:32.248589 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:10:35.293539 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:10:39.431625 140576608098112 submission_runner.py:411] Time since start: 21930.14s, 	Step: 43234, 	{'train/accuracy': 0.9924986958503723, 'train/loss': 0.02389056608080864, 'train/mean_average_precision': 0.5642352845260664, 'validation/accuracy': 0.987065851688385, 'validation/loss': 0.04482008516788483, 'validation/mean_average_precision': 0.2887220173444159, 'validation/num_examples': 43793, 'test/accuracy': 0.9861283302307129, 'test/loss': 0.047874514013528824, 'test/mean_average_precision': 0.27562887168847294, 'test/num_examples': 43793, 'score': 13944.901382684708, 'total_duration': 21930.13982820511, 'accumulated_submission_time': 13944.901382684708, 'accumulated_eval_time': 7982.220856189728, 'accumulated_logging_time': 1.8395116329193115}
I0305 16:10:39.455820 140407882708736 logging_writer.py:48] [43234] accumulated_eval_time=7982.220856, accumulated_logging_time=1.839512, accumulated_submission_time=13944.901383, global_step=43234, preemption_count=0, score=13944.901383, test/accuracy=0.986128, test/loss=0.047875, test/mean_average_precision=0.275629, test/num_examples=43793, total_duration=21930.139828, train/accuracy=0.992499, train/loss=0.023891, train/mean_average_precision=0.564235, validation/accuracy=0.987066, validation/loss=0.044820, validation/mean_average_precision=0.288722, validation/num_examples=43793
I0305 16:11:01.031379 140409291962112 logging_writer.py:48] [43300] global_step=43300, grad_norm=0.0786128118634224, loss=0.025056228041648865
I0305 16:11:33.511739 140407882708736 logging_writer.py:48] [43400] global_step=43400, grad_norm=0.08811492472887039, loss=0.02906854823231697
I0305 16:12:06.025872 140409291962112 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0828154906630516, loss=0.02698868326842785
I0305 16:12:38.406636 140407882708736 logging_writer.py:48] [43600] global_step=43600, grad_norm=0.07358697056770325, loss=0.023969046771526337
I0305 16:13:11.005034 140409291962112 logging_writer.py:48] [43700] global_step=43700, grad_norm=0.06759662181138992, loss=0.025627383962273598
I0305 16:13:44.419858 140407882708736 logging_writer.py:48] [43800] global_step=43800, grad_norm=0.06949775665998459, loss=0.024962330237030983
I0305 16:14:17.335597 140409291962112 logging_writer.py:48] [43900] global_step=43900, grad_norm=0.06837178766727448, loss=0.024507006630301476
I0305 16:14:39.455104 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:16:35.991722 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:16:39.033273 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:16:42.112886 140576608098112 submission_runner.py:411] Time since start: 22292.82s, 	Step: 43970, 	{'train/accuracy': 0.9926086664199829, 'train/loss': 0.02322332188487053, 'train/mean_average_precision': 0.5818679028796002, 'validation/accuracy': 0.9871900677680969, 'validation/loss': 0.044863779097795486, 'validation/mean_average_precision': 0.2902499637499417, 'validation/num_examples': 43793, 'test/accuracy': 0.9862905144691467, 'test/loss': 0.047958508133888245, 'test/mean_average_precision': 0.27449359106219606, 'test/num_examples': 43793, 'score': 14184.867455482483, 'total_duration': 22292.821218252182, 'accumulated_submission_time': 14184.867455482483, 'accumulated_eval_time': 8104.878615617752, 'accumulated_logging_time': 1.8760895729064941}
I0305 16:16:42.136575 140408968165120 logging_writer.py:48] [43970] accumulated_eval_time=8104.878616, accumulated_logging_time=1.876090, accumulated_submission_time=14184.867455, global_step=43970, preemption_count=0, score=14184.867455, test/accuracy=0.986291, test/loss=0.047959, test/mean_average_precision=0.274494, test/num_examples=43793, total_duration=22292.821218, train/accuracy=0.992609, train/loss=0.023223, train/mean_average_precision=0.581868, validation/accuracy=0.987190, validation/loss=0.044864, validation/mean_average_precision=0.290250, validation/num_examples=43793
I0305 16:16:52.071464 140415790769920 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.07292281091213226, loss=0.02686188742518425
I0305 16:17:23.946031 140408968165120 logging_writer.py:48] [44100] global_step=44100, grad_norm=0.07734774798154831, loss=0.022435061633586884
I0305 16:17:56.292237 140415790769920 logging_writer.py:48] [44200] global_step=44200, grad_norm=0.07677669078111649, loss=0.026636745780706406
I0305 16:18:29.001855 140408968165120 logging_writer.py:48] [44300] global_step=44300, grad_norm=0.07743708789348602, loss=0.027392137795686722
I0305 16:19:01.617145 140415790769920 logging_writer.py:48] [44400] global_step=44400, grad_norm=0.08230635523796082, loss=0.025700170546770096
I0305 16:19:34.112391 140408968165120 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.08223781734704971, loss=0.027059145271778107
I0305 16:20:06.580741 140415790769920 logging_writer.py:48] [44600] global_step=44600, grad_norm=0.0790240466594696, loss=0.025296304374933243
I0305 16:20:38.564150 140408968165120 logging_writer.py:48] [44700] global_step=44700, grad_norm=0.09230807423591614, loss=0.02435622550547123
I0305 16:20:42.379652 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:22:39.243751 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:22:42.274039 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:22:45.340400 140576608098112 submission_runner.py:411] Time since start: 22656.05s, 	Step: 44713, 	{'train/accuracy': 0.9928114414215088, 'train/loss': 0.02291768789291382, 'train/mean_average_precision': 0.5719944676468065, 'validation/accuracy': 0.987003743648529, 'validation/loss': 0.04476958140730858, 'validation/mean_average_precision': 0.286275793689501, 'validation/num_examples': 43793, 'test/accuracy': 0.9861140251159668, 'test/loss': 0.04784373566508293, 'test/mean_average_precision': 0.269938147158632, 'test/num_examples': 43793, 'score': 14425.079031467438, 'total_duration': 22656.048730373383, 'accumulated_submission_time': 14425.079031467438, 'accumulated_eval_time': 8227.839316368103, 'accumulated_logging_time': 1.911909580230713}
I0305 16:22:45.366029 140407882708736 logging_writer.py:48] [44713] accumulated_eval_time=8227.839316, accumulated_logging_time=1.911910, accumulated_submission_time=14425.079031, global_step=44713, preemption_count=0, score=14425.079031, test/accuracy=0.986114, test/loss=0.047844, test/mean_average_precision=0.269938, test/num_examples=43793, total_duration=22656.048730, train/accuracy=0.992811, train/loss=0.022918, train/mean_average_precision=0.571994, validation/accuracy=0.987004, validation/loss=0.044770, validation/mean_average_precision=0.286276, validation/num_examples=43793
I0305 16:23:13.603524 140409291962112 logging_writer.py:48] [44800] global_step=44800, grad_norm=0.08459541946649551, loss=0.02603365108370781
I0305 16:23:45.490433 140407882708736 logging_writer.py:48] [44900] global_step=44900, grad_norm=0.06826429814100266, loss=0.024373015388846397
I0305 16:24:17.832022 140409291962112 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0724729672074318, loss=0.024931538850069046
I0305 16:24:49.952635 140407882708736 logging_writer.py:48] [45100] global_step=45100, grad_norm=0.09545009583234787, loss=0.026321854442358017
I0305 16:25:22.164435 140409291962112 logging_writer.py:48] [45200] global_step=45200, grad_norm=0.08276524394750595, loss=0.025865944102406502
I0305 16:25:54.633412 140407882708736 logging_writer.py:48] [45300] global_step=45300, grad_norm=0.0837116613984108, loss=0.02322332002222538
I0305 16:26:27.273927 140409291962112 logging_writer.py:48] [45400] global_step=45400, grad_norm=0.0784200131893158, loss=0.02467110939323902
I0305 16:26:45.467666 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:28:44.220724 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:28:47.226734 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:28:50.210217 140576608098112 submission_runner.py:411] Time since start: 23020.92s, 	Step: 45458, 	{'train/accuracy': 0.9930723309516907, 'train/loss': 0.022061098366975784, 'train/mean_average_precision': 0.6057093471035695, 'validation/accuracy': 0.9870406985282898, 'validation/loss': 0.04487038776278496, 'validation/mean_average_precision': 0.2880019608799084, 'validation/num_examples': 43793, 'test/accuracy': 0.9861544370651245, 'test/loss': 0.047892700880765915, 'test/mean_average_precision': 0.27402433494314554, 'test/num_examples': 43793, 'score': 14665.148543596268, 'total_duration': 23020.918548822403, 'accumulated_submission_time': 14665.148543596268, 'accumulated_eval_time': 8352.581824541092, 'accumulated_logging_time': 1.9499232769012451}
I0305 16:28:50.237087 140408968165120 logging_writer.py:48] [45458] accumulated_eval_time=8352.581825, accumulated_logging_time=1.949923, accumulated_submission_time=14665.148544, global_step=45458, preemption_count=0, score=14665.148544, test/accuracy=0.986154, test/loss=0.047893, test/mean_average_precision=0.274024, test/num_examples=43793, total_duration=23020.918549, train/accuracy=0.993072, train/loss=0.022061, train/mean_average_precision=0.605709, validation/accuracy=0.987041, validation/loss=0.044870, validation/mean_average_precision=0.288002, validation/num_examples=43793
I0305 16:29:04.368374 140415782377216 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.07558874785900116, loss=0.02321961149573326
I0305 16:29:37.091624 140408968165120 logging_writer.py:48] [45600] global_step=45600, grad_norm=0.07435297220945358, loss=0.023386549204587936
I0305 16:30:09.478385 140415782377216 logging_writer.py:48] [45700] global_step=45700, grad_norm=0.10957728326320648, loss=0.021728582680225372
I0305 16:30:41.262559 140408968165120 logging_writer.py:48] [45800] global_step=45800, grad_norm=0.07206492125988007, loss=0.02372143790125847
I0305 16:31:13.894332 140415782377216 logging_writer.py:48] [45900] global_step=45900, grad_norm=0.09755175560712814, loss=0.02628253400325775
I0305 16:31:47.424472 140408968165120 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.07494078576564789, loss=0.024996336549520493
I0305 16:32:20.223009 140415782377216 logging_writer.py:48] [46100] global_step=46100, grad_norm=0.0810294896364212, loss=0.027820702642202377
I0305 16:32:50.527952 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:34:45.706295 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:34:48.753991 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:34:51.733134 140576608098112 submission_runner.py:411] Time since start: 23382.44s, 	Step: 46195, 	{'train/accuracy': 0.9932297468185425, 'train/loss': 0.021555202081799507, 'train/mean_average_precision': 0.611245669579988, 'validation/accuracy': 0.9870281219482422, 'validation/loss': 0.04530685022473335, 'validation/mean_average_precision': 0.28711757120305503, 'validation/num_examples': 43793, 'test/accuracy': 0.9862251877784729, 'test/loss': 0.04827499017119408, 'test/mean_average_precision': 0.27524378348079603, 'test/num_examples': 43793, 'score': 14905.407657384872, 'total_duration': 23382.441463947296, 'accumulated_submission_time': 14905.407657384872, 'accumulated_eval_time': 8473.786965847015, 'accumulated_logging_time': 1.9878568649291992}
I0305 16:34:51.757596 140407882708736 logging_writer.py:48] [46195] accumulated_eval_time=8473.786966, accumulated_logging_time=1.987857, accumulated_submission_time=14905.407657, global_step=46195, preemption_count=0, score=14905.407657, test/accuracy=0.986225, test/loss=0.048275, test/mean_average_precision=0.275244, test/num_examples=43793, total_duration=23382.441464, train/accuracy=0.993230, train/loss=0.021555, train/mean_average_precision=0.611246, validation/accuracy=0.987028, validation/loss=0.045307, validation/mean_average_precision=0.287118, validation/num_examples=43793
I0305 16:34:53.768246 140415790769920 logging_writer.py:48] [46200] global_step=46200, grad_norm=0.08506389707326889, loss=0.02695494517683983
I0305 16:35:26.346111 140407882708736 logging_writer.py:48] [46300] global_step=46300, grad_norm=0.08212863653898239, loss=0.029070748016238213
I0305 16:35:58.890908 140415790769920 logging_writer.py:48] [46400] global_step=46400, grad_norm=0.07771917432546616, loss=0.02399502880871296
I0305 16:36:31.330677 140407882708736 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.08130592852830887, loss=0.026371555402874947
I0305 16:37:03.440225 140415790769920 logging_writer.py:48] [46600] global_step=46600, grad_norm=0.07527493685483932, loss=0.027357084676623344
I0305 16:37:35.549458 140407882708736 logging_writer.py:48] [46700] global_step=46700, grad_norm=0.07111631333827972, loss=0.024779636412858963
I0305 16:38:07.911890 140415790769920 logging_writer.py:48] [46800] global_step=46800, grad_norm=0.09253952652215958, loss=0.02517404593527317
I0305 16:38:39.916604 140407882708736 logging_writer.py:48] [46900] global_step=46900, grad_norm=0.07587798684835434, loss=0.022719508036971092
I0305 16:38:51.951981 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:40:51.341202 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:40:54.363164 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:40:57.342947 140576608098112 submission_runner.py:411] Time since start: 23748.05s, 	Step: 46939, 	{'train/accuracy': 0.9930238723754883, 'train/loss': 0.022141970694065094, 'train/mean_average_precision': 0.5996005868878975, 'validation/accuracy': 0.9870882034301758, 'validation/loss': 0.04536289721727371, 'validation/mean_average_precision': 0.2826583293956821, 'validation/num_examples': 43793, 'test/accuracy': 0.9862580895423889, 'test/loss': 0.048646170645952225, 'test/mean_average_precision': 0.2688644851086066, 'test/num_examples': 43793, 'score': 15145.571384191513, 'total_duration': 23748.051268339157, 'accumulated_submission_time': 15145.571384191513, 'accumulated_eval_time': 8599.17787861824, 'accumulated_logging_time': 2.023483991622925}
I0305 16:40:57.367258 140408968165120 logging_writer.py:48] [46939] accumulated_eval_time=8599.177879, accumulated_logging_time=2.023484, accumulated_submission_time=15145.571384, global_step=46939, preemption_count=0, score=15145.571384, test/accuracy=0.986258, test/loss=0.048646, test/mean_average_precision=0.268864, test/num_examples=43793, total_duration=23748.051268, train/accuracy=0.993024, train/loss=0.022142, train/mean_average_precision=0.599601, validation/accuracy=0.987088, validation/loss=0.045363, validation/mean_average_precision=0.282658, validation/num_examples=43793
I0305 16:41:17.446470 140415782377216 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.07166912406682968, loss=0.024289753288030624
I0305 16:41:50.734179 140408968165120 logging_writer.py:48] [47100] global_step=47100, grad_norm=0.08205001801252365, loss=0.026219293475151062
I0305 16:42:23.140956 140415782377216 logging_writer.py:48] [47200] global_step=47200, grad_norm=0.08865094929933548, loss=0.026746908202767372
I0305 16:42:55.605893 140408968165120 logging_writer.py:48] [47300] global_step=47300, grad_norm=0.07349734008312225, loss=0.026249025017023087
I0305 16:43:28.172774 140415782377216 logging_writer.py:48] [47400] global_step=47400, grad_norm=0.09522365778684616, loss=0.027615690603852272
I0305 16:44:00.325902 140408968165120 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.07668570429086685, loss=0.023863309994339943
I0305 16:44:33.568487 140415782377216 logging_writer.py:48] [47600] global_step=47600, grad_norm=0.08613378554582596, loss=0.023981327190995216
I0305 16:44:57.428241 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:46:57.530312 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:47:00.544172 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:47:03.802072 140576608098112 submission_runner.py:411] Time since start: 24114.51s, 	Step: 47672, 	{'train/accuracy': 0.9928838014602661, 'train/loss': 0.022690821439027786, 'train/mean_average_precision': 0.5954002678011658, 'validation/accuracy': 0.987058162689209, 'validation/loss': 0.04514186456799507, 'validation/mean_average_precision': 0.2881418197880501, 'validation/num_examples': 43793, 'test/accuracy': 0.9862008094787598, 'test/loss': 0.04815571382641792, 'test/mean_average_precision': 0.2730225395600637, 'test/num_examples': 43793, 'score': 15385.599834442139, 'total_duration': 24114.510402441025, 'accumulated_submission_time': 15385.599834442139, 'accumulated_eval_time': 8725.551683664322, 'accumulated_logging_time': 2.058670997619629}
I0305 16:47:03.826603 140407882708736 logging_writer.py:48] [47672] accumulated_eval_time=8725.551684, accumulated_logging_time=2.058671, accumulated_submission_time=15385.599834, global_step=47672, preemption_count=0, score=15385.599834, test/accuracy=0.986201, test/loss=0.048156, test/mean_average_precision=0.273023, test/num_examples=43793, total_duration=24114.510402, train/accuracy=0.992884, train/loss=0.022691, train/mean_average_precision=0.595400, validation/accuracy=0.987058, validation/loss=0.045142, validation/mean_average_precision=0.288142, validation/num_examples=43793
I0305 16:47:13.451574 140409291962112 logging_writer.py:48] [47700] global_step=47700, grad_norm=0.08410012722015381, loss=0.023992950096726418
I0305 16:47:45.639021 140407882708736 logging_writer.py:48] [47800] global_step=47800, grad_norm=0.1108807772397995, loss=0.0241012554615736
I0305 16:48:17.607305 140409291962112 logging_writer.py:48] [47900] global_step=47900, grad_norm=0.07985621690750122, loss=0.021901797503232956
I0305 16:48:49.618698 140407882708736 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.10183363407850266, loss=0.02501583658158779
I0305 16:49:22.016760 140409291962112 logging_writer.py:48] [48100] global_step=48100, grad_norm=0.08448665589094162, loss=0.027085674926638603
I0305 16:49:54.135231 140407882708736 logging_writer.py:48] [48200] global_step=48200, grad_norm=0.09442589432001114, loss=0.025792080909013748
I0305 16:50:26.479520 140409291962112 logging_writer.py:48] [48300] global_step=48300, grad_norm=0.09098590165376663, loss=0.026867100968956947
I0305 16:50:58.676307 140407882708736 logging_writer.py:48] [48400] global_step=48400, grad_norm=0.11252177506685257, loss=0.023721937090158463
I0305 16:51:03.975313 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:53:01.034852 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:53:04.287706 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:53:07.303835 140576608098112 submission_runner.py:411] Time since start: 24478.01s, 	Step: 48417, 	{'train/accuracy': 0.9927613139152527, 'train/loss': 0.022886665537953377, 'train/mean_average_precision': 0.564050308760248, 'validation/accuracy': 0.987027108669281, 'validation/loss': 0.04570500925183296, 'validation/mean_average_precision': 0.28652231033362, 'validation/num_examples': 43793, 'test/accuracy': 0.9862513542175293, 'test/loss': 0.048695534467697144, 'test/mean_average_precision': 0.2771422885173468, 'test/num_examples': 43793, 'score': 15625.717020273209, 'total_duration': 24478.012163877487, 'accumulated_submission_time': 15625.717020273209, 'accumulated_eval_time': 8848.880156755447, 'accumulated_logging_time': 2.0949044227600098}
I0305 16:53:07.328852 140415782377216 logging_writer.py:48] [48417] accumulated_eval_time=8848.880157, accumulated_logging_time=2.094904, accumulated_submission_time=15625.717020, global_step=48417, preemption_count=0, score=15625.717020, test/accuracy=0.986251, test/loss=0.048696, test/mean_average_precision=0.277142, test/num_examples=43793, total_duration=24478.012164, train/accuracy=0.992761, train/loss=0.022887, train/mean_average_precision=0.564050, validation/accuracy=0.987027, validation/loss=0.045705, validation/mean_average_precision=0.286522, validation/num_examples=43793
I0305 16:53:34.505397 140415790769920 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.07629791647195816, loss=0.024488897994160652
I0305 16:54:06.612294 140415782377216 logging_writer.py:48] [48600] global_step=48600, grad_norm=0.08203674107789993, loss=0.02360018901526928
I0305 16:54:40.306277 140415790769920 logging_writer.py:48] [48700] global_step=48700, grad_norm=0.08471239358186722, loss=0.024529116228222847
I0305 16:55:13.672970 140415782377216 logging_writer.py:48] [48800] global_step=48800, grad_norm=0.09848896414041519, loss=0.027091877534985542
I0305 16:55:45.998438 140415790769920 logging_writer.py:48] [48900] global_step=48900, grad_norm=0.07127196341753006, loss=0.021382123231887817
I0305 16:56:19.033983 140415782377216 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.08782932162284851, loss=0.024130357429385185
I0305 16:56:52.296032 140415790769920 logging_writer.py:48] [49100] global_step=49100, grad_norm=0.10530226677656174, loss=0.027557309716939926
I0305 16:57:07.468023 140576608098112 spec.py:321] Evaluating on the training split.
I0305 16:59:05.336183 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 16:59:08.359170 140576608098112 spec.py:349] Evaluating on the test split.
I0305 16:59:11.370750 140576608098112 submission_runner.py:411] Time since start: 24842.08s, 	Step: 49148, 	{'train/accuracy': 0.9930188655853271, 'train/loss': 0.022115573287010193, 'train/mean_average_precision': 0.5984401713121497, 'validation/accuracy': 0.987084150314331, 'validation/loss': 0.04531455412507057, 'validation/mean_average_precision': 0.28559189604023244, 'validation/num_examples': 43793, 'test/accuracy': 0.9862251877784729, 'test/loss': 0.04850993677973747, 'test/mean_average_precision': 0.2718043034407581, 'test/num_examples': 43793, 'score': 15865.82262635231, 'total_duration': 24842.079083919525, 'accumulated_submission_time': 15865.82262635231, 'accumulated_eval_time': 8972.782843828201, 'accumulated_logging_time': 2.1312637329101562}
I0305 16:59:11.395709 140407882708736 logging_writer.py:48] [49148] accumulated_eval_time=8972.782844, accumulated_logging_time=2.131264, accumulated_submission_time=15865.822626, global_step=49148, preemption_count=0, score=15865.822626, test/accuracy=0.986225, test/loss=0.048510, test/mean_average_precision=0.271804, test/num_examples=43793, total_duration=24842.079084, train/accuracy=0.993019, train/loss=0.022116, train/mean_average_precision=0.598440, validation/accuracy=0.987084, validation/loss=0.045315, validation/mean_average_precision=0.285592, validation/num_examples=43793
I0305 16:59:28.647522 140409291962112 logging_writer.py:48] [49200] global_step=49200, grad_norm=0.0761813223361969, loss=0.023911669850349426
I0305 17:00:01.402674 140407882708736 logging_writer.py:48] [49300] global_step=49300, grad_norm=0.08280318230390549, loss=0.023603204637765884
I0305 17:00:33.852415 140409291962112 logging_writer.py:48] [49400] global_step=49400, grad_norm=0.08855917304754257, loss=0.022559501230716705
I0305 17:01:06.313796 140407882708736 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.07486876845359802, loss=0.02342630736529827
I0305 17:01:38.390413 140409291962112 logging_writer.py:48] [49600] global_step=49600, grad_norm=0.07995740324258804, loss=0.023919474333524704
I0305 17:02:10.589475 140407882708736 logging_writer.py:48] [49700] global_step=49700, grad_norm=0.09715218096971512, loss=0.02551347389817238
I0305 17:02:42.972746 140409291962112 logging_writer.py:48] [49800] global_step=49800, grad_norm=0.09428530931472778, loss=0.025375450029969215
I0305 17:03:11.483593 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:05:14.064277 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:05:17.640571 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:05:21.006953 140576608098112 submission_runner.py:411] Time since start: 25211.72s, 	Step: 49889, 	{'train/accuracy': 0.9930760860443115, 'train/loss': 0.0218061376363039, 'train/mean_average_precision': 0.6098008796744399, 'validation/accuracy': 0.9870622158050537, 'validation/loss': 0.045785091817379, 'validation/mean_average_precision': 0.2812710950757494, 'validation/num_examples': 43793, 'test/accuracy': 0.9862938523292542, 'test/loss': 0.04866495728492737, 'test/mean_average_precision': 0.2738028980149617, 'test/num_examples': 43793, 'score': 16105.879657030106, 'total_duration': 25211.715267181396, 'accumulated_submission_time': 16105.879657030106, 'accumulated_eval_time': 9102.306144714355, 'accumulated_logging_time': 2.167288064956665}
I0305 17:05:21.034781 140415782377216 logging_writer.py:48] [49889] accumulated_eval_time=9102.306145, accumulated_logging_time=2.167288, accumulated_submission_time=16105.879657, global_step=49889, preemption_count=0, score=16105.879657, test/accuracy=0.986294, test/loss=0.048665, test/mean_average_precision=0.273803, test/num_examples=43793, total_duration=25211.715267, train/accuracy=0.993076, train/loss=0.021806, train/mean_average_precision=0.609801, validation/accuracy=0.987062, validation/loss=0.045785, validation/mean_average_precision=0.281271, validation/num_examples=43793
I0305 17:05:25.060166 140415790769920 logging_writer.py:48] [49900] global_step=49900, grad_norm=0.08295375853776932, loss=0.023808686062693596
I0305 17:05:58.623935 140415782377216 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.08140726387500763, loss=0.022564494982361794
I0305 17:06:32.605253 140415790769920 logging_writer.py:48] [50100] global_step=50100, grad_norm=0.08149725943803787, loss=0.02264636941254139
I0305 17:07:06.364477 140415782377216 logging_writer.py:48] [50200] global_step=50200, grad_norm=0.08528817445039749, loss=0.024365637451410294
I0305 17:07:39.684841 140415790769920 logging_writer.py:48] [50300] global_step=50300, grad_norm=0.07830995321273804, loss=0.02238006517291069
I0305 17:08:12.943322 140415782377216 logging_writer.py:48] [50400] global_step=50400, grad_norm=0.09981993585824966, loss=0.02581937611103058
I0305 17:08:45.392640 140415790769920 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.09209771454334259, loss=0.024995092302560806
I0305 17:09:18.396482 140415782377216 logging_writer.py:48] [50600] global_step=50600, grad_norm=0.09719543904066086, loss=0.024230901151895523
I0305 17:09:21.015574 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:11:17.643681 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:11:20.667493 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:11:23.657328 140576608098112 submission_runner.py:411] Time since start: 25574.37s, 	Step: 50609, 	{'train/accuracy': 0.9931766986846924, 'train/loss': 0.021431848406791687, 'train/mean_average_precision': 0.619487995667, 'validation/accuracy': 0.9871689677238464, 'validation/loss': 0.045978058129549026, 'validation/mean_average_precision': 0.28589317875376, 'validation/num_examples': 43793, 'test/accuracy': 0.9862997531890869, 'test/loss': 0.04911188781261444, 'test/mean_average_precision': 0.2682895887359769, 'test/num_examples': 43793, 'score': 16345.824395656586, 'total_duration': 25574.36565876007, 'accumulated_submission_time': 16345.824395656586, 'accumulated_eval_time': 9224.947851657867, 'accumulated_logging_time': 2.2070817947387695}
I0305 17:11:23.681872 140407882708736 logging_writer.py:48] [50609] accumulated_eval_time=9224.947852, accumulated_logging_time=2.207082, accumulated_submission_time=16345.824396, global_step=50609, preemption_count=0, score=16345.824396, test/accuracy=0.986300, test/loss=0.049112, test/mean_average_precision=0.268290, test/num_examples=43793, total_duration=25574.365659, train/accuracy=0.993177, train/loss=0.021432, train/mean_average_precision=0.619488, validation/accuracy=0.987169, validation/loss=0.045978, validation/mean_average_precision=0.285893, validation/num_examples=43793
I0305 17:11:53.767141 140408968165120 logging_writer.py:48] [50700] global_step=50700, grad_norm=0.08390885591506958, loss=0.02273215353488922
I0305 17:12:25.963872 140407882708736 logging_writer.py:48] [50800] global_step=50800, grad_norm=0.09188956022262573, loss=0.0274242851883173
I0305 17:12:57.961817 140408968165120 logging_writer.py:48] [50900] global_step=50900, grad_norm=0.0927133858203888, loss=0.024244677275419235
I0305 17:13:30.273345 140407882708736 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.07693862915039062, loss=0.02139965072274208
I0305 17:14:02.775925 140408968165120 logging_writer.py:48] [51100] global_step=51100, grad_norm=0.10177314281463623, loss=0.026528997346758842
I0305 17:14:35.138492 140407882708736 logging_writer.py:48] [51200] global_step=51200, grad_norm=0.0971154272556305, loss=0.024208033457398415
I0305 17:15:07.189512 140408968165120 logging_writer.py:48] [51300] global_step=51300, grad_norm=0.08653483539819717, loss=0.02341531589627266
I0305 17:15:23.798182 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:17:18.604813 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:17:22.892521 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:17:25.963479 140576608098112 submission_runner.py:411] Time since start: 25936.67s, 	Step: 51353, 	{'train/accuracy': 0.9935702085494995, 'train/loss': 0.02028081938624382, 'train/mean_average_precision': 0.6379793391083107, 'validation/accuracy': 0.9870991706848145, 'validation/loss': 0.045844487845897675, 'validation/mean_average_precision': 0.28650389069637644, 'validation/num_examples': 43793, 'test/accuracy': 0.9862328171730042, 'test/loss': 0.04892588034272194, 'test/mean_average_precision': 0.2725946008014016, 'test/num_examples': 43793, 'score': 16585.909583091736, 'total_duration': 25936.671803712845, 'accumulated_submission_time': 16585.909583091736, 'accumulated_eval_time': 9347.113098144531, 'accumulated_logging_time': 2.2424938678741455}
I0305 17:17:25.989205 140409291962112 logging_writer.py:48] [51353] accumulated_eval_time=9347.113098, accumulated_logging_time=2.242494, accumulated_submission_time=16585.909583, global_step=51353, preemption_count=0, score=16585.909583, test/accuracy=0.986233, test/loss=0.048926, test/mean_average_precision=0.272595, test/num_examples=43793, total_duration=25936.671804, train/accuracy=0.993570, train/loss=0.020281, train/mean_average_precision=0.637979, validation/accuracy=0.987099, validation/loss=0.045844, validation/mean_average_precision=0.286504, validation/num_examples=43793
I0305 17:17:41.408516 140415790769920 logging_writer.py:48] [51400] global_step=51400, grad_norm=0.08672522753477097, loss=0.023996626958251
I0305 17:18:13.794008 140409291962112 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.08810892701148987, loss=0.02092851884663105
I0305 17:18:45.801435 140415790769920 logging_writer.py:48] [51600] global_step=51600, grad_norm=0.08570856600999832, loss=0.02383251301944256
I0305 17:19:18.060897 140409291962112 logging_writer.py:48] [51700] global_step=51700, grad_norm=0.09729396551847458, loss=0.02503792569041252
I0305 17:19:50.099531 140415790769920 logging_writer.py:48] [51800] global_step=51800, grad_norm=0.09269728511571884, loss=0.02371133677661419
I0305 17:20:22.400648 140409291962112 logging_writer.py:48] [51900] global_step=51900, grad_norm=0.08957911282777786, loss=0.023215997964143753
I0305 17:20:54.837086 140415790769920 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.0837494283914566, loss=0.021001292392611504
I0305 17:21:25.967079 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:23:23.616396 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:23:26.666685 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:23:29.636645 140576608098112 submission_runner.py:411] Time since start: 26300.34s, 	Step: 52098, 	{'train/accuracy': 0.9937245845794678, 'train/loss': 0.01977001503109932, 'train/mean_average_precision': 0.6558632894885597, 'validation/accuracy': 0.9871072769165039, 'validation/loss': 0.04597201198339462, 'validation/mean_average_precision': 0.2865892628409184, 'validation/num_examples': 43793, 'test/accuracy': 0.9862193465232849, 'test/loss': 0.049068745225667953, 'test/mean_average_precision': 0.2715121900772505, 'test/num_examples': 43793, 'score': 16825.856951475143, 'total_duration': 26300.344979286194, 'accumulated_submission_time': 16825.856951475143, 'accumulated_eval_time': 9470.78262424469, 'accumulated_logging_time': 2.279003143310547}
I0305 17:23:29.661743 140408968165120 logging_writer.py:48] [52098] accumulated_eval_time=9470.782624, accumulated_logging_time=2.279003, accumulated_submission_time=16825.856951, global_step=52098, preemption_count=0, score=16825.856951, test/accuracy=0.986219, test/loss=0.049069, test/mean_average_precision=0.271512, test/num_examples=43793, total_duration=26300.344979, train/accuracy=0.993725, train/loss=0.019770, train/mean_average_precision=0.655863, validation/accuracy=0.987107, validation/loss=0.045972, validation/mean_average_precision=0.286589, validation/num_examples=43793
I0305 17:23:30.645145 140415782377216 logging_writer.py:48] [52100] global_step=52100, grad_norm=0.09763848036527634, loss=0.022735856473445892
I0305 17:24:02.748531 140408968165120 logging_writer.py:48] [52200] global_step=52200, grad_norm=0.1053573414683342, loss=0.026988882571458817
I0305 17:24:34.516674 140415782377216 logging_writer.py:48] [52300] global_step=52300, grad_norm=0.08439213782548904, loss=0.022559965029358864
I0305 17:25:06.648757 140408968165120 logging_writer.py:48] [52400] global_step=52400, grad_norm=0.09112004190683365, loss=0.022859681397676468
I0305 17:25:38.605803 140415782377216 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.10863814502954483, loss=0.02307461015880108
I0305 17:26:10.919324 140408968165120 logging_writer.py:48] [52600] global_step=52600, grad_norm=0.09211181104183197, loss=0.02448243647813797
I0305 17:26:42.987701 140415782377216 logging_writer.py:48] [52700] global_step=52700, grad_norm=0.10719622671604156, loss=0.024624472483992577
I0305 17:27:15.294256 140408968165120 logging_writer.py:48] [52800] global_step=52800, grad_norm=0.09919486194849014, loss=0.023411309346556664
I0305 17:27:29.902690 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:29:29.531381 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:29:32.583381 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:29:35.573454 140576608098112 submission_runner.py:411] Time since start: 26666.28s, 	Step: 52846, 	{'train/accuracy': 0.9934922456741333, 'train/loss': 0.02035524696111679, 'train/mean_average_precision': 0.6389656490329827, 'validation/accuracy': 0.987166166305542, 'validation/loss': 0.046401042491197586, 'validation/mean_average_precision': 0.29041160071093175, 'validation/num_examples': 43793, 'test/accuracy': 0.9862968325614929, 'test/loss': 0.04962412267923355, 'test/mean_average_precision': 0.27313180221446604, 'test/num_examples': 43793, 'score': 17066.066660165787, 'total_duration': 26666.281783103943, 'accumulated_submission_time': 17066.066660165787, 'accumulated_eval_time': 9596.453342676163, 'accumulated_logging_time': 2.315351963043213}
I0305 17:29:35.599602 140409291962112 logging_writer.py:48] [52846] accumulated_eval_time=9596.453343, accumulated_logging_time=2.315352, accumulated_submission_time=17066.066660, global_step=52846, preemption_count=0, score=17066.066660, test/accuracy=0.986297, test/loss=0.049624, test/mean_average_precision=0.273132, test/num_examples=43793, total_duration=26666.281783, train/accuracy=0.993492, train/loss=0.020355, train/mean_average_precision=0.638966, validation/accuracy=0.987166, validation/loss=0.046401, validation/mean_average_precision=0.290412, validation/num_examples=43793
I0305 17:29:53.278712 140415790769920 logging_writer.py:48] [52900] global_step=52900, grad_norm=0.09995055943727493, loss=0.02599112130701542
I0305 17:30:25.703840 140409291962112 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.09205584228038788, loss=0.02336411364376545
I0305 17:30:57.871467 140415790769920 logging_writer.py:48] [53100] global_step=53100, grad_norm=0.08410143852233887, loss=0.02327670156955719
I0305 17:31:30.063077 140409291962112 logging_writer.py:48] [53200] global_step=53200, grad_norm=0.10439929366111755, loss=0.027538113296031952
I0305 17:32:02.322835 140415790769920 logging_writer.py:48] [53300] global_step=53300, grad_norm=0.08578726649284363, loss=0.021663488820195198
I0305 17:32:34.269093 140409291962112 logging_writer.py:48] [53400] global_step=53400, grad_norm=0.09211926907300949, loss=0.021842313930392265
I0305 17:33:06.243406 140415790769920 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.10251300781965256, loss=0.023402230814099312
I0305 17:33:35.773735 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:35:31.603868 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:35:34.612675 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:35:37.591607 140576608098112 submission_runner.py:411] Time since start: 27028.30s, 	Step: 53594, 	{'train/accuracy': 0.9935109615325928, 'train/loss': 0.020448721945285797, 'train/mean_average_precision': 0.635550748244853, 'validation/accuracy': 0.9870272874832153, 'validation/loss': 0.04599516838788986, 'validation/mean_average_precision': 0.2882264457127399, 'validation/num_examples': 43793, 'test/accuracy': 0.9861013889312744, 'test/loss': 0.04916822165250778, 'test/mean_average_precision': 0.2707588206225723, 'test/num_examples': 43793, 'score': 17306.208050727844, 'total_duration': 27028.29993700981, 'accumulated_submission_time': 17306.208050727844, 'accumulated_eval_time': 9718.271196126938, 'accumulated_logging_time': 2.3540172576904297}
I0305 17:35:37.617937 140408968165120 logging_writer.py:48] [53594] accumulated_eval_time=9718.271196, accumulated_logging_time=2.354017, accumulated_submission_time=17306.208051, global_step=53594, preemption_count=0, score=17306.208051, test/accuracy=0.986101, test/loss=0.049168, test/mean_average_precision=0.270759, test/num_examples=43793, total_duration=27028.299937, train/accuracy=0.993511, train/loss=0.020449, train/mean_average_precision=0.635551, validation/accuracy=0.987027, validation/loss=0.045995, validation/mean_average_precision=0.288226, validation/num_examples=43793
I0305 17:35:39.930534 140415782377216 logging_writer.py:48] [53600] global_step=53600, grad_norm=0.10142356157302856, loss=0.02210426703095436
I0305 17:36:12.018729 140408968165120 logging_writer.py:48] [53700] global_step=53700, grad_norm=0.11279857903718948, loss=0.02579198032617569
I0305 17:36:43.968584 140415782377216 logging_writer.py:48] [53800] global_step=53800, grad_norm=0.10314537584781647, loss=0.023724136874079704
I0305 17:37:16.041937 140408968165120 logging_writer.py:48] [53900] global_step=53900, grad_norm=0.10687805712223053, loss=0.024033017456531525
I0305 17:37:48.857380 140415782377216 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.10815764963626862, loss=0.02283015102148056
I0305 17:38:21.364135 140408968165120 logging_writer.py:48] [54100] global_step=54100, grad_norm=0.09565223008394241, loss=0.023247720673680305
I0305 17:38:53.316819 140415782377216 logging_writer.py:48] [54200] global_step=54200, grad_norm=0.12130292505025864, loss=0.02504846639931202
I0305 17:39:25.602882 140408968165120 logging_writer.py:48] [54300] global_step=54300, grad_norm=0.09578578174114227, loss=0.022204335778951645
I0305 17:39:37.763222 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:41:34.441370 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:41:37.473499 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:41:40.474394 140576608098112 submission_runner.py:411] Time since start: 27391.18s, 	Step: 54339, 	{'train/accuracy': 0.9935351014137268, 'train/loss': 0.02039884217083454, 'train/mean_average_precision': 0.6431458559111352, 'validation/accuracy': 0.9870265126228333, 'validation/loss': 0.046319570392370224, 'validation/mean_average_precision': 0.2899425669337316, 'validation/num_examples': 43793, 'test/accuracy': 0.9861831068992615, 'test/loss': 0.04943804070353508, 'test/mean_average_precision': 0.2739239046998224, 'test/num_examples': 43793, 'score': 17546.322751760483, 'total_duration': 27391.182716608047, 'accumulated_submission_time': 17546.322751760483, 'accumulated_eval_time': 9840.982321023941, 'accumulated_logging_time': 2.391303777694702}
I0305 17:41:40.505565 140407882708736 logging_writer.py:48] [54339] accumulated_eval_time=9840.982321, accumulated_logging_time=2.391304, accumulated_submission_time=17546.322752, global_step=54339, preemption_count=0, score=17546.322752, test/accuracy=0.986183, test/loss=0.049438, test/mean_average_precision=0.273924, test/num_examples=43793, total_duration=27391.182717, train/accuracy=0.993535, train/loss=0.020399, train/mean_average_precision=0.643146, validation/accuracy=0.987027, validation/loss=0.046320, validation/mean_average_precision=0.289943, validation/num_examples=43793
I0305 17:42:00.781340 140409291962112 logging_writer.py:48] [54400] global_step=54400, grad_norm=0.08409343659877777, loss=0.018367430195212364
I0305 17:42:33.283751 140407882708736 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.09236651659011841, loss=0.02079157717525959
I0305 17:43:05.918225 140409291962112 logging_writer.py:48] [54600] global_step=54600, grad_norm=0.11904991418123245, loss=0.02455134503543377
I0305 17:43:38.234010 140407882708736 logging_writer.py:48] [54700] global_step=54700, grad_norm=0.0936494916677475, loss=0.021514717489480972
I0305 17:44:10.447561 140409291962112 logging_writer.py:48] [54800] global_step=54800, grad_norm=0.10169178247451782, loss=0.02433418668806553
I0305 17:44:42.621271 140407882708736 logging_writer.py:48] [54900] global_step=54900, grad_norm=0.09546300023794174, loss=0.02245231345295906
I0305 17:45:15.092584 140409291962112 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.09930353611707687, loss=0.021275607869029045
I0305 17:45:40.660828 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:47:37.082066 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:47:40.137843 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:47:43.096267 140576608098112 submission_runner.py:411] Time since start: 27753.80s, 	Step: 55079, 	{'train/accuracy': 0.9935994744300842, 'train/loss': 0.020245837047696114, 'train/mean_average_precision': 0.6338022190807497, 'validation/accuracy': 0.9870439767837524, 'validation/loss': 0.04625082015991211, 'validation/mean_average_precision': 0.28955530294344833, 'validation/num_examples': 43793, 'test/accuracy': 0.9861974120140076, 'test/loss': 0.04954618215560913, 'test/mean_average_precision': 0.27508505881685036, 'test/num_examples': 43793, 'score': 17786.44678425789, 'total_duration': 27753.804602861404, 'accumulated_submission_time': 17786.44678425789, 'accumulated_eval_time': 9963.41772222519, 'accumulated_logging_time': 2.4342310428619385}
I0305 17:47:43.121929 140408968165120 logging_writer.py:48] [55079] accumulated_eval_time=9963.417722, accumulated_logging_time=2.434231, accumulated_submission_time=17786.446784, global_step=55079, preemption_count=0, score=17786.446784, test/accuracy=0.986197, test/loss=0.049546, test/mean_average_precision=0.275085, test/num_examples=43793, total_duration=27753.804603, train/accuracy=0.993599, train/loss=0.020246, train/mean_average_precision=0.633802, validation/accuracy=0.987044, validation/loss=0.046251, validation/mean_average_precision=0.289555, validation/num_examples=43793
I0305 17:47:50.159392 140415782377216 logging_writer.py:48] [55100] global_step=55100, grad_norm=0.09678690880537033, loss=0.020425502210855484
I0305 17:48:22.462395 140408968165120 logging_writer.py:48] [55200] global_step=55200, grad_norm=0.10352003574371338, loss=0.025013217702507973
I0305 17:48:54.044168 140415782377216 logging_writer.py:48] [55300] global_step=55300, grad_norm=0.09548177570104599, loss=0.021313203498721123
I0305 17:49:26.040323 140408968165120 logging_writer.py:48] [55400] global_step=55400, grad_norm=0.1137399673461914, loss=0.02310037426650524
I0305 17:49:57.740063 140415782377216 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.11111234128475189, loss=0.024035342037677765
I0305 17:50:29.843692 140408968165120 logging_writer.py:48] [55600] global_step=55600, grad_norm=0.10598067939281464, loss=0.02168598212301731
I0305 17:51:02.118478 140415782377216 logging_writer.py:48] [55700] global_step=55700, grad_norm=0.10973116755485535, loss=0.021719928830862045
I0305 17:51:34.305635 140408968165120 logging_writer.py:48] [55800] global_step=55800, grad_norm=0.09111811965703964, loss=0.020667845383286476
I0305 17:51:43.171773 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:53:39.312296 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:53:42.389253 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:53:45.371984 140576608098112 submission_runner.py:411] Time since start: 28116.08s, 	Step: 55829, 	{'train/accuracy': 0.9936383962631226, 'train/loss': 0.019924310967326164, 'train/mean_average_precision': 0.6444634510671647, 'validation/accuracy': 0.9870975613594055, 'validation/loss': 0.04666970670223236, 'validation/mean_average_precision': 0.28966356554121103, 'validation/num_examples': 43793, 'test/accuracy': 0.9862349033355713, 'test/loss': 0.04991696774959564, 'test/mean_average_precision': 0.27474796209750985, 'test/num_examples': 43793, 'score': 18026.465641736984, 'total_duration': 28116.08030819893, 'accumulated_submission_time': 18026.465641736984, 'accumulated_eval_time': 10085.61788034439, 'accumulated_logging_time': 2.470750331878662}
I0305 17:53:45.398155 140407882708736 logging_writer.py:48] [55829] accumulated_eval_time=10085.617880, accumulated_logging_time=2.470750, accumulated_submission_time=18026.465642, global_step=55829, preemption_count=0, score=18026.465642, test/accuracy=0.986235, test/loss=0.049917, test/mean_average_precision=0.274748, test/num_examples=43793, total_duration=28116.080308, train/accuracy=0.993638, train/loss=0.019924, train/mean_average_precision=0.644463, validation/accuracy=0.987098, validation/loss=0.046670, validation/mean_average_precision=0.289664, validation/num_examples=43793
I0305 17:54:09.052218 140415790769920 logging_writer.py:48] [55900] global_step=55900, grad_norm=0.12521061301231384, loss=0.02441662736237049
I0305 17:54:41.077819 140407882708736 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.11208590120077133, loss=0.02225589193403721
I0305 17:55:13.444566 140415790769920 logging_writer.py:48] [56100] global_step=56100, grad_norm=0.1022883802652359, loss=0.020770341157913208
I0305 17:55:45.449207 140407882708736 logging_writer.py:48] [56200] global_step=56200, grad_norm=0.11609131842851639, loss=0.024600764736533165
I0305 17:56:17.631430 140415790769920 logging_writer.py:48] [56300] global_step=56300, grad_norm=0.10294663161039352, loss=0.02360296994447708
I0305 17:56:50.049955 140407882708736 logging_writer.py:48] [56400] global_step=56400, grad_norm=0.09814072400331497, loss=0.023435134440660477
I0305 17:57:22.333655 140415790769920 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.09547888487577438, loss=0.02107948251068592
I0305 17:57:45.666906 140576608098112 spec.py:321] Evaluating on the training split.
I0305 17:59:38.495480 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 17:59:41.507709 140576608098112 spec.py:349] Evaluating on the test split.
I0305 17:59:44.521329 140576608098112 submission_runner.py:411] Time since start: 28475.23s, 	Step: 56574, 	{'train/accuracy': 0.9936703443527222, 'train/loss': 0.019641142338514328, 'train/mean_average_precision': 0.6531613462400097, 'validation/accuracy': 0.9871182441711426, 'validation/loss': 0.046835217624902725, 'validation/mean_average_precision': 0.2888919640760225, 'validation/num_examples': 43793, 'test/accuracy': 0.986233651638031, 'test/loss': 0.05032934248447418, 'test/mean_average_precision': 0.27153537898603847, 'test/num_examples': 43793, 'score': 18266.70391392708, 'total_duration': 28475.22965478897, 'accumulated_submission_time': 18266.70391392708, 'accumulated_eval_time': 10204.472261428833, 'accumulated_logging_time': 2.5078279972076416}
I0305 17:59:44.548242 140408968165120 logging_writer.py:48] [56574] accumulated_eval_time=10204.472261, accumulated_logging_time=2.507828, accumulated_submission_time=18266.703914, global_step=56574, preemption_count=0, score=18266.703914, test/accuracy=0.986234, test/loss=0.050329, test/mean_average_precision=0.271535, test/num_examples=43793, total_duration=28475.229655, train/accuracy=0.993670, train/loss=0.019641, train/mean_average_precision=0.653161, validation/accuracy=0.987118, validation/loss=0.046835, validation/mean_average_precision=0.288892, validation/num_examples=43793
I0305 17:59:53.297802 140409291962112 logging_writer.py:48] [56600] global_step=56600, grad_norm=0.10426060855388641, loss=0.022786512970924377
I0305 18:00:25.373929 140408968165120 logging_writer.py:48] [56700] global_step=56700, grad_norm=0.1059156209230423, loss=0.019037384539842606
I0305 18:00:57.874359 140409291962112 logging_writer.py:48] [56800] global_step=56800, grad_norm=0.0986475721001625, loss=0.020581286400556564
I0305 18:01:30.270866 140408968165120 logging_writer.py:48] [56900] global_step=56900, grad_norm=0.1131916418671608, loss=0.024700600653886795
I0305 18:02:02.641297 140409291962112 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.1144113838672638, loss=0.024700883775949478
I0305 18:02:34.508179 140408968165120 logging_writer.py:48] [57100] global_step=57100, grad_norm=0.11687836050987244, loss=0.02224425971508026
I0305 18:03:06.784329 140409291962112 logging_writer.py:48] [57200] global_step=57200, grad_norm=0.09935023635625839, loss=0.02037700265645981
I0305 18:03:39.137024 140408968165120 logging_writer.py:48] [57300] global_step=57300, grad_norm=0.09315639734268188, loss=0.01956639066338539
I0305 18:03:44.557174 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:05:41.683792 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:05:44.741781 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:05:47.787894 140576608098112 submission_runner.py:411] Time since start: 28838.50s, 	Step: 57318, 	{'train/accuracy': 0.9939448237419128, 'train/loss': 0.01902058720588684, 'train/mean_average_precision': 0.6744130159403038, 'validation/accuracy': 0.9870589971542358, 'validation/loss': 0.04679694399237633, 'validation/mean_average_precision': 0.2875065035808982, 'validation/num_examples': 43793, 'test/accuracy': 0.9861376285552979, 'test/loss': 0.05007738620042801, 'test/mean_average_precision': 0.27625264563142654, 'test/num_examples': 43793, 'score': 18506.681567668915, 'total_duration': 28838.496221780777, 'accumulated_submission_time': 18506.681567668915, 'accumulated_eval_time': 10327.702929973602, 'accumulated_logging_time': 2.546010971069336}
I0305 18:05:47.813594 140407882708736 logging_writer.py:48] [57318] accumulated_eval_time=10327.702930, accumulated_logging_time=2.546011, accumulated_submission_time=18506.681568, global_step=57318, preemption_count=0, score=18506.681568, test/accuracy=0.986138, test/loss=0.050077, test/mean_average_precision=0.276253, test/num_examples=43793, total_duration=28838.496222, train/accuracy=0.993945, train/loss=0.019021, train/mean_average_precision=0.674413, validation/accuracy=0.987059, validation/loss=0.046797, validation/mean_average_precision=0.287507, validation/num_examples=43793
I0305 18:06:15.361061 140415790769920 logging_writer.py:48] [57400] global_step=57400, grad_norm=0.10275651514530182, loss=0.020408878102898598
I0305 18:06:48.124754 140407882708736 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.10394136607646942, loss=0.021345678716897964
I0305 18:07:20.525640 140415790769920 logging_writer.py:48] [57600] global_step=57600, grad_norm=0.10512424260377884, loss=0.021407024934887886
I0305 18:07:52.786754 140407882708736 logging_writer.py:48] [57700] global_step=57700, grad_norm=0.08979923278093338, loss=0.020003577694296837
I0305 18:08:25.264891 140415790769920 logging_writer.py:48] [57800] global_step=57800, grad_norm=0.10078456252813339, loss=0.02208469994366169
I0305 18:08:57.142892 140407882708736 logging_writer.py:48] [57900] global_step=57900, grad_norm=0.10357853025197983, loss=0.021759022027254105
I0305 18:09:29.417376 140415790769920 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.1008516401052475, loss=0.02012029103934765
I0305 18:09:48.118891 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:11:46.669263 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:11:49.760273 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:11:52.768366 140576608098112 submission_runner.py:411] Time since start: 29203.48s, 	Step: 58059, 	{'train/accuracy': 0.9942131638526917, 'train/loss': 0.018031014129519463, 'train/mean_average_precision': 0.6876187034913789, 'validation/accuracy': 0.9870078563690186, 'validation/loss': 0.04715845361351967, 'validation/mean_average_precision': 0.28620366343716436, 'validation/num_examples': 43793, 'test/accuracy': 0.9861649870872498, 'test/loss': 0.050599150359630585, 'test/mean_average_precision': 0.2743655426824452, 'test/num_examples': 43793, 'score': 18746.95504975319, 'total_duration': 29203.476695775986, 'accumulated_submission_time': 18746.95504975319, 'accumulated_eval_time': 10452.352368354797, 'accumulated_logging_time': 2.582829475402832}
I0305 18:11:52.795635 140408968165120 logging_writer.py:48] [58059] accumulated_eval_time=10452.352368, accumulated_logging_time=2.582829, accumulated_submission_time=18746.955050, global_step=58059, preemption_count=0, score=18746.955050, test/accuracy=0.986165, test/loss=0.050599, test/mean_average_precision=0.274366, test/num_examples=43793, total_duration=29203.476696, train/accuracy=0.994213, train/loss=0.018031, train/mean_average_precision=0.687619, validation/accuracy=0.987008, validation/loss=0.047158, validation/mean_average_precision=0.286204, validation/num_examples=43793
I0305 18:12:06.530319 140415782377216 logging_writer.py:48] [58100] global_step=58100, grad_norm=0.10596732795238495, loss=0.022179873660206795
I0305 18:12:38.708496 140408968165120 logging_writer.py:48] [58200] global_step=58200, grad_norm=0.09724817425012589, loss=0.01901068724691868
I0305 18:13:11.027212 140415782377216 logging_writer.py:48] [58300] global_step=58300, grad_norm=0.12495612353086472, loss=0.024422375485301018
I0305 18:13:43.049037 140408968165120 logging_writer.py:48] [58400] global_step=58400, grad_norm=0.10783814638853073, loss=0.02449762262403965
I0305 18:14:15.169534 140415782377216 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.12052782624959946, loss=0.023962926119565964
I0305 18:14:47.028734 140408968165120 logging_writer.py:48] [58600] global_step=58600, grad_norm=0.11060623824596405, loss=0.020318740978837013
I0305 18:15:19.340995 140415782377216 logging_writer.py:48] [58700] global_step=58700, grad_norm=0.12095144391059875, loss=0.023601185530424118
I0305 18:15:51.568519 140408968165120 logging_writer.py:48] [58800] global_step=58800, grad_norm=0.10811012238264084, loss=0.024006064981222153
I0305 18:15:52.845159 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:17:47.561015 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:17:50.590844 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:17:53.622382 140576608098112 submission_runner.py:411] Time since start: 29564.33s, 	Step: 58805, 	{'train/accuracy': 0.9944328665733337, 'train/loss': 0.017561275511980057, 'train/mean_average_precision': 0.7030557313454966, 'validation/accuracy': 0.9870532751083374, 'validation/loss': 0.047436848282814026, 'validation/mean_average_precision': 0.28794227019730645, 'validation/num_examples': 43793, 'test/accuracy': 0.9862053990364075, 'test/loss': 0.050916947424411774, 'test/mean_average_precision': 0.27338544185002445, 'test/num_examples': 43793, 'score': 18986.972960710526, 'total_duration': 29564.330701112747, 'accumulated_submission_time': 18986.972960710526, 'accumulated_eval_time': 10573.129535675049, 'accumulated_logging_time': 2.621100902557373}
I0305 18:17:53.649296 140409291962112 logging_writer.py:48] [58805] accumulated_eval_time=10573.129536, accumulated_logging_time=2.621101, accumulated_submission_time=18986.972961, global_step=58805, preemption_count=0, score=18986.972961, test/accuracy=0.986205, test/loss=0.050917, test/mean_average_precision=0.273385, test/num_examples=43793, total_duration=29564.330701, train/accuracy=0.994433, train/loss=0.017561, train/mean_average_precision=0.703056, validation/accuracy=0.987053, validation/loss=0.047437, validation/mean_average_precision=0.287942, validation/num_examples=43793
I0305 18:18:24.446935 140415790769920 logging_writer.py:48] [58900] global_step=58900, grad_norm=0.11052621155977249, loss=0.022975562140345573
I0305 18:18:56.392853 140409291962112 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.10853375494480133, loss=0.01788579486310482
I0305 18:19:28.716590 140415790769920 logging_writer.py:48] [59100] global_step=59100, grad_norm=0.10664021223783493, loss=0.022169461473822594
I0305 18:20:00.539441 140409291962112 logging_writer.py:48] [59200] global_step=59200, grad_norm=0.10312899947166443, loss=0.020113598555326462
I0305 18:20:32.936120 140415790769920 logging_writer.py:48] [59300] global_step=59300, grad_norm=0.11132896691560745, loss=0.022495191544294357
I0305 18:21:05.371942 140409291962112 logging_writer.py:48] [59400] global_step=59400, grad_norm=0.11234552413225174, loss=0.02226637676358223
I0305 18:21:37.245433 140415790769920 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.10666190832853317, loss=0.020084738731384277
I0305 18:21:53.736229 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:23:47.830492 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:23:50.899723 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:23:53.923492 140576608098112 submission_runner.py:411] Time since start: 29924.63s, 	Step: 59552, 	{'train/accuracy': 0.9943233728408813, 'train/loss': 0.01780301332473755, 'train/mean_average_precision': 0.694210838560239, 'validation/accuracy': 0.9870098829269409, 'validation/loss': 0.04714403674006462, 'validation/mean_average_precision': 0.2937513754272167, 'validation/num_examples': 43793, 'test/accuracy': 0.9861890077590942, 'test/loss': 0.05059564486145973, 'test/mean_average_precision': 0.27516602253228856, 'test/num_examples': 43793, 'score': 19227.028086662292, 'total_duration': 29924.63180088997, 'accumulated_submission_time': 19227.028086662292, 'accumulated_eval_time': 10693.316731929779, 'accumulated_logging_time': 2.6599795818328857}
I0305 18:23:53.951183 140407882708736 logging_writer.py:48] [59552] accumulated_eval_time=10693.316732, accumulated_logging_time=2.659980, accumulated_submission_time=19227.028087, global_step=59552, preemption_count=0, score=19227.028087, test/accuracy=0.986189, test/loss=0.050596, test/mean_average_precision=0.275166, test/num_examples=43793, total_duration=29924.631801, train/accuracy=0.994323, train/loss=0.017803, train/mean_average_precision=0.694211, validation/accuracy=0.987010, validation/loss=0.047144, validation/mean_average_precision=0.293751, validation/num_examples=43793
I0305 18:24:09.936700 140408968165120 logging_writer.py:48] [59600] global_step=59600, grad_norm=0.101259246468544, loss=0.019897030666470528
I0305 18:24:42.110753 140407882708736 logging_writer.py:48] [59700] global_step=59700, grad_norm=0.11065724492073059, loss=0.021171653643250465
I0305 18:25:14.402864 140408968165120 logging_writer.py:48] [59800] global_step=59800, grad_norm=0.11774025112390518, loss=0.021808214485645294
I0305 18:25:46.240496 140407882708736 logging_writer.py:48] [59900] global_step=59900, grad_norm=0.11290963739156723, loss=0.02248607575893402
I0305 18:26:18.627015 140408968165120 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.10970930755138397, loss=0.019941190257668495
I0305 18:26:50.802361 140407882708736 logging_writer.py:48] [60100] global_step=60100, grad_norm=0.13170528411865234, loss=0.02468952164053917
I0305 18:27:23.275485 140408968165120 logging_writer.py:48] [60200] global_step=60200, grad_norm=0.12087208032608032, loss=0.02035379968583584
I0305 18:27:53.998657 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:29:49.871483 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:29:53.271105 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:29:56.643028 140576608098112 submission_runner.py:411] Time since start: 30287.35s, 	Step: 60296, 	{'train/accuracy': 0.9941422343254089, 'train/loss': 0.018168844282627106, 'train/mean_average_precision': 0.6758369738154911, 'validation/accuracy': 0.9870870113372803, 'validation/loss': 0.04780871421098709, 'validation/mean_average_precision': 0.2878571093468118, 'validation/num_examples': 43793, 'test/accuracy': 0.9862959980964661, 'test/loss': 0.051175232976675034, 'test/mean_average_precision': 0.2740927208224178, 'test/num_examples': 43793, 'score': 19467.04409813881, 'total_duration': 30287.35133075714, 'accumulated_submission_time': 19467.04409813881, 'accumulated_eval_time': 10815.961034536362, 'accumulated_logging_time': 2.698820114135742}
I0305 18:29:56.673312 140409291962112 logging_writer.py:48] [60296] accumulated_eval_time=10815.961035, accumulated_logging_time=2.698820, accumulated_submission_time=19467.044098, global_step=60296, preemption_count=0, score=19467.044098, test/accuracy=0.986296, test/loss=0.051175, test/mean_average_precision=0.274093, test/num_examples=43793, total_duration=30287.351331, train/accuracy=0.994142, train/loss=0.018169, train/mean_average_precision=0.675837, validation/accuracy=0.987087, validation/loss=0.047809, validation/mean_average_precision=0.287857, validation/num_examples=43793
I0305 18:29:58.429953 140415790769920 logging_writer.py:48] [60300] global_step=60300, grad_norm=0.11639070510864258, loss=0.023128561675548553
I0305 18:30:30.582851 140409291962112 logging_writer.py:48] [60400] global_step=60400, grad_norm=0.10922399908304214, loss=0.02090052329003811
I0305 18:31:02.665814 140415790769920 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.11119852215051651, loss=0.022408653050661087
I0305 18:31:34.941371 140409291962112 logging_writer.py:48] [60600] global_step=60600, grad_norm=0.1346713751554489, loss=0.024993449449539185
I0305 18:32:07.209390 140415790769920 logging_writer.py:48] [60700] global_step=60700, grad_norm=0.1093127653002739, loss=0.018148476257920265
I0305 18:32:39.843805 140409291962112 logging_writer.py:48] [60800] global_step=60800, grad_norm=0.12886138260364532, loss=0.02288171462714672
I0305 18:33:12.259372 140415790769920 logging_writer.py:48] [60900] global_step=60900, grad_norm=0.11248747259378433, loss=0.022677075117826462
I0305 18:33:44.197512 140409291962112 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.13691961765289307, loss=0.021700508892536163
I0305 18:33:56.702628 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:35:52.321657 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:35:55.354817 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:35:58.342555 140576608098112 submission_runner.py:411] Time since start: 30649.05s, 	Step: 61040, 	{'train/accuracy': 0.9941512942314148, 'train/loss': 0.01820985972881317, 'train/mean_average_precision': 0.686735293228531, 'validation/accuracy': 0.9870995879173279, 'validation/loss': 0.04788989573717117, 'validation/mean_average_precision': 0.286116871264883, 'validation/num_examples': 43793, 'test/accuracy': 0.9861696362495422, 'test/loss': 0.05140224099159241, 'test/mean_average_precision': 0.2712695979829206, 'test/num_examples': 43793, 'score': 19707.040642499924, 'total_duration': 30649.05088710785, 'accumulated_submission_time': 19707.040642499924, 'accumulated_eval_time': 10937.600918531418, 'accumulated_logging_time': 2.740966796875}
I0305 18:35:58.370129 140408968165120 logging_writer.py:48] [61040] accumulated_eval_time=10937.600919, accumulated_logging_time=2.740967, accumulated_submission_time=19707.040642, global_step=61040, preemption_count=0, score=19707.040642, test/accuracy=0.986170, test/loss=0.051402, test/mean_average_precision=0.271270, test/num_examples=43793, total_duration=30649.050887, train/accuracy=0.994151, train/loss=0.018210, train/mean_average_precision=0.686735, validation/accuracy=0.987100, validation/loss=0.047890, validation/mean_average_precision=0.286117, validation/num_examples=43793
I0305 18:36:18.187931 140415782377216 logging_writer.py:48] [61100] global_step=61100, grad_norm=0.1406417191028595, loss=0.021411815658211708
I0305 18:36:50.520446 140408968165120 logging_writer.py:48] [61200] global_step=61200, grad_norm=0.13602274656295776, loss=0.018651947379112244
I0305 18:37:22.897234 140415782377216 logging_writer.py:48] [61300] global_step=61300, grad_norm=0.1163567304611206, loss=0.022450827062129974
I0305 18:37:54.806195 140408968165120 logging_writer.py:48] [61400] global_step=61400, grad_norm=0.1200675368309021, loss=0.020467622205615044
I0305 18:38:27.041325 140415782377216 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.1310047060251236, loss=0.021741298958659172
I0305 18:38:59.217455 140408968165120 logging_writer.py:48] [61600] global_step=61600, grad_norm=0.12984299659729004, loss=0.021244846284389496
I0305 18:39:31.350966 140415782377216 logging_writer.py:48] [61700] global_step=61700, grad_norm=0.11199544370174408, loss=0.018739789724349976
I0305 18:39:58.446767 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:41:51.563651 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:41:54.627959 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:41:57.692956 140576608098112 submission_runner.py:411] Time since start: 31008.40s, 	Step: 61786, 	{'train/accuracy': 0.9941979050636292, 'train/loss': 0.017994355410337448, 'train/mean_average_precision': 0.6911118193222282, 'validation/accuracy': 0.9871572256088257, 'validation/loss': 0.04788706451654434, 'validation/mean_average_precision': 0.290800598389781, 'validation/num_examples': 43793, 'test/accuracy': 0.9861999154090881, 'test/loss': 0.05176125839352608, 'test/mean_average_precision': 0.2712998483111147, 'test/num_examples': 43793, 'score': 19947.08567380905, 'total_duration': 31008.40128135681, 'accumulated_submission_time': 19947.08567380905, 'accumulated_eval_time': 11056.847058057785, 'accumulated_logging_time': 2.7800092697143555}
I0305 18:41:57.720226 140407882708736 logging_writer.py:48] [61786] accumulated_eval_time=11056.847058, accumulated_logging_time=2.780009, accumulated_submission_time=19947.085674, global_step=61786, preemption_count=0, score=19947.085674, test/accuracy=0.986200, test/loss=0.051761, test/mean_average_precision=0.271300, test/num_examples=43793, total_duration=31008.401281, train/accuracy=0.994198, train/loss=0.017994, train/mean_average_precision=0.691112, validation/accuracy=0.987157, validation/loss=0.047887, validation/mean_average_precision=0.290801, validation/num_examples=43793
I0305 18:42:02.629822 140415790769920 logging_writer.py:48] [61800] global_step=61800, grad_norm=0.1228162944316864, loss=0.021176649257540703
I0305 18:42:34.744509 140407882708736 logging_writer.py:48] [61900] global_step=61900, grad_norm=0.11888594925403595, loss=0.02262483723461628
I0305 18:43:06.517806 140415790769920 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.11489655822515488, loss=0.019821491092443466
I0305 18:43:38.321655 140407882708736 logging_writer.py:48] [62100] global_step=62100, grad_norm=0.13231126964092255, loss=0.021765857934951782
I0305 18:44:10.508828 140415790769920 logging_writer.py:48] [62200] global_step=62200, grad_norm=0.1259445995092392, loss=0.021980084478855133
I0305 18:44:42.414137 140407882708736 logging_writer.py:48] [62300] global_step=62300, grad_norm=0.1341380923986435, loss=0.024453014135360718
I0305 18:45:15.186700 140415790769920 logging_writer.py:48] [62400] global_step=62400, grad_norm=0.12625040113925934, loss=0.020423945039510727
I0305 18:45:48.031295 140407882708736 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.12144889682531357, loss=0.019538914784789085
I0305 18:45:57.709154 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:47:55.197110 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:47:58.276694 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:48:01.420091 140576608098112 submission_runner.py:411] Time since start: 31372.13s, 	Step: 62531, 	{'train/accuracy': 0.9941907525062561, 'train/loss': 0.01790260709822178, 'train/mean_average_precision': 0.6973760724944296, 'validation/accuracy': 0.9870695471763611, 'validation/loss': 0.04836646467447281, 'validation/mean_average_precision': 0.2877178300183725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861767888069153, 'test/loss': 0.052037857472896576, 'test/mean_average_precision': 0.26907835024324567, 'test/num_examples': 43793, 'score': 20187.041157007217, 'total_duration': 31372.12839603424, 'accumulated_submission_time': 20187.041157007217, 'accumulated_eval_time': 11180.55793595314, 'accumulated_logging_time': 2.8193583488464355}
I0305 18:48:01.456568 140408968165120 logging_writer.py:48] [62531] accumulated_eval_time=11180.557936, accumulated_logging_time=2.819358, accumulated_submission_time=20187.041157, global_step=62531, preemption_count=0, score=20187.041157, test/accuracy=0.986177, test/loss=0.052038, test/mean_average_precision=0.269078, test/num_examples=43793, total_duration=31372.128396, train/accuracy=0.994191, train/loss=0.017903, train/mean_average_precision=0.697376, validation/accuracy=0.987070, validation/loss=0.048366, validation/mean_average_precision=0.287718, validation/num_examples=43793
I0305 18:48:24.162233 140415782377216 logging_writer.py:48] [62600] global_step=62600, grad_norm=0.14017902314662933, loss=0.02135712467133999
I0305 18:48:56.135942 140408968165120 logging_writer.py:48] [62700] global_step=62700, grad_norm=0.12497440725564957, loss=0.0170234814286232
I0305 18:49:28.386373 140415782377216 logging_writer.py:48] [62800] global_step=62800, grad_norm=0.12775413691997528, loss=0.020754775032401085
I0305 18:50:00.578781 140408968165120 logging_writer.py:48] [62900] global_step=62900, grad_norm=0.11139924824237823, loss=0.021091997623443604
I0305 18:50:33.091351 140415782377216 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.12368207424879074, loss=0.01943148858845234
I0305 18:51:05.064513 140408968165120 logging_writer.py:48] [63100] global_step=63100, grad_norm=0.11208666115999222, loss=0.020805159583687782
I0305 18:51:37.233217 140415782377216 logging_writer.py:48] [63200] global_step=63200, grad_norm=0.11396168172359467, loss=0.01946020871400833
I0305 18:52:01.541249 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:53:53.185297 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:53:56.246067 140576608098112 spec.py:349] Evaluating on the test split.
I0305 18:53:59.300189 140576608098112 submission_runner.py:411] Time since start: 31730.01s, 	Step: 63276, 	{'train/accuracy': 0.9944777488708496, 'train/loss': 0.017259057611227036, 'train/mean_average_precision': 0.7072899931696965, 'validation/accuracy': 0.9870573282241821, 'validation/loss': 0.04824541136622429, 'validation/mean_average_precision': 0.28795803002755643, 'validation/num_examples': 43793, 'test/accuracy': 0.9861236810684204, 'test/loss': 0.051955170929431915, 'test/mean_average_precision': 0.2732008237432165, 'test/num_examples': 43793, 'score': 20427.09281229973, 'total_duration': 31730.00851416588, 'accumulated_submission_time': 20427.09281229973, 'accumulated_eval_time': 11298.31682562828, 'accumulated_logging_time': 2.868788719177246}
I0305 18:53:59.327774 140407882708736 logging_writer.py:48] [63276] accumulated_eval_time=11298.316826, accumulated_logging_time=2.868789, accumulated_submission_time=20427.092812, global_step=63276, preemption_count=0, score=20427.092812, test/accuracy=0.986124, test/loss=0.051955, test/mean_average_precision=0.273201, test/num_examples=43793, total_duration=31730.008514, train/accuracy=0.994478, train/loss=0.017259, train/mean_average_precision=0.707290, validation/accuracy=0.987057, validation/loss=0.048245, validation/mean_average_precision=0.287958, validation/num_examples=43793
I0305 18:54:07.592478 140415790769920 logging_writer.py:48] [63300] global_step=63300, grad_norm=0.12414717674255371, loss=0.02035921812057495
I0305 18:54:39.760062 140407882708736 logging_writer.py:48] [63400] global_step=63400, grad_norm=0.13111528754234314, loss=0.022240905091166496
I0305 18:55:11.893430 140415790769920 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.13362079858779907, loss=0.02127864770591259
I0305 18:55:44.655749 140407882708736 logging_writer.py:48] [63600] global_step=63600, grad_norm=0.11715946346521378, loss=0.017790067940950394
I0305 18:56:16.815638 140415790769920 logging_writer.py:48] [63700] global_step=63700, grad_norm=0.11950948089361191, loss=0.020632382482290268
I0305 18:56:49.006105 140407882708736 logging_writer.py:48] [63800] global_step=63800, grad_norm=0.14889642596244812, loss=0.02043825015425682
I0305 18:57:21.219582 140415790769920 logging_writer.py:48] [63900] global_step=63900, grad_norm=0.1276768445968628, loss=0.02032112330198288
I0305 18:57:53.294267 140407882708736 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.12684552371501923, loss=0.019211946055293083
I0305 18:57:59.348888 140576608098112 spec.py:321] Evaluating on the training split.
I0305 18:59:54.064252 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 18:59:57.079900 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:00:00.001123 140576608098112 submission_runner.py:411] Time since start: 32090.71s, 	Step: 64020, 	{'train/accuracy': 0.9946504831314087, 'train/loss': 0.01659744791686535, 'train/mean_average_precision': 0.7181443084231507, 'validation/accuracy': 0.9870849847793579, 'validation/loss': 0.04852207377552986, 'validation/mean_average_precision': 0.2862159947309958, 'validation/num_examples': 43793, 'test/accuracy': 0.986163318157196, 'test/loss': 0.05215165391564369, 'test/mean_average_precision': 0.27280427783351635, 'test/num_examples': 43793, 'score': 20667.083528518677, 'total_duration': 32090.70943880081, 'accumulated_submission_time': 20667.083528518677, 'accumulated_eval_time': 11418.96899819374, 'accumulated_logging_time': 2.9073712825775146}
I0305 19:00:00.029678 140408968165120 logging_writer.py:48] [64020] accumulated_eval_time=11418.968998, accumulated_logging_time=2.907371, accumulated_submission_time=20667.083529, global_step=64020, preemption_count=0, score=20667.083529, test/accuracy=0.986163, test/loss=0.052152, test/mean_average_precision=0.272804, test/num_examples=43793, total_duration=32090.709439, train/accuracy=0.994650, train/loss=0.016597, train/mean_average_precision=0.718144, validation/accuracy=0.987085, validation/loss=0.048522, validation/mean_average_precision=0.286216, validation/num_examples=43793
I0305 19:00:26.233000 140415782377216 logging_writer.py:48] [64100] global_step=64100, grad_norm=0.13635414838790894, loss=0.021588031202554703
I0305 19:00:58.727951 140408968165120 logging_writer.py:48] [64200] global_step=64200, grad_norm=0.13507014513015747, loss=0.020973701030015945
I0305 19:01:32.234778 140415782377216 logging_writer.py:48] [64300] global_step=64300, grad_norm=0.13100314140319824, loss=0.020371628925204277
I0305 19:02:05.652106 140408968165120 logging_writer.py:48] [64400] global_step=64400, grad_norm=0.13511227071285248, loss=0.02240750938653946
I0305 19:02:38.052452 140415782377216 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.13613586127758026, loss=0.022729676216840744
I0305 19:03:10.567051 140408968165120 logging_writer.py:48] [64600] global_step=64600, grad_norm=0.12214905023574829, loss=0.02034175395965576
I0305 19:03:42.869300 140415782377216 logging_writer.py:48] [64700] global_step=64700, grad_norm=0.13060793280601501, loss=0.017923224717378616
I0305 19:04:00.019672 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:05:52.496251 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:05:55.530487 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:05:58.537239 140576608098112 submission_runner.py:411] Time since start: 32449.25s, 	Step: 64754, 	{'train/accuracy': 0.9948357939720154, 'train/loss': 0.016156265512108803, 'train/mean_average_precision': 0.7323550397195834, 'validation/accuracy': 0.9871401786804199, 'validation/loss': 0.04851768910884857, 'validation/mean_average_precision': 0.2874680887460105, 'validation/num_examples': 43793, 'test/accuracy': 0.9861885905265808, 'test/loss': 0.052366696298122406, 'test/mean_average_precision': 0.270310425672684, 'test/num_examples': 43793, 'score': 20907.040336608887, 'total_duration': 32449.245572328568, 'accumulated_submission_time': 20907.040336608887, 'accumulated_eval_time': 11537.486523628235, 'accumulated_logging_time': 2.947197914123535}
I0305 19:05:58.564912 140409291962112 logging_writer.py:48] [64754] accumulated_eval_time=11537.486524, accumulated_logging_time=2.947198, accumulated_submission_time=20907.040337, global_step=64754, preemption_count=0, score=20907.040337, test/accuracy=0.986189, test/loss=0.052367, test/mean_average_precision=0.270310, test/num_examples=43793, total_duration=32449.245572, train/accuracy=0.994836, train/loss=0.016156, train/mean_average_precision=0.732355, validation/accuracy=0.987140, validation/loss=0.048518, validation/mean_average_precision=0.287468, validation/num_examples=43793
I0305 19:06:13.751845 140415790769920 logging_writer.py:48] [64800] global_step=64800, grad_norm=0.12862300872802734, loss=0.01798158511519432
I0305 19:06:46.094320 140409291962112 logging_writer.py:48] [64900] global_step=64900, grad_norm=0.13274197280406952, loss=0.020498428493738174
I0305 19:07:18.262519 140415790769920 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.1306358426809311, loss=0.02336079813539982
I0305 19:07:50.540583 140409291962112 logging_writer.py:48] [65100] global_step=65100, grad_norm=0.13258115947246552, loss=0.019612811505794525
I0305 19:08:22.826982 140415790769920 logging_writer.py:48] [65200] global_step=65200, grad_norm=0.14262916147708893, loss=0.02129248157143593
I0305 19:08:54.898624 140409291962112 logging_writer.py:48] [65300] global_step=65300, grad_norm=0.11116989701986313, loss=0.01857694983482361
I0305 19:09:27.072004 140415790769920 logging_writer.py:48] [65400] global_step=65400, grad_norm=0.13792209327220917, loss=0.021573567762970924
I0305 19:09:58.843802 140409291962112 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.13607646524906158, loss=0.021214820444583893
I0305 19:09:58.848572 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:11:49.268179 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:11:52.310436 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:11:55.287717 140576608098112 submission_runner.py:411] Time since start: 32806.00s, 	Step: 65501, 	{'train/accuracy': 0.9949519634246826, 'train/loss': 0.01564915105700493, 'train/mean_average_precision': 0.7398820880929526, 'validation/accuracy': 0.9871227145195007, 'validation/loss': 0.04873404651880264, 'validation/mean_average_precision': 0.2906366602949252, 'validation/num_examples': 43793, 'test/accuracy': 0.9861925840377808, 'test/loss': 0.05264739319682121, 'test/mean_average_precision': 0.27030206645728694, 'test/num_examples': 43793, 'score': 21147.292327165604, 'total_duration': 32805.996050834656, 'accumulated_submission_time': 21147.292327165604, 'accumulated_eval_time': 11653.925603151321, 'accumulated_logging_time': 2.98614239692688}
I0305 19:11:55.317278 140407882708736 logging_writer.py:48] [65501] accumulated_eval_time=11653.925603, accumulated_logging_time=2.986142, accumulated_submission_time=21147.292327, global_step=65501, preemption_count=0, score=21147.292327, test/accuracy=0.986193, test/loss=0.052647, test/mean_average_precision=0.270302, test/num_examples=43793, total_duration=32805.996051, train/accuracy=0.994952, train/loss=0.015649, train/mean_average_precision=0.739882, validation/accuracy=0.987123, validation/loss=0.048734, validation/mean_average_precision=0.290637, validation/num_examples=43793
I0305 19:12:27.473778 140408968165120 logging_writer.py:48] [65600] global_step=65600, grad_norm=0.11667197942733765, loss=0.02033933438360691
I0305 19:12:59.696004 140407882708736 logging_writer.py:48] [65700] global_step=65700, grad_norm=0.13195911049842834, loss=0.02106328308582306
I0305 19:13:32.019752 140408968165120 logging_writer.py:48] [65800] global_step=65800, grad_norm=0.13465248048305511, loss=0.0203262809664011
I0305 19:14:04.070316 140407882708736 logging_writer.py:48] [65900] global_step=65900, grad_norm=0.1498229205608368, loss=0.0201223473995924
I0305 19:14:36.147631 140408968165120 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.1295720785856247, loss=0.02305438742041588
I0305 19:15:08.374062 140407882708736 logging_writer.py:48] [66100] global_step=66100, grad_norm=0.1289941519498825, loss=0.020759286358952522
I0305 19:15:40.376121 140408968165120 logging_writer.py:48] [66200] global_step=66200, grad_norm=0.13846103847026825, loss=0.01809670776128769
I0305 19:15:55.381908 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:17:48.913212 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:17:51.980355 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:17:54.965623 140576608098112 submission_runner.py:411] Time since start: 33165.67s, 	Step: 66248, 	{'train/accuracy': 0.9950123429298401, 'train/loss': 0.015703314915299416, 'train/mean_average_precision': 0.7419741470556183, 'validation/accuracy': 0.9870402812957764, 'validation/loss': 0.04901812970638275, 'validation/mean_average_precision': 0.29158882852293155, 'validation/num_examples': 43793, 'test/accuracy': 0.9861291646957397, 'test/loss': 0.05271420255303383, 'test/mean_average_precision': 0.2725590229995492, 'test/num_examples': 43793, 'score': 21387.325922966003, 'total_duration': 33165.67395567894, 'accumulated_submission_time': 21387.325922966003, 'accumulated_eval_time': 11773.509294509888, 'accumulated_logging_time': 3.0265464782714844}
I0305 19:17:54.993680 140409291962112 logging_writer.py:48] [66248] accumulated_eval_time=11773.509295, accumulated_logging_time=3.026546, accumulated_submission_time=21387.325923, global_step=66248, preemption_count=0, score=21387.325923, test/accuracy=0.986129, test/loss=0.052714, test/mean_average_precision=0.272559, test/num_examples=43793, total_duration=33165.673956, train/accuracy=0.995012, train/loss=0.015703, train/mean_average_precision=0.741974, validation/accuracy=0.987040, validation/loss=0.049018, validation/mean_average_precision=0.291589, validation/num_examples=43793
I0305 19:18:12.094624 140415782377216 logging_writer.py:48] [66300] global_step=66300, grad_norm=0.12681059539318085, loss=0.01739594340324402
I0305 19:18:44.332773 140409291962112 logging_writer.py:48] [66400] global_step=66400, grad_norm=0.1514119654893875, loss=0.02056269533932209
I0305 19:19:16.321879 140415782377216 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.1227504089474678, loss=0.018853379413485527
I0305 19:19:48.330752 140409291962112 logging_writer.py:48] [66600] global_step=66600, grad_norm=0.13127131760120392, loss=0.019157182425260544
I0305 19:20:20.617115 140415782377216 logging_writer.py:48] [66700] global_step=66700, grad_norm=0.1496213674545288, loss=0.018326520919799805
I0305 19:20:52.566320 140409291962112 logging_writer.py:48] [66800] global_step=66800, grad_norm=0.1277855634689331, loss=0.019277298822999
I0305 19:21:24.861847 140415782377216 logging_writer.py:48] [66900] global_step=66900, grad_norm=0.14138855040073395, loss=0.019670313224196434
I0305 19:21:55.242554 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:23:48.630234 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:23:51.734245 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:23:54.758047 140576608098112 submission_runner.py:411] Time since start: 33525.47s, 	Step: 66996, 	{'train/accuracy': 0.9948969483375549, 'train/loss': 0.015833865851163864, 'train/mean_average_precision': 0.7381022885433084, 'validation/accuracy': 0.9870325922966003, 'validation/loss': 0.04913480579853058, 'validation/mean_average_precision': 0.29172425596365825, 'validation/num_examples': 43793, 'test/accuracy': 0.9861043095588684, 'test/loss': 0.05298231169581413, 'test/mean_average_precision': 0.26930050140609724, 'test/num_examples': 43793, 'score': 21627.543115854263, 'total_duration': 33525.466379880905, 'accumulated_submission_time': 21627.543115854263, 'accumulated_eval_time': 11893.024748325348, 'accumulated_logging_time': 3.0661911964416504}
I0305 19:23:54.786089 140408968165120 logging_writer.py:48] [66996] accumulated_eval_time=11893.024748, accumulated_logging_time=3.066191, accumulated_submission_time=21627.543116, global_step=66996, preemption_count=0, score=21627.543116, test/accuracy=0.986104, test/loss=0.052982, test/mean_average_precision=0.269301, test/num_examples=43793, total_duration=33525.466380, train/accuracy=0.994897, train/loss=0.015834, train/mean_average_precision=0.738102, validation/accuracy=0.987033, validation/loss=0.049135, validation/mean_average_precision=0.291724, validation/num_examples=43793
I0305 19:23:56.564299 140415790769920 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.13917027413845062, loss=0.018574226647615433
I0305 19:24:28.777791 140408968165120 logging_writer.py:48] [67100] global_step=67100, grad_norm=0.12406644970178604, loss=0.017837338149547577
I0305 19:25:00.485133 140415790769920 logging_writer.py:48] [67200] global_step=67200, grad_norm=0.1349470615386963, loss=0.018791744485497475
I0305 19:25:32.569063 140408968165120 logging_writer.py:48] [67300] global_step=67300, grad_norm=0.15160085260868073, loss=0.021985404193401337
I0305 19:26:04.254727 140415790769920 logging_writer.py:48] [67400] global_step=67400, grad_norm=0.13663139939308167, loss=0.02048412710428238
I0305 19:26:36.110846 140408968165120 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.14735788106918335, loss=0.019122745841741562
I0305 19:27:08.234512 140415790769920 logging_writer.py:48] [67600] global_step=67600, grad_norm=0.1302657276391983, loss=0.019179582595825195
I0305 19:27:39.906955 140408968165120 logging_writer.py:48] [67700] global_step=67700, grad_norm=0.1221739798784256, loss=0.019521281123161316
I0305 19:27:55.016495 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:29:47.134722 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:29:50.159415 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:29:53.110566 140576608098112 submission_runner.py:411] Time since start: 33883.82s, 	Step: 67749, 	{'train/accuracy': 0.994721531867981, 'train/loss': 0.016378182917833328, 'train/mean_average_precision': 0.7252492006625263, 'validation/accuracy': 0.9870800971984863, 'validation/loss': 0.04932945966720581, 'validation/mean_average_precision': 0.2926122424504049, 'validation/num_examples': 43793, 'test/accuracy': 0.986127495765686, 'test/loss': 0.05317869782447815, 'test/mean_average_precision': 0.2718304619033597, 'test/num_examples': 43793, 'score': 21867.74240756035, 'total_duration': 33883.81890010834, 'accumulated_submission_time': 21867.74240756035, 'accumulated_eval_time': 12011.118778467178, 'accumulated_logging_time': 3.105543851852417}
I0305 19:29:53.139094 140407882708736 logging_writer.py:48] [67749] accumulated_eval_time=12011.118778, accumulated_logging_time=3.105544, accumulated_submission_time=21867.742408, global_step=67749, preemption_count=0, score=21867.742408, test/accuracy=0.986127, test/loss=0.053179, test/mean_average_precision=0.271830, test/num_examples=43793, total_duration=33883.818900, train/accuracy=0.994722, train/loss=0.016378, train/mean_average_precision=0.725249, validation/accuracy=0.987080, validation/loss=0.049329, validation/mean_average_precision=0.292612, validation/num_examples=43793
I0305 19:30:09.785891 140409291962112 logging_writer.py:48] [67800] global_step=67800, grad_norm=0.1528802514076233, loss=0.02085202746093273
I0305 19:30:41.352334 140407882708736 logging_writer.py:48] [67900] global_step=67900, grad_norm=0.13841955363750458, loss=0.019890472292900085
I0305 19:31:13.673017 140409291962112 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.1304413229227066, loss=0.01941445656120777
I0305 19:31:45.673241 140407882708736 logging_writer.py:48] [68100] global_step=68100, grad_norm=0.13254879415035248, loss=0.0177457332611084
I0305 19:32:18.290995 140409291962112 logging_writer.py:48] [68200] global_step=68200, grad_norm=0.1426764875650406, loss=0.020922334864735603
I0305 19:32:51.184356 140407882708736 logging_writer.py:48] [68300] global_step=68300, grad_norm=0.15125545859336853, loss=0.021744627505540848
I0305 19:33:24.543716 140409291962112 logging_writer.py:48] [68400] global_step=68400, grad_norm=0.12987744808197021, loss=0.018791520968079567
I0305 19:33:53.251150 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:35:54.336062 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:35:57.744426 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:36:01.068523 140576608098112 submission_runner.py:411] Time since start: 34251.78s, 	Step: 68488, 	{'train/accuracy': 0.994666337966919, 'train/loss': 0.01639249548316002, 'train/mean_average_precision': 0.7239857813140791, 'validation/accuracy': 0.9870427250862122, 'validation/loss': 0.0495910719037056, 'validation/mean_average_precision': 0.29098764881197, 'validation/num_examples': 43793, 'test/accuracy': 0.9861001372337341, 'test/loss': 0.05354296416044235, 'test/mean_average_precision': 0.27301447251424704, 'test/num_examples': 43793, 'score': 22107.820801496506, 'total_duration': 34251.776836156845, 'accumulated_submission_time': 22107.820801496506, 'accumulated_eval_time': 12138.936106681824, 'accumulated_logging_time': 3.1450343132019043}
I0305 19:36:01.100770 140408968165120 logging_writer.py:48] [68488] accumulated_eval_time=12138.936107, accumulated_logging_time=3.145034, accumulated_submission_time=22107.820801, global_step=68488, preemption_count=0, score=22107.820801, test/accuracy=0.986100, test/loss=0.053543, test/mean_average_precision=0.273014, test/num_examples=43793, total_duration=34251.776836, train/accuracy=0.994666, train/loss=0.016392, train/mean_average_precision=0.723986, validation/accuracy=0.987043, validation/loss=0.049591, validation/mean_average_precision=0.290988, validation/num_examples=43793
I0305 19:36:05.573252 140415790769920 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.12954393029212952, loss=0.016791297122836113
I0305 19:36:38.478285 140408968165120 logging_writer.py:48] [68600] global_step=68600, grad_norm=0.1438186913728714, loss=0.01856049709022045
I0305 19:37:11.209608 140415790769920 logging_writer.py:48] [68700] global_step=68700, grad_norm=0.14913809299468994, loss=0.021690333262085915
I0305 19:37:43.699510 140408968165120 logging_writer.py:48] [68800] global_step=68800, grad_norm=0.1274283081293106, loss=0.018924664705991745
I0305 19:38:16.404304 140415790769920 logging_writer.py:48] [68900] global_step=68900, grad_norm=0.13080863654613495, loss=0.018434559926390648
I0305 19:38:49.037350 140408968165120 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.13104738295078278, loss=0.02021411806344986
I0305 19:39:21.578872 140415790769920 logging_writer.py:48] [69100] global_step=69100, grad_norm=0.1453419178724289, loss=0.021527763456106186
I0305 19:39:53.366046 140408968165120 logging_writer.py:48] [69200] global_step=69200, grad_norm=0.12356302887201309, loss=0.01935799978673458
I0305 19:40:01.086750 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:41:54.435471 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:41:57.479586 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:42:00.447979 140576608098112 submission_runner.py:411] Time since start: 34611.16s, 	Step: 69225, 	{'train/accuracy': 0.994940996170044, 'train/loss': 0.01571442186832428, 'train/mean_average_precision': 0.7364670876265623, 'validation/accuracy': 0.986950159072876, 'validation/loss': 0.04924878850579262, 'validation/mean_average_precision': 0.2891234404148812, 'validation/num_examples': 43793, 'test/accuracy': 0.9860045313835144, 'test/loss': 0.05306505784392357, 'test/mean_average_precision': 0.27306194421612656, 'test/num_examples': 43793, 'score': 22347.77027964592, 'total_duration': 34611.156307697296, 'accumulated_submission_time': 22347.77027964592, 'accumulated_eval_time': 12258.297290086746, 'accumulated_logging_time': 3.189326524734497}
I0305 19:42:00.477245 140407882708736 logging_writer.py:48] [69225] accumulated_eval_time=12258.297290, accumulated_logging_time=3.189327, accumulated_submission_time=22347.770280, global_step=69225, preemption_count=0, score=22347.770280, test/accuracy=0.986005, test/loss=0.053065, test/mean_average_precision=0.273062, test/num_examples=43793, total_duration=34611.156308, train/accuracy=0.994941, train/loss=0.015714, train/mean_average_precision=0.736467, validation/accuracy=0.986950, validation/loss=0.049249, validation/mean_average_precision=0.289123, validation/num_examples=43793
I0305 19:42:25.377479 140415782377216 logging_writer.py:48] [69300] global_step=69300, grad_norm=0.15214399993419647, loss=0.018386131152510643
I0305 19:42:57.918406 140407882708736 logging_writer.py:48] [69400] global_step=69400, grad_norm=0.14366386830806732, loss=0.021343747153878212
I0305 19:43:30.357424 140415782377216 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.1253308355808258, loss=0.018735703080892563
I0305 19:44:03.228377 140407882708736 logging_writer.py:48] [69600] global_step=69600, grad_norm=0.14351382851600647, loss=0.02041522040963173
I0305 19:44:36.057962 140415782377216 logging_writer.py:48] [69700] global_step=69700, grad_norm=0.12117692083120346, loss=0.018888898193836212
I0305 19:45:08.485161 140407882708736 logging_writer.py:48] [69800] global_step=69800, grad_norm=0.13567790389060974, loss=0.018491234630346298
I0305 19:45:40.423968 140415782377216 logging_writer.py:48] [69900] global_step=69900, grad_norm=0.13741113245487213, loss=0.019081812351942062
I0305 19:46:00.489120 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:47:48.497416 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:47:51.562751 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:47:54.598096 140576608098112 submission_runner.py:411] Time since start: 34965.31s, 	Step: 69963, 	{'train/accuracy': 0.9950496554374695, 'train/loss': 0.015386695973575115, 'train/mean_average_precision': 0.7498461206729499, 'validation/accuracy': 0.9870471954345703, 'validation/loss': 0.049656644463539124, 'validation/mean_average_precision': 0.2911408890764547, 'validation/num_examples': 43793, 'test/accuracy': 0.986139714717865, 'test/loss': 0.05341222137212753, 'test/mean_average_precision': 0.27209897821367673, 'test/num_examples': 43793, 'score': 22587.750163316727, 'total_duration': 34965.306415081024, 'accumulated_submission_time': 22587.750163316727, 'accumulated_eval_time': 12372.406210422516, 'accumulated_logging_time': 3.2301290035247803}
I0305 19:47:54.627072 140409291962112 logging_writer.py:48] [69963] accumulated_eval_time=12372.406210, accumulated_logging_time=3.230129, accumulated_submission_time=22587.750163, global_step=69963, preemption_count=0, score=22587.750163, test/accuracy=0.986140, test/loss=0.053412, test/mean_average_precision=0.272099, test/num_examples=43793, total_duration=34965.306415, train/accuracy=0.995050, train/loss=0.015387, train/mean_average_precision=0.749846, validation/accuracy=0.987047, validation/loss=0.049657, validation/mean_average_precision=0.291141, validation/num_examples=43793
I0305 19:48:06.866866 140415790769920 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.14530223608016968, loss=0.01987239345908165
I0305 19:48:38.689335 140409291962112 logging_writer.py:48] [70100] global_step=70100, grad_norm=0.14747045934200287, loss=0.01911582238972187
I0305 19:49:10.235218 140415790769920 logging_writer.py:48] [70200] global_step=70200, grad_norm=0.1330636888742447, loss=0.01721746288239956
I0305 19:49:42.201744 140409291962112 logging_writer.py:48] [70300] global_step=70300, grad_norm=0.13368137180805206, loss=0.02035008929669857
I0305 19:50:15.632346 140415790769920 logging_writer.py:48] [70400] global_step=70400, grad_norm=0.1269979327917099, loss=0.018928105011582375
I0305 19:50:47.339591 140409291962112 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.13732868432998657, loss=0.018781105056405067
I0305 19:51:19.382154 140415790769920 logging_writer.py:48] [70600] global_step=70600, grad_norm=0.14754031598567963, loss=0.020935751497745514
I0305 19:51:51.506479 140409291962112 logging_writer.py:48] [70700] global_step=70700, grad_norm=0.1326294243335724, loss=0.016924001276493073
I0305 19:51:54.687823 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:53:50.144341 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:53:53.207530 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:53:56.199527 140576608098112 submission_runner.py:411] Time since start: 35326.91s, 	Step: 70711, 	{'train/accuracy': 0.9950863122940063, 'train/loss': 0.01529749296605587, 'train/mean_average_precision': 0.7440555332907148, 'validation/accuracy': 0.9869948625564575, 'validation/loss': 0.049831464886665344, 'validation/mean_average_precision': 0.290006136839753, 'validation/num_examples': 43793, 'test/accuracy': 0.98613041639328, 'test/loss': 0.053623318672180176, 'test/mean_average_precision': 0.27146087923047724, 'test/num_examples': 43793, 'score': 22827.77950668335, 'total_duration': 35326.907858133316, 'accumulated_submission_time': 22827.77950668335, 'accumulated_eval_time': 12493.917872667313, 'accumulated_logging_time': 3.2706449031829834}
I0305 19:53:56.228136 140407882708736 logging_writer.py:48] [70711] accumulated_eval_time=12493.917873, accumulated_logging_time=3.270645, accumulated_submission_time=22827.779507, global_step=70711, preemption_count=0, score=22827.779507, test/accuracy=0.986130, test/loss=0.053623, test/mean_average_precision=0.271461, test/num_examples=43793, total_duration=35326.907858, train/accuracy=0.995086, train/loss=0.015297, train/mean_average_precision=0.744056, validation/accuracy=0.986995, validation/loss=0.049831, validation/mean_average_precision=0.290006, validation/num_examples=43793
I0305 19:54:25.545727 140415782377216 logging_writer.py:48] [70800] global_step=70800, grad_norm=0.14540258049964905, loss=0.020560434088110924
I0305 19:54:57.888104 140407882708736 logging_writer.py:48] [70900] global_step=70900, grad_norm=0.13525351881980896, loss=0.018065644428133965
I0305 19:55:30.163177 140415782377216 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.13695895671844482, loss=0.01840689778327942
I0305 19:56:01.917223 140407882708736 logging_writer.py:48] [71100] global_step=71100, grad_norm=0.1334296613931656, loss=0.016286276280879974
I0305 19:56:34.074186 140415782377216 logging_writer.py:48] [71200] global_step=71200, grad_norm=0.14279279112815857, loss=0.018886949867010117
I0305 19:57:07.628708 140407882708736 logging_writer.py:48] [71300] global_step=71300, grad_norm=0.13694120943546295, loss=0.019657982513308525
I0305 19:57:39.144448 140415782377216 logging_writer.py:48] [71400] global_step=71400, grad_norm=0.12624599039554596, loss=0.01709812879562378
I0305 19:57:56.230396 140576608098112 spec.py:321] Evaluating on the training split.
I0305 19:59:45.955834 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 19:59:49.020926 140576608098112 spec.py:349] Evaluating on the test split.
I0305 19:59:52.037729 140576608098112 submission_runner.py:411] Time since start: 35682.75s, 	Step: 71455, 	{'train/accuracy': 0.9954333305358887, 'train/loss': 0.014372694306075573, 'train/mean_average_precision': 0.7739653157648199, 'validation/accuracy': 0.9869473576545715, 'validation/loss': 0.04986492544412613, 'validation/mean_average_precision': 0.2913538274236733, 'validation/num_examples': 43793, 'test/accuracy': 0.9860959053039551, 'test/loss': 0.053541623055934906, 'test/mean_average_precision': 0.27289775467168986, 'test/num_examples': 43793, 'score': 23067.749517202377, 'total_duration': 35682.74603819847, 'accumulated_submission_time': 23067.749517202377, 'accumulated_eval_time': 12609.725140094757, 'accumulated_logging_time': 3.3116016387939453}
I0305 19:59:52.067505 140408968165120 logging_writer.py:48] [71455] accumulated_eval_time=12609.725140, accumulated_logging_time=3.311602, accumulated_submission_time=23067.749517, global_step=71455, preemption_count=0, score=23067.749517, test/accuracy=0.986096, test/loss=0.053542, test/mean_average_precision=0.272898, test/num_examples=43793, total_duration=35682.746038, train/accuracy=0.995433, train/loss=0.014373, train/mean_average_precision=0.773965, validation/accuracy=0.986947, validation/loss=0.049865, validation/mean_average_precision=0.291354, validation/num_examples=43793
I0305 20:00:07.000583 140415790769920 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.13639989495277405, loss=0.01633637025952339
I0305 20:00:38.832932 140408968165120 logging_writer.py:48] [71600] global_step=71600, grad_norm=0.13560134172439575, loss=0.018311757594347
I0305 20:01:10.671238 140415790769920 logging_writer.py:48] [71700] global_step=71700, grad_norm=0.1428089588880539, loss=0.020970873534679413
I0305 20:01:42.109796 140408968165120 logging_writer.py:48] [71800] global_step=71800, grad_norm=0.14512000977993011, loss=0.018384914845228195
I0305 20:02:14.161204 140415790769920 logging_writer.py:48] [71900] global_step=71900, grad_norm=0.1579158753156662, loss=0.01909453608095646
I0305 20:02:46.460434 140408968165120 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.14952443540096283, loss=0.01884840801358223
I0305 20:03:18.809333 140415790769920 logging_writer.py:48] [72100] global_step=72100, grad_norm=0.15920239686965942, loss=0.018368983641266823
I0305 20:03:50.406598 140408968165120 logging_writer.py:48] [72200] global_step=72200, grad_norm=0.13633432984352112, loss=0.019172046333551407
I0305 20:03:52.319183 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:05:43.006500 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:05:46.090619 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:05:49.069588 140576608098112 submission_runner.py:411] Time since start: 36039.78s, 	Step: 72207, 	{'train/accuracy': 0.9954251646995544, 'train/loss': 0.01436374057084322, 'train/mean_average_precision': 0.768106455239543, 'validation/accuracy': 0.9870321750640869, 'validation/loss': 0.049958277493715286, 'validation/mean_average_precision': 0.29056416570202737, 'validation/num_examples': 43793, 'test/accuracy': 0.9861700534820557, 'test/loss': 0.05378158017992973, 'test/mean_average_precision': 0.2736166316268893, 'test/num_examples': 43793, 'score': 23307.969571113586, 'total_duration': 36039.77791452408, 'accumulated_submission_time': 23307.969571113586, 'accumulated_eval_time': 12726.47549533844, 'accumulated_logging_time': 3.352621078491211}
I0305 20:05:49.098610 140409291962112 logging_writer.py:48] [72207] accumulated_eval_time=12726.475495, accumulated_logging_time=3.352621, accumulated_submission_time=23307.969571, global_step=72207, preemption_count=0, score=23307.969571, test/accuracy=0.986170, test/loss=0.053782, test/mean_average_precision=0.273617, test/num_examples=43793, total_duration=36039.777915, train/accuracy=0.995425, train/loss=0.014364, train/mean_average_precision=0.768106, validation/accuracy=0.987032, validation/loss=0.049958, validation/mean_average_precision=0.290564, validation/num_examples=43793
I0305 20:06:19.423857 140415782377216 logging_writer.py:48] [72300] global_step=72300, grad_norm=0.1366538107395172, loss=0.01846303977072239
I0305 20:06:51.209913 140409291962112 logging_writer.py:48] [72400] global_step=72400, grad_norm=0.1337861716747284, loss=0.017352644354104996
I0305 20:07:23.185773 140415782377216 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.13120713829994202, loss=0.017456820234656334
I0305 20:07:54.985271 140409291962112 logging_writer.py:48] [72600] global_step=72600, grad_norm=0.1408778876066208, loss=0.019089410081505775
I0305 20:08:26.784862 140415782377216 logging_writer.py:48] [72700] global_step=72700, grad_norm=0.15369056165218353, loss=0.020221494138240814
I0305 20:08:58.370311 140409291962112 logging_writer.py:48] [72800] global_step=72800, grad_norm=0.13732637465000153, loss=0.0181136392056942
I0305 20:09:30.526355 140415782377216 logging_writer.py:48] [72900] global_step=72900, grad_norm=0.15749970078468323, loss=0.020953794941306114
I0305 20:09:49.312211 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:11:40.676377 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:11:43.670182 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:11:46.614237 140576608098112 submission_runner.py:411] Time since start: 36397.32s, 	Step: 72960, 	{'train/accuracy': 0.9953681230545044, 'train/loss': 0.014467169530689716, 'train/mean_average_precision': 0.7619073822725733, 'validation/accuracy': 0.9870269298553467, 'validation/loss': 0.050065625458955765, 'validation/mean_average_precision': 0.2910683527730367, 'validation/num_examples': 43793, 'test/accuracy': 0.9861670732498169, 'test/loss': 0.05387284979224205, 'test/mean_average_precision': 0.27333937156198485, 'test/num_examples': 43793, 'score': 23548.14874601364, 'total_duration': 36397.322568655014, 'accumulated_submission_time': 23548.14874601364, 'accumulated_eval_time': 12843.777476787567, 'accumulated_logging_time': 3.3956403732299805}
I0305 20:11:46.643273 140407882708736 logging_writer.py:48] [72960] accumulated_eval_time=12843.777477, accumulated_logging_time=3.395640, accumulated_submission_time=23548.148746, global_step=72960, preemption_count=0, score=23548.148746, test/accuracy=0.986167, test/loss=0.053873, test/mean_average_precision=0.273339, test/num_examples=43793, total_duration=36397.322569, train/accuracy=0.995368, train/loss=0.014467, train/mean_average_precision=0.761907, validation/accuracy=0.987027, validation/loss=0.050066, validation/mean_average_precision=0.291068, validation/num_examples=43793
I0305 20:11:59.766075 140415790769920 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.14768894016742706, loss=0.016911765560507774
I0305 20:12:31.874241 140407882708736 logging_writer.py:48] [73100] global_step=73100, grad_norm=0.14315035939216614, loss=0.01887398213148117
I0305 20:13:04.114080 140415790769920 logging_writer.py:48] [73200] global_step=73200, grad_norm=0.1659187525510788, loss=0.01952776126563549
I0305 20:13:36.152086 140407882708736 logging_writer.py:48] [73300] global_step=73300, grad_norm=0.14032799005508423, loss=0.019486086443066597
I0305 20:14:08.183658 140415790769920 logging_writer.py:48] [73400] global_step=73400, grad_norm=0.14082111418247223, loss=0.017752498388290405
I0305 20:14:40.263972 140407882708736 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.13321875035762787, loss=0.01753149926662445
I0305 20:15:12.964421 140415790769920 logging_writer.py:48] [73600] global_step=73600, grad_norm=0.1401803195476532, loss=0.01774761453270912
I0305 20:15:45.833795 140407882708736 logging_writer.py:48] [73700] global_step=73700, grad_norm=0.12689776718616486, loss=0.016280369833111763
I0305 20:15:46.822458 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:17:36.927802 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:17:40.000512 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:17:42.989366 140576608098112 submission_runner.py:411] Time since start: 36753.70s, 	Step: 73704, 	{'train/accuracy': 0.9953656792640686, 'train/loss': 0.01451911311596632, 'train/mean_average_precision': 0.7680448154889454, 'validation/accuracy': 0.9869895577430725, 'validation/loss': 0.05006621778011322, 'validation/mean_average_precision': 0.28909997769531537, 'validation/num_examples': 43793, 'test/accuracy': 0.9861195087432861, 'test/loss': 0.05397979915142059, 'test/mean_average_precision': 0.27239434284603453, 'test/num_examples': 43793, 'score': 23788.296051740646, 'total_duration': 36753.69769072533, 'accumulated_submission_time': 23788.296051740646, 'accumulated_eval_time': 12959.944328784943, 'accumulated_logging_time': 3.435800552368164}
I0305 20:17:43.025105 140408968165120 logging_writer.py:48] [73704] accumulated_eval_time=12959.944329, accumulated_logging_time=3.435801, accumulated_submission_time=23788.296052, global_step=73704, preemption_count=0, score=23788.296052, test/accuracy=0.986120, test/loss=0.053980, test/mean_average_precision=0.272394, test/num_examples=43793, total_duration=36753.697691, train/accuracy=0.995366, train/loss=0.014519, train/mean_average_precision=0.768045, validation/accuracy=0.986990, validation/loss=0.050066, validation/mean_average_precision=0.289100, validation/num_examples=43793
I0305 20:18:14.834254 140415782377216 logging_writer.py:48] [73800] global_step=73800, grad_norm=0.12777894735336304, loss=0.01756112277507782
I0305 20:18:47.538428 140408968165120 logging_writer.py:48] [73900] global_step=73900, grad_norm=0.14224429428577423, loss=0.017401719465851784
I0305 20:19:20.343709 140415782377216 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.15767879784107208, loss=0.01924222894012928
I0305 20:19:52.862198 140408968165120 logging_writer.py:48] [74100] global_step=74100, grad_norm=0.1347828358411789, loss=0.018711920827627182
I0305 20:20:24.946344 140415782377216 logging_writer.py:48] [74200] global_step=74200, grad_norm=0.13830596208572388, loss=0.017530975863337517
I0305 20:20:57.288599 140408968165120 logging_writer.py:48] [74300] global_step=74300, grad_norm=0.13169942796230316, loss=0.01750335283577442
I0305 20:21:29.937520 140415782377216 logging_writer.py:48] [74400] global_step=74400, grad_norm=0.1432238668203354, loss=0.016321828588843346
I0305 20:21:43.058988 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:23:30.157439 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:23:33.269321 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:23:36.212645 140576608098112 submission_runner.py:411] Time since start: 37106.92s, 	Step: 74442, 	{'train/accuracy': 0.99525386095047, 'train/loss': 0.0147152254357934, 'train/mean_average_precision': 0.7530949013829958, 'validation/accuracy': 0.987002968788147, 'validation/loss': 0.05019928142428398, 'validation/mean_average_precision': 0.29143779943241316, 'validation/num_examples': 43793, 'test/accuracy': 0.9861443638801575, 'test/loss': 0.054072391241788864, 'test/mean_average_precision': 0.27347169809501726, 'test/num_examples': 43793, 'score': 24028.297236442566, 'total_duration': 37106.92096066475, 'accumulated_submission_time': 24028.297236442566, 'accumulated_eval_time': 13073.09792804718, 'accumulated_logging_time': 3.483970880508423}
I0305 20:23:36.242544 140407882708736 logging_writer.py:48] [74442] accumulated_eval_time=13073.097928, accumulated_logging_time=3.483971, accumulated_submission_time=24028.297236, global_step=74442, preemption_count=0, score=24028.297236, test/accuracy=0.986144, test/loss=0.054072, test/mean_average_precision=0.273472, test/num_examples=43793, total_duration=37106.920961, train/accuracy=0.995254, train/loss=0.014715, train/mean_average_precision=0.753095, validation/accuracy=0.987003, validation/loss=0.050199, validation/mean_average_precision=0.291438, validation/num_examples=43793
I0305 20:23:55.058250 140415790769920 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.14070095121860504, loss=0.018308700993657112
I0305 20:24:26.997879 140407882708736 logging_writer.py:48] [74600] global_step=74600, grad_norm=0.14131852984428406, loss=0.018456177785992622
I0305 20:24:59.368304 140415790769920 logging_writer.py:48] [74700] global_step=74700, grad_norm=0.13720130920410156, loss=0.01740202121436596
I0305 20:25:31.185019 140407882708736 logging_writer.py:48] [74800] global_step=74800, grad_norm=0.1569218635559082, loss=0.018761400133371353
I0305 20:26:03.345329 140415790769920 logging_writer.py:48] [74900] global_step=74900, grad_norm=0.13655191659927368, loss=0.019285017624497414
I0305 20:26:35.257441 140407882708736 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.12904570996761322, loss=0.01800028420984745
I0305 20:27:07.230487 140415790769920 logging_writer.py:48] [75100] global_step=75100, grad_norm=0.1458842009305954, loss=0.019190948456525803
I0305 20:27:36.249731 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:29:28.849837 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:29:31.899461 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:29:34.918338 140576608098112 submission_runner.py:411] Time since start: 37465.63s, 	Step: 75192, 	{'train/accuracy': 0.9953786730766296, 'train/loss': 0.014435878954827785, 'train/mean_average_precision': 0.771100602272772, 'validation/accuracy': 0.9869931936264038, 'validation/loss': 0.05010157451033592, 'validation/mean_average_precision': 0.2919096212544834, 'validation/num_examples': 43793, 'test/accuracy': 0.9861211776733398, 'test/loss': 0.05391651391983032, 'test/mean_average_precision': 0.2740543698815366, 'test/num_examples': 43793, 'score': 24268.272846221924, 'total_duration': 37465.626670598984, 'accumulated_submission_time': 24268.272846221924, 'accumulated_eval_time': 13191.766512155533, 'accumulated_logging_time': 3.5248465538024902}
I0305 20:29:34.948082 140408968165120 logging_writer.py:48] [75192] accumulated_eval_time=13191.766512, accumulated_logging_time=3.524847, accumulated_submission_time=24268.272846, global_step=75192, preemption_count=0, score=24268.272846, test/accuracy=0.986121, test/loss=0.053917, test/mean_average_precision=0.274054, test/num_examples=43793, total_duration=37465.626671, train/accuracy=0.995379, train/loss=0.014436, train/mean_average_precision=0.771101, validation/accuracy=0.986993, validation/loss=0.050102, validation/mean_average_precision=0.291910, validation/num_examples=43793
I0305 20:29:37.850583 140415782377216 logging_writer.py:48] [75200] global_step=75200, grad_norm=0.1493007391691208, loss=0.0189591683447361
I0305 20:30:10.062024 140408968165120 logging_writer.py:48] [75300] global_step=75300, grad_norm=0.14083798229694366, loss=0.017069095745682716
I0305 20:30:42.277829 140415782377216 logging_writer.py:48] [75400] global_step=75400, grad_norm=0.14285224676132202, loss=0.018123716115951538
I0305 20:31:14.523771 140408968165120 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.15296122431755066, loss=0.018539370968937874
I0305 20:31:46.422947 140415782377216 logging_writer.py:48] [75600] global_step=75600, grad_norm=0.15584759414196014, loss=0.019499648362398148
I0305 20:32:18.642254 140408968165120 logging_writer.py:48] [75700] global_step=75700, grad_norm=0.1486167311668396, loss=0.01733073964715004
I0305 20:32:50.563645 140415782377216 logging_writer.py:48] [75800] global_step=75800, grad_norm=0.14104793965816498, loss=0.02028803899884224
I0305 20:33:22.571777 140408968165120 logging_writer.py:48] [75900] global_step=75900, grad_norm=0.14019766449928284, loss=0.019160296767950058
I0305 20:33:35.018668 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:35:24.565367 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:35:27.639702 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:35:30.674727 140576608098112 submission_runner.py:411] Time since start: 37821.38s, 	Step: 75940, 	{'train/accuracy': 0.9953892230987549, 'train/loss': 0.014392606914043427, 'train/mean_average_precision': 0.7691021492142511, 'validation/accuracy': 0.9870362281799316, 'validation/loss': 0.05010756850242615, 'validation/mean_average_precision': 0.29277855600035246, 'validation/num_examples': 43793, 'test/accuracy': 0.9861279129981995, 'test/loss': 0.053962018340826035, 'test/mean_average_precision': 0.2749305524965749, 'test/num_examples': 43793, 'score': 24508.311578273773, 'total_duration': 37821.382946014404, 'accumulated_submission_time': 24508.311578273773, 'accumulated_eval_time': 13307.42241358757, 'accumulated_logging_time': 3.565934658050537}
I0305 20:35:30.704543 140409291962112 logging_writer.py:48] [75940] accumulated_eval_time=13307.422414, accumulated_logging_time=3.565935, accumulated_submission_time=24508.311578, global_step=75940, preemption_count=0, score=24508.311578, test/accuracy=0.986128, test/loss=0.053962, test/mean_average_precision=0.274931, test/num_examples=43793, total_duration=37821.382946, train/accuracy=0.995389, train/loss=0.014393, train/mean_average_precision=0.769102, validation/accuracy=0.987036, validation/loss=0.050108, validation/mean_average_precision=0.292779, validation/num_examples=43793
I0305 20:35:50.147488 140415790769920 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.15471822023391724, loss=0.01932659186422825
I0305 20:36:22.880897 140409291962112 logging_writer.py:48] [76100] global_step=76100, grad_norm=0.13398036360740662, loss=0.01896132528781891
I0305 20:36:55.322545 140415790769920 logging_writer.py:48] [76200] global_step=76200, grad_norm=0.1673068106174469, loss=0.020730413496494293
I0305 20:37:27.690078 140409291962112 logging_writer.py:48] [76300] global_step=76300, grad_norm=0.14244121313095093, loss=0.017672162503004074
I0305 20:37:59.466547 140415790769920 logging_writer.py:48] [76400] global_step=76400, grad_norm=0.1550256311893463, loss=0.019802557304501534
I0305 20:38:31.792646 140409291962112 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.11834795773029327, loss=0.016810951754450798
I0305 20:39:03.803722 140415790769920 logging_writer.py:48] [76600] global_step=76600, grad_norm=0.14262960851192474, loss=0.019446896389126778
I0305 20:39:30.987475 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:41:21.368218 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:41:24.816597 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:41:28.130425 140576608098112 submission_runner.py:411] Time since start: 38178.84s, 	Step: 76686, 	{'train/accuracy': 0.9954409003257751, 'train/loss': 0.014349482953548431, 'train/mean_average_precision': 0.7656371137133122, 'validation/accuracy': 0.9870646595954895, 'validation/loss': 0.05016282573342323, 'validation/mean_average_precision': 0.2936275545673407, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.05401529744267464, 'test/mean_average_precision': 0.2739643168130566, 'test/num_examples': 43793, 'score': 24748.563071012497, 'total_duration': 38178.83873414993, 'accumulated_submission_time': 24748.563071012497, 'accumulated_eval_time': 13424.565298557281, 'accumulated_logging_time': 3.6069235801696777}
I0305 20:41:28.166254 140408968165120 logging_writer.py:48] [76686] accumulated_eval_time=13424.565299, accumulated_logging_time=3.606924, accumulated_submission_time=24748.563071, global_step=76686, preemption_count=0, score=24748.563071, test/accuracy=0.986145, test/loss=0.054015, test/mean_average_precision=0.273964, test/num_examples=43793, total_duration=38178.838734, train/accuracy=0.995441, train/loss=0.014349, train/mean_average_precision=0.765637, validation/accuracy=0.987065, validation/loss=0.050163, validation/mean_average_precision=0.293628, validation/num_examples=43793
I0305 20:41:33.175378 140415782377216 logging_writer.py:48] [76700] global_step=76700, grad_norm=0.13962624967098236, loss=0.01719295233488083
I0305 20:42:06.746642 140408968165120 logging_writer.py:48] [76800] global_step=76800, grad_norm=0.1366252452135086, loss=0.018197814002633095
I0305 20:42:39.092196 140415782377216 logging_writer.py:48] [76900] global_step=76900, grad_norm=0.13070395588874817, loss=0.01791752129793167
I0305 20:43:11.203253 140408968165120 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.15556102991104126, loss=0.020139550790190697
I0305 20:43:43.383900 140415782377216 logging_writer.py:48] [77100] global_step=77100, grad_norm=0.1404624879360199, loss=0.018437130376696587
I0305 20:44:16.206708 140408968165120 logging_writer.py:48] [77200] global_step=77200, grad_norm=0.14249032735824585, loss=0.01704828068614006
I0305 20:44:48.432810 140415782377216 logging_writer.py:48] [77300] global_step=77300, grad_norm=0.14020955562591553, loss=0.016938917338848114
I0305 20:45:21.176290 140408968165120 logging_writer.py:48] [77400] global_step=77400, grad_norm=0.14083914458751678, loss=0.017562028020620346
I0305 20:45:28.321862 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:47:19.038582 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:47:22.479822 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:47:25.868535 140576608098112 submission_runner.py:411] Time since start: 38536.58s, 	Step: 77423, 	{'train/accuracy': 0.9954899549484253, 'train/loss': 0.01416554395109415, 'train/mean_average_precision': 0.7759545638285559, 'validation/accuracy': 0.9870675206184387, 'validation/loss': 0.050172798335552216, 'validation/mean_average_precision': 0.29259052351820486, 'validation/num_examples': 43793, 'test/accuracy': 0.9861460328102112, 'test/loss': 0.0540119931101799, 'test/mean_average_precision': 0.27386142732469715, 'test/num_examples': 43793, 'score': 24988.685559511185, 'total_duration': 38536.57685160637, 'accumulated_submission_time': 24988.685559511185, 'accumulated_eval_time': 13542.111910581589, 'accumulated_logging_time': 3.654918909072876}
I0305 20:47:25.901997 140407882708736 logging_writer.py:48] [77423] accumulated_eval_time=13542.111911, accumulated_logging_time=3.654919, accumulated_submission_time=24988.685560, global_step=77423, preemption_count=0, score=24988.685560, test/accuracy=0.986146, test/loss=0.054012, test/mean_average_precision=0.273861, test/num_examples=43793, total_duration=38536.576852, train/accuracy=0.995490, train/loss=0.014166, train/mean_average_precision=0.775955, validation/accuracy=0.987068, validation/loss=0.050173, validation/mean_average_precision=0.292591, validation/num_examples=43793
I0305 20:47:51.579430 140415790769920 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.14456501603126526, loss=0.01882554218173027
I0305 20:48:23.571699 140407882708736 logging_writer.py:48] [77600] global_step=77600, grad_norm=0.13812927901744843, loss=0.015650713816285133
I0305 20:48:55.444891 140415790769920 logging_writer.py:48] [77700] global_step=77700, grad_norm=0.1415255069732666, loss=0.018062762916088104
I0305 20:49:27.553028 140407882708736 logging_writer.py:48] [77800] global_step=77800, grad_norm=0.13383245468139648, loss=0.016953974962234497
I0305 20:49:59.461795 140415790769920 logging_writer.py:48] [77900] global_step=77900, grad_norm=0.1408194750547409, loss=0.01702740043401718
I0305 20:50:31.799496 140407882708736 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.14173264801502228, loss=0.01667444407939911
I0305 20:51:04.219613 140415790769920 logging_writer.py:48] [78100] global_step=78100, grad_norm=0.13426317274570465, loss=0.01809248887002468
I0305 20:51:26.072557 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:53:14.313058 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:53:17.374822 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:53:20.340312 140576608098112 submission_runner.py:411] Time since start: 38891.05s, 	Step: 78169, 	{'train/accuracy': 0.9955411553382874, 'train/loss': 0.013990039005875587, 'train/mean_average_precision': 0.7764028795076405, 'validation/accuracy': 0.9870642423629761, 'validation/loss': 0.050121013075113297, 'validation/mean_average_precision': 0.2925099990148256, 'validation/num_examples': 43793, 'test/accuracy': 0.9861548542976379, 'test/loss': 0.05397595837712288, 'test/mean_average_precision': 0.27420196319591933, 'test/num_examples': 43793, 'score': 25228.494074106216, 'total_duration': 38891.048644304276, 'accumulated_submission_time': 25228.494074106216, 'accumulated_eval_time': 13656.379624128342, 'accumulated_logging_time': 4.030130386352539}
I0305 20:53:20.370578 140408968165120 logging_writer.py:48] [78169] accumulated_eval_time=13656.379624, accumulated_logging_time=4.030130, accumulated_submission_time=25228.494074, global_step=78169, preemption_count=0, score=25228.494074, test/accuracy=0.986155, test/loss=0.053976, test/mean_average_precision=0.274202, test/num_examples=43793, total_duration=38891.048644, train/accuracy=0.995541, train/loss=0.013990, train/mean_average_precision=0.776403, validation/accuracy=0.987064, validation/loss=0.050121, validation/mean_average_precision=0.292510, validation/num_examples=43793
I0305 20:53:30.658082 140415782377216 logging_writer.py:48] [78200] global_step=78200, grad_norm=0.14038436114788055, loss=0.01697569154202938
I0305 20:54:02.835632 140408968165120 logging_writer.py:48] [78300] global_step=78300, grad_norm=0.15115685760974884, loss=0.02020573429763317
I0305 20:54:34.920943 140415782377216 logging_writer.py:48] [78400] global_step=78400, grad_norm=0.1326943337917328, loss=0.01813868246972561
I0305 20:55:07.178259 140408968165120 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.14862386882305145, loss=0.02013232372701168
I0305 20:55:39.379166 140415782377216 logging_writer.py:48] [78600] global_step=78600, grad_norm=0.13927744328975677, loss=0.018405551090836525
I0305 20:56:11.963142 140408968165120 logging_writer.py:48] [78700] global_step=78700, grad_norm=0.1390637904405594, loss=0.017683671787381172
I0305 20:56:44.349468 140415782377216 logging_writer.py:48] [78800] global_step=78800, grad_norm=0.15652215480804443, loss=0.02150374837219715
I0305 20:57:17.267879 140408968165120 logging_writer.py:48] [78900] global_step=78900, grad_norm=0.13752907514572144, loss=0.01713593490421772
I0305 20:57:20.562845 140576608098112 spec.py:321] Evaluating on the training split.
I0305 20:59:16.171017 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 20:59:19.276492 140576608098112 spec.py:349] Evaluating on the test split.
I0305 20:59:22.315296 140576608098112 submission_runner.py:411] Time since start: 39253.02s, 	Step: 78911, 	{'train/accuracy': 0.9954591989517212, 'train/loss': 0.014154761098325253, 'train/mean_average_precision': 0.7724652479037462, 'validation/accuracy': 0.987065851688385, 'validation/loss': 0.05017329007387161, 'validation/mean_average_precision': 0.29211668046331213, 'validation/num_examples': 43793, 'test/accuracy': 0.9861447811126709, 'test/loss': 0.054026540368795395, 'test/mean_average_precision': 0.274390207559422, 'test/num_examples': 43793, 'score': 25468.654727697372, 'total_duration': 39253.02361893654, 'accumulated_submission_time': 25468.654727697372, 'accumulated_eval_time': 13778.132029294968, 'accumulated_logging_time': 4.0714709758758545}
I0305 20:59:22.346142 140409291962112 logging_writer.py:48] [78911] accumulated_eval_time=13778.132029, accumulated_logging_time=4.071471, accumulated_submission_time=25468.654728, global_step=78911, preemption_count=0, score=25468.654728, test/accuracy=0.986145, test/loss=0.054027, test/mean_average_precision=0.274390, test/num_examples=43793, total_duration=39253.023619, train/accuracy=0.995459, train/loss=0.014155, train/mean_average_precision=0.772465, validation/accuracy=0.987066, validation/loss=0.050173, validation/mean_average_precision=0.292117, validation/num_examples=43793
I0305 20:59:51.689649 140415790769920 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.149549663066864, loss=0.018305204808712006
I0305 21:00:24.158581 140409291962112 logging_writer.py:48] [79100] global_step=79100, grad_norm=0.1540195345878601, loss=0.018409626558423042
I0305 21:00:56.379583 140415790769920 logging_writer.py:48] [79200] global_step=79200, grad_norm=0.15185026824474335, loss=0.01667657308280468
I0305 21:01:29.211687 140409291962112 logging_writer.py:48] [79300] global_step=79300, grad_norm=0.14232249557971954, loss=0.01806320622563362
I0305 21:02:01.911096 140415790769920 logging_writer.py:48] [79400] global_step=79400, grad_norm=0.1471584141254425, loss=0.017657920718193054
I0305 21:02:34.586015 140409291962112 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.1456141173839569, loss=0.016878847032785416
I0305 21:03:06.733421 140415790769920 logging_writer.py:48] [79600] global_step=79600, grad_norm=0.15261802077293396, loss=0.01908578909933567
I0305 21:03:22.632183 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:05:14.199684 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:05:17.310379 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:05:20.284111 140576608098112 submission_runner.py:411] Time since start: 39610.99s, 	Step: 79651, 	{'train/accuracy': 0.9955033659934998, 'train/loss': 0.014123479835689068, 'train/mean_average_precision': 0.7787619339885247, 'validation/accuracy': 0.9870723485946655, 'validation/loss': 0.050179753452539444, 'validation/mean_average_precision': 0.29233459798557176, 'validation/num_examples': 43793, 'test/accuracy': 0.9861464500427246, 'test/loss': 0.0540345124900341, 'test/mean_average_precision': 0.2745096935947982, 'test/num_examples': 43793, 'score': 25708.908861875534, 'total_duration': 39610.99244427681, 'accumulated_submission_time': 25708.908861875534, 'accumulated_eval_time': 13895.783916950226, 'accumulated_logging_time': 4.113170385360718}
I0305 21:05:20.314714 140408968165120 logging_writer.py:48] [79651] accumulated_eval_time=13895.783917, accumulated_logging_time=4.113170, accumulated_submission_time=25708.908862, global_step=79651, preemption_count=0, score=25708.908862, test/accuracy=0.986146, test/loss=0.054035, test/mean_average_precision=0.274510, test/num_examples=43793, total_duration=39610.992444, train/accuracy=0.995503, train/loss=0.014123, train/mean_average_precision=0.778762, validation/accuracy=0.987072, validation/loss=0.050180, validation/mean_average_precision=0.292335, validation/num_examples=43793
I0305 21:05:36.410120 140415782377216 logging_writer.py:48] [79700] global_step=79700, grad_norm=0.13912247121334076, loss=0.017296263948082924
I0305 21:06:08.435322 140408968165120 logging_writer.py:48] [79800] global_step=79800, grad_norm=0.13110724091529846, loss=0.01649734377861023
I0305 21:06:40.576430 140415782377216 logging_writer.py:48] [79900] global_step=79900, grad_norm=0.1495240330696106, loss=0.019266456365585327
I0305 21:07:12.623698 140408968165120 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.1471729725599289, loss=0.018847951665520668
I0305 21:07:44.481895 140415782377216 logging_writer.py:48] [80100] global_step=80100, grad_norm=0.15100663900375366, loss=0.019304700195789337
I0305 21:08:16.828612 140408968165120 logging_writer.py:48] [80200] global_step=80200, grad_norm=0.11924581974744797, loss=0.016990037634968758
I0305 21:08:49.215070 140415782377216 logging_writer.py:48] [80300] global_step=80300, grad_norm=0.1342422217130661, loss=0.01707489974796772
I0305 21:09:20.504015 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:11:09.261203 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:11:12.326988 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:11:15.277164 140576608098112 submission_runner.py:411] Time since start: 39965.99s, 	Step: 80398, 	{'train/accuracy': 0.9954550862312317, 'train/loss': 0.014238841831684113, 'train/mean_average_precision': 0.7613149448795697, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29253254886149604, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744157901547382, 'test/num_examples': 43793, 'score': 25949.066460609436, 'total_duration': 39965.985496520996, 'accumulated_submission_time': 25949.066460609436, 'accumulated_eval_time': 14010.557025671005, 'accumulated_logging_time': 4.155007600784302}
I0305 21:11:15.307780 140407882708736 logging_writer.py:48] [80398] accumulated_eval_time=14010.557026, accumulated_logging_time=4.155008, accumulated_submission_time=25949.066461, global_step=80398, preemption_count=0, score=25949.066461, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274416, test/num_examples=43793, total_duration=39965.985497, train/accuracy=0.995455, train/loss=0.014239, train/mean_average_precision=0.761315, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292533, validation/num_examples=43793
I0305 21:11:16.293936 140415790769920 logging_writer.py:48] [80400] global_step=80400, grad_norm=0.1539333313703537, loss=0.017191082239151
I0305 21:11:48.486663 140407882708736 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.12462115287780762, loss=0.01618981547653675
I0305 21:12:20.961520 140415790769920 logging_writer.py:48] [80600] global_step=80600, grad_norm=0.14536713063716888, loss=0.01733485609292984
I0305 21:12:53.031298 140407882708736 logging_writer.py:48] [80700] global_step=80700, grad_norm=0.13560044765472412, loss=0.017515337094664574
I0305 21:13:25.261879 140415790769920 logging_writer.py:48] [80800] global_step=80800, grad_norm=0.14001934230327606, loss=0.01786673441529274
I0305 21:13:57.222079 140407882708736 logging_writer.py:48] [80900] global_step=80900, grad_norm=0.15375344455242157, loss=0.01908286102116108
I0305 21:14:29.217175 140415790769920 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.13231366872787476, loss=0.01888314262032509
I0305 21:15:01.018168 140407882708736 logging_writer.py:48] [81100] global_step=81100, grad_norm=0.12804895639419556, loss=0.01583215221762657
I0305 21:15:15.346571 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:17:04.280896 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:17:07.264320 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:17:10.236786 140576608098112 submission_runner.py:411] Time since start: 40320.95s, 	Step: 81145, 	{'train/accuracy': 0.9954413771629333, 'train/loss': 0.014292030595242977, 'train/mean_average_precision': 0.7743069602250183, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29251318645481067, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27445922620724106, 'test/num_examples': 43793, 'score': 26189.072664499283, 'total_duration': 40320.945110082626, 'accumulated_submission_time': 26189.072664499283, 'accumulated_eval_time': 14125.44719004631, 'accumulated_logging_time': 4.197999954223633}
I0305 21:17:10.267512 140408968165120 logging_writer.py:48] [81145] accumulated_eval_time=14125.447190, accumulated_logging_time=4.198000, accumulated_submission_time=26189.072664, global_step=81145, preemption_count=0, score=26189.072664, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274459, test/num_examples=43793, total_duration=40320.945110, train/accuracy=0.995441, train/loss=0.014292, train/mean_average_precision=0.774307, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292513, validation/num_examples=43793
I0305 21:17:28.462832 140409291962112 logging_writer.py:48] [81200] global_step=81200, grad_norm=0.1612682193517685, loss=0.020021390169858932
I0305 21:18:00.313960 140408968165120 logging_writer.py:48] [81300] global_step=81300, grad_norm=0.14277900755405426, loss=0.0173959881067276
I0305 21:18:32.487243 140409291962112 logging_writer.py:48] [81400] global_step=81400, grad_norm=0.1370057314634323, loss=0.01597840152680874
I0305 21:19:04.887849 140408968165120 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.14345796406269073, loss=0.018061179667711258
I0305 21:19:36.869126 140409291962112 logging_writer.py:48] [81600] global_step=81600, grad_norm=0.16086392104625702, loss=0.01998167671263218
I0305 21:20:09.301262 140408968165120 logging_writer.py:48] [81700] global_step=81700, grad_norm=0.1430990844964981, loss=0.01694907248020172
I0305 21:20:41.475529 140409291962112 logging_writer.py:48] [81800] global_step=81800, grad_norm=0.1421958953142166, loss=0.018005071207880974
I0305 21:21:10.305818 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:23:01.216143 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:23:04.449134 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:23:07.446954 140576608098112 submission_runner.py:411] Time since start: 40678.16s, 	Step: 81891, 	{'train/accuracy': 0.9955335259437561, 'train/loss': 0.01403177622705698, 'train/mean_average_precision': 0.7743152053587273, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29238982545126263, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745432332629763, 'test/num_examples': 43793, 'score': 26429.078844308853, 'total_duration': 40678.15526819229, 'accumulated_submission_time': 26429.078844308853, 'accumulated_eval_time': 14242.58826804161, 'accumulated_logging_time': 4.240504264831543}
I0305 21:23:07.478756 140407882708736 logging_writer.py:48] [81891] accumulated_eval_time=14242.588268, accumulated_logging_time=4.240504, accumulated_submission_time=26429.078844, global_step=81891, preemption_count=0, score=26429.078844, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274543, test/num_examples=43793, total_duration=40678.155268, train/accuracy=0.995534, train/loss=0.014032, train/mean_average_precision=0.774315, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292390, validation/num_examples=43793
I0305 21:23:10.722424 140415782377216 logging_writer.py:48] [81900] global_step=81900, grad_norm=0.14737571775913239, loss=0.01806025765836239
I0305 21:23:42.731962 140407882708736 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.15377125144004822, loss=0.019294913858175278
I0305 21:24:15.335277 140415782377216 logging_writer.py:48] [82100] global_step=82100, grad_norm=0.14478693902492523, loss=0.01823675073683262
I0305 21:24:47.658437 140407882708736 logging_writer.py:48] [82200] global_step=82200, grad_norm=0.15805481374263763, loss=0.01942867413163185
I0305 21:25:20.143691 140415782377216 logging_writer.py:48] [82300] global_step=82300, grad_norm=0.16020946204662323, loss=0.02150021493434906
I0305 21:25:52.171752 140407882708736 logging_writer.py:48] [82400] global_step=82400, grad_norm=0.15763720870018005, loss=0.020298682153224945
I0305 21:26:24.411985 140415782377216 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.14878050982952118, loss=0.019082872197031975
I0305 21:26:56.586478 140407882708736 logging_writer.py:48] [82600] global_step=82600, grad_norm=0.162061870098114, loss=0.019432468339800835
I0305 21:27:07.638437 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:28:59.240504 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:29:02.512561 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:29:05.518618 140576608098112 submission_runner.py:411] Time since start: 41036.23s, 	Step: 82635, 	{'train/accuracy': 0.9954736232757568, 'train/loss': 0.014210385270416737, 'train/mean_average_precision': 0.7793815673647857, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923741737832562, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27439555740947785, 'test/num_examples': 43793, 'score': 26669.20731639862, 'total_duration': 41036.22692847252, 'accumulated_submission_time': 26669.20731639862, 'accumulated_eval_time': 14360.468386650085, 'accumulated_logging_time': 4.283483266830444}
I0305 21:29:05.550772 140408968165120 logging_writer.py:48] [82635] accumulated_eval_time=14360.468387, accumulated_logging_time=4.283483, accumulated_submission_time=26669.207316, global_step=82635, preemption_count=0, score=26669.207316, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274396, test/num_examples=43793, total_duration=41036.226928, train/accuracy=0.995474, train/loss=0.014210, train/mean_average_precision=0.779382, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292374, validation/num_examples=43793
I0305 21:29:26.999680 140409291962112 logging_writer.py:48] [82700] global_step=82700, grad_norm=0.13654594123363495, loss=0.016610408201813698
I0305 21:29:59.305962 140408968165120 logging_writer.py:48] [82800] global_step=82800, grad_norm=0.14031945168972015, loss=0.01865980215370655
I0305 21:30:31.367099 140409291962112 logging_writer.py:48] [82900] global_step=82900, grad_norm=0.1331968903541565, loss=0.01872897334396839
I0305 21:31:03.606158 140408968165120 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.14570432901382446, loss=0.020213866606354713
I0305 21:31:35.328943 140409291962112 logging_writer.py:48] [83100] global_step=83100, grad_norm=0.15407928824424744, loss=0.018632495775818825
I0305 21:32:07.488439 140408968165120 logging_writer.py:48] [83200] global_step=83200, grad_norm=0.14358818531036377, loss=0.018473321571946144
I0305 21:32:40.049116 140409291962112 logging_writer.py:48] [83300] global_step=83300, grad_norm=0.13620910048484802, loss=0.017885517328977585
I0305 21:33:05.684638 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:34:52.717334 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:34:55.773224 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:34:58.759758 140576608098112 submission_runner.py:411] Time since start: 41389.47s, 	Step: 83379, 	{'train/accuracy': 0.9954969882965088, 'train/loss': 0.014058413915336132, 'train/mean_average_precision': 0.7734911544122332, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29237454060272, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744025722481929, 'test/num_examples': 43793, 'score': 26909.310037374496, 'total_duration': 41389.468089580536, 'accumulated_submission_time': 26909.310037374496, 'accumulated_eval_time': 14473.543465852737, 'accumulated_logging_time': 4.326739549636841}
I0305 21:34:58.791023 140415782377216 logging_writer.py:48] [83379] accumulated_eval_time=14473.543466, accumulated_logging_time=4.326740, accumulated_submission_time=26909.310037, global_step=83379, preemption_count=0, score=26909.310037, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274403, test/num_examples=43793, total_duration=41389.468090, train/accuracy=0.995497, train/loss=0.014058, train/mean_average_precision=0.773491, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292375, validation/num_examples=43793
I0305 21:35:06.082449 140415790769920 logging_writer.py:48] [83400] global_step=83400, grad_norm=0.14788112044334412, loss=0.018228352069854736
I0305 21:35:38.325073 140415782377216 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.1387770175933838, loss=0.015530187636613846
I0305 21:36:11.025317 140415790769920 logging_writer.py:48] [83600] global_step=83600, grad_norm=0.14285187423229218, loss=0.018884504213929176
I0305 21:36:44.206023 140415782377216 logging_writer.py:48] [83700] global_step=83700, grad_norm=0.1465277373790741, loss=0.01936681568622589
I0305 21:37:16.739042 140415790769920 logging_writer.py:48] [83800] global_step=83800, grad_norm=0.13301624357700348, loss=0.017070265486836433
I0305 21:37:49.310951 140415782377216 logging_writer.py:48] [83900] global_step=83900, grad_norm=0.1465630680322647, loss=0.018394842743873596
I0305 21:38:21.620043 140415790769920 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.15035736560821533, loss=0.01889444887638092
I0305 21:38:54.180894 140415782377216 logging_writer.py:48] [84100] global_step=84100, grad_norm=0.14501963555812836, loss=0.019049901515245438
I0305 21:38:58.773274 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:40:47.621078 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:40:50.675462 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:40:53.648102 140576608098112 submission_runner.py:411] Time since start: 41744.36s, 	Step: 84115, 	{'train/accuracy': 0.9954795241355896, 'train/loss': 0.014179798774421215, 'train/mean_average_precision': 0.7700450569577544, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924245796731328, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27439142944771633, 'test/num_examples': 43793, 'score': 27149.2596681118, 'total_duration': 41744.3564248085, 'accumulated_submission_time': 27149.2596681118, 'accumulated_eval_time': 14588.418242692947, 'accumulated_logging_time': 4.370355844497681}
I0305 21:40:53.680924 140407882708736 logging_writer.py:48] [84115] accumulated_eval_time=14588.418243, accumulated_logging_time=4.370356, accumulated_submission_time=27149.259668, global_step=84115, preemption_count=0, score=27149.259668, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274391, test/num_examples=43793, total_duration=41744.356425, train/accuracy=0.995480, train/loss=0.014180, train/mean_average_precision=0.770045, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292425, validation/num_examples=43793
I0305 21:41:21.220616 140408968165120 logging_writer.py:48] [84200] global_step=84200, grad_norm=0.1491645723581314, loss=0.018890157341957092
I0305 21:41:52.915620 140407882708736 logging_writer.py:48] [84300] global_step=84300, grad_norm=0.12410744279623032, loss=0.016821902245283127
I0305 21:42:24.835842 140408968165120 logging_writer.py:48] [84400] global_step=84400, grad_norm=0.13950054347515106, loss=0.01839316077530384
I0305 21:42:56.665102 140407882708736 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.13976053893566132, loss=0.01678909733891487
I0305 21:43:28.764822 140408968165120 logging_writer.py:48] [84600] global_step=84600, grad_norm=0.1472339779138565, loss=0.019160760566592216
I0305 21:44:00.455413 140407882708736 logging_writer.py:48] [84700] global_step=84700, grad_norm=0.15153972804546356, loss=0.019489049911499023
I0305 21:44:32.529308 140408968165120 logging_writer.py:48] [84800] global_step=84800, grad_norm=0.13856452703475952, loss=0.0182014312595129
I0305 21:44:53.865663 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:46:44.504067 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:46:47.517357 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:46:50.515721 140576608098112 submission_runner.py:411] Time since start: 42101.22s, 	Step: 84868, 	{'train/accuracy': 0.9954484105110168, 'train/loss': 0.014258011244237423, 'train/mean_average_precision': 0.770631365526065, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925296154257013, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744031934173734, 'test/num_examples': 43793, 'score': 27389.411874771118, 'total_duration': 42101.22405195236, 'accumulated_submission_time': 27389.411874771118, 'accumulated_eval_time': 14705.068259239197, 'accumulated_logging_time': 4.415584087371826}
I0305 21:46:50.548007 140409291962112 logging_writer.py:48] [84868] accumulated_eval_time=14705.068259, accumulated_logging_time=4.415584, accumulated_submission_time=27389.411875, global_step=84868, preemption_count=0, score=27389.411875, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274403, test/num_examples=43793, total_duration=42101.224052, train/accuracy=0.995448, train/loss=0.014258, train/mean_average_precision=0.770631, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292530, validation/num_examples=43793
I0305 21:47:01.113256 140415790769920 logging_writer.py:48] [84900] global_step=84900, grad_norm=0.13738925755023956, loss=0.017063701525330544
I0305 21:47:33.412792 140409291962112 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.15565182268619537, loss=0.01885090582072735
I0305 21:48:05.499309 140415790769920 logging_writer.py:48] [85100] global_step=85100, grad_norm=0.1510854959487915, loss=0.01770370453596115
I0305 21:48:37.613932 140409291962112 logging_writer.py:48] [85200] global_step=85200, grad_norm=0.1374964863061905, loss=0.01803923398256302
I0305 21:49:09.750412 140415790769920 logging_writer.py:48] [85300] global_step=85300, grad_norm=0.13392245769500732, loss=0.01759910210967064
I0305 21:49:41.872501 140409291962112 logging_writer.py:48] [85400] global_step=85400, grad_norm=0.13462360203266144, loss=0.017822636291384697
I0305 21:50:14.003737 140415790769920 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.14773733913898468, loss=0.01666029542684555
I0305 21:50:45.849018 140409291962112 logging_writer.py:48] [85600] global_step=85600, grad_norm=0.12090876698493958, loss=0.017803478986024857
I0305 21:50:50.647439 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:52:39.985000 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:52:43.061928 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:52:46.077637 140576608098112 submission_runner.py:411] Time since start: 42456.79s, 	Step: 85616, 	{'train/accuracy': 0.9955559968948364, 'train/loss': 0.014015146531164646, 'train/mean_average_precision': 0.7786308148010082, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29230866325011895, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.274370387334361, 'test/num_examples': 43793, 'score': 27629.480456590652, 'total_duration': 42456.785959005356, 'accumulated_submission_time': 27629.480456590652, 'accumulated_eval_time': 14820.498401403427, 'accumulated_logging_time': 4.458751201629639}
I0305 21:52:46.109455 140407882708736 logging_writer.py:48] [85616] accumulated_eval_time=14820.498401, accumulated_logging_time=4.458751, accumulated_submission_time=27629.480457, global_step=85616, preemption_count=0, score=27629.480457, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274370, test/num_examples=43793, total_duration=42456.785959, train/accuracy=0.995556, train/loss=0.014015, train/mean_average_precision=0.778631, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292309, validation/num_examples=43793
I0305 21:53:13.941042 140408968165120 logging_writer.py:48] [85700] global_step=85700, grad_norm=0.13445770740509033, loss=0.01809512823820114
I0305 21:53:46.384094 140407882708736 logging_writer.py:48] [85800] global_step=85800, grad_norm=0.14226263761520386, loss=0.018213117495179176
I0305 21:54:19.060314 140408968165120 logging_writer.py:48] [85900] global_step=85900, grad_norm=0.14724650979042053, loss=0.016213566064834595
I0305 21:54:51.262263 140407882708736 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.12204435467720032, loss=0.01524698082357645
I0305 21:55:23.612283 140408968165120 logging_writer.py:48] [86100] global_step=86100, grad_norm=0.1466563642024994, loss=0.020395001396536827
I0305 21:55:55.884549 140407882708736 logging_writer.py:48] [86200] global_step=86200, grad_norm=0.13001424074172974, loss=0.017135418951511383
I0305 21:56:28.204766 140408968165120 logging_writer.py:48] [86300] global_step=86300, grad_norm=0.15263541042804718, loss=0.017523912712931633
I0305 21:56:46.334577 140576608098112 spec.py:321] Evaluating on the training split.
I0305 21:58:32.650625 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 21:58:35.719397 140576608098112 spec.py:349] Evaluating on the test split.
I0305 21:58:38.753038 140576608098112 submission_runner.py:411] Time since start: 42809.46s, 	Step: 86357, 	{'train/accuracy': 0.9954648613929749, 'train/loss': 0.014220686629414558, 'train/mean_average_precision': 0.7778873312937167, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2925448356793363, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27451661895287927, 'test/num_examples': 43793, 'score': 27869.674690246582, 'total_duration': 42809.461366176605, 'accumulated_submission_time': 27869.674690246582, 'accumulated_eval_time': 14932.916816949844, 'accumulated_logging_time': 4.501690149307251}
I0305 21:58:38.786066 140415782377216 logging_writer.py:48] [86357] accumulated_eval_time=14932.916817, accumulated_logging_time=4.501690, accumulated_submission_time=27869.674690, global_step=86357, preemption_count=0, score=27869.674690, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274517, test/num_examples=43793, total_duration=42809.461366, train/accuracy=0.995465, train/loss=0.014221, train/mean_average_precision=0.777887, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292545, validation/num_examples=43793
I0305 21:58:53.231786 140415790769920 logging_writer.py:48] [86400] global_step=86400, grad_norm=0.1619654893875122, loss=0.019590724259614944
I0305 21:59:26.345803 140415782377216 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.1452832967042923, loss=0.016117023304104805
I0305 21:59:59.091574 140415790769920 logging_writer.py:48] [86600] global_step=86600, grad_norm=0.13723084330558777, loss=0.017832931131124496
I0305 22:00:31.586292 140415782377216 logging_writer.py:48] [86700] global_step=86700, grad_norm=0.13988623023033142, loss=0.01866297423839569
I0305 22:01:04.001148 140415790769920 logging_writer.py:48] [86800] global_step=86800, grad_norm=0.1608608365058899, loss=0.019595591351389885
I0305 22:01:36.463933 140415782377216 logging_writer.py:48] [86900] global_step=86900, grad_norm=0.1449330747127533, loss=0.0180071871727705
I0305 22:02:09.154063 140415790769920 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.13233479857444763, loss=0.017653189599514008
I0305 22:02:38.837655 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:04:31.270547 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:04:34.366119 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:04:37.429044 140576608098112 submission_runner.py:411] Time since start: 43168.14s, 	Step: 87093, 	{'train/accuracy': 0.9954904317855835, 'train/loss': 0.014112512581050396, 'train/mean_average_precision': 0.7748273898665948, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29241149382851334, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744101209278129, 'test/num_examples': 43793, 'score': 28109.69511294365, 'total_duration': 43168.13737511635, 'accumulated_submission_time': 28109.69511294365, 'accumulated_eval_time': 15051.508164167404, 'accumulated_logging_time': 4.545614242553711}
I0305 22:04:37.461626 140407882708736 logging_writer.py:48] [87093] accumulated_eval_time=15051.508164, accumulated_logging_time=4.545614, accumulated_submission_time=28109.695113, global_step=87093, preemption_count=0, score=28109.695113, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274410, test/num_examples=43793, total_duration=43168.137375, train/accuracy=0.995490, train/loss=0.014113, train/mean_average_precision=0.774827, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292411, validation/num_examples=43793
I0305 22:04:40.068500 140408968165120 logging_writer.py:48] [87100] global_step=87100, grad_norm=0.14154328405857086, loss=0.017741581425070763
I0305 22:05:12.665880 140407882708736 logging_writer.py:48] [87200] global_step=87200, grad_norm=0.1254468411207199, loss=0.01483154483139515
I0305 22:05:45.632109 140408968165120 logging_writer.py:48] [87300] global_step=87300, grad_norm=0.14346884191036224, loss=0.01652335375547409
I0305 22:06:18.961196 140407882708736 logging_writer.py:48] [87400] global_step=87400, grad_norm=0.15152820944786072, loss=0.01901349611580372
I0305 22:06:51.414860 140408968165120 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.1557951122522354, loss=0.017085721716284752
I0305 22:07:24.319041 140407882708736 logging_writer.py:48] [87600] global_step=87600, grad_norm=0.1432027965784073, loss=0.016821708530187607
I0305 22:07:57.820416 140408968165120 logging_writer.py:48] [87700] global_step=87700, grad_norm=0.14050817489624023, loss=0.017076201736927032
I0305 22:08:30.138216 140407882708736 logging_writer.py:48] [87800] global_step=87800, grad_norm=0.14570024609565735, loss=0.017080459743738174
I0305 22:08:37.670149 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:10:30.236760 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:10:33.380019 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:10:36.455717 140576608098112 submission_runner.py:411] Time since start: 43527.16s, 	Step: 87824, 	{'train/accuracy': 0.9954723715782166, 'train/loss': 0.014251687563955784, 'train/mean_average_precision': 0.7732605363318497, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29253611652288153, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743490402265405, 'test/num_examples': 43793, 'score': 28349.8712515831, 'total_duration': 43527.16404628754, 'accumulated_submission_time': 28349.8712515831, 'accumulated_eval_time': 15170.293683290482, 'accumulated_logging_time': 4.589420795440674}
I0305 22:10:36.488931 140409291962112 logging_writer.py:48] [87824] accumulated_eval_time=15170.293683, accumulated_logging_time=4.589421, accumulated_submission_time=28349.871252, global_step=87824, preemption_count=0, score=28349.871252, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274349, test/num_examples=43793, total_duration=43527.164046, train/accuracy=0.995472, train/loss=0.014252, train/mean_average_precision=0.773261, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292536, validation/num_examples=43793
I0305 22:11:01.190436 140415790769920 logging_writer.py:48] [87900] global_step=87900, grad_norm=0.14398398995399475, loss=0.019501961767673492
I0305 22:11:34.022831 140409291962112 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.13892489671707153, loss=0.01948734000325203
I0305 22:12:07.601387 140415790769920 logging_writer.py:48] [88100] global_step=88100, grad_norm=0.13509654998779297, loss=0.017622560262680054
I0305 22:12:40.744110 140409291962112 logging_writer.py:48] [88200] global_step=88200, grad_norm=0.14788712561130524, loss=0.020813679322600365
I0305 22:13:15.064754 140415790769920 logging_writer.py:48] [88300] global_step=88300, grad_norm=0.14800620079040527, loss=0.01975458674132824
I0305 22:13:48.486762 140409291962112 logging_writer.py:48] [88400] global_step=88400, grad_norm=0.1387699693441391, loss=0.017009852454066277
I0305 22:14:22.154575 140415790769920 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.1466771513223648, loss=0.019182464107871056
I0305 22:14:36.467041 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:16:28.465590 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:16:31.517014 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:16:34.862923 140576608098112 submission_runner.py:411] Time since start: 43885.57s, 	Step: 88544, 	{'train/accuracy': 0.9954808950424194, 'train/loss': 0.01415772270411253, 'train/mean_average_precision': 0.7625540593532746, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29245573763190835, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27453616701298245, 'test/num_examples': 43793, 'score': 28589.813091516495, 'total_duration': 43885.57111406326, 'accumulated_submission_time': 28589.813091516495, 'accumulated_eval_time': 15288.689393758774, 'accumulated_logging_time': 4.63522481918335}
I0305 22:16:34.897355 140407882708736 logging_writer.py:48] [88544] accumulated_eval_time=15288.689394, accumulated_logging_time=4.635225, accumulated_submission_time=28589.813092, global_step=88544, preemption_count=0, score=28589.813092, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274536, test/num_examples=43793, total_duration=43885.571114, train/accuracy=0.995481, train/loss=0.014158, train/mean_average_precision=0.762554, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292456, validation/num_examples=43793
I0305 22:16:53.762591 140415782377216 logging_writer.py:48] [88600] global_step=88600, grad_norm=0.16147442162036896, loss=0.020008740946650505
I0305 22:17:26.780520 140407882708736 logging_writer.py:48] [88700] global_step=88700, grad_norm=0.1316925287246704, loss=0.01694612391293049
I0305 22:17:59.188991 140415782377216 logging_writer.py:48] [88800] global_step=88800, grad_norm=0.13189207017421722, loss=0.01826591230928898
I0305 22:18:31.407139 140407882708736 logging_writer.py:48] [88900] global_step=88900, grad_norm=0.1546812802553177, loss=0.01985105127096176
I0305 22:19:03.492465 140415782377216 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.1426270604133606, loss=0.018478915095329285
I0305 22:19:35.954594 140407882708736 logging_writer.py:48] [89100] global_step=89100, grad_norm=0.15769943594932556, loss=0.019612977281212807
I0305 22:20:08.294200 140415782377216 logging_writer.py:48] [89200] global_step=89200, grad_norm=0.14536738395690918, loss=0.01881498098373413
I0305 22:20:35.071333 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:22:22.537808 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:22:25.587212 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:22:28.599821 140576608098112 submission_runner.py:411] Time since start: 44239.31s, 	Step: 89284, 	{'train/accuracy': 0.9955047965049744, 'train/loss': 0.014082049950957298, 'train/mean_average_precision': 0.7791833701354678, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923814411188966, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744330019098556, 'test/num_examples': 43793, 'score': 28829.955509662628, 'total_duration': 44239.30815052986, 'accumulated_submission_time': 28829.955509662628, 'accumulated_eval_time': 15402.21783900261, 'accumulated_logging_time': 4.680805921554565}
I0305 22:22:28.632385 140408968165120 logging_writer.py:48] [89284] accumulated_eval_time=15402.217839, accumulated_logging_time=4.680806, accumulated_submission_time=28829.955510, global_step=89284, preemption_count=0, score=28829.955510, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274433, test/num_examples=43793, total_duration=44239.308151, train/accuracy=0.995505, train/loss=0.014082, train/mean_average_precision=0.779183, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292381, validation/num_examples=43793
I0305 22:22:34.068974 140415790769920 logging_writer.py:48] [89300] global_step=89300, grad_norm=0.16386154294013977, loss=0.02128485217690468
I0305 22:23:06.303498 140408968165120 logging_writer.py:48] [89400] global_step=89400, grad_norm=0.15415716171264648, loss=0.020367272198200226
I0305 22:23:38.245657 140415790769920 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.15425731241703033, loss=0.019092092290520668
I0305 22:24:10.548660 140408968165120 logging_writer.py:48] [89600] global_step=89600, grad_norm=0.14205162227153778, loss=0.01963859610259533
I0305 22:24:42.568148 140415790769920 logging_writer.py:48] [89700] global_step=89700, grad_norm=0.1612786054611206, loss=0.019561557099223137
I0305 22:25:14.884857 140408968165120 logging_writer.py:48] [89800] global_step=89800, grad_norm=0.15941940248012543, loss=0.017955416813492775
I0305 22:25:46.865513 140415790769920 logging_writer.py:48] [89900] global_step=89900, grad_norm=0.15045155584812164, loss=0.019133327528834343
I0305 22:26:19.384540 140408968165120 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.13623444736003876, loss=0.019427886232733727
I0305 22:26:28.730477 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:28:18.903696 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:28:21.903418 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:28:24.952936 140576608098112 submission_runner.py:411] Time since start: 44595.66s, 	Step: 90030, 	{'train/accuracy': 0.9955043792724609, 'train/loss': 0.014120706357061863, 'train/mean_average_precision': 0.7797977344418124, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923577434288198, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744070792908892, 'test/num_examples': 43793, 'score': 29070.022459983826, 'total_duration': 44595.66125655174, 'accumulated_submission_time': 29070.022459983826, 'accumulated_eval_time': 15518.440243244171, 'accumulated_logging_time': 4.724467754364014}
I0305 22:28:24.985359 140407882708736 logging_writer.py:48] [90030] accumulated_eval_time=15518.440243, accumulated_logging_time=4.724468, accumulated_submission_time=29070.022460, global_step=90030, preemption_count=0, score=29070.022460, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274407, test/num_examples=43793, total_duration=44595.661257, train/accuracy=0.995504, train/loss=0.014121, train/mean_average_precision=0.779798, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292358, validation/num_examples=43793
I0305 22:28:48.108017 140415782377216 logging_writer.py:48] [90100] global_step=90100, grad_norm=0.17170223593711853, loss=0.02164505422115326
I0305 22:29:20.545865 140407882708736 logging_writer.py:48] [90200] global_step=90200, grad_norm=0.12986978888511658, loss=0.017471913248300552
I0305 22:29:52.627276 140415782377216 logging_writer.py:48] [90300] global_step=90300, grad_norm=0.1383543461561203, loss=0.018076978623867035
I0305 22:30:24.615874 140407882708736 logging_writer.py:48] [90400] global_step=90400, grad_norm=0.13994061946868896, loss=0.01667764037847519
I0305 22:30:56.409159 140415782377216 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.1438046544790268, loss=0.01970326155424118
I0305 22:31:28.519497 140407882708736 logging_writer.py:48] [90600] global_step=90600, grad_norm=0.14492936432361603, loss=0.01779385656118393
I0305 22:32:00.521302 140415782377216 logging_writer.py:48] [90700] global_step=90700, grad_norm=0.13605916500091553, loss=0.017153386026620865
I0305 22:32:25.044108 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:34:19.270643 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:34:22.321891 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:34:25.324880 140576608098112 submission_runner.py:411] Time since start: 44956.03s, 	Step: 90777, 	{'train/accuracy': 0.9954771399497986, 'train/loss': 0.01419143844395876, 'train/mean_average_precision': 0.7664620953486616, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29256205217180614, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2745075266508673, 'test/num_examples': 43793, 'score': 29310.049928426743, 'total_duration': 44956.033213377, 'accumulated_submission_time': 29310.049928426743, 'accumulated_eval_time': 15638.720978021622, 'accumulated_logging_time': 4.768067836761475}
I0305 22:34:25.358298 140408968165120 logging_writer.py:48] [90777] accumulated_eval_time=15638.720978, accumulated_logging_time=4.768068, accumulated_submission_time=29310.049928, global_step=90777, preemption_count=0, score=29310.049928, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274508, test/num_examples=43793, total_duration=44956.033213, train/accuracy=0.995477, train/loss=0.014191, train/mean_average_precision=0.766462, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292562, validation/num_examples=43793
I0305 22:34:33.180351 140415790769920 logging_writer.py:48] [90800] global_step=90800, grad_norm=0.14363761246204376, loss=0.017663123086094856
I0305 22:35:05.765798 140408968165120 logging_writer.py:48] [90900] global_step=90900, grad_norm=0.12130281329154968, loss=0.015729939565062523
I0305 22:35:38.359496 140415790769920 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.14552627503871918, loss=0.01960058882832527
I0305 22:36:10.840941 140408968165120 logging_writer.py:48] [91100] global_step=91100, grad_norm=0.12394264340400696, loss=0.017691534012556076
I0305 22:36:43.632215 140415790769920 logging_writer.py:48] [91200] global_step=91200, grad_norm=0.16063550114631653, loss=0.020412364974617958
I0305 22:37:16.368304 140408968165120 logging_writer.py:48] [91300] global_step=91300, grad_norm=0.15677803754806519, loss=0.02018282748758793
I0305 22:37:48.754529 140415790769920 logging_writer.py:48] [91400] global_step=91400, grad_norm=0.13518527150154114, loss=0.01581546477973461
I0305 22:38:21.048478 140408968165120 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.14870236814022064, loss=0.020165465772151947
I0305 22:38:25.524854 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:40:13.417827 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:40:16.529951 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:40:19.538017 140576608098112 submission_runner.py:411] Time since start: 45310.25s, 	Step: 91515, 	{'train/accuracy': 0.9954972863197327, 'train/loss': 0.014135231263935566, 'train/mean_average_precision': 0.7745224158954835, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923887581809313, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27456100679909695, 'test/num_examples': 43793, 'score': 29550.185745477676, 'total_duration': 45310.246342897415, 'accumulated_submission_time': 29550.185745477676, 'accumulated_eval_time': 15752.73408961296, 'accumulated_logging_time': 4.812305450439453}
I0305 22:40:19.571814 140407882708736 logging_writer.py:48] [91515] accumulated_eval_time=15752.734090, accumulated_logging_time=4.812305, accumulated_submission_time=29550.185745, global_step=91515, preemption_count=0, score=29550.185745, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274561, test/num_examples=43793, total_duration=45310.246343, train/accuracy=0.995497, train/loss=0.014135, train/mean_average_precision=0.774522, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292389, validation/num_examples=43793
I0305 22:40:47.296373 140409291962112 logging_writer.py:48] [91600] global_step=91600, grad_norm=0.14625900983810425, loss=0.019389253109693527
I0305 22:41:19.765735 140407882708736 logging_writer.py:48] [91700] global_step=91700, grad_norm=0.14417879283428192, loss=0.01665552332997322
I0305 22:41:51.779437 140409291962112 logging_writer.py:48] [91800] global_step=91800, grad_norm=0.13913777470588684, loss=0.015906115993857384
I0305 22:42:25.069450 140407882708736 logging_writer.py:48] [91900] global_step=91900, grad_norm=0.13312934339046478, loss=0.01546645350754261
I0305 22:42:58.422719 140409291962112 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.13869017362594604, loss=0.017357071861624718
I0305 22:43:30.861476 140407882708736 logging_writer.py:48] [92100] global_step=92100, grad_norm=0.1510559618473053, loss=0.01930861361324787
I0305 22:44:02.875881 140409291962112 logging_writer.py:48] [92200] global_step=92200, grad_norm=0.13529343903064728, loss=0.015485338866710663
I0305 22:44:19.629298 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:46:05.369458 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:46:10.004676 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:46:12.964535 140576608098112 submission_runner.py:411] Time since start: 45663.67s, 	Step: 92253, 	{'train/accuracy': 0.9954531192779541, 'train/loss': 0.01423732005059719, 'train/mean_average_precision': 0.7638528805363961, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29243233570997706, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27453457447165747, 'test/num_examples': 43793, 'score': 29790.211117506027, 'total_duration': 45663.672857284546, 'accumulated_submission_time': 29790.211117506027, 'accumulated_eval_time': 15866.069275140762, 'accumulated_logging_time': 4.857031583786011}
I0305 22:46:12.997896 140408968165120 logging_writer.py:48] [92253] accumulated_eval_time=15866.069275, accumulated_logging_time=4.857032, accumulated_submission_time=29790.211118, global_step=92253, preemption_count=0, score=29790.211118, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274535, test/num_examples=43793, total_duration=45663.672857, train/accuracy=0.995453, train/loss=0.014237, train/mean_average_precision=0.763853, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292432, validation/num_examples=43793
I0305 22:46:29.367319 140415782377216 logging_writer.py:48] [92300] global_step=92300, grad_norm=0.15239664912223816, loss=0.018056359142065048
I0305 22:47:04.030150 140408968165120 logging_writer.py:48] [92400] global_step=92400, grad_norm=0.14741924405097961, loss=0.018786877393722534
I0305 22:47:36.674376 140415782377216 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.1340426504611969, loss=0.01681717298924923
I0305 22:48:09.240327 140408968165120 logging_writer.py:48] [92600] global_step=92600, grad_norm=0.1366385519504547, loss=0.019945334643125534
I0305 22:48:41.684546 140415782377216 logging_writer.py:48] [92700] global_step=92700, grad_norm=0.15771402418613434, loss=0.018519850447773933
I0305 22:49:14.115546 140408968165120 logging_writer.py:48] [92800] global_step=92800, grad_norm=0.13053277134895325, loss=0.01797574758529663
I0305 22:49:46.064251 140415782377216 logging_writer.py:48] [92900] global_step=92900, grad_norm=0.1303035020828247, loss=0.016295716166496277
I0305 22:50:13.027386 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:52:02.430157 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:52:05.449088 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:52:08.485331 140576608098112 submission_runner.py:411] Time since start: 46019.19s, 	Step: 92984, 	{'train/accuracy': 0.9955169558525085, 'train/loss': 0.014061023481190205, 'train/mean_average_precision': 0.7800021202800156, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29240816599057085, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27437041354738456, 'test/num_examples': 43793, 'score': 30030.20610809326, 'total_duration': 46019.193653821945, 'accumulated_submission_time': 30030.20610809326, 'accumulated_eval_time': 15981.527184009552, 'accumulated_logging_time': 4.903205156326294}
I0305 22:52:08.518542 140407882708736 logging_writer.py:48] [92984] accumulated_eval_time=15981.527184, accumulated_logging_time=4.903205, accumulated_submission_time=30030.206108, global_step=92984, preemption_count=0, score=30030.206108, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274370, test/num_examples=43793, total_duration=46019.193654, train/accuracy=0.995517, train/loss=0.014061, train/mean_average_precision=0.780002, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292408, validation/num_examples=43793
I0305 22:52:14.234295 140415790769920 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.13957521319389343, loss=0.019433848559856415
I0305 22:52:46.740778 140407882708736 logging_writer.py:48] [93100] global_step=93100, grad_norm=0.1416451334953308, loss=0.018359549343585968
I0305 22:53:19.010355 140415790769920 logging_writer.py:48] [93200] global_step=93200, grad_norm=0.16210690140724182, loss=0.020152123644948006
I0305 22:53:51.491361 140407882708736 logging_writer.py:48] [93300] global_step=93300, grad_norm=0.1404949426651001, loss=0.016884705051779747
I0305 22:54:25.026803 140415790769920 logging_writer.py:48] [93400] global_step=93400, grad_norm=0.13621363043785095, loss=0.017394142225384712
I0305 22:54:58.121436 140407882708736 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.1538516879081726, loss=0.018922461196780205
I0305 22:55:30.566061 140415790769920 logging_writer.py:48] [93600] global_step=93600, grad_norm=0.13808010518550873, loss=0.018188349902629852
I0305 22:56:02.669621 140407882708736 logging_writer.py:48] [93700] global_step=93700, grad_norm=0.13197113573551178, loss=0.018519867211580276
I0305 22:56:08.781480 140576608098112 spec.py:321] Evaluating on the training split.
I0305 22:57:59.018898 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 22:58:02.230333 140576608098112 spec.py:349] Evaluating on the test split.
I0305 22:58:05.239108 140576608098112 submission_runner.py:411] Time since start: 46375.95s, 	Step: 93720, 	{'train/accuracy': 0.9955341219902039, 'train/loss': 0.01410527341067791, 'train/mean_average_precision': 0.7757578925328368, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29239972857590946, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27446853609808697, 'test/num_examples': 43793, 'score': 30270.435495376587, 'total_duration': 46375.94743990898, 'accumulated_submission_time': 30270.435495376587, 'accumulated_eval_time': 16097.984763383865, 'accumulated_logging_time': 4.948692083358765}
I0305 22:58:05.272059 140408968165120 logging_writer.py:48] [93720] accumulated_eval_time=16097.984763, accumulated_logging_time=4.948692, accumulated_submission_time=30270.435495, global_step=93720, preemption_count=0, score=30270.435495, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274469, test/num_examples=43793, total_duration=46375.947440, train/accuracy=0.995534, train/loss=0.014105, train/mean_average_precision=0.775758, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292400, validation/num_examples=43793
I0305 22:58:31.969319 140409291962112 logging_writer.py:48] [93800] global_step=93800, grad_norm=0.1338120996952057, loss=0.017859820276498795
I0305 22:59:04.088057 140408968165120 logging_writer.py:48] [93900] global_step=93900, grad_norm=0.1412264108657837, loss=0.018486306071281433
I0305 22:59:35.715394 140409291962112 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.1489643007516861, loss=0.021374912932515144
I0305 23:00:07.690479 140408968165120 logging_writer.py:48] [94100] global_step=94100, grad_norm=0.15831266343593597, loss=0.01888195052742958
I0305 23:00:39.427756 140409291962112 logging_writer.py:48] [94200] global_step=94200, grad_norm=0.14134065806865692, loss=0.01710708811879158
I0305 23:01:11.368425 140408968165120 logging_writer.py:48] [94300] global_step=94300, grad_norm=0.14602410793304443, loss=0.0196168664842844
I0305 23:01:43.290158 140409291962112 logging_writer.py:48] [94400] global_step=94400, grad_norm=0.1379271149635315, loss=0.01702303998172283
I0305 23:02:05.299941 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:03:54.596107 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:03:57.607577 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:04:00.611248 140576608098112 submission_runner.py:411] Time since start: 46731.32s, 	Step: 94469, 	{'train/accuracy': 0.9954125881195068, 'train/loss': 0.01429128460586071, 'train/mean_average_precision': 0.7713674171966016, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29236731277570904, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27433491412643324, 'test/num_examples': 43793, 'score': 30510.429839372635, 'total_duration': 46731.31958055496, 'accumulated_submission_time': 30510.429839372635, 'accumulated_eval_time': 16213.296025753021, 'accumulated_logging_time': 4.994197368621826}
I0305 23:04:00.644639 140407882708736 logging_writer.py:48] [94469] accumulated_eval_time=16213.296026, accumulated_logging_time=4.994197, accumulated_submission_time=30510.429839, global_step=94469, preemption_count=0, score=30510.429839, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274335, test/num_examples=43793, total_duration=46731.319581, train/accuracy=0.995413, train/loss=0.014291, train/mean_average_precision=0.771367, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292367, validation/num_examples=43793
I0305 23:04:11.133948 140415782377216 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.13169656693935394, loss=0.01855427958071232
I0305 23:04:43.107068 140407882708736 logging_writer.py:48] [94600] global_step=94600, grad_norm=0.13391751050949097, loss=0.0182022787630558
I0305 23:05:14.915129 140415782377216 logging_writer.py:48] [94700] global_step=94700, grad_norm=0.1582481861114502, loss=0.01865842193365097
I0305 23:05:46.672136 140407882708736 logging_writer.py:48] [94800] global_step=94800, grad_norm=0.14999757707118988, loss=0.018414538353681564
I0305 23:06:18.827378 140415782377216 logging_writer.py:48] [94900] global_step=94900, grad_norm=0.1447962522506714, loss=0.01608922705054283
I0305 23:06:50.625091 140407882708736 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.14160266518592834, loss=0.018929261714220047
I0305 23:07:23.554631 140415782377216 logging_writer.py:48] [95100] global_step=95100, grad_norm=0.160109281539917, loss=0.018630029633641243
I0305 23:07:56.661744 140407882708736 logging_writer.py:48] [95200] global_step=95200, grad_norm=0.15489502251148224, loss=0.018467728048563004
I0305 23:08:00.670290 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:09:50.290940 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:09:53.351608 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:09:56.335330 140576608098112 submission_runner.py:411] Time since start: 47087.04s, 	Step: 95213, 	{'train/accuracy': 0.9955213665962219, 'train/loss': 0.014077221043407917, 'train/mean_average_precision': 0.7769381347581835, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2924208216760564, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743825265260335, 'test/num_examples': 43793, 'score': 30750.42320585251, 'total_duration': 47087.043660879135, 'accumulated_submission_time': 30750.42320585251, 'accumulated_eval_time': 16328.961030006409, 'accumulated_logging_time': 5.038869380950928}
I0305 23:09:56.369604 140408968165120 logging_writer.py:48] [95213] accumulated_eval_time=16328.961030, accumulated_logging_time=5.038869, accumulated_submission_time=30750.423206, global_step=95213, preemption_count=0, score=30750.423206, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274383, test/num_examples=43793, total_duration=47087.043661, train/accuracy=0.995521, train/loss=0.014077, train/mean_average_precision=0.776938, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292421, validation/num_examples=43793
I0305 23:10:24.866268 140409291962112 logging_writer.py:48] [95300] global_step=95300, grad_norm=0.13961204886436462, loss=0.018149228766560555
I0305 23:10:57.054292 140408968165120 logging_writer.py:48] [95400] global_step=95400, grad_norm=0.1481269747018814, loss=0.017853135243058205
I0305 23:11:29.362890 140409291962112 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.14216294884681702, loss=0.016944237053394318
I0305 23:12:01.546070 140408968165120 logging_writer.py:48] [95600] global_step=95600, grad_norm=0.14832355082035065, loss=0.01668994314968586
I0305 23:12:33.735386 140409291962112 logging_writer.py:48] [95700] global_step=95700, grad_norm=0.14807969331741333, loss=0.018454065546393394
I0305 23:13:06.206698 140408968165120 logging_writer.py:48] [95800] global_step=95800, grad_norm=0.13383662700653076, loss=0.01675417646765709
I0305 23:13:38.222167 140409291962112 logging_writer.py:48] [95900] global_step=95900, grad_norm=0.15500202775001526, loss=0.01744510792195797
I0305 23:13:56.654118 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:15:43.023010 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:15:46.049045 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:15:49.021315 140576608098112 submission_runner.py:411] Time since start: 47439.73s, 	Step: 95959, 	{'train/accuracy': 0.9954837560653687, 'train/loss': 0.014148159883916378, 'train/mean_average_precision': 0.7681165605917525, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29254407966952445, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743611788337915, 'test/num_examples': 43793, 'score': 30990.67506980896, 'total_duration': 47439.72964859009, 'accumulated_submission_time': 30990.67506980896, 'accumulated_eval_time': 16441.328189611435, 'accumulated_logging_time': 5.085657596588135}
I0305 23:15:49.055275 140407882708736 logging_writer.py:48] [95959] accumulated_eval_time=16441.328190, accumulated_logging_time=5.085658, accumulated_submission_time=30990.675070, global_step=95959, preemption_count=0, score=30990.675070, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274361, test/num_examples=43793, total_duration=47439.729649, train/accuracy=0.995484, train/loss=0.014148, train/mean_average_precision=0.768117, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292544, validation/num_examples=43793
I0305 23:16:02.905082 140415790769920 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.12990278005599976, loss=0.017680447548627853
I0305 23:16:35.409585 140407882708736 logging_writer.py:48] [96100] global_step=96100, grad_norm=0.13586467504501343, loss=0.016417570412158966
I0305 23:17:07.977986 140415790769920 logging_writer.py:48] [96200] global_step=96200, grad_norm=0.14051003754138947, loss=0.016979826614260674
I0305 23:17:39.909207 140407882708736 logging_writer.py:48] [96300] global_step=96300, grad_norm=0.13770988583564758, loss=0.015936510637402534
I0305 23:18:12.817964 140415790769920 logging_writer.py:48] [96400] global_step=96400, grad_norm=0.1358964890241623, loss=0.017429344356060028
I0305 23:18:45.156322 140407882708736 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.13859300315380096, loss=0.018973639234900475
I0305 23:19:17.298683 140415790769920 logging_writer.py:48] [96600] global_step=96600, grad_norm=0.13153165578842163, loss=0.018248645588755608
I0305 23:19:49.115602 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:21:38.064321 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:21:41.203351 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:21:44.194642 140576608098112 submission_runner.py:411] Time since start: 47794.90s, 	Step: 96699, 	{'train/accuracy': 0.9954470992088318, 'train/loss': 0.014258240349590778, 'train/mean_average_precision': 0.7757481490303596, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29237989862936276, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743702992049815, 'test/num_examples': 43793, 'score': 31230.70459461212, 'total_duration': 47794.902975559235, 'accumulated_submission_time': 31230.70459461212, 'accumulated_eval_time': 16556.407190322876, 'accumulated_logging_time': 5.13048243522644}
I0305 23:21:44.228404 140409291962112 logging_writer.py:48] [96699] accumulated_eval_time=16556.407190, accumulated_logging_time=5.130482, accumulated_submission_time=31230.704595, global_step=96699, preemption_count=0, score=31230.704595, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274370, test/num_examples=43793, total_duration=47794.902976, train/accuracy=0.995447, train/loss=0.014258, train/mean_average_precision=0.775748, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292380, validation/num_examples=43793
I0305 23:21:44.909272 140415782377216 logging_writer.py:48] [96700] global_step=96700, grad_norm=0.14059635996818542, loss=0.01680106110870838
I0305 23:22:17.583151 140409291962112 logging_writer.py:48] [96800] global_step=96800, grad_norm=0.14566932618618011, loss=0.018351390957832336
I0305 23:22:49.997683 140415782377216 logging_writer.py:48] [96900] global_step=96900, grad_norm=0.133688285946846, loss=0.018509462475776672
I0305 23:23:22.354135 140409291962112 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.14791589975357056, loss=0.017962416633963585
I0305 23:23:54.849943 140415782377216 logging_writer.py:48] [97100] global_step=97100, grad_norm=0.1443590372800827, loss=0.01791858673095703
I0305 23:24:26.980409 140409291962112 logging_writer.py:48] [97200] global_step=97200, grad_norm=0.1359855979681015, loss=0.016867367550730705
I0305 23:24:58.895752 140415782377216 logging_writer.py:48] [97300] global_step=97300, grad_norm=0.14479373395442963, loss=0.016933590173721313
I0305 23:25:31.190817 140409291962112 logging_writer.py:48] [97400] global_step=97400, grad_norm=0.1398678869009018, loss=0.016904601827263832
I0305 23:25:44.419593 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:27:29.577557 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:27:35.197427 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:27:38.578690 140576608098112 submission_runner.py:411] Time since start: 48149.29s, 	Step: 97442, 	{'train/accuracy': 0.9955335855484009, 'train/loss': 0.01406300812959671, 'train/mean_average_precision': 0.7739189992361805, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924558861565163, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27437706250287347, 'test/num_examples': 43793, 'score': 31470.862909078598, 'total_duration': 48149.28699564934, 'accumulated_submission_time': 31470.862909078598, 'accumulated_eval_time': 16670.566215515137, 'accumulated_logging_time': 5.1765077114105225}
I0305 23:27:38.619062 140408968165120 logging_writer.py:48] [97442] accumulated_eval_time=16670.566216, accumulated_logging_time=5.176508, accumulated_submission_time=31470.862909, global_step=97442, preemption_count=0, score=31470.862909, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274377, test/num_examples=43793, total_duration=48149.286996, train/accuracy=0.995534, train/loss=0.014063, train/mean_average_precision=0.773919, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292456, validation/num_examples=43793
I0305 23:27:58.205590 140415790769920 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.15042626857757568, loss=0.017660394310951233
I0305 23:28:30.650234 140408968165120 logging_writer.py:48] [97600] global_step=97600, grad_norm=0.14653655886650085, loss=0.018336117267608643
I0305 23:29:02.651964 140415790769920 logging_writer.py:48] [97700] global_step=97700, grad_norm=0.14792069792747498, loss=0.020258890464901924
I0305 23:29:34.289913 140408968165120 logging_writer.py:48] [97800] global_step=97800, grad_norm=0.15063811838626862, loss=0.018218453973531723
I0305 23:30:06.197873 140415790769920 logging_writer.py:48] [97900] global_step=97900, grad_norm=0.14673055708408356, loss=0.0177778173238039
I0305 23:30:38.546994 140408968165120 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.153489887714386, loss=0.019763490185141563
I0305 23:31:10.628192 140415790769920 logging_writer.py:48] [98100] global_step=98100, grad_norm=0.13259972631931305, loss=0.01759079098701477
I0305 23:31:38.642788 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:33:25.568151 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:33:28.664582 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:33:31.725642 140576608098112 submission_runner.py:411] Time since start: 48502.43s, 	Step: 98189, 	{'train/accuracy': 0.9955210089683533, 'train/loss': 0.014052504673600197, 'train/mean_average_precision': 0.7769833804928025, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2925534718224171, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27444251426680427, 'test/num_examples': 43793, 'score': 31710.853369951248, 'total_duration': 48502.43397283554, 'accumulated_submission_time': 31710.853369951248, 'accumulated_eval_time': 16783.64903306961, 'accumulated_logging_time': 5.22965407371521}
I0305 23:33:31.760110 140407882708736 logging_writer.py:48] [98189] accumulated_eval_time=16783.649033, accumulated_logging_time=5.229654, accumulated_submission_time=31710.853370, global_step=98189, preemption_count=0, score=31710.853370, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274443, test/num_examples=43793, total_duration=48502.433973, train/accuracy=0.995521, train/loss=0.014053, train/mean_average_precision=0.776983, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292553, validation/num_examples=43793
I0305 23:33:35.698937 140409291962112 logging_writer.py:48] [98200] global_step=98200, grad_norm=0.13164310157299042, loss=0.016996238380670547
I0305 23:34:08.048726 140407882708736 logging_writer.py:48] [98300] global_step=98300, grad_norm=0.13334831595420837, loss=0.018084656447172165
I0305 23:34:40.143588 140409291962112 logging_writer.py:48] [98400] global_step=98400, grad_norm=0.12410064041614532, loss=0.018034247681498528
I0305 23:35:12.234769 140407882708736 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.15000033378601074, loss=0.017117265611886978
I0305 23:35:44.417865 140409291962112 logging_writer.py:48] [98600] global_step=98600, grad_norm=0.12999998033046722, loss=0.0172954760491848
I0305 23:36:16.908420 140407882708736 logging_writer.py:48] [98700] global_step=98700, grad_norm=0.14213956892490387, loss=0.01846005581319332
I0305 23:36:48.678752 140409291962112 logging_writer.py:48] [98800] global_step=98800, grad_norm=0.13963988423347473, loss=0.01705067977309227
I0305 23:37:21.119059 140407882708736 logging_writer.py:48] [98900] global_step=98900, grad_norm=0.14724428951740265, loss=0.015806127339601517
I0305 23:37:31.802222 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:39:21.326266 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:39:24.388230 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:39:27.355474 140576608098112 submission_runner.py:411] Time since start: 48858.06s, 	Step: 98932, 	{'train/accuracy': 0.9954484701156616, 'train/loss': 0.014266662299633026, 'train/mean_average_precision': 0.7728061638280789, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29254070664519594, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27448934080340776, 'test/num_examples': 43793, 'score': 31950.86460542679, 'total_duration': 48858.06380486488, 'accumulated_submission_time': 31950.86460542679, 'accumulated_eval_time': 16899.202238321304, 'accumulated_logging_time': 5.274975776672363}
I0305 23:39:27.389481 140415782377216 logging_writer.py:48] [98932] accumulated_eval_time=16899.202238, accumulated_logging_time=5.274976, accumulated_submission_time=31950.864605, global_step=98932, preemption_count=0, score=31950.864605, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274489, test/num_examples=43793, total_duration=48858.063805, train/accuracy=0.995448, train/loss=0.014267, train/mean_average_precision=0.772806, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292541, validation/num_examples=43793
I0305 23:39:49.428919 140415790769920 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.1284334659576416, loss=0.01626603491604328
I0305 23:40:21.891030 140415782377216 logging_writer.py:48] [99100] global_step=99100, grad_norm=0.1468352973461151, loss=0.018424896523356438
I0305 23:40:54.871726 140415790769920 logging_writer.py:48] [99200] global_step=99200, grad_norm=0.12965819239616394, loss=0.015919659286737442
I0305 23:41:27.895623 140415782377216 logging_writer.py:48] [99300] global_step=99300, grad_norm=0.15776792168617249, loss=0.01977040246129036
I0305 23:42:00.716819 140415790769920 logging_writer.py:48] [99400] global_step=99400, grad_norm=0.14269155263900757, loss=0.01683598756790161
I0305 23:42:32.813744 140415782377216 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.143411785364151, loss=0.0177618395537138
I0305 23:43:04.717664 140415790769920 logging_writer.py:48] [99600] global_step=99600, grad_norm=0.15019449591636658, loss=0.01745106838643551
I0305 23:43:27.578049 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:45:15.361685 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:45:18.400186 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:45:21.396062 140576608098112 submission_runner.py:411] Time since start: 49212.10s, 	Step: 99673, 	{'train/accuracy': 0.9955064058303833, 'train/loss': 0.014039936475455761, 'train/mean_average_precision': 0.769985629437685, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29237428368983875, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27447650860349054, 'test/num_examples': 43793, 'score': 32191.01792359352, 'total_duration': 49212.104393959045, 'accumulated_submission_time': 32191.01792359352, 'accumulated_eval_time': 17013.020210027695, 'accumulated_logging_time': 5.321348190307617}
I0305 23:45:21.430850 140408968165120 logging_writer.py:48] [99673] accumulated_eval_time=17013.020210, accumulated_logging_time=5.321348, accumulated_submission_time=32191.017924, global_step=99673, preemption_count=0, score=32191.017924, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274477, test/num_examples=43793, total_duration=49212.104394, train/accuracy=0.995506, train/loss=0.014040, train/mean_average_precision=0.769986, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292374, validation/num_examples=43793
I0305 23:45:30.274618 140409291962112 logging_writer.py:48] [99700] global_step=99700, grad_norm=0.13695946335792542, loss=0.017167624086141586
I0305 23:46:02.115980 140408968165120 logging_writer.py:48] [99800] global_step=99800, grad_norm=0.14559072256088257, loss=0.015594682656228542
I0305 23:46:34.170916 140409291962112 logging_writer.py:48] [99900] global_step=99900, grad_norm=0.13787494599819183, loss=0.018444878980517387
I0305 23:47:06.365727 140408968165120 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.13407932221889496, loss=0.01690569519996643
I0305 23:47:38.370724 140409291962112 logging_writer.py:48] [100100] global_step=100100, grad_norm=0.14841827750205994, loss=0.018448682501912117
I0305 23:48:10.635375 140408968165120 logging_writer.py:48] [100200] global_step=100200, grad_norm=0.1558104008436203, loss=0.01983209140598774
I0305 23:48:42.776264 140409291962112 logging_writer.py:48] [100300] global_step=100300, grad_norm=0.15856559574604034, loss=0.016526710242033005
I0305 23:49:15.641759 140408968165120 logging_writer.py:48] [100400] global_step=100400, grad_norm=0.17010828852653503, loss=0.01939515210688114
I0305 23:49:21.562391 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:51:09.863368 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:51:12.909787 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:51:15.903909 140576608098112 submission_runner.py:411] Time since start: 49566.61s, 	Step: 100419, 	{'train/accuracy': 0.9954321980476379, 'train/loss': 0.014379989355802536, 'train/mean_average_precision': 0.7721081716111647, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29235791272347406, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743564112248008, 'test/num_examples': 43793, 'score': 32431.11732816696, 'total_duration': 49566.612240314484, 'accumulated_submission_time': 32431.11732816696, 'accumulated_eval_time': 17127.36168217659, 'accumulated_logging_time': 5.368618726730347}
I0305 23:51:15.938583 140407882708736 logging_writer.py:48] [100419] accumulated_eval_time=17127.361682, accumulated_logging_time=5.368619, accumulated_submission_time=32431.117328, global_step=100419, preemption_count=0, score=32431.117328, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274356, test/num_examples=43793, total_duration=49566.612240, train/accuracy=0.995432, train/loss=0.014380, train/mean_average_precision=0.772108, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292358, validation/num_examples=43793
I0305 23:51:42.438512 140415790769920 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.13641254603862762, loss=0.019313553348183632
I0305 23:52:14.509850 140407882708736 logging_writer.py:48] [100600] global_step=100600, grad_norm=0.12950994074344635, loss=0.01653888449072838
I0305 23:52:46.462929 140415790769920 logging_writer.py:48] [100700] global_step=100700, grad_norm=0.14846983551979065, loss=0.018035301938652992
I0305 23:53:18.654343 140407882708736 logging_writer.py:48] [100800] global_step=100800, grad_norm=0.13648159801959991, loss=0.017878783866763115
I0305 23:53:50.783589 140415790769920 logging_writer.py:48] [100900] global_step=100900, grad_norm=0.13398824632167816, loss=0.01753857731819153
I0305 23:54:23.245040 140407882708736 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.13081634044647217, loss=0.01709025539457798
I0305 23:54:55.989314 140415790769920 logging_writer.py:48] [101100] global_step=101100, grad_norm=0.12369883060455322, loss=0.015115108340978622
I0305 23:55:16.014747 140576608098112 spec.py:321] Evaluating on the training split.
I0305 23:56:58.855152 140576608098112 spec.py:333] Evaluating on the validation split.
I0305 23:57:02.097071 140576608098112 spec.py:349] Evaluating on the test split.
I0305 23:57:05.153939 140576608098112 submission_runner.py:411] Time since start: 49915.86s, 	Step: 101162, 	{'train/accuracy': 0.9955034255981445, 'train/loss': 0.014101027511060238, 'train/mean_average_precision': 0.7755480809717403, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29245202083669164, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.274346909907928, 'test/num_examples': 43793, 'score': 32671.162100553513, 'total_duration': 49915.86226391792, 'accumulated_submission_time': 32671.162100553513, 'accumulated_eval_time': 17236.50082373619, 'accumulated_logging_time': 5.4143126010894775}
I0305 23:57:05.188522 140409291962112 logging_writer.py:48] [101162] accumulated_eval_time=17236.500824, accumulated_logging_time=5.414313, accumulated_submission_time=32671.162101, global_step=101162, preemption_count=0, score=32671.162101, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274347, test/num_examples=43793, total_duration=49915.862264, train/accuracy=0.995503, train/loss=0.014101, train/mean_average_precision=0.775548, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292452, validation/num_examples=43793
I0305 23:57:17.917191 140415782377216 logging_writer.py:48] [101200] global_step=101200, grad_norm=0.13172392547130585, loss=0.01740601286292076
I0305 23:57:50.289277 140409291962112 logging_writer.py:48] [101300] global_step=101300, grad_norm=0.15862450003623962, loss=0.017883066087961197
I0305 23:58:22.383835 140415782377216 logging_writer.py:48] [101400] global_step=101400, grad_norm=0.15400229394435883, loss=0.01864723302423954
I0305 23:58:54.523780 140409291962112 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.13347770273685455, loss=0.01697549782693386
I0305 23:59:27.447868 140415782377216 logging_writer.py:48] [101600] global_step=101600, grad_norm=0.1296340823173523, loss=0.015752054750919342
I0306 00:00:00.512828 140409291962112 logging_writer.py:48] [101700] global_step=101700, grad_norm=0.13102255761623383, loss=0.016439899802207947
I0306 00:00:33.090012 140415782377216 logging_writer.py:48] [101800] global_step=101800, grad_norm=0.14935196936130524, loss=0.020112372934818268
I0306 00:01:05.396439 140409291962112 logging_writer.py:48] [101900] global_step=101900, grad_norm=0.1312912255525589, loss=0.019012000411748886
I0306 00:01:05.402312 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:02:47.576915 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:02:50.616248 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:02:53.616324 140576608098112 submission_runner.py:411] Time since start: 50264.32s, 	Step: 101901, 	{'train/accuracy': 0.9954982399940491, 'train/loss': 0.01410677656531334, 'train/mean_average_precision': 0.7797891037776852, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29240091467300344, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743002376678368, 'test/num_examples': 43793, 'score': 32911.343249082565, 'total_duration': 50264.32465529442, 'accumulated_submission_time': 32911.343249082565, 'accumulated_eval_time': 17344.714772701263, 'accumulated_logging_time': 5.459990978240967}
I0306 00:02:53.650698 140407882708736 logging_writer.py:48] [101901] accumulated_eval_time=17344.714773, accumulated_logging_time=5.459991, accumulated_submission_time=32911.343249, global_step=101901, preemption_count=0, score=32911.343249, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274300, test/num_examples=43793, total_duration=50264.324655, train/accuracy=0.995498, train/loss=0.014107, train/mean_average_precision=0.779789, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292401, validation/num_examples=43793
I0306 00:03:25.579531 140408968165120 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.1399119645357132, loss=0.018072694540023804
I0306 00:03:57.010249 140407882708736 logging_writer.py:48] [102100] global_step=102100, grad_norm=0.15993700921535492, loss=0.02113650180399418
I0306 00:04:28.901814 140408968165120 logging_writer.py:48] [102200] global_step=102200, grad_norm=0.14761076867580414, loss=0.019023997709155083
I0306 00:05:00.583661 140407882708736 logging_writer.py:48] [102300] global_step=102300, grad_norm=0.13372333347797394, loss=0.01699959859251976
I0306 00:05:32.579719 140408968165120 logging_writer.py:48] [102400] global_step=102400, grad_norm=0.16848017275333405, loss=0.01833101361989975
I0306 00:06:04.151428 140407882708736 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.15546387434005737, loss=0.02005935087800026
I0306 00:06:35.696723 140408968165120 logging_writer.py:48] [102600] global_step=102600, grad_norm=0.14019499719142914, loss=0.01775745116174221
I0306 00:06:53.788643 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:08:37.420574 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:08:40.447535 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:08:43.442560 140576608098112 submission_runner.py:411] Time since start: 50614.15s, 	Step: 102658, 	{'train/accuracy': 0.9954644441604614, 'train/loss': 0.014184223487973213, 'train/mean_average_precision': 0.7669780497456298, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925304588886242, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27443760555251184, 'test/num_examples': 43793, 'score': 33151.44903755188, 'total_duration': 50614.150799274445, 'accumulated_submission_time': 33151.44903755188, 'accumulated_eval_time': 17454.36855649948, 'accumulated_logging_time': 5.506225824356079}
I0306 00:08:43.477563 140409291962112 logging_writer.py:48] [102658] accumulated_eval_time=17454.368556, accumulated_logging_time=5.506226, accumulated_submission_time=33151.449038, global_step=102658, preemption_count=0, score=33151.449038, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274438, test/num_examples=43793, total_duration=50614.150799, train/accuracy=0.995464, train/loss=0.014184, train/mean_average_precision=0.766978, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292530, validation/num_examples=43793
I0306 00:08:57.238746 140415790769920 logging_writer.py:48] [102700] global_step=102700, grad_norm=0.131084144115448, loss=0.018170004710555077
I0306 00:09:29.124977 140409291962112 logging_writer.py:48] [102800] global_step=102800, grad_norm=0.1407538652420044, loss=0.016544057056307793
I0306 00:10:01.098132 140415790769920 logging_writer.py:48] [102900] global_step=102900, grad_norm=0.12891413271427155, loss=0.015389733947813511
I0306 00:10:32.740818 140409291962112 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.14314962923526764, loss=0.017745062708854675
I0306 00:11:04.882783 140415790769920 logging_writer.py:48] [103100] global_step=103100, grad_norm=0.13991034030914307, loss=0.0170575063675642
I0306 00:11:36.282643 140409291962112 logging_writer.py:48] [103200] global_step=103200, grad_norm=0.12985709309577942, loss=0.016664806753396988
I0306 00:12:08.637304 140415790769920 logging_writer.py:48] [103300] global_step=103300, grad_norm=0.1407979279756546, loss=0.018107781186699867
I0306 00:12:41.339295 140409291962112 logging_writer.py:48] [103400] global_step=103400, grad_norm=0.14446023106575012, loss=0.01761002652347088
I0306 00:12:43.636886 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:14:29.641875 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:14:32.637196 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:14:35.684677 140576608098112 submission_runner.py:411] Time since start: 50966.39s, 	Step: 103408, 	{'train/accuracy': 0.9954658150672913, 'train/loss': 0.01423284038901329, 'train/mean_average_precision': 0.7757999081072839, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925922920047358, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27441528637681967, 'test/num_examples': 43793, 'score': 33391.576731443405, 'total_duration': 50966.39300918579, 'accumulated_submission_time': 33391.576731443405, 'accumulated_eval_time': 17566.41631412506, 'accumulated_logging_time': 5.552358627319336}
I0306 00:14:35.719460 140407882708736 logging_writer.py:48] [103408] accumulated_eval_time=17566.416314, accumulated_logging_time=5.552359, accumulated_submission_time=33391.576731, global_step=103408, preemption_count=0, score=33391.576731, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274415, test/num_examples=43793, total_duration=50966.393009, train/accuracy=0.995466, train/loss=0.014233, train/mean_average_precision=0.775800, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292592, validation/num_examples=43793
I0306 00:15:05.835801 140408968165120 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.1451941728591919, loss=0.01667892560362816
I0306 00:15:37.437277 140407882708736 logging_writer.py:48] [103600] global_step=103600, grad_norm=0.14116241037845612, loss=0.018586095422506332
I0306 00:16:09.473141 140408968165120 logging_writer.py:48] [103700] global_step=103700, grad_norm=0.1363711655139923, loss=0.01669214852154255
I0306 00:16:41.590657 140407882708736 logging_writer.py:48] [103800] global_step=103800, grad_norm=0.14666298031806946, loss=0.01883612386882305
I0306 00:17:13.475376 140408968165120 logging_writer.py:48] [103900] global_step=103900, grad_norm=0.13402403891086578, loss=0.015834536403417587
I0306 00:17:45.147166 140407882708736 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.14643855392932892, loss=0.01807469129562378
I0306 00:18:16.774373 140408968165120 logging_writer.py:48] [104100] global_step=104100, grad_norm=0.1399446576833725, loss=0.01750425435602665
I0306 00:18:35.697073 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:20:21.342093 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:20:24.381604 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:20:27.337187 140576608098112 submission_runner.py:411] Time since start: 51318.05s, 	Step: 104161, 	{'train/accuracy': 0.9954906105995178, 'train/loss': 0.0141477445140481, 'train/mean_average_precision': 0.7707154556254288, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29234054776294394, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744654155294517, 'test/num_examples': 43793, 'score': 33631.52317357063, 'total_duration': 51318.0455186367, 'accumulated_submission_time': 33631.52317357063, 'accumulated_eval_time': 17678.056384563446, 'accumulated_logging_time': 5.598142623901367}
I0306 00:20:27.371993 140415782377216 logging_writer.py:48] [104161] accumulated_eval_time=17678.056385, accumulated_logging_time=5.598143, accumulated_submission_time=33631.523174, global_step=104161, preemption_count=0, score=33631.523174, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274465, test/num_examples=43793, total_duration=51318.045519, train/accuracy=0.995491, train/loss=0.014148, train/mean_average_precision=0.770715, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292341, validation/num_examples=43793
I0306 00:20:40.134019 140415790769920 logging_writer.py:48] [104200] global_step=104200, grad_norm=0.13166135549545288, loss=0.01811560057103634
I0306 00:21:11.950274 140415782377216 logging_writer.py:48] [104300] global_step=104300, grad_norm=0.1340850591659546, loss=0.01611427031457424
I0306 00:21:43.835067 140415790769920 logging_writer.py:48] [104400] global_step=104400, grad_norm=0.1353505402803421, loss=0.0167673509567976
I0306 00:22:15.881219 140415782377216 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.12356726825237274, loss=0.016541052609682083
I0306 00:22:47.685372 140415790769920 logging_writer.py:48] [104600] global_step=104600, grad_norm=0.13070593774318695, loss=0.01756942644715309
I0306 00:23:19.689906 140415782377216 logging_writer.py:48] [104700] global_step=104700, grad_norm=0.14319971203804016, loss=0.017554685473442078
I0306 00:23:51.521712 140415790769920 logging_writer.py:48] [104800] global_step=104800, grad_norm=0.1450732946395874, loss=0.019022954627871513
I0306 00:24:23.555197 140415782377216 logging_writer.py:48] [104900] global_step=104900, grad_norm=0.13174575567245483, loss=0.016982490196824074
I0306 00:24:27.629029 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:26:10.001944 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:26:12.998927 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:26:15.956920 140576608098112 submission_runner.py:411] Time since start: 51666.67s, 	Step: 104914, 	{'train/accuracy': 0.9955100417137146, 'train/loss': 0.014104155823588371, 'train/mean_average_precision': 0.7736689829168021, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2924022043200743, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27439166614816834, 'test/num_examples': 43793, 'score': 33871.74913954735, 'total_duration': 51666.66525363922, 'accumulated_submission_time': 33871.74913954735, 'accumulated_eval_time': 17786.38423061371, 'accumulated_logging_time': 5.643944025039673}
I0306 00:26:15.991977 140407882708736 logging_writer.py:48] [104914] accumulated_eval_time=17786.384231, accumulated_logging_time=5.643944, accumulated_submission_time=33871.749140, global_step=104914, preemption_count=0, score=33871.749140, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274392, test/num_examples=43793, total_duration=51666.665254, train/accuracy=0.995510, train/loss=0.014104, train/mean_average_precision=0.773669, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292402, validation/num_examples=43793
I0306 00:26:44.135475 140408968165120 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.1623574197292328, loss=0.020590970292687416
I0306 00:27:16.411914 140407882708736 logging_writer.py:48] [105100] global_step=105100, grad_norm=0.1368533819913864, loss=0.018414519727230072
I0306 00:27:48.961364 140408968165120 logging_writer.py:48] [105200] global_step=105200, grad_norm=0.13615363836288452, loss=0.01736799068748951
I0306 00:28:22.016361 140407882708736 logging_writer.py:48] [105300] global_step=105300, grad_norm=0.16914105415344238, loss=0.019992748275399208
I0306 00:28:54.798505 140408968165120 logging_writer.py:48] [105400] global_step=105400, grad_norm=0.14638261497020721, loss=0.020032037049531937
I0306 00:29:27.334453 140407882708736 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.1460578888654709, loss=0.018377525731921196
I0306 00:29:59.363153 140408968165120 logging_writer.py:48] [105600] global_step=105600, grad_norm=0.1643458902835846, loss=0.020557885989546776
I0306 00:30:16.098137 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:31:55.576583 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:31:58.644398 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:32:01.668305 140576608098112 submission_runner.py:411] Time since start: 52012.38s, 	Step: 105653, 	{'train/accuracy': 0.9955135583877563, 'train/loss': 0.014115297235548496, 'train/mean_average_precision': 0.7804668605179181, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29266943136731954, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27444814338587953, 'test/num_examples': 43793, 'score': 34111.82308912277, 'total_duration': 52012.376638650894, 'accumulated_submission_time': 34111.82308912277, 'accumulated_eval_time': 17891.954362392426, 'accumulated_logging_time': 5.690004587173462}
I0306 00:32:01.703312 140409291962112 logging_writer.py:48] [105653] accumulated_eval_time=17891.954362, accumulated_logging_time=5.690005, accumulated_submission_time=34111.823089, global_step=105653, preemption_count=0, score=34111.823089, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274448, test/num_examples=43793, total_duration=52012.376639, train/accuracy=0.995514, train/loss=0.014115, train/mean_average_precision=0.780467, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292669, validation/num_examples=43793
I0306 00:32:17.186085 140415782377216 logging_writer.py:48] [105700] global_step=105700, grad_norm=0.14974142611026764, loss=0.018876899033784866
I0306 00:32:49.098346 140409291962112 logging_writer.py:48] [105800] global_step=105800, grad_norm=0.1314113289117813, loss=0.017379995435476303
I0306 00:33:21.462216 140415782377216 logging_writer.py:48] [105900] global_step=105900, grad_norm=0.1360161304473877, loss=0.017922092229127884
I0306 00:33:53.691318 140409291962112 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.14280593395233154, loss=0.017716409638524055
I0306 00:34:25.840481 140415782377216 logging_writer.py:48] [106100] global_step=106100, grad_norm=0.1341000497341156, loss=0.017401473596692085
I0306 00:34:57.676965 140409291962112 logging_writer.py:48] [106200] global_step=106200, grad_norm=0.1304531842470169, loss=0.017265360802412033
I0306 00:35:30.169624 140415782377216 logging_writer.py:48] [106300] global_step=106300, grad_norm=0.13218148052692413, loss=0.018802078440785408
I0306 00:36:01.763677 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:37:48.424288 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:37:51.834899 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:37:55.169722 140576608098112 submission_runner.py:411] Time since start: 52365.88s, 	Step: 106399, 	{'train/accuracy': 0.9954696297645569, 'train/loss': 0.014168737456202507, 'train/mean_average_precision': 0.7746599640365843, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.292687754129531, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27442387966958703, 'test/num_examples': 43793, 'score': 34351.85087966919, 'total_duration': 52365.878015995026, 'accumulated_submission_time': 34351.85087966919, 'accumulated_eval_time': 18005.360337734222, 'accumulated_logging_time': 5.737663269042969}
I0306 00:37:55.210443 140407882708736 logging_writer.py:48] [106399] accumulated_eval_time=18005.360338, accumulated_logging_time=5.737663, accumulated_submission_time=34351.850880, global_step=106399, preemption_count=0, score=34351.850880, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274424, test/num_examples=43793, total_duration=52365.878016, train/accuracy=0.995470, train/loss=0.014169, train/mean_average_precision=0.774660, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292688, validation/num_examples=43793
I0306 00:37:55.916608 140415790769920 logging_writer.py:48] [106400] global_step=106400, grad_norm=0.14230571687221527, loss=0.0203545019030571
I0306 00:38:28.751410 140407882708736 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.14162367582321167, loss=0.018213465809822083
I0306 00:39:01.595880 140415790769920 logging_writer.py:48] [106600] global_step=106600, grad_norm=0.14434032142162323, loss=0.01837524026632309
I0306 00:39:33.620397 140407882708736 logging_writer.py:48] [106700] global_step=106700, grad_norm=0.15181557834148407, loss=0.02039214037358761
I0306 00:40:05.371523 140415790769920 logging_writer.py:48] [106800] global_step=106800, grad_norm=0.14092756807804108, loss=0.01643006131052971
I0306 00:40:37.166590 140407882708736 logging_writer.py:48] [106900] global_step=106900, grad_norm=0.15805824100971222, loss=0.020181868225336075
I0306 00:41:09.121133 140415790769920 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.14686943590641022, loss=0.0189439095556736
I0306 00:41:40.703699 140407882708736 logging_writer.py:48] [107100] global_step=107100, grad_norm=0.13477647304534912, loss=0.017682718113064766
I0306 00:41:55.218250 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:43:42.056475 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:43:45.498225 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:43:48.910410 140576608098112 submission_runner.py:411] Time since start: 52719.62s, 	Step: 107146, 	{'train/accuracy': 0.9955094456672668, 'train/loss': 0.01402148324996233, 'train/mean_average_precision': 0.7750776297005886, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29247950259387323, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27438537112228834, 'test/num_examples': 43793, 'score': 34591.82379245758, 'total_duration': 52719.61870455742, 'accumulated_submission_time': 34591.82379245758, 'accumulated_eval_time': 18119.052418470383, 'accumulated_logging_time': 5.792099714279175}
I0306 00:43:48.951132 140409291962112 logging_writer.py:48] [107146] accumulated_eval_time=18119.052418, accumulated_logging_time=5.792100, accumulated_submission_time=34591.823792, global_step=107146, preemption_count=0, score=34591.823792, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274385, test/num_examples=43793, total_duration=52719.618705, train/accuracy=0.995509, train/loss=0.014021, train/mean_average_precision=0.775078, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292480, validation/num_examples=43793
I0306 00:44:07.367829 140415782377216 logging_writer.py:48] [107200] global_step=107200, grad_norm=0.14713506400585175, loss=0.016707587987184525
I0306 00:44:40.825740 140409291962112 logging_writer.py:48] [107300] global_step=107300, grad_norm=0.17172829806804657, loss=0.021992875263094902
I0306 00:45:14.431337 140415782377216 logging_writer.py:48] [107400] global_step=107400, grad_norm=0.14283417165279388, loss=0.018779708072543144
I0306 00:45:47.519834 140409291962112 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.14318232238292694, loss=0.01752743497490883
I0306 00:46:20.333859 140415782377216 logging_writer.py:48] [107600] global_step=107600, grad_norm=0.13776837289333344, loss=0.017797822132706642
I0306 00:46:52.388571 140409291962112 logging_writer.py:48] [107700] global_step=107700, grad_norm=0.1401888132095337, loss=0.016333071514964104
I0306 00:47:24.677231 140415782377216 logging_writer.py:48] [107800] global_step=107800, grad_norm=0.1427283138036728, loss=0.018714282661676407
I0306 00:47:49.018366 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:49:28.531839 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:49:31.549301 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:49:34.500173 140576608098112 submission_runner.py:411] Time since start: 53065.21s, 	Step: 107877, 	{'train/accuracy': 0.9954585433006287, 'train/loss': 0.014249203726649284, 'train/mean_average_precision': 0.7664326035292928, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925241672229261, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744068619439111, 'test/num_examples': 43793, 'score': 34831.85524511337, 'total_duration': 53065.20850133896, 'accumulated_submission_time': 34831.85524511337, 'accumulated_eval_time': 18224.53417825699, 'accumulated_logging_time': 5.845448970794678}
I0306 00:49:34.535926 140407882708736 logging_writer.py:48] [107877] accumulated_eval_time=18224.534178, accumulated_logging_time=5.845449, accumulated_submission_time=34831.855245, global_step=107877, preemption_count=0, score=34831.855245, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274407, test/num_examples=43793, total_duration=53065.208501, train/accuracy=0.995459, train/loss=0.014249, train/mean_average_precision=0.766433, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292524, validation/num_examples=43793
I0306 00:49:42.327925 140415790769920 logging_writer.py:48] [107900] global_step=107900, grad_norm=0.14749108254909515, loss=0.018818283453583717
I0306 00:50:14.695567 140407882708736 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.14081305265426636, loss=0.01843874715268612
I0306 00:50:46.533528 140415790769920 logging_writer.py:48] [108100] global_step=108100, grad_norm=0.14793726801872253, loss=0.01921001262962818
I0306 00:51:19.350032 140407882708736 logging_writer.py:48] [108200] global_step=108200, grad_norm=0.1486089676618576, loss=0.020076025277376175
I0306 00:51:52.375631 140415790769920 logging_writer.py:48] [108300] global_step=108300, grad_norm=0.11873851716518402, loss=0.01665995828807354
I0306 00:52:25.311864 140407882708736 logging_writer.py:48] [108400] global_step=108400, grad_norm=0.11959090828895569, loss=0.013908122666180134
I0306 00:52:57.917003 140415790769920 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.14893977344036102, loss=0.019804032519459724
I0306 00:53:31.282069 140407882708736 logging_writer.py:48] [108600] global_step=108600, grad_norm=0.14425328373908997, loss=0.018550116568803787
I0306 00:53:34.521258 140576608098112 spec.py:321] Evaluating on the training split.
I0306 00:55:17.998742 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 00:55:21.028464 140576608098112 spec.py:349] Evaluating on the test split.
I0306 00:55:23.973189 140576608098112 submission_runner.py:411] Time since start: 53414.68s, 	Step: 108610, 	{'train/accuracy': 0.9954603314399719, 'train/loss': 0.014225433580577374, 'train/mean_average_precision': 0.7700719906062522, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29244259418498003, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27442914793851464, 'test/num_examples': 43793, 'score': 35071.80486369133, 'total_duration': 53414.68152284622, 'accumulated_submission_time': 35071.80486369133, 'accumulated_eval_time': 18333.98607826233, 'accumulated_logging_time': 5.893911361694336}
I0306 00:55:24.008476 140409291962112 logging_writer.py:48] [108610] accumulated_eval_time=18333.986078, accumulated_logging_time=5.893911, accumulated_submission_time=35071.804864, global_step=108610, preemption_count=0, score=35071.804864, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274429, test/num_examples=43793, total_duration=53414.681523, train/accuracy=0.995460, train/loss=0.014225, train/mean_average_precision=0.770072, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292443, validation/num_examples=43793
I0306 00:55:53.237045 140415782377216 logging_writer.py:48] [108700] global_step=108700, grad_norm=0.14856469631195068, loss=0.019615137949585915
I0306 00:56:25.751461 140409291962112 logging_writer.py:48] [108800] global_step=108800, grad_norm=0.1634674221277237, loss=0.01884406805038452
I0306 00:56:57.740019 140415782377216 logging_writer.py:48] [108900] global_step=108900, grad_norm=0.1523221880197525, loss=0.019865985959768295
I0306 00:57:29.548891 140409291962112 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.12101498991250992, loss=0.01716652326285839
I0306 00:58:01.527507 140415782377216 logging_writer.py:48] [109100] global_step=109100, grad_norm=0.14719609916210175, loss=0.017382582649588585
I0306 00:58:33.367521 140409291962112 logging_writer.py:48] [109200] global_step=109200, grad_norm=0.15056322515010834, loss=0.01838230900466442
I0306 00:59:05.050035 140415782377216 logging_writer.py:48] [109300] global_step=109300, grad_norm=0.14837312698364258, loss=0.018906574696302414
I0306 00:59:24.028301 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:01:08.153638 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:01:11.191994 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:01:14.165797 140576608098112 submission_runner.py:411] Time since start: 53764.87s, 	Step: 109360, 	{'train/accuracy': 0.9955194592475891, 'train/loss': 0.01415284164249897, 'train/mean_average_precision': 0.7760508584807527, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923588761610667, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743291088378314, 'test/num_examples': 43793, 'score': 35311.79198503494, 'total_duration': 53764.87401008606, 'accumulated_submission_time': 35311.79198503494, 'accumulated_eval_time': 18444.12341618538, 'accumulated_logging_time': 5.941523551940918}
I0306 01:01:14.201864 140407882708736 logging_writer.py:48] [109360] accumulated_eval_time=18444.123416, accumulated_logging_time=5.941524, accumulated_submission_time=35311.791985, global_step=109360, preemption_count=0, score=35311.791985, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274329, test/num_examples=43793, total_duration=53764.874010, train/accuracy=0.995519, train/loss=0.014153, train/mean_average_precision=0.776051, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292359, validation/num_examples=43793
I0306 01:01:27.287425 140408968165120 logging_writer.py:48] [109400] global_step=109400, grad_norm=0.13361789286136627, loss=0.018218250945210457
I0306 01:01:59.181937 140407882708736 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.14110153913497925, loss=0.016728475689888
I0306 01:02:30.990896 140408968165120 logging_writer.py:48] [109600] global_step=109600, grad_norm=0.13026300072669983, loss=0.01529520284384489
I0306 01:03:03.229966 140407882708736 logging_writer.py:48] [109700] global_step=109700, grad_norm=0.1524333655834198, loss=0.01792003959417343
I0306 01:03:35.532418 140408968165120 logging_writer.py:48] [109800] global_step=109800, grad_norm=0.16548800468444824, loss=0.02100214548408985
I0306 01:04:07.590872 140407882708736 logging_writer.py:48] [109900] global_step=109900, grad_norm=0.13727715611457825, loss=0.018931545317173004
I0306 01:04:39.822128 140408968165120 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.14816346764564514, loss=0.017084691673517227
I0306 01:05:12.111874 140407882708736 logging_writer.py:48] [110100] global_step=110100, grad_norm=0.14323650300502777, loss=0.01579952985048294
I0306 01:05:14.343740 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:07:03.646011 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:07:06.647287 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:07:09.654834 140576608098112 submission_runner.py:411] Time since start: 54120.36s, 	Step: 110108, 	{'train/accuracy': 0.9955502152442932, 'train/loss': 0.013993489556014538, 'train/mean_average_precision': 0.7846037522654938, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923811446565916, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744111213048259, 'test/num_examples': 43793, 'score': 35551.902698516846, 'total_duration': 54120.363154649734, 'accumulated_submission_time': 35551.902698516846, 'accumulated_eval_time': 18559.434452056885, 'accumulated_logging_time': 5.98870325088501}
I0306 01:07:09.690984 140415782377216 logging_writer.py:48] [110108] accumulated_eval_time=18559.434452, accumulated_logging_time=5.988703, accumulated_submission_time=35551.902699, global_step=110108, preemption_count=0, score=35551.902699, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274411, test/num_examples=43793, total_duration=54120.363155, train/accuracy=0.995550, train/loss=0.013993, train/mean_average_precision=0.784604, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292381, validation/num_examples=43793
I0306 01:07:39.296481 140415790769920 logging_writer.py:48] [110200] global_step=110200, grad_norm=0.1421743929386139, loss=0.016578329727053642
I0306 01:08:11.221858 140415782377216 logging_writer.py:48] [110300] global_step=110300, grad_norm=0.14074236154556274, loss=0.017535515129566193
I0306 01:08:42.895231 140415790769920 logging_writer.py:48] [110400] global_step=110400, grad_norm=0.1358734369277954, loss=0.01691151224076748
I0306 01:09:15.143034 140415782377216 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.15627118945121765, loss=0.019038286060094833
I0306 01:09:46.767169 140415790769920 logging_writer.py:48] [110600] global_step=110600, grad_norm=0.15482614934444427, loss=0.01899285800755024
I0306 01:10:19.090521 140415782377216 logging_writer.py:48] [110700] global_step=110700, grad_norm=0.14356845617294312, loss=0.018981557339429855
I0306 01:10:51.336963 140415790769920 logging_writer.py:48] [110800] global_step=110800, grad_norm=0.1365838646888733, loss=0.017944056540727615
I0306 01:11:09.684376 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:12:55.410693 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:12:58.761265 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:13:02.128407 140576608098112 submission_runner.py:411] Time since start: 54472.84s, 	Step: 110858, 	{'train/accuracy': 0.9954262971878052, 'train/loss': 0.01426724623888731, 'train/mean_average_precision': 0.7683804375932366, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29239247916789796, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744625788209498, 'test/num_examples': 43793, 'score': 35791.86517548561, 'total_duration': 54472.836720466614, 'accumulated_submission_time': 35791.86517548561, 'accumulated_eval_time': 18671.87842154503, 'accumulated_logging_time': 6.035488843917847}
I0306 01:13:02.183313 140407882708736 logging_writer.py:48] [110858] accumulated_eval_time=18671.878422, accumulated_logging_time=6.035489, accumulated_submission_time=35791.865175, global_step=110858, preemption_count=0, score=35791.865175, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274463, test/num_examples=43793, total_duration=54472.836720, train/accuracy=0.995426, train/loss=0.014267, train/mean_average_precision=0.768380, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292392, validation/num_examples=43793
I0306 01:13:15.938509 140409291962112 logging_writer.py:48] [110900] global_step=110900, grad_norm=0.15676619112491608, loss=0.018383342772722244
I0306 01:13:48.682598 140407882708736 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.13163650035858154, loss=0.01723284274339676
I0306 01:14:20.853591 140409291962112 logging_writer.py:48] [111100] global_step=111100, grad_norm=0.15085966885089874, loss=0.019594090059399605
I0306 01:14:52.660604 140407882708736 logging_writer.py:48] [111200] global_step=111200, grad_norm=0.1409633755683899, loss=0.018103618174791336
I0306 01:15:24.909048 140409291962112 logging_writer.py:48] [111300] global_step=111300, grad_norm=0.15260152518749237, loss=0.016769826412200928
I0306 01:15:56.869232 140407882708736 logging_writer.py:48] [111400] global_step=111400, grad_norm=0.14455007016658783, loss=0.01870211958885193
I0306 01:16:28.762796 140409291962112 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.1471690982580185, loss=0.019453512504696846
I0306 01:17:00.819507 140407882708736 logging_writer.py:48] [111600] global_step=111600, grad_norm=0.16089943051338196, loss=0.01884644664824009
I0306 01:17:02.256692 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:18:48.516846 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:18:51.532016 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:18:54.546864 140576608098112 submission_runner.py:411] Time since start: 54825.26s, 	Step: 111605, 	{'train/accuracy': 0.9955164790153503, 'train/loss': 0.014073743484914303, 'train/mean_average_precision': 0.7725581660335283, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29243695629415406, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27443414571882857, 'test/num_examples': 43793, 'score': 36031.905665159225, 'total_duration': 54825.25518202782, 'accumulated_submission_time': 36031.905665159225, 'accumulated_eval_time': 18784.168533802032, 'accumulated_logging_time': 6.103245735168457}
I0306 01:18:54.583108 140408968165120 logging_writer.py:48] [111605] accumulated_eval_time=18784.168534, accumulated_logging_time=6.103246, accumulated_submission_time=36031.905665, global_step=111605, preemption_count=0, score=36031.905665, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274434, test/num_examples=43793, total_duration=54825.255182, train/accuracy=0.995516, train/loss=0.014074, train/mean_average_precision=0.772558, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292437, validation/num_examples=43793
I0306 01:19:25.459201 140415790769920 logging_writer.py:48] [111700] global_step=111700, grad_norm=0.16244809329509735, loss=0.020753683522343636
I0306 01:19:57.336822 140408968165120 logging_writer.py:48] [111800] global_step=111800, grad_norm=0.14377981424331665, loss=0.018283603712916374
I0306 01:20:29.070160 140415790769920 logging_writer.py:48] [111900] global_step=111900, grad_norm=0.16087587177753448, loss=0.02132950723171234
I0306 01:21:00.946812 140408968165120 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.1587449163198471, loss=0.019832409918308258
I0306 01:21:32.943831 140415790769920 logging_writer.py:48] [112100] global_step=112100, grad_norm=0.1452893763780594, loss=0.017025144770741463
I0306 01:22:05.062068 140408968165120 logging_writer.py:48] [112200] global_step=112200, grad_norm=0.15027745068073273, loss=0.01796349510550499
I0306 01:22:37.112704 140415790769920 logging_writer.py:48] [112300] global_step=112300, grad_norm=0.14372317492961884, loss=0.018698357045650482
I0306 01:22:54.694075 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:24:34.954191 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:24:39.932613 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:24:42.980462 140576608098112 submission_runner.py:411] Time since start: 55173.69s, 	Step: 112356, 	{'train/accuracy': 0.9954671859741211, 'train/loss': 0.014209458604454994, 'train/mean_average_precision': 0.7682750228101535, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29241338268043726, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27448496300535913, 'test/num_examples': 43793, 'score': 36271.98492002487, 'total_duration': 55173.6886715889, 'accumulated_submission_time': 36271.98492002487, 'accumulated_eval_time': 18892.454761505127, 'accumulated_logging_time': 6.1505467891693115}
I0306 01:24:43.017434 140407882708736 logging_writer.py:48] [112356] accumulated_eval_time=18892.454762, accumulated_logging_time=6.150547, accumulated_submission_time=36271.984920, global_step=112356, preemption_count=0, score=36271.984920, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274485, test/num_examples=43793, total_duration=55173.688672, train/accuracy=0.995467, train/loss=0.014209, train/mean_average_precision=0.768275, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292413, validation/num_examples=43793
I0306 01:24:57.916630 140409291962112 logging_writer.py:48] [112400] global_step=112400, grad_norm=0.14980687201023102, loss=0.02006862312555313
I0306 01:25:30.209236 140407882708736 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.1312742531299591, loss=0.018048258498311043
I0306 01:26:02.294506 140409291962112 logging_writer.py:48] [112600] global_step=112600, grad_norm=0.1652090698480606, loss=0.020710991695523262
I0306 01:26:34.618724 140407882708736 logging_writer.py:48] [112700] global_step=112700, grad_norm=0.140891432762146, loss=0.018581153824925423
I0306 01:27:06.898088 140409291962112 logging_writer.py:48] [112800] global_step=112800, grad_norm=0.14093278348445892, loss=0.0196254700422287
I0306 01:27:39.029716 140407882708736 logging_writer.py:48] [112900] global_step=112900, grad_norm=0.14788277447223663, loss=0.01910468004643917
I0306 01:28:11.461124 140409291962112 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.15014415979385376, loss=0.018814267590641975
I0306 01:28:43.153561 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:30:21.240437 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:30:24.287148 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:30:27.279145 140576608098112 submission_runner.py:411] Time since start: 55517.99s, 	Step: 113099, 	{'train/accuracy': 0.9955037236213684, 'train/loss': 0.014144578948616982, 'train/mean_average_precision': 0.7743666140906535, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2924845483570632, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2743955276193887, 'test/num_examples': 43793, 'score': 36512.089584350586, 'total_duration': 55517.987456321716, 'accumulated_submission_time': 36512.089584350586, 'accumulated_eval_time': 18996.58028960228, 'accumulated_logging_time': 6.198683500289917}
I0306 01:30:27.315620 140408968165120 logging_writer.py:48] [113099] accumulated_eval_time=18996.580290, accumulated_logging_time=6.198684, accumulated_submission_time=36512.089584, global_step=113099, preemption_count=0, score=36512.089584, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274396, test/num_examples=43793, total_duration=55517.987456, train/accuracy=0.995504, train/loss=0.014145, train/mean_average_precision=0.774367, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292485, validation/num_examples=43793
I0306 01:30:28.023366 140415782377216 logging_writer.py:48] [113100] global_step=113100, grad_norm=0.1461097151041031, loss=0.018717320635914803
I0306 01:31:00.292263 140408968165120 logging_writer.py:48] [113200] global_step=113200, grad_norm=0.13582870364189148, loss=0.018424857407808304
I0306 01:31:32.533430 140415782377216 logging_writer.py:48] [113300] global_step=113300, grad_norm=0.13265080749988556, loss=0.016815923154354095
I0306 01:32:05.290249 140408968165120 logging_writer.py:48] [113400] global_step=113400, grad_norm=0.13525690138339996, loss=0.016297196969389915
I0306 01:32:37.375108 140415782377216 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.15183134377002716, loss=0.017233751714229584
I0306 01:33:09.865136 140408968165120 logging_writer.py:48] [113600] global_step=113600, grad_norm=0.1604919135570526, loss=0.01968647725880146
I0306 01:33:42.223609 140415782377216 logging_writer.py:48] [113700] global_step=113700, grad_norm=0.13661770522594452, loss=0.017924435436725616
I0306 01:34:14.469315 140408968165120 logging_writer.py:48] [113800] global_step=113800, grad_norm=0.15077568590641022, loss=0.01919757016003132
I0306 01:34:27.552684 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:36:07.145441 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:36:10.192042 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:36:15.013746 140576608098112 submission_runner.py:411] Time since start: 55865.72s, 	Step: 113842, 	{'train/accuracy': 0.9955094456672668, 'train/loss': 0.014125462621450424, 'train/mean_average_precision': 0.7793063321714918, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29259664043115974, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743417720933081, 'test/num_examples': 43793, 'score': 36752.29545927048, 'total_duration': 55865.72206258774, 'accumulated_submission_time': 36752.29545927048, 'accumulated_eval_time': 19104.041301965714, 'accumulated_logging_time': 6.246399402618408}
I0306 01:36:15.053158 140409291962112 logging_writer.py:48] [113842] accumulated_eval_time=19104.041302, accumulated_logging_time=6.246399, accumulated_submission_time=36752.295459, global_step=113842, preemption_count=0, score=36752.295459, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274342, test/num_examples=43793, total_duration=55865.722063, train/accuracy=0.995509, train/loss=0.014125, train/mean_average_precision=0.779306, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292597, validation/num_examples=43793
I0306 01:36:34.215651 140415790769920 logging_writer.py:48] [113900] global_step=113900, grad_norm=0.1491204798221588, loss=0.017785653471946716
I0306 01:37:06.747800 140409291962112 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.14450950920581818, loss=0.01971379667520523
I0306 01:37:38.745279 140415790769920 logging_writer.py:48] [114100] global_step=114100, grad_norm=0.14396695792675018, loss=0.017522616311907768
I0306 01:38:10.943663 140409291962112 logging_writer.py:48] [114200] global_step=114200, grad_norm=0.15452414751052856, loss=0.017540762200951576
I0306 01:38:43.110591 140415790769920 logging_writer.py:48] [114300] global_step=114300, grad_norm=0.1481117457151413, loss=0.01849641464650631
I0306 01:39:15.172076 140409291962112 logging_writer.py:48] [114400] global_step=114400, grad_norm=0.13660819828510284, loss=0.016698237508535385
I0306 01:39:47.176678 140415790769920 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.13071812689304352, loss=0.017849788069725037
I0306 01:40:15.294988 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:42:03.370951 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:42:06.423799 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:42:09.492964 140576608098112 submission_runner.py:411] Time since start: 56220.20s, 	Step: 114587, 	{'train/accuracy': 0.9954116940498352, 'train/loss': 0.014296358451247215, 'train/mean_average_precision': 0.7696221359037616, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29236348871934237, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744716788874261, 'test/num_examples': 43793, 'score': 36992.5061249733, 'total_duration': 56220.20129728317, 'accumulated_submission_time': 36992.5061249733, 'accumulated_eval_time': 19218.23925757408, 'accumulated_logging_time': 6.296609163284302}
I0306 01:42:09.529092 140407882708736 logging_writer.py:48] [114587] accumulated_eval_time=19218.239258, accumulated_logging_time=6.296609, accumulated_submission_time=36992.506125, global_step=114587, preemption_count=0, score=36992.506125, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274472, test/num_examples=43793, total_duration=56220.201297, train/accuracy=0.995412, train/loss=0.014296, train/mean_average_precision=0.769622, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292363, validation/num_examples=43793
I0306 01:42:14.199782 140415782377216 logging_writer.py:48] [114600] global_step=114600, grad_norm=0.16015484929084778, loss=0.019769033417105675
I0306 01:42:46.269453 140407882708736 logging_writer.py:48] [114700] global_step=114700, grad_norm=0.17260202765464783, loss=0.019045254215598106
I0306 01:43:18.113069 140415782377216 logging_writer.py:48] [114800] global_step=114800, grad_norm=0.13527587056159973, loss=0.017649175599217415
I0306 01:43:49.936715 140407882708736 logging_writer.py:48] [114900] global_step=114900, grad_norm=0.15982428193092346, loss=0.018645960837602615
I0306 01:44:21.620372 140415782377216 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.14774729311466217, loss=0.019342049956321716
I0306 01:44:53.296064 140407882708736 logging_writer.py:48] [115100] global_step=115100, grad_norm=0.12993088364601135, loss=0.017204077914357185
I0306 01:45:25.172889 140415782377216 logging_writer.py:48] [115200] global_step=115200, grad_norm=0.14953254163265228, loss=0.01756950281560421
I0306 01:45:56.926136 140407882708736 logging_writer.py:48] [115300] global_step=115300, grad_norm=0.14827917516231537, loss=0.017906634137034416
I0306 01:46:09.602439 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:47:48.989585 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:47:52.141903 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:47:57.007662 140576608098112 submission_runner.py:411] Time since start: 56567.72s, 	Step: 115340, 	{'train/accuracy': 0.9955100417137146, 'train/loss': 0.01410288643091917, 'train/mean_average_precision': 0.7763613971947132, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29247796897876904, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744870863371934, 'test/num_examples': 43793, 'score': 37232.54695749283, 'total_duration': 56567.71599459648, 'accumulated_submission_time': 37232.54695749283, 'accumulated_eval_time': 19325.64443707466, 'accumulated_logging_time': 6.345672607421875}
I0306 01:47:57.046113 140409291962112 logging_writer.py:48] [115340] accumulated_eval_time=19325.644437, accumulated_logging_time=6.345673, accumulated_submission_time=37232.546957, global_step=115340, preemption_count=0, score=37232.546957, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274487, test/num_examples=43793, total_duration=56567.715995, train/accuracy=0.995510, train/loss=0.014103, train/mean_average_precision=0.776361, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292478, validation/num_examples=43793
I0306 01:48:16.932496 140415790769920 logging_writer.py:48] [115400] global_step=115400, grad_norm=0.13815325498580933, loss=0.01709713414311409
I0306 01:48:48.872055 140409291962112 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.17825357615947723, loss=0.020659666508436203
I0306 01:49:21.161905 140415790769920 logging_writer.py:48] [115600] global_step=115600, grad_norm=0.1444643884897232, loss=0.017374683171510696
I0306 01:49:53.145164 140409291962112 logging_writer.py:48] [115700] global_step=115700, grad_norm=0.1338580697774887, loss=0.018845824524760246
I0306 01:50:25.683162 140415790769920 logging_writer.py:48] [115800] global_step=115800, grad_norm=0.14960041642189026, loss=0.01806408166885376
I0306 01:50:57.904662 140409291962112 logging_writer.py:48] [115900] global_step=115900, grad_norm=0.1448543518781662, loss=0.017971932888031006
I0306 01:51:29.962457 140415790769920 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.12181774526834488, loss=0.015883011743426323
I0306 01:51:57.103334 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:53:39.144356 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:53:42.247025 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:53:45.256877 140576608098112 submission_runner.py:411] Time since start: 56915.97s, 	Step: 116085, 	{'train/accuracy': 0.9954637289047241, 'train/loss': 0.014224603772163391, 'train/mean_average_precision': 0.7704293389531462, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923070868083733, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744327779240913, 'test/num_examples': 43793, 'score': 37472.57381772995, 'total_duration': 56915.96519422531, 'accumulated_submission_time': 37472.57381772995, 'accumulated_eval_time': 19433.797921419144, 'accumulated_logging_time': 6.394840955734253}
I0306 01:53:45.297759 140407882708736 logging_writer.py:48] [116085] accumulated_eval_time=19433.797921, accumulated_logging_time=6.394841, accumulated_submission_time=37472.573818, global_step=116085, preemption_count=0, score=37472.573818, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274433, test/num_examples=43793, total_duration=56915.965194, train/accuracy=0.995464, train/loss=0.014225, train/mean_average_precision=0.770429, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292307, validation/num_examples=43793
I0306 01:53:50.585824 140415782377216 logging_writer.py:48] [116100] global_step=116100, grad_norm=0.13141301274299622, loss=0.017253650352358818
I0306 01:54:22.706038 140407882708736 logging_writer.py:48] [116200] global_step=116200, grad_norm=0.15021705627441406, loss=0.01828518696129322
I0306 01:54:54.478844 140415782377216 logging_writer.py:48] [116300] global_step=116300, grad_norm=0.13934803009033203, loss=0.017331670969724655
I0306 01:55:26.294958 140407882708736 logging_writer.py:48] [116400] global_step=116400, grad_norm=0.13479824364185333, loss=0.01703065074980259
I0306 01:55:57.991135 140415782377216 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.16253256797790527, loss=0.019115759059786797
I0306 01:56:29.891172 140407882708736 logging_writer.py:48] [116600] global_step=116600, grad_norm=0.1528501808643341, loss=0.01729195937514305
I0306 01:57:01.333909 140415782377216 logging_writer.py:48] [116700] global_step=116700, grad_norm=0.1539449691772461, loss=0.019779851660132408
I0306 01:57:33.331334 140407882708736 logging_writer.py:48] [116800] global_step=116800, grad_norm=0.1494010090827942, loss=0.01693037524819374
I0306 01:57:45.369237 140576608098112 spec.py:321] Evaluating on the training split.
I0306 01:59:27.214313 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 01:59:30.276130 140576608098112 spec.py:349] Evaluating on the test split.
I0306 01:59:33.307017 140576608098112 submission_runner.py:411] Time since start: 57264.02s, 	Step: 116839, 	{'train/accuracy': 0.9954558610916138, 'train/loss': 0.01420577708631754, 'train/mean_average_precision': 0.7722315712608714, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924067980397405, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27447557744963014, 'test/num_examples': 43793, 'score': 37712.61404252052, 'total_duration': 57264.01523518562, 'accumulated_submission_time': 37712.61404252052, 'accumulated_eval_time': 19541.73554468155, 'accumulated_logging_time': 6.446910858154297}
I0306 01:59:33.344705 140409291962112 logging_writer.py:48] [116839] accumulated_eval_time=19541.735545, accumulated_logging_time=6.446911, accumulated_submission_time=37712.614043, global_step=116839, preemption_count=0, score=37712.614043, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274476, test/num_examples=43793, total_duration=57264.015235, train/accuracy=0.995456, train/loss=0.014206, train/mean_average_precision=0.772232, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292407, validation/num_examples=43793
I0306 01:59:53.322569 140415790769920 logging_writer.py:48] [116900] global_step=116900, grad_norm=0.16526921093463898, loss=0.017846155911684036
I0306 02:00:25.398543 140409291962112 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.12996602058410645, loss=0.015498626977205276
I0306 02:00:57.111666 140415790769920 logging_writer.py:48] [117100] global_step=117100, grad_norm=0.14804190397262573, loss=0.018305188044905663
I0306 02:01:29.179806 140409291962112 logging_writer.py:48] [117200] global_step=117200, grad_norm=0.14986826479434967, loss=0.01827682927250862
I0306 02:02:00.879435 140415790769920 logging_writer.py:48] [117300] global_step=117300, grad_norm=0.1662977635860443, loss=0.019599631428718567
I0306 02:02:32.606868 140409291962112 logging_writer.py:48] [117400] global_step=117400, grad_norm=0.13426098227500916, loss=0.020140567794442177
I0306 02:03:04.537401 140415790769920 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.1345752328634262, loss=0.016840757802128792
I0306 02:03:33.526283 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:05:13.625954 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:05:16.616158 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:05:19.641932 140576608098112 submission_runner.py:411] Time since start: 57610.35s, 	Step: 117592, 	{'train/accuracy': 0.995553195476532, 'train/loss': 0.013992968946695328, 'train/mean_average_precision': 0.7718386775875689, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29237223587864664, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27446449738914663, 'test/num_examples': 43793, 'score': 37952.76475858688, 'total_duration': 57610.35025882721, 'accumulated_submission_time': 37952.76475858688, 'accumulated_eval_time': 19647.851148843765, 'accumulated_logging_time': 6.495280981063843}
I0306 02:05:19.679442 140407882708736 logging_writer.py:48] [117592] accumulated_eval_time=19647.851149, accumulated_logging_time=6.495281, accumulated_submission_time=37952.764759, global_step=117592, preemption_count=0, score=37952.764759, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274464, test/num_examples=43793, total_duration=57610.350259, train/accuracy=0.995553, train/loss=0.013993, train/mean_average_precision=0.771839, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292372, validation/num_examples=43793
I0306 02:05:22.652666 140415782377216 logging_writer.py:48] [117600] global_step=117600, grad_norm=0.16218334436416626, loss=0.018474124372005463
I0306 02:05:55.095024 140407882708736 logging_writer.py:48] [117700] global_step=117700, grad_norm=0.14612925052642822, loss=0.01914380118250847
I0306 02:06:27.094172 140415782377216 logging_writer.py:48] [117800] global_step=117800, grad_norm=0.12910433113574982, loss=0.019185923039913177
I0306 02:06:58.888900 140407882708736 logging_writer.py:48] [117900] global_step=117900, grad_norm=0.1671726405620575, loss=0.0205834973603487
I0306 02:07:30.969267 140415782377216 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.13543549180030823, loss=0.018182633444666862
I0306 02:08:02.989804 140407882708736 logging_writer.py:48] [118100] global_step=118100, grad_norm=0.13923969864845276, loss=0.018653567880392075
I0306 02:08:34.883775 140415782377216 logging_writer.py:48] [118200] global_step=118200, grad_norm=0.14528563618659973, loss=0.019820859655737877
I0306 02:09:07.229433 140407882708736 logging_writer.py:48] [118300] global_step=118300, grad_norm=0.1408657729625702, loss=0.013777950778603554
I0306 02:09:19.774328 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:11:11.505943 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:11:14.570364 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:11:17.580717 140576608098112 submission_runner.py:411] Time since start: 57968.29s, 	Step: 118339, 	{'train/accuracy': 0.9954729080200195, 'train/loss': 0.014159295707941055, 'train/mean_average_precision': 0.7737373209301305, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29235214871185716, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27450968454496427, 'test/num_examples': 43793, 'score': 38192.82896399498, 'total_duration': 57968.28904867172, 'accumulated_submission_time': 38192.82896399498, 'accumulated_eval_time': 19765.65750479698, 'accumulated_logging_time': 6.543762683868408}
I0306 02:11:17.618817 140408968165120 logging_writer.py:48] [118339] accumulated_eval_time=19765.657505, accumulated_logging_time=6.543763, accumulated_submission_time=38192.828964, global_step=118339, preemption_count=0, score=38192.828964, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274510, test/num_examples=43793, total_duration=57968.289049, train/accuracy=0.995473, train/loss=0.014159, train/mean_average_precision=0.773737, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292352, validation/num_examples=43793
I0306 02:11:37.406869 140409291962112 logging_writer.py:48] [118400] global_step=118400, grad_norm=0.15386433899402618, loss=0.02038365788757801
I0306 02:12:09.696306 140408968165120 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.11809155344963074, loss=0.016306988894939423
I0306 02:12:41.404160 140409291962112 logging_writer.py:48] [118600] global_step=118600, grad_norm=0.14871783554553986, loss=0.019950704649090767
I0306 02:13:13.526563 140408968165120 logging_writer.py:48] [118700] global_step=118700, grad_norm=0.14114685356616974, loss=0.017562372609972954
I0306 02:13:45.336679 140409291962112 logging_writer.py:48] [118800] global_step=118800, grad_norm=0.12810829281806946, loss=0.01492936909198761
I0306 02:14:17.530510 140408968165120 logging_writer.py:48] [118900] global_step=118900, grad_norm=0.1470627784729004, loss=0.017947105690836906
I0306 02:14:49.676847 140409291962112 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.15563072264194489, loss=0.019426636397838593
I0306 02:15:17.797126 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:17:00.228364 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:17:03.522762 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:17:06.522755 140576608098112 submission_runner.py:411] Time since start: 58317.23s, 	Step: 119088, 	{'train/accuracy': 0.9955180287361145, 'train/loss': 0.014150379225611687, 'train/mean_average_precision': 0.7726563510216342, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2924237165053917, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27455363274127054, 'test/num_examples': 43793, 'score': 38432.975707530975, 'total_duration': 58317.231088876724, 'accumulated_submission_time': 38432.975707530975, 'accumulated_eval_time': 19874.38309621811, 'accumulated_logging_time': 6.593130588531494}
I0306 02:17:06.560222 140407882708736 logging_writer.py:48] [119088] accumulated_eval_time=19874.383096, accumulated_logging_time=6.593131, accumulated_submission_time=38432.975708, global_step=119088, preemption_count=0, score=38432.975708, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274554, test/num_examples=43793, total_duration=58317.231089, train/accuracy=0.995518, train/loss=0.014150, train/mean_average_precision=0.772656, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292424, validation/num_examples=43793
I0306 02:17:10.825042 140415782377216 logging_writer.py:48] [119100] global_step=119100, grad_norm=0.1618766337633133, loss=0.018958989530801773
I0306 02:17:42.676817 140407882708736 logging_writer.py:48] [119200] global_step=119200, grad_norm=0.15083691477775574, loss=0.019313283264636993
I0306 02:18:14.654919 140415782377216 logging_writer.py:48] [119300] global_step=119300, grad_norm=0.1300928294658661, loss=0.016756629571318626
I0306 02:18:46.796584 140407882708736 logging_writer.py:48] [119400] global_step=119400, grad_norm=0.13043732941150665, loss=0.01880395971238613
I0306 02:19:19.154682 140415782377216 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.1588447540998459, loss=0.017271073535084724
I0306 02:19:50.637807 140407882708736 logging_writer.py:48] [119600] global_step=119600, grad_norm=0.16354726254940033, loss=0.019258001819252968
I0306 02:20:22.869488 140415782377216 logging_writer.py:48] [119700] global_step=119700, grad_norm=0.129318505525589, loss=0.01648394577205181
I0306 02:20:54.582587 140407882708736 logging_writer.py:48] [119800] global_step=119800, grad_norm=0.13521094620227814, loss=0.017107421532273293
I0306 02:21:06.750366 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:22:52.411143 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:22:55.537626 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:22:58.539575 140576608098112 submission_runner.py:411] Time since start: 58669.25s, 	Step: 119839, 	{'train/accuracy': 0.9954690933227539, 'train/loss': 0.014174409210681915, 'train/mean_average_precision': 0.7726981784844295, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923949008598069, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27440364839743714, 'test/num_examples': 43793, 'score': 38673.135501623154, 'total_duration': 58669.247754096985, 'accumulated_submission_time': 38673.135501623154, 'accumulated_eval_time': 19986.172110557556, 'accumulated_logging_time': 6.641467571258545}
I0306 02:22:58.576823 140408968165120 logging_writer.py:48] [119839] accumulated_eval_time=19986.172111, accumulated_logging_time=6.641468, accumulated_submission_time=38673.135502, global_step=119839, preemption_count=0, score=38673.135502, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274404, test/num_examples=43793, total_duration=58669.247754, train/accuracy=0.995469, train/loss=0.014174, train/mean_average_precision=0.772698, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292395, validation/num_examples=43793
I0306 02:23:18.482290 140409291962112 logging_writer.py:48] [119900] global_step=119900, grad_norm=0.14768679440021515, loss=0.016616154462099075
I0306 02:23:49.969976 140408968165120 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.12999340891838074, loss=0.017264699563384056
I0306 02:24:22.225873 140409291962112 logging_writer.py:48] [120100] global_step=120100, grad_norm=0.1402444839477539, loss=0.01748616062104702
I0306 02:24:53.980460 140408968165120 logging_writer.py:48] [120200] global_step=120200, grad_norm=0.1463804543018341, loss=0.018315250054001808
I0306 02:25:25.966431 140409291962112 logging_writer.py:48] [120300] global_step=120300, grad_norm=0.14595702290534973, loss=0.017769452184438705
I0306 02:25:58.193102 140408968165120 logging_writer.py:48] [120400] global_step=120400, grad_norm=0.1477162092924118, loss=0.01834387704730034
I0306 02:26:31.306588 140409291962112 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.1439017802476883, loss=0.018202807754278183
I0306 02:26:58.858148 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:28:39.336300 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:28:42.384229 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:28:45.352897 140576608098112 submission_runner.py:411] Time since start: 59016.06s, 	Step: 120584, 	{'train/accuracy': 0.9954341650009155, 'train/loss': 0.014273026026785374, 'train/mean_average_precision': 0.7724481400829802, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29247847086465295, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27443142074903737, 'test/num_examples': 43793, 'score': 38913.38436675072, 'total_duration': 59016.061228990555, 'accumulated_submission_time': 38913.38436675072, 'accumulated_eval_time': 20092.66683936119, 'accumulated_logging_time': 6.6898863315582275}
I0306 02:28:45.391291 140407882708736 logging_writer.py:48] [120584] accumulated_eval_time=20092.666839, accumulated_logging_time=6.689886, accumulated_submission_time=38913.384367, global_step=120584, preemption_count=0, score=38913.384367, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274431, test/num_examples=43793, total_duration=59016.061229, train/accuracy=0.995434, train/loss=0.014273, train/mean_average_precision=0.772448, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292478, validation/num_examples=43793
I0306 02:28:51.264410 140415790769920 logging_writer.py:48] [120600] global_step=120600, grad_norm=0.1356448382139206, loss=0.0167063158005476
I0306 02:29:23.738743 140407882708736 logging_writer.py:48] [120700] global_step=120700, grad_norm=0.1386989802122116, loss=0.01761440932750702
I0306 02:29:56.043166 140415790769920 logging_writer.py:48] [120800] global_step=120800, grad_norm=0.1317209005355835, loss=0.015702899545431137
I0306 02:30:28.219768 140407882708736 logging_writer.py:48] [120900] global_step=120900, grad_norm=0.13539281487464905, loss=0.0178357120603323
I0306 02:31:00.850496 140415790769920 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.13425903022289276, loss=0.017215518280863762
I0306 02:31:32.783596 140407882708736 logging_writer.py:48] [121100] global_step=121100, grad_norm=0.15541471540927887, loss=0.0171903558075428
I0306 02:32:04.607941 140415790769920 logging_writer.py:48] [121200] global_step=121200, grad_norm=0.1391746699810028, loss=0.015606353990733624
I0306 02:32:37.926548 140407882708736 logging_writer.py:48] [121300] global_step=121300, grad_norm=0.13889679312705994, loss=0.017737140879034996
I0306 02:32:45.493112 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:34:34.229419 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:34:37.289272 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:34:40.369291 140576608098112 submission_runner.py:411] Time since start: 59371.08s, 	Step: 121324, 	{'train/accuracy': 0.9955268502235413, 'train/loss': 0.014081204310059547, 'train/mean_average_precision': 0.7726060218973945, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924184000733607, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27444007386440233, 'test/num_examples': 43793, 'score': 39153.452647686005, 'total_duration': 59371.07761955261, 'accumulated_submission_time': 39153.452647686005, 'accumulated_eval_time': 20207.542983531952, 'accumulated_logging_time': 6.741236925125122}
I0306 02:34:40.411806 140408968165120 logging_writer.py:48] [121324] accumulated_eval_time=20207.542984, accumulated_logging_time=6.741237, accumulated_submission_time=39153.452648, global_step=121324, preemption_count=0, score=39153.452648, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274440, test/num_examples=43793, total_duration=59371.077620, train/accuracy=0.995527, train/loss=0.014081, train/mean_average_precision=0.772606, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292418, validation/num_examples=43793
I0306 02:35:05.341911 140409291962112 logging_writer.py:48] [121400] global_step=121400, grad_norm=0.1305921971797943, loss=0.017267677932977676
I0306 02:35:36.884039 140408968165120 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.1621537208557129, loss=0.019803373143076897
I0306 02:36:08.960430 140409291962112 logging_writer.py:48] [121600] global_step=121600, grad_norm=0.1430690437555313, loss=0.018271569162607193
I0306 02:36:40.976983 140408968165120 logging_writer.py:48] [121700] global_step=121700, grad_norm=0.1305970549583435, loss=0.016051171347498894
I0306 02:37:12.646255 140409291962112 logging_writer.py:48] [121800] global_step=121800, grad_norm=0.1464279741048813, loss=0.018361225724220276
I0306 02:37:44.193224 140408968165120 logging_writer.py:48] [121900] global_step=121900, grad_norm=0.12680763006210327, loss=0.01672443188726902
I0306 02:38:16.436176 140409291962112 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.13513192534446716, loss=0.016697747632861137
I0306 02:38:40.456090 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:40:26.932019 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:40:30.025424 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:40:33.014007 140576608098112 submission_runner.py:411] Time since start: 59723.72s, 	Step: 122076, 	{'train/accuracy': 0.9955320358276367, 'train/loss': 0.014074597507715225, 'train/mean_average_precision': 0.7766342497186973, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924523927774579, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.274348726685141, 'test/num_examples': 43793, 'score': 39393.46521615982, 'total_duration': 59723.722333431244, 'accumulated_submission_time': 39393.46521615982, 'accumulated_eval_time': 20320.10085272789, 'accumulated_logging_time': 6.794872522354126}
I0306 02:40:33.052363 140407882708736 logging_writer.py:48] [122076] accumulated_eval_time=20320.100853, accumulated_logging_time=6.794873, accumulated_submission_time=39393.465216, global_step=122076, preemption_count=0, score=39393.465216, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274349, test/num_examples=43793, total_duration=59723.722333, train/accuracy=0.995532, train/loss=0.014075, train/mean_average_precision=0.776634, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292452, validation/num_examples=43793
I0306 02:40:41.291950 140415790769920 logging_writer.py:48] [122100] global_step=122100, grad_norm=0.13327433168888092, loss=0.0174344964325428
I0306 02:41:13.488842 140407882708736 logging_writer.py:48] [122200] global_step=122200, grad_norm=0.1422663778066635, loss=0.01834893971681595
I0306 02:41:45.694284 140415790769920 logging_writer.py:48] [122300] global_step=122300, grad_norm=0.13516287505626678, loss=0.0168137326836586
I0306 02:42:17.878674 140407882708736 logging_writer.py:48] [122400] global_step=122400, grad_norm=0.14695793390274048, loss=0.018793387338519096
I0306 02:42:49.795412 140415790769920 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.1458684355020523, loss=0.018937837332487106
I0306 02:43:21.585238 140407882708736 logging_writer.py:48] [122600] global_step=122600, grad_norm=0.12512530386447906, loss=0.015666648745536804
I0306 02:43:53.398878 140415790769920 logging_writer.py:48] [122700] global_step=122700, grad_norm=0.14129571616649628, loss=0.018540488556027412
I0306 02:44:25.611989 140407882708736 logging_writer.py:48] [122800] global_step=122800, grad_norm=0.14088134467601776, loss=0.017704766243696213
I0306 02:44:33.039835 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:46:17.763263 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:46:20.858493 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:46:23.910131 140576608098112 submission_runner.py:411] Time since start: 60074.62s, 	Step: 122824, 	{'train/accuracy': 0.9954253435134888, 'train/loss': 0.014281541109085083, 'train/mean_average_precision': 0.7695000327559407, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2924960667603323, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744166570766237, 'test/num_examples': 43793, 'score': 39633.420177698135, 'total_duration': 60074.61845064163, 'accumulated_submission_time': 39633.420177698135, 'accumulated_eval_time': 20430.971091747284, 'accumulated_logging_time': 6.845506191253662}
I0306 02:46:23.949244 140408968165120 logging_writer.py:48] [122824] accumulated_eval_time=20430.971092, accumulated_logging_time=6.845506, accumulated_submission_time=39633.420178, global_step=122824, preemption_count=0, score=39633.420178, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274417, test/num_examples=43793, total_duration=60074.618451, train/accuracy=0.995425, train/loss=0.014282, train/mean_average_precision=0.769500, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292496, validation/num_examples=43793
I0306 02:46:48.647800 140415782377216 logging_writer.py:48] [122900] global_step=122900, grad_norm=0.15035836398601532, loss=0.018358787521719933
I0306 02:47:20.942818 140408968165120 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.14621590077877045, loss=0.0188766960054636
I0306 02:47:52.878730 140415782377216 logging_writer.py:48] [123100] global_step=123100, grad_norm=0.13354714214801788, loss=0.01870395429432392
I0306 02:48:25.254674 140408968165120 logging_writer.py:48] [123200] global_step=123200, grad_norm=0.1397048830986023, loss=0.01689254865050316
I0306 02:48:57.010450 140415782377216 logging_writer.py:48] [123300] global_step=123300, grad_norm=0.1465020626783371, loss=0.018166955560445786
I0306 02:49:29.244019 140408968165120 logging_writer.py:48] [123400] global_step=123400, grad_norm=0.13912613689899445, loss=0.015711139887571335
I0306 02:50:00.934444 140415782377216 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.13071753084659576, loss=0.016329223290085793
I0306 02:50:23.935163 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:52:06.608902 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:52:09.957862 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:52:13.324138 140576608098112 submission_runner.py:411] Time since start: 60424.03s, 	Step: 123573, 	{'train/accuracy': 0.9954928755760193, 'train/loss': 0.014117361046373844, 'train/mean_average_precision': 0.7755888247643258, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2924262429932207, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27445403289333853, 'test/num_examples': 43793, 'score': 39873.37377142906, 'total_duration': 60424.03245639801, 'accumulated_submission_time': 39873.37377142906, 'accumulated_eval_time': 20540.36001110077, 'accumulated_logging_time': 6.896385669708252}
I0306 02:52:13.369578 140409291962112 logging_writer.py:48] [123573] accumulated_eval_time=20540.360011, accumulated_logging_time=6.896386, accumulated_submission_time=39873.373771, global_step=123573, preemption_count=0, score=39873.373771, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274454, test/num_examples=43793, total_duration=60424.032456, train/accuracy=0.995493, train/loss=0.014117, train/mean_average_precision=0.775589, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292426, validation/num_examples=43793
I0306 02:52:22.706026 140415790769920 logging_writer.py:48] [123600] global_step=123600, grad_norm=0.15677520632743835, loss=0.018785681575536728
I0306 02:52:55.728788 140409291962112 logging_writer.py:48] [123700] global_step=123700, grad_norm=0.1480683833360672, loss=0.01791044883430004
I0306 02:53:28.781413 140415790769920 logging_writer.py:48] [123800] global_step=123800, grad_norm=0.14060695469379425, loss=0.017243381589651108
I0306 02:54:01.732821 140409291962112 logging_writer.py:48] [123900] global_step=123900, grad_norm=0.1328558772802353, loss=0.017692353576421738
I0306 02:54:34.347368 140415790769920 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.13849091529846191, loss=0.01508395466953516
I0306 02:55:07.210510 140409291962112 logging_writer.py:48] [124100] global_step=124100, grad_norm=0.1489708572626114, loss=0.01866297423839569
I0306 02:55:39.778728 140415790769920 logging_writer.py:48] [124200] global_step=124200, grad_norm=0.14592178165912628, loss=0.018438445404171944
I0306 02:56:12.307153 140409291962112 logging_writer.py:48] [124300] global_step=124300, grad_norm=0.1290615350008011, loss=0.01814388297498226
I0306 02:56:13.556066 140576608098112 spec.py:321] Evaluating on the training split.
I0306 02:57:57.555174 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 02:58:00.587967 140576608098112 spec.py:349] Evaluating on the test split.
I0306 02:58:03.819620 140576608098112 submission_runner.py:411] Time since start: 60774.53s, 	Step: 124305, 	{'train/accuracy': 0.9954410195350647, 'train/loss': 0.01426288764923811, 'train/mean_average_precision': 0.7707968891314905, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2924575249004623, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743404442294432, 'test/num_examples': 43793, 'score': 40113.52621340752, 'total_duration': 60774.52795505524, 'accumulated_submission_time': 40113.52621340752, 'accumulated_eval_time': 20650.623522281647, 'accumulated_logging_time': 6.954053163528442}
I0306 02:58:03.858950 140407882708736 logging_writer.py:48] [124305] accumulated_eval_time=20650.623522, accumulated_logging_time=6.954053, accumulated_submission_time=40113.526213, global_step=124305, preemption_count=0, score=40113.526213, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274340, test/num_examples=43793, total_duration=60774.527955, train/accuracy=0.995441, train/loss=0.014263, train/mean_average_precision=0.770797, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292458, validation/num_examples=43793
I0306 02:58:34.897233 140408968165120 logging_writer.py:48] [124400] global_step=124400, grad_norm=0.12915100157260895, loss=0.019348401576280594
I0306 02:59:06.922302 140407882708736 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.14757686853408813, loss=0.01931311935186386
I0306 02:59:38.935660 140408968165120 logging_writer.py:48] [124600] global_step=124600, grad_norm=0.14569741487503052, loss=0.018315784633159637
I0306 03:00:10.937794 140407882708736 logging_writer.py:48] [124700] global_step=124700, grad_norm=0.14013084769248962, loss=0.019583679735660553
I0306 03:00:42.921018 140408968165120 logging_writer.py:48] [124800] global_step=124800, grad_norm=0.14898833632469177, loss=0.018587470054626465
I0306 03:01:15.118580 140407882708736 logging_writer.py:48] [124900] global_step=124900, grad_norm=0.13551121950149536, loss=0.016584118828177452
I0306 03:01:47.371910 140408968165120 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.1338873654603958, loss=0.017456766217947006
I0306 03:02:03.996190 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:03:47.319905 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:03:50.359341 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:03:53.366985 140576608098112 submission_runner.py:411] Time since start: 61124.08s, 	Step: 125052, 	{'train/accuracy': 0.9955234527587891, 'train/loss': 0.014029952697455883, 'train/mean_average_precision': 0.7771478198568447, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2923724415569426, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744823761778551, 'test/num_examples': 43793, 'score': 40353.63005948067, 'total_duration': 61124.075313568115, 'accumulated_submission_time': 40353.63005948067, 'accumulated_eval_time': 20759.99427127838, 'accumulated_logging_time': 7.006211042404175}
I0306 03:03:53.405766 140409291962112 logging_writer.py:48] [125052] accumulated_eval_time=20759.994271, accumulated_logging_time=7.006211, accumulated_submission_time=40353.630059, global_step=125052, preemption_count=0, score=40353.630059, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274482, test/num_examples=43793, total_duration=61124.075314, train/accuracy=0.995523, train/loss=0.014030, train/mean_average_precision=0.777148, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292372, validation/num_examples=43793
I0306 03:04:09.480687 140415782377216 logging_writer.py:48] [125100] global_step=125100, grad_norm=0.13492374122142792, loss=0.018521593883633614
I0306 03:04:41.914027 140409291962112 logging_writer.py:48] [125200] global_step=125200, grad_norm=0.1575615257024765, loss=0.017557965591549873
I0306 03:05:14.466244 140415782377216 logging_writer.py:48] [125300] global_step=125300, grad_norm=0.14322035014629364, loss=0.017132990062236786
I0306 03:05:46.647711 140409291962112 logging_writer.py:48] [125400] global_step=125400, grad_norm=0.13541290163993835, loss=0.017054075375199318
I0306 03:06:18.990012 140415782377216 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.1624123901128769, loss=0.020678969100117683
I0306 03:06:50.990619 140409291962112 logging_writer.py:48] [125600] global_step=125600, grad_norm=0.14683353900909424, loss=0.01685103215277195
I0306 03:07:23.068317 140415782377216 logging_writer.py:48] [125700] global_step=125700, grad_norm=0.14446578919887543, loss=0.01793077401816845
I0306 03:07:53.538538 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:09:36.533071 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:09:39.597931 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:09:42.640532 140576608098112 submission_runner.py:411] Time since start: 61473.35s, 	Step: 125798, 	{'train/accuracy': 0.9955294132232666, 'train/loss': 0.014070314355194569, 'train/mean_average_precision': 0.7733585278918658, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923784314654618, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744143988411106, 'test/num_examples': 43793, 'score': 40593.73137521744, 'total_duration': 61473.34885716438, 'accumulated_submission_time': 40593.73137521744, 'accumulated_eval_time': 20869.096217632294, 'accumulated_logging_time': 7.056287527084351}
I0306 03:09:42.679469 140407882708736 logging_writer.py:48] [125798] accumulated_eval_time=20869.096218, accumulated_logging_time=7.056288, accumulated_submission_time=40593.731375, global_step=125798, preemption_count=0, score=40593.731375, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274414, test/num_examples=43793, total_duration=61473.348857, train/accuracy=0.995529, train/loss=0.014070, train/mean_average_precision=0.773359, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292378, validation/num_examples=43793
I0306 03:09:43.689091 140408968165120 logging_writer.py:48] [125800] global_step=125800, grad_norm=0.13398447632789612, loss=0.016947081312537193
I0306 03:10:15.448472 140407882708736 logging_writer.py:48] [125900] global_step=125900, grad_norm=0.13290615379810333, loss=0.017572714015841484
I0306 03:10:47.389153 140408968165120 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.12671655416488647, loss=0.016518890857696533
I0306 03:11:19.271211 140407882708736 logging_writer.py:48] [126100] global_step=126100, grad_norm=0.13626842200756073, loss=0.017242809757590294
I0306 03:11:50.795684 140408968165120 logging_writer.py:48] [126200] global_step=126200, grad_norm=0.14557380974292755, loss=0.017606422305107117
I0306 03:12:22.442257 140407882708736 logging_writer.py:48] [126300] global_step=126300, grad_norm=0.12206212431192398, loss=0.01634848862886429
I0306 03:12:54.261645 140408968165120 logging_writer.py:48] [126400] global_step=126400, grad_norm=0.13522395491600037, loss=0.01936975121498108
I0306 03:13:25.863911 140407882708736 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.14777864515781403, loss=0.018968744203448296
I0306 03:13:42.918259 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:15:24.327635 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:15:27.440798 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:15:30.500638 140576608098112 submission_runner.py:411] Time since start: 61821.21s, 	Step: 126555, 	{'train/accuracy': 0.9954558610916138, 'train/loss': 0.01423877663910389, 'train/mean_average_precision': 0.7651018401588172, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29261354077997964, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743924223147777, 'test/num_examples': 43793, 'score': 40833.93912935257, 'total_duration': 61821.20895910263, 'accumulated_submission_time': 40833.93912935257, 'accumulated_eval_time': 20976.678540229797, 'accumulated_logging_time': 7.106051445007324}
I0306 03:15:30.541048 140415782377216 logging_writer.py:48] [126555] accumulated_eval_time=20976.678540, accumulated_logging_time=7.106051, accumulated_submission_time=40833.939129, global_step=126555, preemption_count=0, score=40833.939129, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274392, test/num_examples=43793, total_duration=61821.208959, train/accuracy=0.995456, train/loss=0.014239, train/mean_average_precision=0.765102, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292614, validation/num_examples=43793
I0306 03:15:45.284225 140415790769920 logging_writer.py:48] [126600] global_step=126600, grad_norm=0.15238721668720245, loss=0.018440192565321922
I0306 03:16:17.253259 140415782377216 logging_writer.py:48] [126700] global_step=126700, grad_norm=0.12857697904109955, loss=0.015098455362021923
I0306 03:16:48.869371 140415790769920 logging_writer.py:48] [126800] global_step=126800, grad_norm=0.14159923791885376, loss=0.019116314128041267
I0306 03:17:20.733744 140415782377216 logging_writer.py:48] [126900] global_step=126900, grad_norm=0.15061648190021515, loss=0.017705000936985016
I0306 03:17:52.399570 140415790769920 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.13643969595432281, loss=0.017168007791042328
I0306 03:18:24.155641 140415782377216 logging_writer.py:48] [127100] global_step=127100, grad_norm=0.13531562685966492, loss=0.017797473818063736
I0306 03:18:55.574319 140415790769920 logging_writer.py:48] [127200] global_step=127200, grad_norm=0.16261711716651917, loss=0.018758729100227356
I0306 03:19:27.920231 140415782377216 logging_writer.py:48] [127300] global_step=127300, grad_norm=0.1412985771894455, loss=0.017551811411976814
I0306 03:19:30.823471 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:21:14.071520 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:21:17.413952 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:21:20.721830 140576608098112 submission_runner.py:411] Time since start: 62171.43s, 	Step: 127310, 	{'train/accuracy': 0.9954926371574402, 'train/loss': 0.014150331728160381, 'train/mean_average_precision': 0.7795920817475045, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29254700669340933, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27433930793389333, 'test/num_examples': 43793, 'score': 41074.18945026398, 'total_duration': 62171.43013072014, 'accumulated_submission_time': 41074.18945026398, 'accumulated_eval_time': 21086.576821565628, 'accumulated_logging_time': 7.158508539199829}
I0306 03:21:20.766326 140408968165120 logging_writer.py:48] [127310] accumulated_eval_time=21086.576822, accumulated_logging_time=7.158509, accumulated_submission_time=41074.189450, global_step=127310, preemption_count=0, score=41074.189450, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274339, test/num_examples=43793, total_duration=62171.430131, train/accuracy=0.995493, train/loss=0.014150, train/mean_average_precision=0.779592, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292547, validation/num_examples=43793
I0306 03:21:51.012967 140409291962112 logging_writer.py:48] [127400] global_step=127400, grad_norm=0.13913126289844513, loss=0.01789379119873047
I0306 03:22:24.127623 140408968165120 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.13674820959568024, loss=0.016866708174347878
I0306 03:22:56.636434 140409291962112 logging_writer.py:48] [127600] global_step=127600, grad_norm=0.13164953887462616, loss=0.016937164589762688
I0306 03:23:28.935449 140408968165120 logging_writer.py:48] [127700] global_step=127700, grad_norm=0.15219934284687042, loss=0.020237062126398087
I0306 03:24:01.200400 140409291962112 logging_writer.py:48] [127800] global_step=127800, grad_norm=0.1437751203775406, loss=0.01826777122914791
I0306 03:24:33.461307 140408968165120 logging_writer.py:48] [127900] global_step=127900, grad_norm=0.14276468753814697, loss=0.017717737704515457
I0306 03:25:05.644460 140409291962112 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.13648676872253418, loss=0.019198860973119736
I0306 03:25:21.024667 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:27:04.072789 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:27:07.146559 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:27:10.140661 140576608098112 submission_runner.py:411] Time since start: 62520.85s, 	Step: 128049, 	{'train/accuracy': 0.9954724311828613, 'train/loss': 0.014165924862027168, 'train/mean_average_precision': 0.7580778686618597, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2922960154879298, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744622774438503, 'test/num_examples': 43793, 'score': 41314.413110256195, 'total_duration': 62520.848984479904, 'accumulated_submission_time': 41314.413110256195, 'accumulated_eval_time': 21195.692762613297, 'accumulated_logging_time': 7.215036869049072}
I0306 03:27:10.180121 140407882708736 logging_writer.py:48] [128049] accumulated_eval_time=21195.692763, accumulated_logging_time=7.215037, accumulated_submission_time=41314.413110, global_step=128049, preemption_count=0, score=41314.413110, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274462, test/num_examples=43793, total_duration=62520.848984, train/accuracy=0.995472, train/loss=0.014166, train/mean_average_precision=0.758078, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292296, validation/num_examples=43793
I0306 03:27:26.883930 140415782377216 logging_writer.py:48] [128100] global_step=128100, grad_norm=0.14428681135177612, loss=0.017925266176462173
I0306 03:27:58.910749 140407882708736 logging_writer.py:48] [128200] global_step=128200, grad_norm=0.14335480332374573, loss=0.017388643696904182
I0306 03:28:30.779153 140415782377216 logging_writer.py:48] [128300] global_step=128300, grad_norm=0.14107610285282135, loss=0.017856009304523468
I0306 03:29:03.041969 140407882708736 logging_writer.py:48] [128400] global_step=128400, grad_norm=0.15008297562599182, loss=0.018684284761548042
I0306 03:29:34.830642 140415782377216 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.14333732426166534, loss=0.017163950949907303
I0306 03:30:06.834260 140407882708736 logging_writer.py:48] [128600] global_step=128600, grad_norm=0.13592243194580078, loss=0.015978828072547913
I0306 03:30:38.658673 140415782377216 logging_writer.py:48] [128700] global_step=128700, grad_norm=0.15501786768436432, loss=0.01910904236137867
I0306 03:31:10.376010 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:32:54.820228 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:32:57.922894 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:33:00.932297 140576608098112 submission_runner.py:411] Time since start: 62871.64s, 	Step: 128800, 	{'train/accuracy': 0.9954509735107422, 'train/loss': 0.014306887984275818, 'train/mean_average_precision': 0.772381038299001, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2925018996634979, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27445220662231323, 'test/num_examples': 43793, 'score': 41554.57661104202, 'total_duration': 62871.640630960464, 'accumulated_submission_time': 41554.57661104202, 'accumulated_eval_time': 21306.24901342392, 'accumulated_logging_time': 7.266671419143677}
I0306 03:33:00.971424 140408968165120 logging_writer.py:48] [128800] accumulated_eval_time=21306.249013, accumulated_logging_time=7.266671, accumulated_submission_time=41554.576611, global_step=128800, preemption_count=0, score=41554.576611, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274452, test/num_examples=43793, total_duration=62871.640631, train/accuracy=0.995451, train/loss=0.014307, train/mean_average_precision=0.772381, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292502, validation/num_examples=43793
I0306 03:33:01.450468 140409291962112 logging_writer.py:48] [128800] global_step=128800, grad_norm=0.13094493746757507, loss=0.016970019787549973
I0306 03:33:33.601647 140408968165120 logging_writer.py:48] [128900] global_step=128900, grad_norm=0.152709499001503, loss=0.018576467409729958
I0306 03:34:05.847742 140409291962112 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.15678369998931885, loss=0.021102789789438248
I0306 03:34:38.433119 140408968165120 logging_writer.py:48] [129100] global_step=129100, grad_norm=0.15219275653362274, loss=0.020388804376125336
I0306 03:35:10.992762 140409291962112 logging_writer.py:48] [129200] global_step=129200, grad_norm=0.14337158203125, loss=0.018345333635807037
I0306 03:35:44.071945 140408968165120 logging_writer.py:48] [129300] global_step=129300, grad_norm=0.14183153212070465, loss=0.01884358935058117
I0306 03:36:16.783372 140409291962112 logging_writer.py:48] [129400] global_step=129400, grad_norm=0.13750030100345612, loss=0.01819353550672531
I0306 03:36:49.521277 140408968165120 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.14078925549983978, loss=0.019285690039396286
I0306 03:37:01.160711 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:38:47.034183 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:38:52.837016 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:38:56.184910 140576608098112 submission_runner.py:411] Time since start: 63226.89s, 	Step: 129536, 	{'train/accuracy': 0.9955395460128784, 'train/loss': 0.014033522456884384, 'train/mean_average_precision': 0.7769832884068464, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925394755638397, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744407621982706, 'test/num_examples': 43793, 'score': 41794.73320269585, 'total_duration': 63226.893221616745, 'accumulated_submission_time': 41794.73320269585, 'accumulated_eval_time': 21421.27315235138, 'accumulated_logging_time': 7.3166587352752686}
I0306 03:38:56.232073 140407882708736 logging_writer.py:48] [129536] accumulated_eval_time=21421.273152, accumulated_logging_time=7.316659, accumulated_submission_time=41794.733203, global_step=129536, preemption_count=0, score=41794.733203, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274441, test/num_examples=43793, total_duration=63226.893222, train/accuracy=0.995540, train/loss=0.014034, train/mean_average_precision=0.776983, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292539, validation/num_examples=43793
I0306 03:39:17.591931 140415782377216 logging_writer.py:48] [129600] global_step=129600, grad_norm=0.1313551813364029, loss=0.018404819071292877
I0306 03:39:49.580795 140407882708736 logging_writer.py:48] [129700] global_step=129700, grad_norm=0.16449229419231415, loss=0.019614065065979958
I0306 03:40:22.405496 140415782377216 logging_writer.py:48] [129800] global_step=129800, grad_norm=0.15363577008247375, loss=0.019335174933075905
I0306 03:40:55.357578 140407882708736 logging_writer.py:48] [129900] global_step=129900, grad_norm=0.14914964139461517, loss=0.01994319073855877
I0306 03:41:28.158737 140415782377216 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.14185431599617004, loss=0.018574899062514305
I0306 03:42:00.860644 140407882708736 logging_writer.py:48] [130100] global_step=130100, grad_norm=0.15086640417575836, loss=0.019496925175189972
I0306 03:42:33.555679 140415782377216 logging_writer.py:48] [130200] global_step=130200, grad_norm=0.13167718052864075, loss=0.01988161727786064
I0306 03:42:56.433332 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:44:39.701357 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:44:42.789187 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:44:45.783424 140576608098112 submission_runner.py:411] Time since start: 63576.49s, 	Step: 130272, 	{'train/accuracy': 0.9954925775527954, 'train/loss': 0.014148651622235775, 'train/mean_average_precision': 0.7728318242745001, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2924767408212498, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744175331022145, 'test/num_examples': 43793, 'score': 42034.901480674744, 'total_duration': 63576.49175739288, 'accumulated_submission_time': 42034.901480674744, 'accumulated_eval_time': 21530.62320971489, 'accumulated_logging_time': 7.376136064529419}
I0306 03:44:45.823811 140409291962112 logging_writer.py:48] [130272] accumulated_eval_time=21530.623210, accumulated_logging_time=7.376136, accumulated_submission_time=42034.901481, global_step=130272, preemption_count=0, score=42034.901481, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274418, test/num_examples=43793, total_duration=63576.491757, train/accuracy=0.995493, train/loss=0.014149, train/mean_average_precision=0.772832, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292477, validation/num_examples=43793
I0306 03:44:55.241361 140415790769920 logging_writer.py:48] [130300] global_step=130300, grad_norm=0.1492571383714676, loss=0.018767807632684708
I0306 03:45:27.842683 140409291962112 logging_writer.py:48] [130400] global_step=130400, grad_norm=0.15395669639110565, loss=0.017934530973434448
I0306 03:46:00.258673 140415790769920 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.14859777688980103, loss=0.018932145088911057
I0306 03:46:33.555180 140409291962112 logging_writer.py:48] [130600] global_step=130600, grad_norm=0.13943199813365936, loss=0.01876183971762657
I0306 03:47:06.548028 140415790769920 logging_writer.py:48] [130700] global_step=130700, grad_norm=0.1650865525007248, loss=0.0194248016923666
I0306 03:47:39.642580 140409291962112 logging_writer.py:48] [130800] global_step=130800, grad_norm=0.1553487479686737, loss=0.019661474972963333
I0306 03:48:13.207262 140415790769920 logging_writer.py:48] [130900] global_step=130900, grad_norm=0.14503571391105652, loss=0.018004432320594788
I0306 03:48:45.895183 140409291962112 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.1444510966539383, loss=0.01834852248430252
I0306 03:48:45.900825 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:50:28.815590 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:50:31.928230 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:50:36.985725 140576608098112 submission_runner.py:411] Time since start: 63927.69s, 	Step: 131001, 	{'train/accuracy': 0.9954299330711365, 'train/loss': 0.014200999401509762, 'train/mean_average_precision': 0.7792628467588556, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2925354472598478, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744591741016137, 'test/num_examples': 43793, 'score': 42274.94419336319, 'total_duration': 63927.694029569626, 'accumulated_submission_time': 42274.94419336319, 'accumulated_eval_time': 21641.708025217056, 'accumulated_logging_time': 7.427554130554199}
I0306 03:50:37.027921 140407882708736 logging_writer.py:48] [131001] accumulated_eval_time=21641.708025, accumulated_logging_time=7.427554, accumulated_submission_time=42274.944193, global_step=131001, preemption_count=0, score=42274.944193, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274459, test/num_examples=43793, total_duration=63927.694030, train/accuracy=0.995430, train/loss=0.014201, train/mean_average_precision=0.779263, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292535, validation/num_examples=43793
I0306 03:51:09.120879 140408968165120 logging_writer.py:48] [131100] global_step=131100, grad_norm=0.14893361926078796, loss=0.017760813236236572
I0306 03:51:40.999934 140407882708736 logging_writer.py:48] [131200] global_step=131200, grad_norm=0.14889991283416748, loss=0.018972190096974373
I0306 03:52:13.931145 140408968165120 logging_writer.py:48] [131300] global_step=131300, grad_norm=0.1318977326154709, loss=0.01754002273082733
I0306 03:52:47.049498 140407882708736 logging_writer.py:48] [131400] global_step=131400, grad_norm=0.14970146119594574, loss=0.018399490043520927
I0306 03:53:20.448063 140408968165120 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.12991692125797272, loss=0.01728292554616928
I0306 03:53:53.355401 140407882708736 logging_writer.py:48] [131600] global_step=131600, grad_norm=0.14673446118831635, loss=0.018001848831772804
I0306 03:54:26.148856 140408968165120 logging_writer.py:48] [131700] global_step=131700, grad_norm=0.13573051989078522, loss=0.019062938168644905
I0306 03:54:37.208582 140576608098112 spec.py:321] Evaluating on the training split.
I0306 03:56:22.608782 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 03:56:25.669044 140576608098112 spec.py:349] Evaluating on the test split.
I0306 03:56:28.673480 140576608098112 submission_runner.py:411] Time since start: 64279.38s, 	Step: 131735, 	{'train/accuracy': 0.9955693483352661, 'train/loss': 0.013986327685415745, 'train/mean_average_precision': 0.7674853021467718, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923617745264467, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744910385094342, 'test/num_examples': 43793, 'score': 42515.090604782104, 'total_duration': 64279.381809711456, 'accumulated_submission_time': 42515.090604782104, 'accumulated_eval_time': 21753.172875642776, 'accumulated_logging_time': 7.480916500091553}
I0306 03:56:28.714032 140409291962112 logging_writer.py:48] [131735] accumulated_eval_time=21753.172876, accumulated_logging_time=7.480917, accumulated_submission_time=42515.090605, global_step=131735, preemption_count=0, score=42515.090605, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274491, test/num_examples=43793, total_duration=64279.381810, train/accuracy=0.995569, train/loss=0.013986, train/mean_average_precision=0.767485, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292362, validation/num_examples=43793
I0306 03:56:50.207522 140415790769920 logging_writer.py:48] [131800] global_step=131800, grad_norm=0.14945772290229797, loss=0.019580835476517677
I0306 03:57:23.171121 140409291962112 logging_writer.py:48] [131900] global_step=131900, grad_norm=0.14056281745433807, loss=0.017549313604831696
I0306 03:57:55.503156 140415790769920 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.1322740763425827, loss=0.017461756244301796
I0306 03:58:27.909919 140409291962112 logging_writer.py:48] [132100] global_step=132100, grad_norm=0.15093843638896942, loss=0.019637756049633026
I0306 03:59:00.232306 140415790769920 logging_writer.py:48] [132200] global_step=132200, grad_norm=0.15186083316802979, loss=0.018838101997971535
I0306 03:59:32.862470 140409291962112 logging_writer.py:48] [132300] global_step=132300, grad_norm=0.12448923289775848, loss=0.016128264367580414
I0306 04:00:05.532593 140415790769920 logging_writer.py:48] [132400] global_step=132400, grad_norm=0.13425776362419128, loss=0.01825016178190708
I0306 04:00:28.918998 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:02:10.955363 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:02:14.325495 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:02:20.135353 140576608098112 submission_runner.py:411] Time since start: 64630.84s, 	Step: 132474, 	{'train/accuracy': 0.9954354166984558, 'train/loss': 0.014276333153247833, 'train/mean_average_precision': 0.7732393560158943, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29250812474247545, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27448543423974425, 'test/num_examples': 43793, 'score': 42755.26333451271, 'total_duration': 64630.84366893768, 'accumulated_submission_time': 42755.26333451271, 'accumulated_eval_time': 21864.389171123505, 'accumulated_logging_time': 7.533536434173584}
I0306 04:02:20.182807 140407882708736 logging_writer.py:48] [132474] accumulated_eval_time=21864.389171, accumulated_logging_time=7.533536, accumulated_submission_time=42755.263335, global_step=132474, preemption_count=0, score=42755.263335, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274485, test/num_examples=43793, total_duration=64630.843669, train/accuracy=0.995435, train/loss=0.014276, train/mean_average_precision=0.773239, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292508, validation/num_examples=43793
I0306 04:02:29.391769 140408968165120 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.148452028632164, loss=0.017948409542441368
I0306 04:03:03.097813 140407882708736 logging_writer.py:48] [132600] global_step=132600, grad_norm=0.14450272917747498, loss=0.01760023646056652
I0306 04:03:36.260380 140408968165120 logging_writer.py:48] [132700] global_step=132700, grad_norm=0.1392965018749237, loss=0.01645263098180294
I0306 04:04:09.633424 140407882708736 logging_writer.py:48] [132800] global_step=132800, grad_norm=0.1414952427148819, loss=0.018443813547492027
I0306 04:04:43.790015 140408968165120 logging_writer.py:48] [132900] global_step=132900, grad_norm=0.14012297987937927, loss=0.015830712392926216
I0306 04:05:16.984438 140407882708736 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.13220885396003723, loss=0.016888435930013657
I0306 04:05:50.154123 140408968165120 logging_writer.py:48] [133100] global_step=133100, grad_norm=0.1374817192554474, loss=0.018491106107831
I0306 04:06:20.306475 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:08:04.411074 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:08:07.537941 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:08:10.505211 140576608098112 submission_runner.py:411] Time since start: 64981.21s, 	Step: 133192, 	{'train/accuracy': 0.9955052733421326, 'train/loss': 0.014124082401394844, 'train/mean_average_precision': 0.7706519939588254, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29254423625127207, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745147941177334, 'test/num_examples': 43793, 'score': 42995.351095438, 'total_duration': 64981.21354317665, 'accumulated_submission_time': 42995.351095438, 'accumulated_eval_time': 21974.587879419327, 'accumulated_logging_time': 7.593179702758789}
I0306 04:08:10.546477 140409291962112 logging_writer.py:48] [133192] accumulated_eval_time=21974.587879, accumulated_logging_time=7.593180, accumulated_submission_time=42995.351095, global_step=133192, preemption_count=0, score=42995.351095, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274515, test/num_examples=43793, total_duration=64981.213543, train/accuracy=0.995505, train/loss=0.014124, train/mean_average_precision=0.770652, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292544, validation/num_examples=43793
I0306 04:08:13.441244 140415782377216 logging_writer.py:48] [133200] global_step=133200, grad_norm=0.1529322862625122, loss=0.020253190770745277
I0306 04:08:45.187953 140409291962112 logging_writer.py:48] [133300] global_step=133300, grad_norm=0.14179977774620056, loss=0.016112474724650383
I0306 04:09:17.869328 140415782377216 logging_writer.py:48] [133400] global_step=133400, grad_norm=0.15401913225650787, loss=0.017255190759897232
I0306 04:09:50.011169 140409291962112 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.13311778008937836, loss=0.017573446035385132
I0306 04:10:22.232390 140415782377216 logging_writer.py:48] [133600] global_step=133600, grad_norm=0.1374642699956894, loss=0.017690857872366905
I0306 04:10:54.372073 140409291962112 logging_writer.py:48] [133700] global_step=133700, grad_norm=0.1414799988269806, loss=0.019940150901675224
I0306 04:11:26.740246 140415782377216 logging_writer.py:48] [133800] global_step=133800, grad_norm=0.13575105369091034, loss=0.01579945906996727
I0306 04:11:58.846205 140409291962112 logging_writer.py:48] [133900] global_step=133900, grad_norm=0.15034066140651703, loss=0.016516506671905518
I0306 04:12:10.542611 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:13:50.403007 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:13:53.420936 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:13:56.379215 140576608098112 submission_runner.py:411] Time since start: 65327.09s, 	Step: 133937, 	{'train/accuracy': 0.9955182671546936, 'train/loss': 0.01407657004892826, 'train/mean_average_precision': 0.7777763457581376, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.292485678695066, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744378650368807, 'test/num_examples': 43793, 'score': 43235.31395435333, 'total_duration': 65327.087550640106, 'accumulated_submission_time': 43235.31395435333, 'accumulated_eval_time': 22080.42444038391, 'accumulated_logging_time': 7.647066116333008}
I0306 04:13:56.420448 140407882708736 logging_writer.py:48] [133937] accumulated_eval_time=22080.424440, accumulated_logging_time=7.647066, accumulated_submission_time=43235.313954, global_step=133937, preemption_count=0, score=43235.313954, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274438, test/num_examples=43793, total_duration=65327.087551, train/accuracy=0.995518, train/loss=0.014077, train/mean_average_precision=0.777776, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292486, validation/num_examples=43793
I0306 04:14:17.011463 140415790769920 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.13276222348213196, loss=0.01745573990046978
I0306 04:14:49.265336 140407882708736 logging_writer.py:48] [134100] global_step=134100, grad_norm=0.13792292773723602, loss=0.016705229878425598
I0306 04:15:21.581314 140415790769920 logging_writer.py:48] [134200] global_step=134200, grad_norm=0.16756810247898102, loss=0.01920751854777336
I0306 04:15:53.334323 140407882708736 logging_writer.py:48] [134300] global_step=134300, grad_norm=0.14348381757736206, loss=0.016877084970474243
I0306 04:16:25.505871 140415790769920 logging_writer.py:48] [134400] global_step=134400, grad_norm=0.1511368602514267, loss=0.017341021448373795
I0306 04:16:57.439786 140407882708736 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.15055742859840393, loss=0.01702132448554039
I0306 04:17:29.648324 140415790769920 logging_writer.py:48] [134600] global_step=134600, grad_norm=0.14617960155010223, loss=0.017092125490307808
I0306 04:17:56.435899 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:19:38.283235 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:19:41.316740 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:19:44.288770 140576608098112 submission_runner.py:411] Time since start: 65675.00s, 	Step: 134684, 	{'train/accuracy': 0.9954232573509216, 'train/loss': 0.014325281605124474, 'train/mean_average_precision': 0.7749691994204116, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.292514974609005, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27446997170309995, 'test/num_examples': 43793, 'score': 43475.29788017273, 'total_duration': 65674.99710512161, 'accumulated_submission_time': 43475.29788017273, 'accumulated_eval_time': 22188.27728843689, 'accumulated_logging_time': 7.6993937492370605}
I0306 04:19:44.329341 140409291962112 logging_writer.py:48] [134684] accumulated_eval_time=22188.277288, accumulated_logging_time=7.699394, accumulated_submission_time=43475.297880, global_step=134684, preemption_count=0, score=43475.297880, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274470, test/num_examples=43793, total_duration=65674.997105, train/accuracy=0.995423, train/loss=0.014325, train/mean_average_precision=0.774969, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292515, validation/num_examples=43793
I0306 04:19:50.325776 140415782377216 logging_writer.py:48] [134700] global_step=134700, grad_norm=0.14209382236003876, loss=0.01687229424715042
I0306 04:20:23.393060 140409291962112 logging_writer.py:48] [134800] global_step=134800, grad_norm=0.1495693475008011, loss=0.020023521035909653
I0306 04:20:55.416676 140415782377216 logging_writer.py:48] [134900] global_step=134900, grad_norm=0.13981454074382782, loss=0.017701638862490654
I0306 04:21:27.652116 140409291962112 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.14235922694206238, loss=0.017332352697849274
I0306 04:21:59.319827 140415782377216 logging_writer.py:48] [135100] global_step=135100, grad_norm=0.13691329956054688, loss=0.01876720041036606
I0306 04:22:31.147065 140409291962112 logging_writer.py:48] [135200] global_step=135200, grad_norm=0.1339397132396698, loss=0.018474094569683075
I0306 04:23:02.912744 140415782377216 logging_writer.py:48] [135300] global_step=135300, grad_norm=0.13751141726970673, loss=0.017370980232954025
I0306 04:23:34.692148 140409291962112 logging_writer.py:48] [135400] global_step=135400, grad_norm=0.13436220586299896, loss=0.016968779265880585
I0306 04:23:44.365569 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:25:20.244756 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:25:23.284259 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:25:28.175763 140576608098112 submission_runner.py:411] Time since start: 66018.88s, 	Step: 135432, 	{'train/accuracy': 0.9955505132675171, 'train/loss': 0.013994750566780567, 'train/mean_average_precision': 0.771572487802147, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924638056018725, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743817458990037, 'test/num_examples': 43793, 'score': 43715.30280971527, 'total_duration': 66018.88409805298, 'accumulated_submission_time': 43715.30280971527, 'accumulated_eval_time': 22292.08743953705, 'accumulated_logging_time': 7.750726938247681}
I0306 04:25:28.219686 140408968165120 logging_writer.py:48] [135432] accumulated_eval_time=22292.087440, accumulated_logging_time=7.750727, accumulated_submission_time=43715.302810, global_step=135432, preemption_count=0, score=43715.302810, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274382, test/num_examples=43793, total_duration=66018.884098, train/accuracy=0.995551, train/loss=0.013995, train/mean_average_precision=0.771572, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292464, validation/num_examples=43793
I0306 04:25:50.245544 140415790769920 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.13051234185695648, loss=0.017467860132455826
I0306 04:26:22.199018 140408968165120 logging_writer.py:48] [135600] global_step=135600, grad_norm=0.13754570484161377, loss=0.01607682555913925
I0306 04:26:54.244100 140415790769920 logging_writer.py:48] [135700] global_step=135700, grad_norm=0.15139563381671906, loss=0.016804851591587067
I0306 04:27:26.013440 140408968165120 logging_writer.py:48] [135800] global_step=135800, grad_norm=0.15267197787761688, loss=0.01990327052772045
I0306 04:27:57.787348 140415790769920 logging_writer.py:48] [135900] global_step=135900, grad_norm=0.1602121889591217, loss=0.019523056223988533
I0306 04:28:30.814075 140408968165120 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.126792773604393, loss=0.015952089801430702
I0306 04:29:03.869798 140415790769920 logging_writer.py:48] [136100] global_step=136100, grad_norm=0.13159923255443573, loss=0.016191253438591957
I0306 04:29:28.187005 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:31:12.333676 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:31:15.845953 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:31:19.232471 140576608098112 submission_runner.py:411] Time since start: 66369.94s, 	Step: 136175, 	{'train/accuracy': 0.9954281449317932, 'train/loss': 0.014341083355247974, 'train/mean_average_precision': 0.7698603206108076, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29232840045652947, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27431651393146184, 'test/num_examples': 43793, 'score': 43955.23675751686, 'total_duration': 66369.940782547, 'accumulated_submission_time': 43955.23675751686, 'accumulated_eval_time': 22403.13285589218, 'accumulated_logging_time': 7.805722713470459}
I0306 04:31:19.278059 140409291962112 logging_writer.py:48] [136175] accumulated_eval_time=22403.132856, accumulated_logging_time=7.805723, accumulated_submission_time=43955.236758, global_step=136175, preemption_count=0, score=43955.236758, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274317, test/num_examples=43793, total_duration=66369.940783, train/accuracy=0.995428, train/loss=0.014341, train/mean_average_precision=0.769860, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292328, validation/num_examples=43793
I0306 04:31:27.880720 140415782377216 logging_writer.py:48] [136200] global_step=136200, grad_norm=0.1359739452600479, loss=0.015825053676962852
I0306 04:32:00.896346 140409291962112 logging_writer.py:48] [136300] global_step=136300, grad_norm=0.15552888810634613, loss=0.018245482817292213
I0306 04:32:33.355800 140415782377216 logging_writer.py:48] [136400] global_step=136400, grad_norm=0.1381901353597641, loss=0.018062392249703407
I0306 04:33:05.712734 140409291962112 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.15741311013698578, loss=0.019158056005835533
I0306 04:33:38.316491 140415782377216 logging_writer.py:48] [136600] global_step=136600, grad_norm=0.15281352400779724, loss=0.018241483718156815
I0306 04:34:10.530879 140409291962112 logging_writer.py:48] [136700] global_step=136700, grad_norm=0.15592730045318604, loss=0.018815046176314354
I0306 04:34:42.409123 140415782377216 logging_writer.py:48] [136800] global_step=136800, grad_norm=0.1366511732339859, loss=0.01914110593497753
I0306 04:35:14.360675 140409291962112 logging_writer.py:48] [136900] global_step=136900, grad_norm=0.14724929630756378, loss=0.017547020688652992
I0306 04:35:19.529189 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:36:58.538048 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:37:01.592151 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:37:04.647939 140576608098112 submission_runner.py:411] Time since start: 66715.36s, 	Step: 136917, 	{'train/accuracy': 0.9954986572265625, 'train/loss': 0.0141018470749259, 'train/mean_average_precision': 0.7790549032148038, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29253486056951195, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27440153372438647, 'test/num_examples': 43793, 'score': 44195.45515322685, 'total_duration': 66715.35626888275, 'accumulated_submission_time': 44195.45515322685, 'accumulated_eval_time': 22508.251557588577, 'accumulated_logging_time': 7.863274812698364}
I0306 04:37:04.689644 140407882708736 logging_writer.py:48] [136917] accumulated_eval_time=22508.251558, accumulated_logging_time=7.863275, accumulated_submission_time=44195.455153, global_step=136917, preemption_count=0, score=44195.455153, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274402, test/num_examples=43793, total_duration=66715.356269, train/accuracy=0.995499, train/loss=0.014102, train/mean_average_precision=0.779055, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292535, validation/num_examples=43793
I0306 04:37:31.502686 140415790769920 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.14662674069404602, loss=0.01668001152575016
I0306 04:38:03.566578 140407882708736 logging_writer.py:48] [137100] global_step=137100, grad_norm=0.13457989692687988, loss=0.016397640109062195
I0306 04:38:35.524978 140415790769920 logging_writer.py:48] [137200] global_step=137200, grad_norm=0.13794972002506256, loss=0.0190602857619524
I0306 04:39:07.677422 140407882708736 logging_writer.py:48] [137300] global_step=137300, grad_norm=0.16483594477176666, loss=0.019355032593011856
I0306 04:39:40.549987 140415790769920 logging_writer.py:48] [137400] global_step=137400, grad_norm=0.1377466917037964, loss=0.01869848184287548
I0306 04:40:14.324872 140407882708736 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.1307487189769745, loss=0.017288338392972946
I0306 04:40:46.713118 140415790769920 logging_writer.py:48] [137600] global_step=137600, grad_norm=0.1415141522884369, loss=0.01943444460630417
I0306 04:41:04.937664 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:42:46.915825 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:42:49.933093 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:42:52.884723 140576608098112 submission_runner.py:411] Time since start: 67063.59s, 	Step: 137657, 	{'train/accuracy': 0.9955148100852966, 'train/loss': 0.014121757820248604, 'train/mean_average_precision': 0.7732282225552818, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2923614648157295, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27438127154180153, 'test/num_examples': 43793, 'score': 44435.672504901886, 'total_duration': 67063.59295868874, 'accumulated_submission_time': 44435.672504901886, 'accumulated_eval_time': 22616.19847869873, 'accumulated_logging_time': 7.916009187698364}
I0306 04:42:52.926897 140408968165120 logging_writer.py:48] [137657] accumulated_eval_time=22616.198479, accumulated_logging_time=7.916009, accumulated_submission_time=44435.672505, global_step=137657, preemption_count=0, score=44435.672505, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274381, test/num_examples=43793, total_duration=67063.592959, train/accuracy=0.995515, train/loss=0.014122, train/mean_average_precision=0.773228, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292361, validation/num_examples=43793
I0306 04:43:07.448630 140409291962112 logging_writer.py:48] [137700] global_step=137700, grad_norm=0.1349753439426422, loss=0.019435185939073563
I0306 04:43:39.258800 140408968165120 logging_writer.py:48] [137800] global_step=137800, grad_norm=0.13741271197795868, loss=0.0169740729033947
I0306 04:44:11.604067 140409291962112 logging_writer.py:48] [137900] global_step=137900, grad_norm=0.16853535175323486, loss=0.01871967688202858
I0306 04:44:43.647499 140408968165120 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.14422857761383057, loss=0.01654175855219364
I0306 04:45:15.942582 140409291962112 logging_writer.py:48] [138100] global_step=138100, grad_norm=0.144459530711174, loss=0.01866116188466549
I0306 04:45:47.946010 140408968165120 logging_writer.py:48] [138200] global_step=138200, grad_norm=0.13653726875782013, loss=0.016885336488485336
I0306 04:46:20.190704 140409291962112 logging_writer.py:48] [138300] global_step=138300, grad_norm=0.14812380075454712, loss=0.018675662577152252
I0306 04:46:52.159755 140408968165120 logging_writer.py:48] [138400] global_step=138400, grad_norm=0.16745929419994354, loss=0.019025715067982674
I0306 04:46:53.119085 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:48:33.879664 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:48:36.853349 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:48:41.786598 140576608098112 submission_runner.py:411] Time since start: 67412.49s, 	Step: 138404, 	{'train/accuracy': 0.9954774975776672, 'train/loss': 0.014152494259178638, 'train/mean_average_precision': 0.7739244373319915, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923292070342875, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27460928366193954, 'test/num_examples': 43793, 'score': 44675.83380961418, 'total_duration': 67412.49493050575, 'accumulated_submission_time': 44675.83380961418, 'accumulated_eval_time': 22724.865947008133, 'accumulated_logging_time': 7.969079494476318}
I0306 04:48:41.829414 140407882708736 logging_writer.py:48] [138404] accumulated_eval_time=22724.865947, accumulated_logging_time=7.969079, accumulated_submission_time=44675.833810, global_step=138404, preemption_count=0, score=44675.833810, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274609, test/num_examples=43793, total_duration=67412.494931, train/accuracy=0.995477, train/loss=0.014152, train/mean_average_precision=0.773924, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292329, validation/num_examples=43793
I0306 04:49:13.285103 140415790769920 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.13610205054283142, loss=0.018434375524520874
I0306 04:49:45.500960 140407882708736 logging_writer.py:48] [138600] global_step=138600, grad_norm=0.14499695599079132, loss=0.018073949962854385
I0306 04:50:17.593760 140415790769920 logging_writer.py:48] [138700] global_step=138700, grad_norm=0.12629497051239014, loss=0.01590995490550995
I0306 04:50:49.595452 140407882708736 logging_writer.py:48] [138800] global_step=138800, grad_norm=0.1294780820608139, loss=0.017078999429941177
I0306 04:51:21.447256 140415790769920 logging_writer.py:48] [138900] global_step=138900, grad_norm=0.17032696306705475, loss=0.018156148493289948
I0306 04:51:53.318579 140407882708736 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.14741086959838867, loss=0.016930527985095978
I0306 04:52:25.251559 140415790769920 logging_writer.py:48] [139100] global_step=139100, grad_norm=0.14404678344726562, loss=0.018631530925631523
I0306 04:52:41.905054 140576608098112 spec.py:321] Evaluating on the training split.
I0306 04:54:23.352219 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 04:54:26.368918 140576608098112 spec.py:349] Evaluating on the test split.
I0306 04:54:29.349191 140576608098112 submission_runner.py:411] Time since start: 67760.06s, 	Step: 139153, 	{'train/accuracy': 0.9954907298088074, 'train/loss': 0.014138809405267239, 'train/mean_average_precision': 0.7785713528209731, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2926596141909245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27440208056238247, 'test/num_examples': 43793, 'score': 44915.87771701813, 'total_duration': 67760.05751276016, 'accumulated_submission_time': 44915.87771701813, 'accumulated_eval_time': 22832.310029745102, 'accumulated_logging_time': 8.023373365402222}
I0306 04:54:29.397080 140408968165120 logging_writer.py:48] [139153] accumulated_eval_time=22832.310030, accumulated_logging_time=8.023373, accumulated_submission_time=44915.877717, global_step=139153, preemption_count=0, score=44915.877717, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274402, test/num_examples=43793, total_duration=67760.057513, train/accuracy=0.995491, train/loss=0.014139, train/mean_average_precision=0.778571, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292660, validation/num_examples=43793
I0306 04:54:44.965391 140409291962112 logging_writer.py:48] [139200] global_step=139200, grad_norm=0.1548072099685669, loss=0.019187768921256065
I0306 04:55:17.367478 140408968165120 logging_writer.py:48] [139300] global_step=139300, grad_norm=0.13911943137645721, loss=0.018075237050652504
I0306 04:55:49.054938 140409291962112 logging_writer.py:48] [139400] global_step=139400, grad_norm=0.14159919321537018, loss=0.020140621811151505
I0306 04:56:21.281642 140408968165120 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.15043549239635468, loss=0.017481883987784386
I0306 04:56:53.160634 140409291962112 logging_writer.py:48] [139600] global_step=139600, grad_norm=0.13173170387744904, loss=0.01792430877685547
I0306 04:57:25.181840 140408968165120 logging_writer.py:48] [139700] global_step=139700, grad_norm=0.14602744579315186, loss=0.01740768551826477
I0306 04:57:57.667135 140409291962112 logging_writer.py:48] [139800] global_step=139800, grad_norm=0.139805406332016, loss=0.01780976913869381
I0306 04:58:29.634046 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:00:10.141849 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:00:13.245327 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:00:16.249064 140576608098112 submission_runner.py:411] Time since start: 68106.96s, 	Step: 139900, 	{'train/accuracy': 0.995424747467041, 'train/loss': 0.014287000522017479, 'train/mean_average_precision': 0.761383769673432, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29253140705219904, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744188849967219, 'test/num_examples': 43793, 'score': 45156.082431316376, 'total_duration': 68106.95738077164, 'accumulated_submission_time': 45156.082431316376, 'accumulated_eval_time': 22938.924989938736, 'accumulated_logging_time': 8.08329463005066}
I0306 05:00:16.292104 140407882708736 logging_writer.py:48] [139900] accumulated_eval_time=22938.924990, accumulated_logging_time=8.083295, accumulated_submission_time=45156.082431, global_step=139900, preemption_count=0, score=45156.082431, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274419, test/num_examples=43793, total_duration=68106.957381, train/accuracy=0.995425, train/loss=0.014287, train/mean_average_precision=0.761384, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292531, validation/num_examples=43793
I0306 05:00:16.673561 140415790769920 logging_writer.py:48] [139900] global_step=139900, grad_norm=0.16506634652614594, loss=0.017902730032801628
I0306 05:00:49.199733 140407882708736 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.1648743599653244, loss=0.01908436045050621
I0306 05:01:21.560704 140415790769920 logging_writer.py:48] [140100] global_step=140100, grad_norm=0.1500059813261032, loss=0.019612131640315056
I0306 05:01:53.774997 140407882708736 logging_writer.py:48] [140200] global_step=140200, grad_norm=0.1459255963563919, loss=0.015334929339587688
I0306 05:02:26.294208 140415790769920 logging_writer.py:48] [140300] global_step=140300, grad_norm=0.14933878183364868, loss=0.01903463900089264
I0306 05:02:58.345699 140407882708736 logging_writer.py:48] [140400] global_step=140400, grad_norm=0.14724288880825043, loss=0.018182137981057167
I0306 05:03:31.208330 140415790769920 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.14939865469932556, loss=0.017188187688589096
I0306 05:04:03.372166 140407882708736 logging_writer.py:48] [140600] global_step=140600, grad_norm=0.15448492765426636, loss=0.016224931925535202
I0306 05:04:16.438580 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:06:00.548929 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:06:03.870703 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:06:06.937499 140576608098112 submission_runner.py:411] Time since start: 68457.65s, 	Step: 140642, 	{'train/accuracy': 0.9955102205276489, 'train/loss': 0.014116574078798294, 'train/mean_average_precision': 0.7750042961359505, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924117972188245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27456544914534264, 'test/num_examples': 43793, 'score': 45396.19762325287, 'total_duration': 68457.6458311081, 'accumulated_submission_time': 45396.19762325287, 'accumulated_eval_time': 23049.423865556717, 'accumulated_logging_time': 8.13773775100708}
I0306 05:06:06.979198 140408968165120 logging_writer.py:48] [140642] accumulated_eval_time=23049.423866, accumulated_logging_time=8.137738, accumulated_submission_time=45396.197623, global_step=140642, preemption_count=0, score=45396.197623, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274565, test/num_examples=43793, total_duration=68457.645831, train/accuracy=0.995510, train/loss=0.014117, train/mean_average_precision=0.775004, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292412, validation/num_examples=43793
I0306 05:06:26.296978 140415782377216 logging_writer.py:48] [140700] global_step=140700, grad_norm=0.13658729195594788, loss=0.017481200397014618
I0306 05:06:58.845218 140408968165120 logging_writer.py:48] [140800] global_step=140800, grad_norm=0.1407448798418045, loss=0.01683170534670353
I0306 05:07:30.981736 140415782377216 logging_writer.py:48] [140900] global_step=140900, grad_norm=0.12576118111610413, loss=0.01671905815601349
I0306 05:08:03.089040 140408968165120 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.14928606152534485, loss=0.019559165462851524
I0306 05:08:35.345867 140415782377216 logging_writer.py:48] [141100] global_step=141100, grad_norm=0.16058623790740967, loss=0.019143223762512207
I0306 05:09:08.114836 140408968165120 logging_writer.py:48] [141200] global_step=141200, grad_norm=0.15076683461666107, loss=0.017856057733297348
I0306 05:09:40.898679 140415782377216 logging_writer.py:48] [141300] global_step=141300, grad_norm=0.15679101645946503, loss=0.01911471039056778
I0306 05:10:07.071469 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:11:47.433617 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:11:50.560957 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:11:55.600821 140576608098112 submission_runner.py:411] Time since start: 68806.31s, 	Step: 141381, 	{'train/accuracy': 0.9955143928527832, 'train/loss': 0.014093535020947456, 'train/mean_average_precision': 0.777542621227472, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29238620316756114, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27446592695644023, 'test/num_examples': 43793, 'score': 45636.255640506744, 'total_duration': 68806.30915427208, 'accumulated_submission_time': 45636.255640506744, 'accumulated_eval_time': 23157.95317864418, 'accumulated_logging_time': 8.191657781600952}
I0306 05:11:55.644525 140407882708736 logging_writer.py:48] [141381] accumulated_eval_time=23157.953179, accumulated_logging_time=8.191658, accumulated_submission_time=45636.255641, global_step=141381, preemption_count=0, score=45636.255641, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274466, test/num_examples=43793, total_duration=68806.309154, train/accuracy=0.995514, train/loss=0.014094, train/mean_average_precision=0.777543, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292386, validation/num_examples=43793
I0306 05:12:02.503738 140409291962112 logging_writer.py:48] [141400] global_step=141400, grad_norm=0.14077848196029663, loss=0.01790272444486618
I0306 05:12:34.246261 140407882708736 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.13953858613967896, loss=0.01749296300113201
I0306 05:13:06.319208 140409291962112 logging_writer.py:48] [141600] global_step=141600, grad_norm=0.1479547768831253, loss=0.018900854513049126
I0306 05:13:38.406005 140407882708736 logging_writer.py:48] [141700] global_step=141700, grad_norm=0.16470102965831757, loss=0.01846517249941826
I0306 05:14:11.066810 140409291962112 logging_writer.py:48] [141800] global_step=141800, grad_norm=0.13593675196170807, loss=0.017132900655269623
I0306 05:14:43.188383 140407882708736 logging_writer.py:48] [141900] global_step=141900, grad_norm=0.13349789381027222, loss=0.017850549891591072
I0306 05:15:15.471208 140409291962112 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.13887198269367218, loss=0.01845865324139595
I0306 05:15:47.736629 140407882708736 logging_writer.py:48] [142100] global_step=142100, grad_norm=0.15085691213607788, loss=0.018501700833439827
I0306 05:15:55.694449 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:17:37.496499 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:17:40.837455 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:17:44.113536 140576608098112 submission_runner.py:411] Time since start: 69154.82s, 	Step: 142126, 	{'train/accuracy': 0.9954879283905029, 'train/loss': 0.01414844673126936, 'train/mean_average_precision': 0.7753354730267371, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923758806923691, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745318743089971, 'test/num_examples': 43793, 'score': 45876.27267360687, 'total_duration': 69154.82185125351, 'accumulated_submission_time': 45876.27267360687, 'accumulated_eval_time': 23266.372206687927, 'accumulated_logging_time': 8.247857809066772}
I0306 05:17:44.161118 140408968165120 logging_writer.py:48] [142126] accumulated_eval_time=23266.372207, accumulated_logging_time=8.247858, accumulated_submission_time=45876.272674, global_step=142126, preemption_count=0, score=45876.272674, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274532, test/num_examples=43793, total_duration=69154.821851, train/accuracy=0.995488, train/loss=0.014148, train/mean_average_precision=0.775335, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292376, validation/num_examples=43793
I0306 05:18:08.928103 140415790769920 logging_writer.py:48] [142200] global_step=142200, grad_norm=0.1444721668958664, loss=0.017913751304149628
I0306 05:18:41.601491 140408968165120 logging_writer.py:48] [142300] global_step=142300, grad_norm=0.14624249935150146, loss=0.017322730273008347
I0306 05:19:14.408339 140415790769920 logging_writer.py:48] [142400] global_step=142400, grad_norm=0.14067721366882324, loss=0.017306670546531677
I0306 05:19:47.135905 140408968165120 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.16706155240535736, loss=0.020413435995578766
I0306 05:20:20.897932 140415790769920 logging_writer.py:48] [142600] global_step=142600, grad_norm=0.12919200956821442, loss=0.016118763014674187
I0306 05:20:53.758430 140408968165120 logging_writer.py:48] [142700] global_step=142700, grad_norm=0.1312645971775055, loss=0.01672588475048542
I0306 05:21:26.678639 140415790769920 logging_writer.py:48] [142800] global_step=142800, grad_norm=0.17325115203857422, loss=0.01946841925382614
I0306 05:21:44.381396 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:23:25.084569 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:23:28.530583 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:23:31.906968 140576608098112 submission_runner.py:411] Time since start: 69502.62s, 	Step: 142855, 	{'train/accuracy': 0.9955083727836609, 'train/loss': 0.014060660265386105, 'train/mean_average_precision': 0.783613736914508, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923354873325368, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27444583250691423, 'test/num_examples': 43793, 'score': 46116.456998348236, 'total_duration': 69502.61528301239, 'accumulated_submission_time': 46116.456998348236, 'accumulated_eval_time': 23373.897718191147, 'accumulated_logging_time': 8.308566331863403}
I0306 05:23:31.957909 140407882708736 logging_writer.py:48] [142855] accumulated_eval_time=23373.897718, accumulated_logging_time=8.308566, accumulated_submission_time=46116.456998, global_step=142855, preemption_count=0, score=46116.456998, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274446, test/num_examples=43793, total_duration=69502.615283, train/accuracy=0.995508, train/loss=0.014061, train/mean_average_precision=0.783614, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292335, validation/num_examples=43793
I0306 05:23:47.128223 140415782377216 logging_writer.py:48] [142900] global_step=142900, grad_norm=0.13790982961654663, loss=0.017311381176114082
I0306 05:24:20.056268 140407882708736 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.15080776810646057, loss=0.01798393949866295
I0306 05:24:53.146848 140415782377216 logging_writer.py:48] [143100] global_step=143100, grad_norm=0.1659870147705078, loss=0.01947992853820324
I0306 05:25:26.825071 140407882708736 logging_writer.py:48] [143200] global_step=143200, grad_norm=0.14257517457008362, loss=0.016129983589053154
I0306 05:25:59.612370 140415782377216 logging_writer.py:48] [143300] global_step=143300, grad_norm=0.16273917257785797, loss=0.01899256557226181
I0306 05:26:32.268553 140407882708736 logging_writer.py:48] [143400] global_step=143400, grad_norm=0.14084085822105408, loss=0.017788851633667946
I0306 05:27:05.014366 140415782377216 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.13788102567195892, loss=0.01742764562368393
I0306 05:27:32.011542 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:29:18.407316 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:29:21.466215 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:29:24.476256 140576608098112 submission_runner.py:411] Time since start: 69855.18s, 	Step: 143584, 	{'train/accuracy': 0.9954497814178467, 'train/loss': 0.014285489916801453, 'train/mean_average_precision': 0.7609779749806512, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29250920094195554, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744276065726812, 'test/num_examples': 43793, 'score': 46356.47380423546, 'total_duration': 69855.18458938599, 'accumulated_submission_time': 46356.47380423546, 'accumulated_eval_time': 23486.36240339279, 'accumulated_logging_time': 8.371394395828247}
I0306 05:29:24.520009 140364426737408 logging_writer.py:48] [143584] accumulated_eval_time=23486.362403, accumulated_logging_time=8.371394, accumulated_submission_time=46356.473804, global_step=143584, preemption_count=0, score=46356.473804, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274428, test/num_examples=43793, total_duration=69855.184589, train/accuracy=0.995450, train/loss=0.014285, train/mean_average_precision=0.760978, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292509, validation/num_examples=43793
I0306 05:29:30.232092 140409291962112 logging_writer.py:48] [143600] global_step=143600, grad_norm=0.14990907907485962, loss=0.01688845455646515
I0306 05:30:03.166176 140364426737408 logging_writer.py:48] [143700] global_step=143700, grad_norm=0.12625716626644135, loss=0.016298111528158188
I0306 05:30:35.431429 140409291962112 logging_writer.py:48] [143800] global_step=143800, grad_norm=0.14350470900535583, loss=0.01647481508553028
I0306 05:31:07.624042 140364426737408 logging_writer.py:48] [143900] global_step=143900, grad_norm=0.1412697732448578, loss=0.015819329768419266
I0306 05:31:40.228498 140409291962112 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.14390139281749725, loss=0.019577892497181892
I0306 05:32:12.333940 140364426737408 logging_writer.py:48] [144100] global_step=144100, grad_norm=0.13398702442646027, loss=0.01656341925263405
I0306 05:32:44.223419 140409291962112 logging_writer.py:48] [144200] global_step=144200, grad_norm=0.14903214573860168, loss=0.01912081427872181
I0306 05:33:16.396170 140364426737408 logging_writer.py:48] [144300] global_step=144300, grad_norm=0.13542871177196503, loss=0.01754259131848812
I0306 05:33:24.630886 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:35:03.718766 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:35:08.875142 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:35:11.953338 140576608098112 submission_runner.py:411] Time since start: 70202.66s, 	Step: 144327, 	{'train/accuracy': 0.9954796433448792, 'train/loss': 0.014203819446265697, 'train/mean_average_precision': 0.7661755426876338, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2926325091584312, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.2744851973090345, 'test/num_examples': 43793, 'score': 46596.55307674408, 'total_duration': 70202.66167020798, 'accumulated_submission_time': 46596.55307674408, 'accumulated_eval_time': 23593.684818267822, 'accumulated_logging_time': 8.426125526428223}
I0306 05:35:11.997075 140407882708736 logging_writer.py:48] [144327] accumulated_eval_time=23593.684818, accumulated_logging_time=8.426126, accumulated_submission_time=46596.553077, global_step=144327, preemption_count=0, score=46596.553077, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274485, test/num_examples=43793, total_duration=70202.661670, train/accuracy=0.995480, train/loss=0.014204, train/mean_average_precision=0.766176, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292633, validation/num_examples=43793
I0306 05:35:36.089630 140415790507776 logging_writer.py:48] [144400] global_step=144400, grad_norm=0.15595872700214386, loss=0.017731552943587303
I0306 05:36:08.571171 140407882708736 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.1405596137046814, loss=0.01867346465587616
I0306 05:36:41.276424 140415790507776 logging_writer.py:48] [144600] global_step=144600, grad_norm=0.13804882764816284, loss=0.01954747922718525
I0306 05:37:13.920293 140407882708736 logging_writer.py:48] [144700] global_step=144700, grad_norm=0.14039647579193115, loss=0.01940835267305374
I0306 05:37:45.997868 140415790507776 logging_writer.py:48] [144800] global_step=144800, grad_norm=0.15403681993484497, loss=0.01931452937424183
I0306 05:38:17.862632 140407882708736 logging_writer.py:48] [144900] global_step=144900, grad_norm=0.12693065404891968, loss=0.016361616551876068
I0306 05:38:49.697230 140415790507776 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.1453259140253067, loss=0.01849503070116043
I0306 05:39:12.012457 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:40:51.770751 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:40:54.856078 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:40:57.825835 140576608098112 submission_runner.py:411] Time since start: 70548.53s, 	Step: 145070, 	{'train/accuracy': 0.9954817295074463, 'train/loss': 0.014115733094513416, 'train/mean_average_precision': 0.7795299761374892, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2923431012403145, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.274352949729243, 'test/num_examples': 43793, 'score': 46836.53562474251, 'total_duration': 70548.53416538239, 'accumulated_submission_time': 46836.53562474251, 'accumulated_eval_time': 23699.498154878616, 'accumulated_logging_time': 8.482213497161865}
I0306 05:40:57.870127 140364426737408 logging_writer.py:48] [145070] accumulated_eval_time=23699.498155, accumulated_logging_time=8.482213, accumulated_submission_time=46836.535625, global_step=145070, preemption_count=0, score=46836.535625, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274353, test/num_examples=43793, total_duration=70548.534165, train/accuracy=0.995482, train/loss=0.014116, train/mean_average_precision=0.779530, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292343, validation/num_examples=43793
I0306 05:41:08.130879 140409291962112 logging_writer.py:48] [145100] global_step=145100, grad_norm=0.13770566880702972, loss=0.017832476645708084
I0306 05:41:40.336112 140364426737408 logging_writer.py:48] [145200] global_step=145200, grad_norm=0.1306801587343216, loss=0.01684044487774372
I0306 05:42:12.924525 140409291962112 logging_writer.py:48] [145300] global_step=145300, grad_norm=0.14544039964675903, loss=0.017556481063365936
I0306 05:42:45.578749 140364426737408 logging_writer.py:48] [145400] global_step=145400, grad_norm=0.15104573965072632, loss=0.020425034686923027
I0306 05:43:18.002392 140409291962112 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.13763859868049622, loss=0.018135594204068184
I0306 05:43:50.234797 140364426737408 logging_writer.py:48] [145600] global_step=145600, grad_norm=0.13780367374420166, loss=0.019323991611599922
I0306 05:44:22.379596 140409291962112 logging_writer.py:48] [145700] global_step=145700, grad_norm=0.14225231111049652, loss=0.0186020378023386
I0306 05:44:54.647198 140364426737408 logging_writer.py:48] [145800] global_step=145800, grad_norm=0.1321195662021637, loss=0.01818283647298813
I0306 05:44:57.841260 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:46:38.726225 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:46:41.833221 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:46:44.829635 140576608098112 submission_runner.py:411] Time since start: 70895.54s, 	Step: 145811, 	{'train/accuracy': 0.9955083131790161, 'train/loss': 0.014139887876808643, 'train/mean_average_precision': 0.7773677847036007, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924520914694519, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744340913738719, 'test/num_examples': 43793, 'score': 47076.47433376312, 'total_duration': 70895.5379679203, 'accumulated_submission_time': 47076.47433376312, 'accumulated_eval_time': 23806.48648405075, 'accumulated_logging_time': 8.537409782409668}
I0306 05:46:44.872330 140407882708736 logging_writer.py:48] [145811] accumulated_eval_time=23806.486484, accumulated_logging_time=8.537410, accumulated_submission_time=47076.474334, global_step=145811, preemption_count=0, score=47076.474334, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274434, test/num_examples=43793, total_duration=70895.537968, train/accuracy=0.995508, train/loss=0.014140, train/mean_average_precision=0.777368, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292452, validation/num_examples=43793
I0306 05:47:13.789874 140415790507776 logging_writer.py:48] [145900] global_step=145900, grad_norm=0.1312662810087204, loss=0.01737731322646141
I0306 05:47:45.680801 140407882708736 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.13870254158973694, loss=0.019332129508256912
I0306 05:48:17.665872 140415790507776 logging_writer.py:48] [146100] global_step=146100, grad_norm=0.1436075121164322, loss=0.01772414520382881
I0306 05:48:49.760942 140407882708736 logging_writer.py:48] [146200] global_step=146200, grad_norm=0.15664827823638916, loss=0.017409609630703926
I0306 05:49:21.995074 140415790507776 logging_writer.py:48] [146300] global_step=146300, grad_norm=0.14650969207286835, loss=0.017836879938840866
I0306 05:49:53.761358 140407882708736 logging_writer.py:48] [146400] global_step=146400, grad_norm=0.1420598030090332, loss=0.016898268833756447
I0306 05:50:26.011620 140415790507776 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.14637510478496552, loss=0.018225451931357384
I0306 05:50:44.881658 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:52:26.478664 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:52:29.502332 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:52:32.471421 140576608098112 submission_runner.py:411] Time since start: 71243.18s, 	Step: 146559, 	{'train/accuracy': 0.9955046772956848, 'train/loss': 0.01411769725382328, 'train/mean_average_precision': 0.7730506080338739, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29245910813687076, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27456712736733235, 'test/num_examples': 43793, 'score': 47316.4525346756, 'total_duration': 71243.1797504425, 'accumulated_submission_time': 47316.4525346756, 'accumulated_eval_time': 23914.076204538345, 'accumulated_logging_time': 8.591312885284424}
I0306 05:52:32.514968 140364426737408 logging_writer.py:48] [146559] accumulated_eval_time=23914.076205, accumulated_logging_time=8.591313, accumulated_submission_time=47316.452535, global_step=146559, preemption_count=0, score=47316.452535, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274567, test/num_examples=43793, total_duration=71243.179750, train/accuracy=0.995505, train/loss=0.014118, train/mean_average_precision=0.773051, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292459, validation/num_examples=43793
I0306 05:52:45.922830 140409291962112 logging_writer.py:48] [146600] global_step=146600, grad_norm=0.14732088148593903, loss=0.019344042986631393
I0306 05:53:18.143625 140364426737408 logging_writer.py:48] [146700] global_step=146700, grad_norm=0.1705896556377411, loss=0.020161082968115807
I0306 05:53:49.772710 140409291962112 logging_writer.py:48] [146800] global_step=146800, grad_norm=0.13305771350860596, loss=0.017095118761062622
I0306 05:54:22.223040 140364426737408 logging_writer.py:48] [146900] global_step=146900, grad_norm=0.14559929072856903, loss=0.017786473035812378
I0306 05:54:54.797618 140409291962112 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.1253451406955719, loss=0.015793444588780403
I0306 05:55:27.051804 140364426737408 logging_writer.py:48] [147100] global_step=147100, grad_norm=0.14421917498111725, loss=0.017318595200777054
I0306 05:55:59.256884 140409291962112 logging_writer.py:48] [147200] global_step=147200, grad_norm=0.13866709172725677, loss=0.0194229856133461
I0306 05:56:32.524772 140576608098112 spec.py:321] Evaluating on the training split.
I0306 05:58:11.201971 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 05:58:14.379696 140576608098112 spec.py:349] Evaluating on the test split.
I0306 05:58:17.337360 140576608098112 submission_runner.py:411] Time since start: 71588.05s, 	Step: 147295, 	{'train/accuracy': 0.9955081939697266, 'train/loss': 0.01409273874014616, 'train/mean_average_precision': 0.7726374976251774, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29249231693901945, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27435908517892993, 'test/num_examples': 43793, 'score': 47556.43048453331, 'total_duration': 71588.04568099976, 'accumulated_submission_time': 47556.43048453331, 'accumulated_eval_time': 24018.888756752014, 'accumulated_logging_time': 8.645967245101929}
I0306 05:58:17.381844 140407882708736 logging_writer.py:48] [147295] accumulated_eval_time=24018.888757, accumulated_logging_time=8.645967, accumulated_submission_time=47556.430485, global_step=147295, preemption_count=0, score=47556.430485, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274359, test/num_examples=43793, total_duration=71588.045681, train/accuracy=0.995508, train/loss=0.014093, train/mean_average_precision=0.772637, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292492, validation/num_examples=43793
I0306 05:58:19.511139 140415790507776 logging_writer.py:48] [147300] global_step=147300, grad_norm=0.12744292616844177, loss=0.017039712518453598
I0306 05:58:51.721483 140407882708736 logging_writer.py:48] [147400] global_step=147400, grad_norm=0.12749680876731873, loss=0.016510121524333954
I0306 05:59:23.742639 140415790507776 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.126844584941864, loss=0.017773598432540894
I0306 05:59:55.406127 140407882708736 logging_writer.py:48] [147600] global_step=147600, grad_norm=0.1477847546339035, loss=0.016320206224918365
I0306 06:00:27.415255 140415790507776 logging_writer.py:48] [147700] global_step=147700, grad_norm=0.14955931901931763, loss=0.018883105367422104
I0306 06:00:59.205064 140407882708736 logging_writer.py:48] [147800] global_step=147800, grad_norm=0.14532719552516937, loss=0.01997707411646843
I0306 06:01:31.749978 140415790507776 logging_writer.py:48] [147900] global_step=147900, grad_norm=0.14022640883922577, loss=0.01943078637123108
I0306 06:02:03.606781 140407882708736 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.1360892653465271, loss=0.016403719782829285
I0306 06:02:17.352997 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:03:56.697458 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:03:59.684747 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:04:02.896610 140576608098112 submission_runner.py:411] Time since start: 71933.60s, 	Step: 148044, 	{'train/accuracy': 0.995439887046814, 'train/loss': 0.014242757111787796, 'train/mean_average_precision': 0.7694661257596855, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29242280230300305, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744173503944239, 'test/num_examples': 43793, 'score': 47796.37060403824, 'total_duration': 71933.60494160652, 'accumulated_submission_time': 47796.37060403824, 'accumulated_eval_time': 24124.432327747345, 'accumulated_logging_time': 8.701493501663208}
I0306 06:04:02.939230 140364426737408 logging_writer.py:48] [148044] accumulated_eval_time=24124.432328, accumulated_logging_time=8.701494, accumulated_submission_time=47796.370604, global_step=148044, preemption_count=0, score=47796.370604, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274417, test/num_examples=43793, total_duration=71933.604942, train/accuracy=0.995440, train/loss=0.014243, train/mean_average_precision=0.769466, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292423, validation/num_examples=43793
I0306 06:04:21.293331 140409291962112 logging_writer.py:48] [148100] global_step=148100, grad_norm=0.145494282245636, loss=0.016214773058891296
I0306 06:04:53.062756 140364426737408 logging_writer.py:48] [148200] global_step=148200, grad_norm=0.1542404741048813, loss=0.018347127363085747
I0306 06:05:25.262123 140409291962112 logging_writer.py:48] [148300] global_step=148300, grad_norm=0.1561240255832672, loss=0.02053009159862995
I0306 06:05:57.342131 140364426737408 logging_writer.py:48] [148400] global_step=148400, grad_norm=0.13831230998039246, loss=0.019101453945040703
I0306 06:06:29.298536 140409291962112 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.15073762834072113, loss=0.018254941329360008
I0306 06:07:01.325261 140364426737408 logging_writer.py:48] [148600] global_step=148600, grad_norm=0.14593453705310822, loss=0.019556792452931404
I0306 06:07:33.161277 140409291962112 logging_writer.py:48] [148700] global_step=148700, grad_norm=0.12720492482185364, loss=0.017717426642775536
I0306 06:08:03.042796 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:09:42.636763 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:09:45.674615 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:09:48.633839 140576608098112 submission_runner.py:411] Time since start: 72279.34s, 	Step: 148793, 	{'train/accuracy': 0.9954816102981567, 'train/loss': 0.01417029183357954, 'train/mean_average_precision': 0.7750624725733591, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924158822165662, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743672697535215, 'test/num_examples': 43793, 'score': 48036.4431014061, 'total_duration': 72279.34216976166, 'accumulated_submission_time': 48036.4431014061, 'accumulated_eval_time': 24230.023329257965, 'accumulated_logging_time': 8.754802942276001}
I0306 06:09:48.678800 140408968165120 logging_writer.py:48] [148793] accumulated_eval_time=24230.023329, accumulated_logging_time=8.754803, accumulated_submission_time=48036.443101, global_step=148793, preemption_count=0, score=48036.443101, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274367, test/num_examples=43793, total_duration=72279.342170, train/accuracy=0.995482, train/loss=0.014170, train/mean_average_precision=0.775062, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292416, validation/num_examples=43793
I0306 06:09:51.503272 140415790507776 logging_writer.py:48] [148800] global_step=148800, grad_norm=0.12878946959972382, loss=0.01648976095020771
I0306 06:10:23.894052 140408968165120 logging_writer.py:48] [148900] global_step=148900, grad_norm=0.15114402770996094, loss=0.018692538142204285
I0306 06:10:55.764358 140415790507776 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.12977562844753265, loss=0.01664590835571289
I0306 06:11:27.784721 140408968165120 logging_writer.py:48] [149100] global_step=149100, grad_norm=0.13989609479904175, loss=0.01597106084227562
I0306 06:12:00.114219 140415790507776 logging_writer.py:48] [149200] global_step=149200, grad_norm=0.15796516835689545, loss=0.018535979092121124
I0306 06:12:32.109649 140408968165120 logging_writer.py:48] [149300] global_step=149300, grad_norm=0.14029547572135925, loss=0.01849445141851902
I0306 06:13:04.054268 140415790507776 logging_writer.py:48] [149400] global_step=149400, grad_norm=0.13937228918075562, loss=0.01921449415385723
I0306 06:13:36.098489 140408968165120 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.1432459056377411, loss=0.018980512395501137
I0306 06:13:48.875428 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:15:32.736985 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:15:35.763988 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:15:38.709836 140576608098112 submission_runner.py:411] Time since start: 72629.42s, 	Step: 149541, 	{'train/accuracy': 0.9955478310585022, 'train/loss': 0.01401436235755682, 'train/mean_average_precision': 0.7779941175755147, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2925357045022977, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745146309739536, 'test/num_examples': 43793, 'score': 48276.60923433304, 'total_duration': 72629.41816806793, 'accumulated_submission_time': 48276.60923433304, 'accumulated_eval_time': 24339.857694149017, 'accumulated_logging_time': 8.81046986579895}
I0306 06:15:38.752764 140407882708736 logging_writer.py:48] [149541] accumulated_eval_time=24339.857694, accumulated_logging_time=8.810470, accumulated_submission_time=48276.609234, global_step=149541, preemption_count=0, score=48276.609234, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274515, test/num_examples=43793, total_duration=72629.418168, train/accuracy=0.995548, train/loss=0.014014, train/mean_average_precision=0.777994, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292536, validation/num_examples=43793
I0306 06:15:58.018290 140409291962112 logging_writer.py:48] [149600] global_step=149600, grad_norm=0.13923050463199615, loss=0.01609758660197258
I0306 06:16:30.032577 140407882708736 logging_writer.py:48] [149700] global_step=149700, grad_norm=0.13193446397781372, loss=0.01700657047331333
I0306 06:17:01.587774 140409291962112 logging_writer.py:48] [149800] global_step=149800, grad_norm=0.14742153882980347, loss=0.018268611282110214
I0306 06:17:33.361386 140407882708736 logging_writer.py:48] [149900] global_step=149900, grad_norm=0.13097050786018372, loss=0.017057856544852257
I0306 06:18:05.114676 140409291962112 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.15656252205371857, loss=0.01599149778485298
I0306 06:18:36.532421 140407882708736 logging_writer.py:48] [150100] global_step=150100, grad_norm=0.12629921734333038, loss=0.01707415096461773
I0306 06:19:08.394848 140409291962112 logging_writer.py:48] [150200] global_step=150200, grad_norm=0.14064659178256989, loss=0.018630748614668846
I0306 06:19:38.809180 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:21:16.976941 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:21:20.022784 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:21:23.024595 140576608098112 submission_runner.py:411] Time since start: 72973.73s, 	Step: 150297, 	{'train/accuracy': 0.9954404234886169, 'train/loss': 0.014284079894423485, 'train/mean_average_precision': 0.7704192706591224, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923980627222845, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744235536170402, 'test/num_examples': 43793, 'score': 48516.63416361809, 'total_duration': 72973.73292207718, 'accumulated_submission_time': 48516.63416361809, 'accumulated_eval_time': 24444.07306456566, 'accumulated_logging_time': 8.864438533782959}
I0306 06:21:23.073437 140364426737408 logging_writer.py:48] [150297] accumulated_eval_time=24444.073065, accumulated_logging_time=8.864439, accumulated_submission_time=48516.634164, global_step=150297, preemption_count=0, score=48516.634164, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274424, test/num_examples=43793, total_duration=72973.732922, train/accuracy=0.995440, train/loss=0.014284, train/mean_average_precision=0.770419, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292398, validation/num_examples=43793
I0306 06:21:24.450495 140408968165120 logging_writer.py:48] [150300] global_step=150300, grad_norm=0.14944912493228912, loss=0.017098868265748024
I0306 06:21:56.234278 140364426737408 logging_writer.py:48] [150400] global_step=150400, grad_norm=0.14030593633651733, loss=0.01923280954360962
I0306 06:22:28.111111 140408968165120 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.14221766591072083, loss=0.018689140677452087
I0306 06:22:59.438139 140364426737408 logging_writer.py:48] [150600] global_step=150600, grad_norm=0.1342179775238037, loss=0.01634257100522518
I0306 06:23:31.250626 140408968165120 logging_writer.py:48] [150700] global_step=150700, grad_norm=0.13724924623966217, loss=0.018435919657349586
I0306 06:24:03.033934 140364426737408 logging_writer.py:48] [150800] global_step=150800, grad_norm=0.13990947604179382, loss=0.01833399198949337
I0306 06:24:34.463956 140408968165120 logging_writer.py:48] [150900] global_step=150900, grad_norm=0.13752157986164093, loss=0.018002592027187347
I0306 06:25:05.706817 140364426737408 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.1521623581647873, loss=0.019427619874477386
I0306 06:25:23.084774 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:27:04.463385 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:27:07.504828 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:27:10.502282 140576608098112 submission_runner.py:411] Time since start: 73321.21s, 	Step: 151056, 	{'train/accuracy': 0.9955169558525085, 'train/loss': 0.01406917069107294, 'train/mean_average_precision': 0.7767232637434311, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923393462303533, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744903066878821, 'test/num_examples': 43793, 'score': 48756.612805366516, 'total_duration': 73321.21061086655, 'accumulated_submission_time': 48756.612805366516, 'accumulated_eval_time': 24551.490526914597, 'accumulated_logging_time': 8.925953388214111}
I0306 06:27:10.546131 140407882708736 logging_writer.py:48] [151056] accumulated_eval_time=24551.490527, accumulated_logging_time=8.925953, accumulated_submission_time=48756.612805, global_step=151056, preemption_count=0, score=48756.612805, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274490, test/num_examples=43793, total_duration=73321.210611, train/accuracy=0.995517, train/loss=0.014069, train/mean_average_precision=0.776723, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292339, validation/num_examples=43793
I0306 06:27:25.451519 140409291962112 logging_writer.py:48] [151100] global_step=151100, grad_norm=0.13528607785701752, loss=0.015680385753512383
I0306 06:27:57.609009 140407882708736 logging_writer.py:48] [151200] global_step=151200, grad_norm=0.14790451526641846, loss=0.01644381694495678
I0306 06:28:29.896324 140409291962112 logging_writer.py:48] [151300] global_step=151300, grad_norm=0.15704858303070068, loss=0.01921079494059086
I0306 06:29:01.830569 140407882708736 logging_writer.py:48] [151400] global_step=151400, grad_norm=0.1471405327320099, loss=0.017157794907689095
I0306 06:29:34.053260 140409291962112 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.13414670526981354, loss=0.01753382757306099
I0306 06:30:06.607830 140407882708736 logging_writer.py:48] [151600] global_step=151600, grad_norm=0.1539319008588791, loss=0.019051359966397285
I0306 06:30:38.222943 140409291962112 logging_writer.py:48] [151700] global_step=151700, grad_norm=0.1400105208158493, loss=0.017310773953795433
I0306 06:31:10.344462 140407882708736 logging_writer.py:48] [151800] global_step=151800, grad_norm=0.15123912692070007, loss=0.018915066495537758
I0306 06:31:10.665590 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:32:52.923817 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:32:55.993662 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:32:58.974225 140576608098112 submission_runner.py:411] Time since start: 73669.68s, 	Step: 151802, 	{'train/accuracy': 0.9954544305801392, 'train/loss': 0.014199693687260151, 'train/mean_average_precision': 0.7650808939684499, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2923891979268853, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27434541367817333, 'test/num_examples': 43793, 'score': 48996.37939405441, 'total_duration': 73669.68244862556, 'accumulated_submission_time': 48996.37939405441, 'accumulated_eval_time': 24659.799003601074, 'accumulated_logging_time': 9.302623271942139}
I0306 06:32:59.018062 140362251773696 logging_writer.py:48] [151802] accumulated_eval_time=24659.799004, accumulated_logging_time=9.302623, accumulated_submission_time=48996.379394, global_step=151802, preemption_count=0, score=48996.379394, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274345, test/num_examples=43793, total_duration=73669.682449, train/accuracy=0.995454, train/loss=0.014200, train/mean_average_precision=0.765081, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292389, validation/num_examples=43793
I0306 06:33:31.551883 140408968165120 logging_writer.py:48] [151900] global_step=151900, grad_norm=0.1314648538827896, loss=0.015692133456468582
I0306 06:34:04.011761 140362251773696 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.1323823481798172, loss=0.014761347323656082
I0306 06:34:36.322188 140408968165120 logging_writer.py:48] [152100] global_step=152100, grad_norm=0.13762669265270233, loss=0.017320480197668076
I0306 06:35:08.451379 140362251773696 logging_writer.py:48] [152200] global_step=152200, grad_norm=0.1601952612400055, loss=0.020626066252589226
I0306 06:35:40.238073 140408968165120 logging_writer.py:48] [152300] global_step=152300, grad_norm=0.14289803802967072, loss=0.017748719081282616
I0306 06:36:12.451129 140362251773696 logging_writer.py:48] [152400] global_step=152400, grad_norm=0.15191932022571564, loss=0.01695428416132927
I0306 06:36:44.574565 140408968165120 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.1546623557806015, loss=0.01971070095896721
I0306 06:36:59.107713 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:38:39.888540 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:38:42.980196 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:38:45.966439 140576608098112 submission_runner.py:411] Time since start: 74016.67s, 	Step: 152546, 	{'train/accuracy': 0.9955030083656311, 'train/loss': 0.014186572283506393, 'train/mean_average_precision': 0.7775761097654365, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29238068198727396, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27441720486954607, 'test/num_examples': 43793, 'score': 49236.437601566315, 'total_duration': 74016.67475938797, 'accumulated_submission_time': 49236.437601566315, 'accumulated_eval_time': 24766.657678842545, 'accumulated_logging_time': 9.358002424240112}
I0306 06:38:46.014612 140407882708736 logging_writer.py:48] [152546] accumulated_eval_time=24766.657679, accumulated_logging_time=9.358002, accumulated_submission_time=49236.437602, global_step=152546, preemption_count=0, score=49236.437602, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274417, test/num_examples=43793, total_duration=74016.674759, train/accuracy=0.995503, train/loss=0.014187, train/mean_average_precision=0.777576, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292381, validation/num_examples=43793
I0306 06:39:04.032610 140409291962112 logging_writer.py:48] [152600] global_step=152600, grad_norm=0.1260860413312912, loss=0.016756562516093254
I0306 06:39:36.179511 140407882708736 logging_writer.py:48] [152700] global_step=152700, grad_norm=0.1553262323141098, loss=0.018442418426275253
I0306 06:40:08.556005 140409291962112 logging_writer.py:48] [152800] global_step=152800, grad_norm=0.142738476395607, loss=0.017219003289937973
I0306 06:40:41.126771 140407882708736 logging_writer.py:48] [152900] global_step=152900, grad_norm=0.14471353590488434, loss=0.01885593682527542
I0306 06:41:13.709789 140409291962112 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.14987657964229584, loss=0.017703935503959656
I0306 06:41:45.708563 140407882708736 logging_writer.py:48] [153100] global_step=153100, grad_norm=0.1305609494447708, loss=0.01633317582309246
I0306 06:42:18.049054 140409291962112 logging_writer.py:48] [153200] global_step=153200, grad_norm=0.13868166506290436, loss=0.0191514752805233
I0306 06:42:46.177173 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:44:20.876845 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:44:24.098450 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:44:27.160629 140576608098112 submission_runner.py:411] Time since start: 74357.87s, 	Step: 153288, 	{'train/accuracy': 0.9954869747161865, 'train/loss': 0.01411436777561903, 'train/mean_average_precision': 0.775209529543543, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2923222150566751, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27438338179061494, 'test/num_examples': 43793, 'score': 49476.56862664223, 'total_duration': 74357.86896109581, 'accumulated_submission_time': 49476.56862664223, 'accumulated_eval_time': 24867.64109492302, 'accumulated_logging_time': 9.417367696762085}
I0306 06:44:27.205292 140362251773696 logging_writer.py:48] [153288] accumulated_eval_time=24867.641095, accumulated_logging_time=9.417368, accumulated_submission_time=49476.568627, global_step=153288, preemption_count=0, score=49476.568627, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274383, test/num_examples=43793, total_duration=74357.868961, train/accuracy=0.995487, train/loss=0.014114, train/mean_average_precision=0.775210, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292322, validation/num_examples=43793
I0306 06:44:31.512437 140408968165120 logging_writer.py:48] [153300] global_step=153300, grad_norm=0.13065089285373688, loss=0.0169063787907362
I0306 06:45:03.694832 140362251773696 logging_writer.py:48] [153400] global_step=153400, grad_norm=0.1470608115196228, loss=0.01695009507238865
I0306 06:45:36.249743 140408968165120 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.1421426683664322, loss=0.016893016174435616
I0306 06:46:08.831067 140362251773696 logging_writer.py:48] [153600] global_step=153600, grad_norm=0.13880440592765808, loss=0.017283961176872253
I0306 06:46:40.992779 140408968165120 logging_writer.py:48] [153700] global_step=153700, grad_norm=0.14824716746807098, loss=0.019676076248288155
I0306 06:47:13.813611 140362251773696 logging_writer.py:48] [153800] global_step=153800, grad_norm=0.1721484363079071, loss=0.01852766051888466
I0306 06:47:46.344868 140408968165120 logging_writer.py:48] [153900] global_step=153900, grad_norm=0.1388777643442154, loss=0.01865106262266636
I0306 06:48:19.355291 140362251773696 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.1380937397480011, loss=0.015978701412677765
I0306 06:48:27.215661 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:50:07.679462 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:50:10.792510 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:50:13.921724 140576608098112 submission_runner.py:411] Time since start: 74704.63s, 	Step: 154025, 	{'train/accuracy': 0.9954938888549805, 'train/loss': 0.01415193174034357, 'train/mean_average_precision': 0.7746333298750403, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2925510130236306, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27443893548791515, 'test/num_examples': 43793, 'score': 49716.54668498039, 'total_duration': 74704.63004040718, 'accumulated_submission_time': 49716.54668498039, 'accumulated_eval_time': 24974.34710907936, 'accumulated_logging_time': 9.47364616394043}
I0306 06:50:13.966460 140364426737408 logging_writer.py:48] [154025] accumulated_eval_time=24974.347109, accumulated_logging_time=9.473646, accumulated_submission_time=49716.546685, global_step=154025, preemption_count=0, score=49716.546685, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274439, test/num_examples=43793, total_duration=74704.630040, train/accuracy=0.995494, train/loss=0.014152, train/mean_average_precision=0.774633, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292551, validation/num_examples=43793
I0306 06:50:38.002898 140407882708736 logging_writer.py:48] [154100] global_step=154100, grad_norm=0.15410710871219635, loss=0.018298208713531494
I0306 06:51:10.135568 140364426737408 logging_writer.py:48] [154200] global_step=154200, grad_norm=0.15219025313854218, loss=0.01869814097881317
I0306 06:51:42.325450 140407882708736 logging_writer.py:48] [154300] global_step=154300, grad_norm=0.17064033448696136, loss=0.018322652205824852
I0306 06:52:14.131171 140364426737408 logging_writer.py:48] [154400] global_step=154400, grad_norm=0.14987508952617645, loss=0.019041797146201134
I0306 06:52:45.935846 140407882708736 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.1583670675754547, loss=0.019062688574194908
I0306 06:53:17.712978 140364426737408 logging_writer.py:48] [154600] global_step=154600, grad_norm=0.15394321084022522, loss=0.01946636103093624
I0306 06:53:50.031672 140407882708736 logging_writer.py:48] [154700] global_step=154700, grad_norm=0.1576535552740097, loss=0.017930060625076294
I0306 06:54:14.096302 140576608098112 spec.py:321] Evaluating on the training split.
I0306 06:55:52.044533 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 06:55:55.162950 140576608098112 spec.py:349] Evaluating on the test split.
I0306 06:55:58.213272 140576608098112 submission_runner.py:411] Time since start: 75048.92s, 	Step: 154776, 	{'train/accuracy': 0.995477020740509, 'train/loss': 0.014191939495503902, 'train/mean_average_precision': 0.772443339172785, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.292435496395953, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744790929915433, 'test/num_examples': 43793, 'score': 49956.64419960976, 'total_duration': 75048.92160797119, 'accumulated_submission_time': 49956.64419960976, 'accumulated_eval_time': 25078.46404027939, 'accumulated_logging_time': 9.529872179031372}
I0306 06:55:58.257307 140362251773696 logging_writer.py:48] [154776] accumulated_eval_time=25078.464040, accumulated_logging_time=9.529872, accumulated_submission_time=49956.644200, global_step=154776, preemption_count=0, score=49956.644200, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274479, test/num_examples=43793, total_duration=75048.921608, train/accuracy=0.995477, train/loss=0.014192, train/mean_average_precision=0.772443, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292435, validation/num_examples=43793
I0306 06:56:06.331939 140409291962112 logging_writer.py:48] [154800] global_step=154800, grad_norm=0.14466150104999542, loss=0.019848015159368515
I0306 06:56:38.443107 140362251773696 logging_writer.py:48] [154900] global_step=154900, grad_norm=0.1273883879184723, loss=0.015091581270098686
I0306 06:57:10.576190 140409291962112 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.17226848006248474, loss=0.019008461385965347
I0306 06:57:42.743470 140362251773696 logging_writer.py:48] [155100] global_step=155100, grad_norm=0.14686676859855652, loss=0.021547231823205948
I0306 06:58:15.074213 140409291962112 logging_writer.py:48] [155200] global_step=155200, grad_norm=0.1389668583869934, loss=0.017820483073592186
I0306 06:58:47.328514 140362251773696 logging_writer.py:48] [155300] global_step=155300, grad_norm=0.1613641232252121, loss=0.019285373389720917
I0306 06:59:19.791935 140409291962112 logging_writer.py:48] [155400] global_step=155400, grad_norm=0.1462099552154541, loss=0.018315067514777184
I0306 06:59:51.900361 140362251773696 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.14259769022464752, loss=0.016698982566595078
I0306 06:59:58.515385 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:01:41.444527 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:01:44.579504 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:01:47.690129 140576608098112 submission_runner.py:411] Time since start: 75398.40s, 	Step: 155522, 	{'train/accuracy': 0.9955002665519714, 'train/loss': 0.014126512221992016, 'train/mean_average_precision': 0.76504731206366, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2924108013284655, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.274395693634054, 'test/num_examples': 43793, 'score': 50196.8704969883, 'total_duration': 75398.39845585823, 'accumulated_submission_time': 50196.8704969883, 'accumulated_eval_time': 25187.638740301132, 'accumulated_logging_time': 9.584931373596191}
I0306 07:01:47.734475 140407882708736 logging_writer.py:48] [155522] accumulated_eval_time=25187.638740, accumulated_logging_time=9.584931, accumulated_submission_time=50196.870497, global_step=155522, preemption_count=0, score=50196.870497, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274396, test/num_examples=43793, total_duration=75398.398456, train/accuracy=0.995500, train/loss=0.014127, train/mean_average_precision=0.765047, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292411, validation/num_examples=43793
I0306 07:02:13.208920 140408968165120 logging_writer.py:48] [155600] global_step=155600, grad_norm=0.15661455690860748, loss=0.019241252914071083
I0306 07:02:44.881277 140407882708736 logging_writer.py:48] [155700] global_step=155700, grad_norm=0.15235398709774017, loss=0.017997195944190025
I0306 07:03:17.104976 140408968165120 logging_writer.py:48] [155800] global_step=155800, grad_norm=0.1454421728849411, loss=0.018489733338356018
I0306 07:03:48.635733 140407882708736 logging_writer.py:48] [155900] global_step=155900, grad_norm=0.13615746796131134, loss=0.01701069064438343
I0306 07:04:20.735036 140408968165120 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.12684760987758636, loss=0.016966748982667923
I0306 07:04:52.627123 140407882708736 logging_writer.py:48] [156100] global_step=156100, grad_norm=0.13177426159381866, loss=0.016588833183050156
I0306 07:05:24.583781 140408968165120 logging_writer.py:48] [156200] global_step=156200, grad_norm=0.13786935806274414, loss=0.0178008321672678
I0306 07:05:47.833384 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:07:30.427587 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:07:33.541672 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:07:36.539482 140576608098112 submission_runner.py:411] Time since start: 75747.25s, 	Step: 156274, 	{'train/accuracy': 0.9954464435577393, 'train/loss': 0.014275760389864445, 'train/mean_average_precision': 0.7749395935942144, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2924427218899189, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27441787207828783, 'test/num_examples': 43793, 'score': 50436.93785953522, 'total_duration': 75747.2478158474, 'accumulated_submission_time': 50436.93785953522, 'accumulated_eval_time': 25296.344801425934, 'accumulated_logging_time': 9.640064239501953}
I0306 07:07:36.585001 140364426737408 logging_writer.py:48] [156274] accumulated_eval_time=25296.344801, accumulated_logging_time=9.640064, accumulated_submission_time=50436.937860, global_step=156274, preemption_count=0, score=50436.937860, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274418, test/num_examples=43793, total_duration=75747.247816, train/accuracy=0.995446, train/loss=0.014276, train/mean_average_precision=0.774940, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292443, validation/num_examples=43793
I0306 07:07:45.238996 140409291962112 logging_writer.py:48] [156300] global_step=156300, grad_norm=0.13601209223270416, loss=0.01830247975885868
I0306 07:08:17.230389 140364426737408 logging_writer.py:48] [156400] global_step=156400, grad_norm=0.15048664808273315, loss=0.017980046570301056
I0306 07:08:49.303036 140409291962112 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.14595989882946014, loss=0.01839391328394413
I0306 07:09:21.528136 140364426737408 logging_writer.py:48] [156600] global_step=156600, grad_norm=0.1330651193857193, loss=0.017524370923638344
I0306 07:09:53.359701 140409291962112 logging_writer.py:48] [156700] global_step=156700, grad_norm=0.13469748198986053, loss=0.016215818002820015
I0306 07:10:25.767062 140364426737408 logging_writer.py:48] [156800] global_step=156800, grad_norm=0.14822834730148315, loss=0.01858990453183651
I0306 07:10:57.835841 140409291962112 logging_writer.py:48] [156900] global_step=156900, grad_norm=0.15421321988105774, loss=0.017912575975060463
I0306 07:11:29.582338 140364426737408 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.14383691549301147, loss=0.01771477982401848
I0306 07:11:36.842568 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:13:20.835717 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:13:23.873582 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:13:26.834879 140576608098112 submission_runner.py:411] Time since start: 76097.54s, 	Step: 157024, 	{'train/accuracy': 0.9955170154571533, 'train/loss': 0.01405609492212534, 'train/mean_average_precision': 0.773829621089907, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29228853146805917, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27432990484128655, 'test/num_examples': 43793, 'score': 50677.16159534454, 'total_duration': 76097.5432009697, 'accumulated_submission_time': 50677.16159534454, 'accumulated_eval_time': 25406.3370552063, 'accumulated_logging_time': 9.698690176010132}
I0306 07:13:26.879580 140362251773696 logging_writer.py:48] [157024] accumulated_eval_time=25406.337055, accumulated_logging_time=9.698690, accumulated_submission_time=50677.161595, global_step=157024, preemption_count=0, score=50677.161595, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274330, test/num_examples=43793, total_duration=76097.543201, train/accuracy=0.995517, train/loss=0.014056, train/mean_average_precision=0.773830, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292289, validation/num_examples=43793
I0306 07:13:51.949087 140407882708736 logging_writer.py:48] [157100] global_step=157100, grad_norm=0.14251786470413208, loss=0.017846565693616867
I0306 07:14:24.575483 140362251773696 logging_writer.py:48] [157200] global_step=157200, grad_norm=0.1395862102508545, loss=0.01727188378572464
I0306 07:14:56.562254 140407882708736 logging_writer.py:48] [157300] global_step=157300, grad_norm=0.13596434891223907, loss=0.01617390662431717
I0306 07:15:28.790545 140362251773696 logging_writer.py:48] [157400] global_step=157400, grad_norm=0.14172972738742828, loss=0.020161021500825882
I0306 07:16:00.445174 140407882708736 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.14492453634738922, loss=0.019303621724247932
I0306 07:16:32.706479 140362251773696 logging_writer.py:48] [157600] global_step=157600, grad_norm=0.14690522849559784, loss=0.019412154331803322
I0306 07:17:05.077099 140407882708736 logging_writer.py:48] [157700] global_step=157700, grad_norm=0.16781173646450043, loss=0.01886438950896263
I0306 07:17:27.048807 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:19:05.673846 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:19:08.813176 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:19:11.868583 140576608098112 submission_runner.py:411] Time since start: 76442.58s, 	Step: 157769, 	{'train/accuracy': 0.9955052137374878, 'train/loss': 0.014119334518909454, 'train/mean_average_precision': 0.7832026154449414, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29245382173069473, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.054033685475587845, 'test/mean_average_precision': 0.27444815424183777, 'test/num_examples': 43793, 'score': 50917.29948115349, 'total_duration': 76442.57691502571, 'accumulated_submission_time': 50917.29948115349, 'accumulated_eval_time': 25511.156789064407, 'accumulated_logging_time': 9.754071712493896}
I0306 07:19:11.913223 140408968165120 logging_writer.py:48] [157769] accumulated_eval_time=25511.156789, accumulated_logging_time=9.754072, accumulated_submission_time=50917.299481, global_step=157769, preemption_count=0, score=50917.299481, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274448, test/num_examples=43793, total_duration=76442.576915, train/accuracy=0.995505, train/loss=0.014119, train/mean_average_precision=0.783203, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292454, validation/num_examples=43793
I0306 07:19:22.274045 140409291962112 logging_writer.py:48] [157800] global_step=157800, grad_norm=0.1334802359342575, loss=0.0168150644749403
I0306 07:19:54.104517 140408968165120 logging_writer.py:48] [157900] global_step=157900, grad_norm=0.1457657665014267, loss=0.01833873614668846
I0306 07:20:26.127339 140409291962112 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.14733926951885223, loss=0.017580512911081314
I0306 07:20:57.871931 140408968165120 logging_writer.py:48] [158100] global_step=158100, grad_norm=0.1597139686346054, loss=0.02227959781885147
I0306 07:21:29.769847 140409291962112 logging_writer.py:48] [158200] global_step=158200, grad_norm=0.14449377357959747, loss=0.018523314967751503
I0306 07:22:01.761106 140408968165120 logging_writer.py:48] [158300] global_step=158300, grad_norm=0.1421365737915039, loss=0.01935938186943531
I0306 07:22:33.846137 140409291962112 logging_writer.py:48] [158400] global_step=158400, grad_norm=0.13674324750900269, loss=0.017953645437955856
I0306 07:23:05.505715 140408968165120 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.1447174847126007, loss=0.019576946273446083
I0306 07:23:11.877897 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:24:53.057615 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:24:56.107528 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:24:59.081303 140576608098112 submission_runner.py:411] Time since start: 76789.79s, 	Step: 158521, 	{'train/accuracy': 0.995479941368103, 'train/loss': 0.014167251996695995, 'train/mean_average_precision': 0.7708454919205815, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29243749673825636, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744134394539258, 'test/num_examples': 43793, 'score': 51157.23203110695, 'total_duration': 76789.78963327408, 'accumulated_submission_time': 51157.23203110695, 'accumulated_eval_time': 25618.360147476196, 'accumulated_logging_time': 9.810832738876343}
I0306 07:24:59.127084 140362251773696 logging_writer.py:48] [158521] accumulated_eval_time=25618.360147, accumulated_logging_time=9.810833, accumulated_submission_time=51157.232031, global_step=158521, preemption_count=0, score=51157.232031, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274413, test/num_examples=43793, total_duration=76789.789633, train/accuracy=0.995480, train/loss=0.014167, train/mean_average_precision=0.770845, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292437, validation/num_examples=43793
I0306 07:25:24.850593 140407882708736 logging_writer.py:48] [158600] global_step=158600, grad_norm=0.1425652801990509, loss=0.01704840548336506
I0306 07:25:56.279903 140362251773696 logging_writer.py:48] [158700] global_step=158700, grad_norm=0.14128956198692322, loss=0.017009859904646873
I0306 07:26:28.327981 140407882708736 logging_writer.py:48] [158800] global_step=158800, grad_norm=0.13470801711082458, loss=0.017522454261779785
I0306 07:26:59.944408 140362251773696 logging_writer.py:48] [158900] global_step=158900, grad_norm=0.14867545664310455, loss=0.017371010035276413
I0306 07:27:32.044311 140407882708736 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.13845814764499664, loss=0.018892690539360046
I0306 07:28:04.997892 140362251773696 logging_writer.py:48] [159100] global_step=159100, grad_norm=0.13834884762763977, loss=0.01764942705631256
I0306 07:28:36.728286 140407882708736 logging_writer.py:48] [159200] global_step=159200, grad_norm=0.12809062004089355, loss=0.017005544155836105
I0306 07:28:59.327248 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:30:40.015472 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:30:43.111875 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:30:46.080560 140576608098112 submission_runner.py:411] Time since start: 77136.79s, 	Step: 159271, 	{'train/accuracy': 0.9954665899276733, 'train/loss': 0.014204711653292179, 'train/mean_average_precision': 0.7754148010822717, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2925999843117496, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745207814921742, 'test/num_examples': 43793, 'score': 51397.40039587021, 'total_duration': 77136.78889465332, 'accumulated_submission_time': 51397.40039587021, 'accumulated_eval_time': 25725.11342215538, 'accumulated_logging_time': 9.86826467514038}
I0306 07:30:46.126166 140408968165120 logging_writer.py:48] [159271] accumulated_eval_time=25725.113422, accumulated_logging_time=9.868265, accumulated_submission_time=51397.400396, global_step=159271, preemption_count=0, score=51397.400396, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274521, test/num_examples=43793, total_duration=77136.788895, train/accuracy=0.995467, train/loss=0.014205, train/mean_average_precision=0.775415, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292600, validation/num_examples=43793
I0306 07:30:55.735902 140409291962112 logging_writer.py:48] [159300] global_step=159300, grad_norm=0.12948237359523773, loss=0.015831300988793373
I0306 07:31:27.657838 140408968165120 logging_writer.py:48] [159400] global_step=159400, grad_norm=0.1416960209608078, loss=0.019589638337492943
I0306 07:31:59.440413 140409291962112 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.12295758724212646, loss=0.01554056629538536
I0306 07:32:31.347007 140408968165120 logging_writer.py:48] [159600] global_step=159600, grad_norm=0.15249712765216827, loss=0.019750824198126793
I0306 07:33:03.228743 140409291962112 logging_writer.py:48] [159700] global_step=159700, grad_norm=0.138874351978302, loss=0.01831887662410736
I0306 07:33:35.176729 140408968165120 logging_writer.py:48] [159800] global_step=159800, grad_norm=0.14249886572360992, loss=0.018170975148677826
I0306 07:34:06.715994 140409291962112 logging_writer.py:48] [159900] global_step=159900, grad_norm=0.13902120292186737, loss=0.01797330565750599
I0306 07:34:38.457404 140408968165120 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.1436682492494583, loss=0.018137922510504723
I0306 07:34:46.298583 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:36:25.097746 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:36:28.149960 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:36:31.078532 140576608098112 submission_runner.py:411] Time since start: 77481.79s, 	Step: 160026, 	{'train/accuracy': 0.9954699277877808, 'train/loss': 0.01419955212622881, 'train/mean_average_precision': 0.7704376398776334, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29231894014797427, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2745612762808954, 'test/num_examples': 43793, 'score': 51637.54203295708, 'total_duration': 77481.7868654728, 'accumulated_submission_time': 51637.54203295708, 'accumulated_eval_time': 25829.893330335617, 'accumulated_logging_time': 9.92480731010437}
I0306 07:36:31.124003 140362251773696 logging_writer.py:48] [160026] accumulated_eval_time=25829.893330, accumulated_logging_time=9.924807, accumulated_submission_time=51637.542033, global_step=160026, preemption_count=0, score=51637.542033, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274561, test/num_examples=43793, total_duration=77481.786865, train/accuracy=0.995470, train/loss=0.014200, train/mean_average_precision=0.770438, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292319, validation/num_examples=43793
I0306 07:36:55.041305 140364426737408 logging_writer.py:48] [160100] global_step=160100, grad_norm=0.14404672384262085, loss=0.019219109788537025
I0306 07:37:27.352533 140362251773696 logging_writer.py:48] [160200] global_step=160200, grad_norm=0.12274149805307388, loss=0.016597576439380646
I0306 07:37:59.559545 140364426737408 logging_writer.py:48] [160300] global_step=160300, grad_norm=0.13119082152843475, loss=0.017962930724024773
I0306 07:38:31.460284 140362251773696 logging_writer.py:48] [160400] global_step=160400, grad_norm=0.13717100024223328, loss=0.019702451303601265
I0306 07:39:03.395568 140364426737408 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.14895546436309814, loss=0.01811070740222931
I0306 07:39:35.602991 140362251773696 logging_writer.py:48] [160600] global_step=160600, grad_norm=0.14894260466098785, loss=0.01755267009139061
I0306 07:40:07.858474 140364426737408 logging_writer.py:48] [160700] global_step=160700, grad_norm=0.143583744764328, loss=0.01802274025976658
I0306 07:40:31.309515 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:42:06.322038 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:42:09.323831 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:42:12.292197 140576608098112 submission_runner.py:411] Time since start: 77823.00s, 	Step: 160775, 	{'train/accuracy': 0.9954940676689148, 'train/loss': 0.014141826890408993, 'train/mean_average_precision': 0.7768644225591141, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923581265098338, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27447065624245726, 'test/num_examples': 43793, 'score': 51877.696434020996, 'total_duration': 77823.0005209446, 'accumulated_submission_time': 51877.696434020996, 'accumulated_eval_time': 25930.875960588455, 'accumulated_logging_time': 9.9811851978302}
I0306 07:42:12.338168 140407882708736 logging_writer.py:48] [160775] accumulated_eval_time=25930.875961, accumulated_logging_time=9.981185, accumulated_submission_time=51877.696434, global_step=160775, preemption_count=0, score=51877.696434, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274471, test/num_examples=43793, total_duration=77823.000521, train/accuracy=0.995494, train/loss=0.014142, train/mean_average_precision=0.776864, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292358, validation/num_examples=43793
I0306 07:42:20.668129 140408968165120 logging_writer.py:48] [160800] global_step=160800, grad_norm=0.16417036950588226, loss=0.020670318976044655
I0306 07:42:52.486495 140407882708736 logging_writer.py:48] [160900] global_step=160900, grad_norm=0.13827675580978394, loss=0.01777595467865467
I0306 07:43:24.271727 140408968165120 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.14512088894844055, loss=0.019421188160777092
I0306 07:43:56.065086 140407882708736 logging_writer.py:48] [161100] global_step=161100, grad_norm=0.12833020091056824, loss=0.017042012885212898
I0306 07:44:27.890482 140408968165120 logging_writer.py:48] [161200] global_step=161200, grad_norm=0.13886605203151703, loss=0.017886916175484657
I0306 07:45:00.215748 140407882708736 logging_writer.py:48] [161300] global_step=161300, grad_norm=0.14664369821548462, loss=0.019088786095380783
I0306 07:45:31.908544 140408968165120 logging_writer.py:48] [161400] global_step=161400, grad_norm=0.15503010153770447, loss=0.018510065972805023
I0306 07:46:03.949397 140407882708736 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.13930504024028778, loss=0.017101909965276718
I0306 07:46:12.531291 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:47:54.690428 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:47:57.693204 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:48:00.687421 140576608098112 submission_runner.py:411] Time since start: 78171.40s, 	Step: 161528, 	{'train/accuracy': 0.995476245880127, 'train/loss': 0.014211134053766727, 'train/mean_average_precision': 0.7777987133311279, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.2924305651890328, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27439821613348686, 'test/num_examples': 43793, 'score': 52117.85686635971, 'total_duration': 78171.39575123787, 'accumulated_submission_time': 52117.85686635971, 'accumulated_eval_time': 26039.03204393387, 'accumulated_logging_time': 10.039782524108887}
I0306 07:48:00.742805 140362251773696 logging_writer.py:48] [161528] accumulated_eval_time=26039.032044, accumulated_logging_time=10.039783, accumulated_submission_time=52117.856866, global_step=161528, preemption_count=0, score=52117.856866, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274398, test/num_examples=43793, total_duration=78171.395751, train/accuracy=0.995476, train/loss=0.014211, train/mean_average_precision=0.777799, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292431, validation/num_examples=43793
I0306 07:48:24.401303 140409291962112 logging_writer.py:48] [161600] global_step=161600, grad_norm=0.15160349011421204, loss=0.019948218017816544
I0306 07:48:56.599200 140362251773696 logging_writer.py:48] [161700] global_step=161700, grad_norm=0.13563883304595947, loss=0.016953594982624054
I0306 07:49:29.016793 140409291962112 logging_writer.py:48] [161800] global_step=161800, grad_norm=0.1396607756614685, loss=0.016864949837327003
I0306 07:50:01.454947 140362251773696 logging_writer.py:48] [161900] global_step=161900, grad_norm=0.1374731808900833, loss=0.017650356516242027
I0306 07:50:33.606542 140409291962112 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.15582701563835144, loss=0.02028452605009079
I0306 07:51:05.397588 140362251773696 logging_writer.py:48] [162100] global_step=162100, grad_norm=0.13195553421974182, loss=0.01620067097246647
I0306 07:51:37.286581 140409291962112 logging_writer.py:48] [162200] global_step=162200, grad_norm=0.15416879951953888, loss=0.02044863998889923
I0306 07:52:00.947664 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:53:39.349191 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:53:42.379298 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:53:45.350625 140576608098112 submission_runner.py:411] Time since start: 78516.06s, 	Step: 162275, 	{'train/accuracy': 0.9954986572265625, 'train/loss': 0.014100734144449234, 'train/mean_average_precision': 0.7723155867022389, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2925567746610274, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27440696789532437, 'test/num_examples': 43793, 'score': 52358.030159950256, 'total_duration': 78516.05895709991, 'accumulated_submission_time': 52358.030159950256, 'accumulated_eval_time': 26143.434961795807, 'accumulated_logging_time': 10.106394290924072}
I0306 07:53:45.396450 140407882708736 logging_writer.py:48] [162275] accumulated_eval_time=26143.434962, accumulated_logging_time=10.106394, accumulated_submission_time=52358.030160, global_step=162275, preemption_count=0, score=52358.030160, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274407, test/num_examples=43793, total_duration=78516.058957, train/accuracy=0.995499, train/loss=0.014101, train/mean_average_precision=0.772316, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292557, validation/num_examples=43793
I0306 07:53:53.783036 140408968165120 logging_writer.py:48] [162300] global_step=162300, grad_norm=0.13944277167320251, loss=0.016285989433526993
I0306 07:54:25.944450 140407882708736 logging_writer.py:48] [162400] global_step=162400, grad_norm=0.14086554944515228, loss=0.01868407242000103
I0306 07:54:57.623053 140408968165120 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.14885643124580383, loss=0.017822405323386192
I0306 07:55:29.466238 140407882708736 logging_writer.py:48] [162600] global_step=162600, grad_norm=0.1506493091583252, loss=0.019762543961405754
I0306 07:56:01.310183 140408968165120 logging_writer.py:48] [162700] global_step=162700, grad_norm=0.14305990934371948, loss=0.018090642988681793
I0306 07:56:34.374310 140407882708736 logging_writer.py:48] [162800] global_step=162800, grad_norm=0.12198681384325027, loss=0.016329389065504074
I0306 07:57:07.213591 140408968165120 logging_writer.py:48] [162900] global_step=162900, grad_norm=0.15183784067630768, loss=0.018465399742126465
I0306 07:57:38.835953 140407882708736 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.1301451325416565, loss=0.017191611230373383
I0306 07:57:45.551013 140576608098112 spec.py:321] Evaluating on the training split.
I0306 07:59:22.964422 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 07:59:26.033472 140576608098112 spec.py:349] Evaluating on the test split.
I0306 07:59:29.046328 140576608098112 submission_runner.py:411] Time since start: 78859.75s, 	Step: 163022, 	{'train/accuracy': 0.9955146312713623, 'train/loss': 0.014097247272729874, 'train/mean_average_precision': 0.7758655269731343, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29235043317970505, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27442309717690694, 'test/num_examples': 43793, 'score': 52598.15224838257, 'total_duration': 78859.75465726852, 'accumulated_submission_time': 52598.15224838257, 'accumulated_eval_time': 26246.93022799492, 'accumulated_logging_time': 10.163278579711914}
I0306 07:59:29.093826 140364426737408 logging_writer.py:48] [163022] accumulated_eval_time=26246.930228, accumulated_logging_time=10.163279, accumulated_submission_time=52598.152248, global_step=163022, preemption_count=0, score=52598.152248, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274423, test/num_examples=43793, total_duration=78859.754657, train/accuracy=0.995515, train/loss=0.014097, train/mean_average_precision=0.775866, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292350, validation/num_examples=43793
I0306 07:59:54.233022 140409291962112 logging_writer.py:48] [163100] global_step=163100, grad_norm=0.13845562934875488, loss=0.019079480320215225
I0306 08:00:26.720601 140364426737408 logging_writer.py:48] [163200] global_step=163200, grad_norm=0.1230211928486824, loss=0.016007075086236
I0306 08:00:58.500940 140409291962112 logging_writer.py:48] [163300] global_step=163300, grad_norm=0.14196071028709412, loss=0.018463604152202606
I0306 08:01:30.726399 140364426737408 logging_writer.py:48] [163400] global_step=163400, grad_norm=0.1402292102575302, loss=0.017110703513026237
I0306 08:02:02.681469 140409291962112 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.13948582112789154, loss=0.01850484311580658
I0306 08:02:35.210739 140364426737408 logging_writer.py:48] [163600] global_step=163600, grad_norm=0.13784907758235931, loss=0.01633227989077568
I0306 08:03:07.166138 140409291962112 logging_writer.py:48] [163700] global_step=163700, grad_norm=0.1303437054157257, loss=0.016502512618899345
I0306 08:03:29.366226 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:05:05.368975 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:05:08.433628 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:05:11.467895 140576608098112 submission_runner.py:411] Time since start: 79202.18s, 	Step: 163771, 	{'train/accuracy': 0.9954249858856201, 'train/loss': 0.014271891675889492, 'train/mean_average_precision': 0.7642837725594658, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29239462792956783, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744978733779977, 'test/num_examples': 43793, 'score': 52838.39267086983, 'total_duration': 79202.17622852325, 'accumulated_submission_time': 52838.39267086983, 'accumulated_eval_time': 26349.03185725212, 'accumulated_logging_time': 10.222217321395874}
I0306 08:05:11.515130 140407882708736 logging_writer.py:48] [163771] accumulated_eval_time=26349.031857, accumulated_logging_time=10.222217, accumulated_submission_time=52838.392671, global_step=163771, preemption_count=0, score=52838.392671, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274498, test/num_examples=43793, total_duration=79202.176229, train/accuracy=0.995425, train/loss=0.014272, train/mean_average_precision=0.764284, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292395, validation/num_examples=43793
I0306 08:05:21.265604 140408968165120 logging_writer.py:48] [163800] global_step=163800, grad_norm=0.1390666961669922, loss=0.0176092516630888
I0306 08:05:53.521210 140407882708736 logging_writer.py:48] [163900] global_step=163900, grad_norm=0.13878250122070312, loss=0.01665780507028103
I0306 08:06:26.120012 140408968165120 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.12423589080572128, loss=0.016880394890904427
I0306 08:06:58.360148 140407882708736 logging_writer.py:48] [164100] global_step=164100, grad_norm=0.14445339143276215, loss=0.018242022022604942
I0306 08:07:30.846634 140408968165120 logging_writer.py:48] [164200] global_step=164200, grad_norm=0.1351788341999054, loss=0.018069874495267868
I0306 08:08:03.759093 140407882708736 logging_writer.py:48] [164300] global_step=164300, grad_norm=0.16033631563186646, loss=0.0195073951035738
I0306 08:08:35.850387 140408968165120 logging_writer.py:48] [164400] global_step=164400, grad_norm=0.14551474153995514, loss=0.019201302900910378
I0306 08:09:08.211808 140407882708736 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.14924432337284088, loss=0.021258899942040443
I0306 08:09:11.492830 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:10:49.784204 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:10:54.969144 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:10:57.936738 140576608098112 submission_runner.py:411] Time since start: 79548.65s, 	Step: 164511, 	{'train/accuracy': 0.9954708814620972, 'train/loss': 0.01419944316148758, 'train/mean_average_precision': 0.7770340807670562, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.29233200089252726, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2743908303667785, 'test/num_examples': 43793, 'score': 53078.33946681023, 'total_duration': 79548.64507198334, 'accumulated_submission_time': 53078.33946681023, 'accumulated_eval_time': 26455.47572350502, 'accumulated_logging_time': 10.280250310897827}
I0306 08:10:57.984081 140364426737408 logging_writer.py:48] [164511] accumulated_eval_time=26455.475724, accumulated_logging_time=10.280250, accumulated_submission_time=53078.339467, global_step=164511, preemption_count=0, score=53078.339467, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274391, test/num_examples=43793, total_duration=79548.645072, train/accuracy=0.995471, train/loss=0.014199, train/mean_average_precision=0.777034, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292332, validation/num_examples=43793
I0306 08:11:26.643672 140409291962112 logging_writer.py:48] [164600] global_step=164600, grad_norm=0.13918031752109528, loss=0.016911610960960388
I0306 08:11:58.454029 140364426737408 logging_writer.py:48] [164700] global_step=164700, grad_norm=0.14383874833583832, loss=0.019043413922190666
I0306 08:12:30.243099 140409291962112 logging_writer.py:48] [164800] global_step=164800, grad_norm=0.14442700147628784, loss=0.018107600510120392
I0306 08:13:02.360773 140364426737408 logging_writer.py:48] [164900] global_step=164900, grad_norm=0.150705024600029, loss=0.019175736233592033
I0306 08:13:34.418356 140409291962112 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.12640032172203064, loss=0.016118712723255157
I0306 08:14:06.209034 140364426737408 logging_writer.py:48] [165100] global_step=165100, grad_norm=0.13108834624290466, loss=0.018956732004880905
I0306 08:14:38.064567 140409291962112 logging_writer.py:48] [165200] global_step=165200, grad_norm=0.13702335953712463, loss=0.01760140061378479
I0306 08:14:58.133484 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:16:35.475752 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:16:38.519867 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:16:41.533195 140576608098112 submission_runner.py:411] Time since start: 79892.24s, 	Step: 165264, 	{'train/accuracy': 0.9955559968948364, 'train/loss': 0.014011014252901077, 'train/mean_average_precision': 0.7774745704495394, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2923164205091282, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744477263950919, 'test/num_examples': 43793, 'score': 53318.45733141899, 'total_duration': 79892.24152255058, 'accumulated_submission_time': 53318.45733141899, 'accumulated_eval_time': 26558.875393629074, 'accumulated_logging_time': 10.338693141937256}
I0306 08:16:41.580804 140362251773696 logging_writer.py:48] [165264] accumulated_eval_time=26558.875394, accumulated_logging_time=10.338693, accumulated_submission_time=53318.457331, global_step=165264, preemption_count=0, score=53318.457331, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274448, test/num_examples=43793, total_duration=79892.241523, train/accuracy=0.995556, train/loss=0.014011, train/mean_average_precision=0.777475, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292316, validation/num_examples=43793
I0306 08:16:53.517526 140408968165120 logging_writer.py:48] [165300] global_step=165300, grad_norm=0.145157590508461, loss=0.017404543235898018
I0306 08:17:25.474429 140362251773696 logging_writer.py:48] [165400] global_step=165400, grad_norm=0.1441061943769455, loss=0.01642058975994587
I0306 08:17:57.524667 140408968165120 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.14308208227157593, loss=0.018976543098688126
I0306 08:18:29.543014 140362251773696 logging_writer.py:48] [165600] global_step=165600, grad_norm=0.1584019660949707, loss=0.019006038084626198
I0306 08:19:01.833741 140408968165120 logging_writer.py:48] [165700] global_step=165700, grad_norm=0.13186132907867432, loss=0.015614669770002365
I0306 08:19:33.549059 140362251773696 logging_writer.py:48] [165800] global_step=165800, grad_norm=0.13928161561489105, loss=0.018467450514435768
I0306 08:20:05.784050 140408968165120 logging_writer.py:48] [165900] global_step=165900, grad_norm=0.13016435503959656, loss=0.01782018505036831
I0306 08:20:37.772481 140362251773696 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.13219265639781952, loss=0.017083171755075455
I0306 08:20:41.669500 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:22:18.776188 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:22:21.879215 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:22:24.973508 140576608098112 submission_runner.py:411] Time since start: 80235.68s, 	Step: 166013, 	{'train/accuracy': 0.9955283999443054, 'train/loss': 0.014115395024418831, 'train/mean_average_precision': 0.7682561890850204, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.2923437326480599, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744653453568398, 'test/num_examples': 43793, 'score': 53558.514235019684, 'total_duration': 80235.681828022, 'accumulated_submission_time': 53558.514235019684, 'accumulated_eval_time': 26662.17934703827, 'accumulated_logging_time': 10.398459672927856}
I0306 08:22:25.020244 140407882708736 logging_writer.py:48] [166013] accumulated_eval_time=26662.179347, accumulated_logging_time=10.398460, accumulated_submission_time=53558.514235, global_step=166013, preemption_count=0, score=53558.514235, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274465, test/num_examples=43793, total_duration=80235.681828, train/accuracy=0.995528, train/loss=0.014115, train/mean_average_precision=0.768256, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292344, validation/num_examples=43793
I0306 08:22:53.541390 140409291962112 logging_writer.py:48] [166100] global_step=166100, grad_norm=0.13347381353378296, loss=0.017798569053411484
I0306 08:23:25.484174 140407882708736 logging_writer.py:48] [166200] global_step=166200, grad_norm=0.1374412477016449, loss=0.018646985292434692
I0306 08:23:57.614311 140409291962112 logging_writer.py:48] [166300] global_step=166300, grad_norm=0.14672501385211945, loss=0.01842040754854679
I0306 08:24:30.115932 140407882708736 logging_writer.py:48] [166400] global_step=166400, grad_norm=0.15817303955554962, loss=0.019736427813768387
I0306 08:25:02.458366 140409291962112 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.14073409140110016, loss=0.018270129337906837
I0306 08:25:34.676666 140407882708736 logging_writer.py:48] [166600] global_step=166600, grad_norm=0.12919139862060547, loss=0.015842361375689507
I0306 08:26:06.462779 140409291962112 logging_writer.py:48] [166700] global_step=166700, grad_norm=0.1674000769853592, loss=0.018169287592172623
I0306 08:26:25.134399 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:28:08.703879 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:28:11.871687 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:28:14.897107 140576608098112 submission_runner.py:411] Time since start: 80585.61s, 	Step: 166758, 	{'train/accuracy': 0.9954188466072083, 'train/loss': 0.014226348139345646, 'train/mean_average_precision': 0.7785996373453546, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29234703759049513, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27442455031709445, 'test/num_examples': 43793, 'score': 53798.596177339554, 'total_duration': 80585.60542726517, 'accumulated_submission_time': 53798.596177339554, 'accumulated_eval_time': 26771.942001581192, 'accumulated_logging_time': 10.45738959312439}
I0306 08:28:14.943962 140362251773696 logging_writer.py:48] [166758] accumulated_eval_time=26771.942002, accumulated_logging_time=10.457390, accumulated_submission_time=53798.596177, global_step=166758, preemption_count=0, score=53798.596177, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274425, test/num_examples=43793, total_duration=80585.605427, train/accuracy=0.995419, train/loss=0.014226, train/mean_average_precision=0.778600, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292347, validation/num_examples=43793
I0306 08:28:28.610662 140364426737408 logging_writer.py:48] [166800] global_step=166800, grad_norm=0.13880974054336548, loss=0.01727573201060295
I0306 08:28:59.937214 140362251773696 logging_writer.py:48] [166900] global_step=166900, grad_norm=0.14987003803253174, loss=0.019827093929052353
I0306 08:29:31.838658 140364426737408 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.1307140588760376, loss=0.017388034611940384
I0306 08:30:03.663305 140362251773696 logging_writer.py:48] [167100] global_step=167100, grad_norm=0.1345524787902832, loss=0.01848292350769043
I0306 08:30:36.586520 140364426737408 logging_writer.py:48] [167200] global_step=167200, grad_norm=0.1427476406097412, loss=0.019233815371990204
I0306 08:31:09.311082 140362251773696 logging_writer.py:48] [167300] global_step=167300, grad_norm=0.14230351150035858, loss=0.01780768856406212
I0306 08:31:42.124981 140364426737408 logging_writer.py:48] [167400] global_step=167400, grad_norm=0.1569553017616272, loss=0.01846255362033844
I0306 08:32:15.076690 140362251773696 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.13210724294185638, loss=0.015797186642885208
I0306 08:32:15.081895 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:33:57.552767 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:34:00.746651 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:34:04.051799 140576608098112 submission_runner.py:411] Time since start: 80934.76s, 	Step: 167501, 	{'train/accuracy': 0.9954667687416077, 'train/loss': 0.014189929701387882, 'train/mean_average_precision': 0.7694678128577732, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178978592157364, 'validation/mean_average_precision': 0.29241661685149245, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27432747634512605, 'test/num_examples': 43793, 'score': 54038.70073080063, 'total_duration': 80934.76012897491, 'accumulated_submission_time': 54038.70073080063, 'accumulated_eval_time': 26880.91184401512, 'accumulated_logging_time': 10.514888763427734}
I0306 08:34:04.098950 140407882708736 logging_writer.py:48] [167501] accumulated_eval_time=26880.911844, accumulated_logging_time=10.514889, accumulated_submission_time=54038.700731, global_step=167501, preemption_count=0, score=54038.700731, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274327, test/num_examples=43793, total_duration=80934.760129, train/accuracy=0.995467, train/loss=0.014190, train/mean_average_precision=0.769468, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292417, validation/num_examples=43793
I0306 08:34:36.570817 140409291962112 logging_writer.py:48] [167600] global_step=167600, grad_norm=0.13234837353229523, loss=0.017375782132148743
I0306 08:35:08.698883 140407882708736 logging_writer.py:48] [167700] global_step=167700, grad_norm=0.13768739998340607, loss=0.015801822766661644
I0306 08:35:40.659535 140409291962112 logging_writer.py:48] [167800] global_step=167800, grad_norm=0.15742158889770508, loss=0.019645247608423233
I0306 08:36:12.736315 140407882708736 logging_writer.py:48] [167900] global_step=167900, grad_norm=0.11739581823348999, loss=0.01520079467445612
I0306 08:36:45.123114 140409291962112 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.15552109479904175, loss=0.019638996571302414
I0306 08:37:17.427258 140407882708736 logging_writer.py:48] [168100] global_step=168100, grad_norm=0.14205780625343323, loss=0.017766162753105164
I0306 08:37:49.548305 140409291962112 logging_writer.py:48] [168200] global_step=168200, grad_norm=0.12845054268836975, loss=0.017152566462755203
I0306 08:38:04.192907 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:39:49.108238 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:39:52.470402 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:39:55.768629 140576608098112 submission_runner.py:411] Time since start: 81286.48s, 	Step: 168246, 	{'train/accuracy': 0.9954833388328552, 'train/loss': 0.014200116507709026, 'train/mean_average_precision': 0.7711502631017008, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29244476217498894, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27443460431142264, 'test/num_examples': 43793, 'score': 54278.76337981224, 'total_duration': 81286.47692489624, 'accumulated_submission_time': 54278.76337981224, 'accumulated_eval_time': 26992.4874894619, 'accumulated_logging_time': 10.573052167892456}
I0306 08:39:55.823288 140364426737408 logging_writer.py:48] [168246] accumulated_eval_time=26992.487489, accumulated_logging_time=10.573052, accumulated_submission_time=54278.763380, global_step=168246, preemption_count=0, score=54278.763380, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274435, test/num_examples=43793, total_duration=81286.476925, train/accuracy=0.995483, train/loss=0.014200, train/mean_average_precision=0.771150, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292445, validation/num_examples=43793
I0306 08:40:14.081724 140408968165120 logging_writer.py:48] [168300] global_step=168300, grad_norm=0.13853596150875092, loss=0.01864183507859707
I0306 08:40:46.867945 140364426737408 logging_writer.py:48] [168400] global_step=168400, grad_norm=0.1674366146326065, loss=0.018889838829636574
I0306 08:41:19.391484 140408968165120 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.1424700766801834, loss=0.019529368728399277
I0306 08:41:51.652352 140364426737408 logging_writer.py:48] [168600] global_step=168600, grad_norm=0.14237430691719055, loss=0.020955901592969894
I0306 08:42:23.717556 140408968165120 logging_writer.py:48] [168700] global_step=168700, grad_norm=0.12626071274280548, loss=0.015721319243311882
I0306 08:42:55.440782 140364426737408 logging_writer.py:48] [168800] global_step=168800, grad_norm=0.14231261610984802, loss=0.017793046310544014
I0306 08:43:27.756595 140408968165120 logging_writer.py:48] [168900] global_step=168900, grad_norm=0.14963947236537933, loss=0.018429096788167953
I0306 08:43:55.816241 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:45:34.338666 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:45:37.386970 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:45:40.366115 140576608098112 submission_runner.py:411] Time since start: 81631.07s, 	Step: 168989, 	{'train/accuracy': 0.9955009818077087, 'train/loss': 0.014132117852568626, 'train/mean_average_precision': 0.7773532093488216, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.2924251409088802, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744506152585214, 'test/num_examples': 43793, 'score': 54518.72223806381, 'total_duration': 81631.07444930077, 'accumulated_submission_time': 54518.72223806381, 'accumulated_eval_time': 27097.03732419014, 'accumulated_logging_time': 10.639933347702026}
I0306 08:45:40.413247 140362251773696 logging_writer.py:48] [168989] accumulated_eval_time=27097.037324, accumulated_logging_time=10.639933, accumulated_submission_time=54518.722238, global_step=168989, preemption_count=0, score=54518.722238, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274451, test/num_examples=43793, total_duration=81631.074449, train/accuracy=0.995501, train/loss=0.014132, train/mean_average_precision=0.777353, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292425, validation/num_examples=43793
I0306 08:45:44.276539 140407882708736 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.14427539706230164, loss=0.017464149743318558
I0306 08:46:16.101653 140362251773696 logging_writer.py:48] [169100] global_step=169100, grad_norm=0.12728874385356903, loss=0.016290590167045593
I0306 08:46:48.532851 140407882708736 logging_writer.py:48] [169200] global_step=169200, grad_norm=0.12490933388471603, loss=0.01630224846303463
I0306 08:47:20.993316 140362251773696 logging_writer.py:48] [169300] global_step=169300, grad_norm=0.1387370526790619, loss=0.018636837601661682
I0306 08:47:52.627545 140407882708736 logging_writer.py:48] [169400] global_step=169400, grad_norm=0.1291997730731964, loss=0.018090898171067238
I0306 08:48:24.754813 140362251773696 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.15168045461177826, loss=0.01900044083595276
I0306 08:48:56.890794 140407882708736 logging_writer.py:48] [169600] global_step=169600, grad_norm=0.14025144279003143, loss=0.01736469380557537
I0306 08:49:28.995989 140362251773696 logging_writer.py:48] [169700] global_step=169700, grad_norm=0.1445416510105133, loss=0.01859263889491558
I0306 08:49:40.535118 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:51:19.616305 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:51:22.641776 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:51:25.648826 140576608098112 submission_runner.py:411] Time since start: 81976.36s, 	Step: 169737, 	{'train/accuracy': 0.9955348372459412, 'train/loss': 0.014073893427848816, 'train/mean_average_precision': 0.7799641597540772, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017897114157677, 'validation/mean_average_precision': 0.29249505470970516, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27433063964299914, 'test/num_examples': 43793, 'score': 54758.813244104385, 'total_duration': 81976.3571498394, 'accumulated_submission_time': 54758.813244104385, 'accumulated_eval_time': 27202.150987625122, 'accumulated_logging_time': 10.69777512550354}
I0306 08:51:25.695678 140364426737408 logging_writer.py:48] [169737] accumulated_eval_time=27202.150988, accumulated_logging_time=10.697775, accumulated_submission_time=54758.813244, global_step=169737, preemption_count=0, score=54758.813244, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274331, test/num_examples=43793, total_duration=81976.357150, train/accuracy=0.995535, train/loss=0.014074, train/mean_average_precision=0.779964, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292495, validation/num_examples=43793
I0306 08:51:46.352316 140408968165120 logging_writer.py:48] [169800] global_step=169800, grad_norm=0.1378631740808487, loss=0.019005995243787766
I0306 08:52:18.477175 140364426737408 logging_writer.py:48] [169900] global_step=169900, grad_norm=0.15721122920513153, loss=0.01834237389266491
I0306 08:52:49.894712 140408968165120 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.15373735129833221, loss=0.020182961598038673
I0306 08:53:21.896177 140364426737408 logging_writer.py:48] [170100] global_step=170100, grad_norm=0.13388079404830933, loss=0.016156649217009544
I0306 08:53:53.654313 140408968165120 logging_writer.py:48] [170200] global_step=170200, grad_norm=0.13304224610328674, loss=0.01616460457444191
I0306 08:54:25.456058 140364426737408 logging_writer.py:48] [170300] global_step=170300, grad_norm=0.14378422498703003, loss=0.01789727993309498
I0306 08:54:57.034258 140408968165120 logging_writer.py:48] [170400] global_step=170400, grad_norm=0.14518342912197113, loss=0.021049005910754204
I0306 08:55:25.693323 140576608098112 spec.py:321] Evaluating on the training split.
I0306 08:57:05.385925 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 08:57:08.442108 140576608098112 spec.py:349] Evaluating on the test split.
I0306 08:57:11.488138 140576608098112 submission_runner.py:411] Time since start: 82322.20s, 	Step: 170490, 	{'train/accuracy': 0.995444118976593, 'train/loss': 0.014174314215779305, 'train/mean_average_precision': 0.7645382818223485, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.05017896741628647, 'validation/mean_average_precision': 0.2925678030672734, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.27441999955167334, 'test/num_examples': 43793, 'score': 54998.77826976776, 'total_duration': 82322.19646334648, 'accumulated_submission_time': 54998.77826976776, 'accumulated_eval_time': 27307.945759534836, 'accumulated_logging_time': 10.756629467010498}
I0306 08:57:11.536084 140407882708736 logging_writer.py:48] [170490] accumulated_eval_time=27307.945760, accumulated_logging_time=10.756629, accumulated_submission_time=54998.778270, global_step=170490, preemption_count=0, score=54998.778270, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274420, test/num_examples=43793, total_duration=82322.196463, train/accuracy=0.995444, train/loss=0.014174, train/mean_average_precision=0.764538, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292568, validation/num_examples=43793
I0306 08:57:15.270573 140409291962112 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.14735975861549377, loss=0.018162477761507034
I0306 08:57:47.809398 140407882708736 logging_writer.py:48] [170600] global_step=170600, grad_norm=0.14950969815254211, loss=0.018798787146806717
I0306 08:58:20.149731 140409291962112 logging_writer.py:48] [170700] global_step=170700, grad_norm=0.1482905000448227, loss=0.018818238750100136
I0306 08:58:52.189494 140407882708736 logging_writer.py:48] [170800] global_step=170800, grad_norm=0.15420372784137726, loss=0.01856851577758789
I0306 08:59:24.154603 140409291962112 logging_writer.py:48] [170900] global_step=170900, grad_norm=0.15057195723056793, loss=0.0192690659314394
I0306 08:59:56.940209 140407882708736 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.13401150703430176, loss=0.018000120297074318
I0306 09:00:29.244235 140409291962112 logging_writer.py:48] [171100] global_step=171100, grad_norm=0.1357184499502182, loss=0.017245296388864517
I0306 09:01:02.295722 140407882708736 logging_writer.py:48] [171200] global_step=171200, grad_norm=0.1544165313243866, loss=0.019579347223043442
I0306 09:01:11.760722 140576608098112 spec.py:321] Evaluating on the training split.
I0306 09:02:56.621932 140576608098112 spec.py:333] Evaluating on the validation split.
I0306 09:03:00.100313 140576608098112 spec.py:349] Evaluating on the test split.
I0306 09:03:03.701978 140576608098112 submission_runner.py:411] Time since start: 82674.41s, 	Step: 171230, 	{'train/accuracy': 0.995492696762085, 'train/loss': 0.014181965962052345, 'train/mean_average_precision': 0.7743298517611776, 'validation/accuracy': 0.987072765827179, 'validation/loss': 0.050178974866867065, 'validation/mean_average_precision': 0.29255196553221835, 'validation/num_examples': 43793, 'test/accuracy': 0.9861477017402649, 'test/loss': 0.05403367802500725, 'test/mean_average_precision': 0.2744220064947314, 'test/num_examples': 43793, 'score': 55238.96902322769, 'total_duration': 82674.4102742672, 'accumulated_submission_time': 55238.96902322769, 'accumulated_eval_time': 27419.886949539185, 'accumulated_logging_time': 10.8176851272583}
I0306 09:03:03.759702 140364426737408 logging_writer.py:48] [171230] accumulated_eval_time=27419.886950, accumulated_logging_time=10.817685, accumulated_submission_time=55238.969023, global_step=171230, preemption_count=0, score=55238.969023, test/accuracy=0.986148, test/loss=0.054034, test/mean_average_precision=0.274422, test/num_examples=43793, total_duration=82674.410274, train/accuracy=0.995493, train/loss=0.014182, train/mean_average_precision=0.774330, validation/accuracy=0.987073, validation/loss=0.050179, validation/mean_average_precision=0.292552, validation/num_examples=43793
I0306 09:03:27.339150 140408968165120 logging_writer.py:48] [171300] global_step=171300, grad_norm=0.12910175323486328, loss=0.014955924823880196
I0306 09:04:00.276605 140364426737408 logging_writer.py:48] [171400] global_step=171400, grad_norm=0.1581382006406784, loss=0.021028824150562286
I0306 09:04:33.034043 140408968165120 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.1478489637374878, loss=0.018499627709388733
I0306 09:05:05.526878 140364426737408 logging_writer.py:48] [171600] global_step=171600, grad_norm=0.15393564105033875, loss=0.018130695447325706
I0306 09:05:38.414731 140408968165120 logging_writer.py:48] [171700] global_step=171700, grad_norm=0.14574594795703888, loss=0.018085811287164688
I0306 09:06:11.294717 140364426737408 logging_writer.py:48] [171800] global_step=171800, grad_norm=0.1318587362766266, loss=0.016591889783740044
I0306 09:06:16.108101 140408968165120 logging_writer.py:48] [171816] global_step=171816, preemption_count=0, score=55431.250526
I0306 09:06:16.163941 140576608098112 checkpoints.py:490] Saving checkpoint at step: 171816
I0306 09:06:16.298200 140576608098112 checkpoints.py:422] Saved checkpoint at /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1/checkpoint_171816
I0306 09:06:16.299707 140576608098112 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/prize_qualification_self_tuning/study_2/ogbg_jax/trial_1/checkpoint_171816.
I0306 09:06:16.488345 140576608098112 submission_runner.py:676] Final ogbg score: 55431.25052642822
Dataset ogbg_molpcba downloaded and prepared to /root/data/ogbg_molpcba/0.1.3. Subsequent calls will reuse this data.
